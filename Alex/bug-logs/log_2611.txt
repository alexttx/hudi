+ set -u
+ n=2611
+ shift
+ bash /home/alex/github/alexttx/hudi/docker/setup_demo.sh
 Container trino-worker-1  Stopping
 Container datanode1  Stopping
 Container adhoc-1  Stopping
 Container presto-worker-1  Stopping
 Container graphite  Stopping
 Container kafkabroker  Stopping
 Container zookeeper  Stopping
 Container spark-worker-1  Stopping
 Container adhoc-2  Stopping
 Container zookeeper  Stopped
 Container zookeeper  Removing
 Container zookeeper  Removed
 Container kafkabroker  Stopped
 Container kafkabroker  Removing
 Container kafkabroker  Removed
 Container presto-worker-1  Stopped
 Container presto-worker-1  Removing
 Container presto-worker-1  Removed
 Container graphite  Stopped
 Container graphite  Removing
 Container graphite  Removed
 Container adhoc-2  Stopped
 Container adhoc-2  Removing
 Container adhoc-2  Removed
 Container spark-worker-1  Stopped
 Container spark-worker-1  Removing
 Container spark-worker-1  Removed
 Container adhoc-1  Stopped
 Container adhoc-1  Removing
 Container adhoc-1  Removed
 Container sparkmaster  Stopping
 Container presto-coordinator-1  Stopping
 Container trino-worker-1  Stopped
 Container trino-worker-1  Removing
 Container trino-worker-1  Removed
 Container trino-coordinator-1  Stopping
 Container datanode1  Stopped
 Container datanode1  Removing
 Container datanode1  Removed
 Container historyserver  Stopping
 Container presto-coordinator-1  Stopped
 Container presto-coordinator-1  Removing
 Container presto-coordinator-1  Removed
 Container sparkmaster  Stopped
 Container sparkmaster  Removing
 Container sparkmaster  Removed
 Container hiveserver  Stopping
 Container historyserver  Stopped
 Container historyserver  Removing
 Container historyserver  Removed
 Container trino-coordinator-1  Stopped
 Container trino-coordinator-1  Removing
 Container trino-coordinator-1  Removed
 Container hiveserver  Stopped
 Container hiveserver  Removing
 Container hiveserver  Removed
 Container hivemetastore  Stopping
 Container hivemetastore  Stopped
 Container hivemetastore  Removing
 Container hivemetastore  Removed
 Container namenode  Stopping
 Container hive-metastore-postgresql  Stopping
 Container hive-metastore-postgresql  Stopped
 Container hive-metastore-postgresql  Removing
 Container hive-metastore-postgresql  Removed
 Container namenode  Stopped
 Container namenode  Removing
 Container namenode  Removed
 Network hudi  Removing
 Network hudi  Removed
Pulling docker demo images ...
 hivemetastore Skipped - Image is already being pulled by hiveserver 
 presto-worker-1 Pulling 
 zookeeper Pulling 
 presto-coordinator-1 Skipped - Image is already being pulled by presto-worker-1 
 graphite Pulling 
 historyserver Pulling 
 kafka Pulling 
 datanode1 Pulling 
 hive-metastore-postgresql Pulling 
 namenode Pulling 
 trino-worker-1 Pulling 
 spark-worker-1 Pulling 
 adhoc-1 Pulling 
 adhoc-2 Skipped - Image is already being pulled by adhoc-1 
 sparkmaster Pulling 
 hiveserver Pulling 
 trino-coordinator-1 Pulling 
 hive-metastore-postgresql Pulled 
 zookeeper Pulled 
 graphite Pulled 
 historyserver Pulled 
 presto-worker-1 Pulled 
 sparkmaster Pulled 
 datanode1 Pulled 
 spark-worker-1 Pulled 
 namenode Pulled 
 adhoc-1 Pulled 
 hiveserver Pulled 
 trino-worker-1 Pulled 
 trino-coordinator-1 Pulled 
 kafka Pulled 
 Network hudi  Creating
 Network hudi  Created
 Container namenode  Creating
 Container graphite  Creating
 Container zookeeper  Creating
 Container hive-metastore-postgresql  Creating
 Container kafkabroker  Creating
 Container kafkabroker  Created
 Container zookeeper  Created
 Container hive-metastore-postgresql  Created
 Container graphite  Created
 Container namenode  Created
 Container historyserver  Creating
 Container hivemetastore  Creating
 Container historyserver  Created
 Container datanode1  Creating
 Container hivemetastore  Created
 Container trino-coordinator-1  Creating
 Container hiveserver  Creating
 Container presto-coordinator-1  Creating
 Container presto-coordinator-1  Created
 Container datanode1  Created
 Container hiveserver  Created
 Container presto-worker-1  Creating
 Container sparkmaster  Creating
 Container trino-coordinator-1  Created
 Container trino-worker-1  Creating
 Container presto-worker-1  Created
 Container sparkmaster  Created
 Container adhoc-2  Creating
 Container adhoc-1  Creating
 Container spark-worker-1  Creating
 Container trino-worker-1  Created
 Container spark-worker-1  Created
 Container adhoc-2  Created
 Container adhoc-1  Created
 Container kafkabroker  Starting
 Container namenode  Starting
 Container hive-metastore-postgresql  Starting
 Container zookeeper  Starting
 Container graphite  Starting
 Container kafkabroker  Started
 Container zookeeper  Started
 Container hive-metastore-postgresql  Started
 Container graphite  Started
 Container namenode  Started
 Container historyserver  Starting
 Container hivemetastore  Starting
 Container historyserver  Started
 Container datanode1  Starting
 Container hivemetastore  Started
 Container presto-coordinator-1  Starting
 Container trino-coordinator-1  Starting
 Container hiveserver  Starting
 Container datanode1  Started
 Container presto-coordinator-1  Started
 Container trino-coordinator-1  Started
 Container hiveserver  Started
 Container sparkmaster  Starting
 Container presto-worker-1  Starting
 Container trino-worker-1  Starting
 Container presto-worker-1  Started
 Container sparkmaster  Started
 Container adhoc-2  Starting
 Container adhoc-1  Starting
 Container spark-worker-1  Starting
 Container trino-worker-1  Started
 Container adhoc-1  Started
 Container adhoc-2  Started
 Container spark-worker-1  Started
Copying spark default config and setting up configs
Copying spark default config and setting up configs
+ cat /home/alex/github/alexttx/hudi/docker/demo/data/batch_1.json
+ head -n2611
+ kcat -b kafkabroker -t stock_ticks -P
+ kcat -b kafkabroker -L -J
+ jq -C .
[1;39m{
  [0m[34;1m"originating_broker"[0m[1;39m: [0m[1;39m{
    [0m[34;1m"id"[0m[1;39m: [0m[0;39m1001[0m[1;39m,
    [0m[34;1m"name"[0m[1;39m: [0m[0;32m"kafkabroker:9092/1001"[0m[1;39m
  [1;39m}[0m[1;39m,
  [0m[34;1m"query"[0m[1;39m: [0m[1;39m{
    [0m[34;1m"topic"[0m[1;39m: [0m[0;32m"*"[0m[1;39m
  [1;39m}[0m[1;39m,
  [0m[34;1m"controllerid"[0m[1;39m: [0m[0;39m1001[0m[1;39m,
  [0m[34;1m"brokers"[0m[1;39m: [0m[1;39m[
    [1;39m{
      [0m[34;1m"id"[0m[1;39m: [0m[0;39m1001[0m[1;39m,
      [0m[34;1m"name"[0m[1;39m: [0m[0;32m"kafkabroker:9092"[0m[1;39m
    [1;39m}[0m[1;39m
  [1;39m][0m[1;39m,
  [0m[34;1m"topics"[0m[1;39m: [0m[1;39m[
    [1;39m{
      [0m[34;1m"topic"[0m[1;39m: [0m[0;32m"stock_ticks"[0m[1;39m,
      [0m[34;1m"partitions"[0m[1;39m: [0m[1;39m[
        [1;39m{
          [0m[34;1m"partition"[0m[1;39m: [0m[0;39m0[0m[1;39m,
          [0m[34;1m"leader"[0m[1;39m: [0m[0;39m1001[0m[1;39m,
          [0m[34;1m"replicas"[0m[1;39m: [0m[1;39m[
            [1;39m{
              [0m[34;1m"id"[0m[1;39m: [0m[0;39m1001[0m[1;39m
            [1;39m}[0m[1;39m
          [1;39m][0m[1;39m,
          [0m[34;1m"isrs"[0m[1;39m: [0m[1;39m[
            [1;39m{
              [0m[34;1m"id"[0m[1;39m: [0m[0;39m1001[0m[1;39m
            [1;39m}[0m[1;39m
          [1;39m][0m[1;39m
        [1;39m}[0m[1;39m
      [1;39m][0m[1;39m
    [1;39m}[0m[1;39m
  [1;39m][0m[1;39m
[1;39m}[0m
+ docker exec -i adhoc-1 /bin/bash -x
+ spark-submit --class org.apache.hudi.utilities.streamer.HoodieStreamer /var/hoodie/ws/docker/hoodie/hadoop/hive_base/target/hoodie-utilities.jar --table-type MERGE_ON_READ --source-class org.apache.hudi.utilities.sources.JsonKafkaSource --source-ordering-field ts --target-base-path /user/hive/warehouse/stock_ticks_mor --target-table stock_ticks_mor --props /var/demo/config/kafka-source.properties --schemaprovider-class org.apache.hudi.utilities.schema.FilebasedSchemaProvider --disable-compaction
24/01/05 02:11:07 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
24/01/05 02:11:07 WARN streamer.SchedulerConfGenerator: Job Scheduling Configs will not be in effect as spark.scheduler.mode is not set to FAIR at instantiation time. Continuing without scheduling configs
24/01/05 02:11:07 INFO spark.SparkContext: Running Spark version 2.4.4
24/01/05 02:11:07 INFO spark.SparkContext: Submitted application: streamer-stock_ticks_mor
24/01/05 02:11:07 INFO spark.SecurityManager: Changing view acls to: root
24/01/05 02:11:07 INFO spark.SecurityManager: Changing modify acls to: root
24/01/05 02:11:07 INFO spark.SecurityManager: Changing view acls groups to: 
24/01/05 02:11:07 INFO spark.SecurityManager: Changing modify acls groups to: 
24/01/05 02:11:07 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
24/01/05 02:11:07 INFO Configuration.deprecation: mapred.output.compression.codec is deprecated. Instead, use mapreduce.output.fileoutputformat.compress.codec
24/01/05 02:11:07 INFO Configuration.deprecation: mapred.output.compress is deprecated. Instead, use mapreduce.output.fileoutputformat.compress
24/01/05 02:11:07 INFO Configuration.deprecation: mapred.output.compression.type is deprecated. Instead, use mapreduce.output.fileoutputformat.compress.type
24/01/05 02:11:07 INFO util.Utils: Successfully started service 'sparkDriver' on port 38551.
24/01/05 02:11:07 INFO spark.SparkEnv: Registering MapOutputTracker
24/01/05 02:11:07 INFO spark.SparkEnv: Registering BlockManagerMaster
24/01/05 02:11:07 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
24/01/05 02:11:07 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
24/01/05 02:11:07 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-f2ea2cc7-97f6-442d-acd5-3d2bb8f27928
24/01/05 02:11:07 INFO memory.MemoryStore: MemoryStore started with capacity 366.3 MB
24/01/05 02:11:07 INFO spark.SparkEnv: Registering OutputCommitCoordinator
24/01/05 02:11:07 INFO util.log: Logging initialized @1702ms
24/01/05 02:11:07 INFO server.Server: jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
24/01/05 02:11:07 INFO server.Server: Started @1762ms
24/01/05 02:11:08 INFO server.AbstractConnector: Started ServerConnector@61019f59{HTTP/1.1,[http/1.1]}{0.0.0.0:8090}
24/01/05 02:11:08 INFO util.Utils: Successfully started service 'SparkUI' on port 8090.
24/01/05 02:11:08 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3e1162e7{/jobs,null,AVAILABLE,@Spark}
24/01/05 02:11:08 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@17f460bb{/jobs/json,null,AVAILABLE,@Spark}
24/01/05 02:11:08 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@64a1923a{/jobs/job,null,AVAILABLE,@Spark}
24/01/05 02:11:08 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@18ca3c62{/jobs/job/json,null,AVAILABLE,@Spark}
24/01/05 02:11:08 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2c0f7678{/stages,null,AVAILABLE,@Spark}
24/01/05 02:11:08 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@44d70181{/stages/json,null,AVAILABLE,@Spark}
24/01/05 02:11:08 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6aa648b9{/stages/stage,null,AVAILABLE,@Spark}
24/01/05 02:11:08 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@88a8218{/stages/stage/json,null,AVAILABLE,@Spark}
24/01/05 02:11:08 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@50b1f030{/stages/pool,null,AVAILABLE,@Spark}
24/01/05 02:11:08 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4163f1cd{/stages/pool/json,null,AVAILABLE,@Spark}
24/01/05 02:11:08 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5fa05212{/storage,null,AVAILABLE,@Spark}
24/01/05 02:11:08 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3e681bc{/storage/json,null,AVAILABLE,@Spark}
24/01/05 02:11:08 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5c09d180{/storage/rdd,null,AVAILABLE,@Spark}
24/01/05 02:11:08 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@23aae55{/storage/rdd/json,null,AVAILABLE,@Spark}
24/01/05 02:11:08 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5f574cc2{/environment,null,AVAILABLE,@Spark}
24/01/05 02:11:08 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@680bddf5{/environment/json,null,AVAILABLE,@Spark}
24/01/05 02:11:08 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7a9c84a5{/executors,null,AVAILABLE,@Spark}
24/01/05 02:11:08 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2d83c5a5{/executors/json,null,AVAILABLE,@Spark}
24/01/05 02:11:08 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@48d7ad8b{/executors/threadDump,null,AVAILABLE,@Spark}
24/01/05 02:11:08 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7e053511{/executors/threadDump/json,null,AVAILABLE,@Spark}
24/01/05 02:11:08 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@60222fd8{/static,null,AVAILABLE,@Spark}
24/01/05 02:11:08 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@37ff4054{/,null,AVAILABLE,@Spark}
24/01/05 02:11:08 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@894858{/api,null,AVAILABLE,@Spark}
24/01/05 02:11:08 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@74cf8b28{/jobs/job/kill,null,AVAILABLE,@Spark}
24/01/05 02:11:08 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@36c54a56{/stages/stage/kill,null,AVAILABLE,@Spark}
24/01/05 02:11:08 INFO ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://adhoc-1:8090
24/01/05 02:11:08 INFO spark.SparkContext: Added JAR file:/var/hoodie/ws/docker/hoodie/hadoop/hive_base/target/hoodie-utilities.jar at spark://adhoc-1:38551/jars/hoodie-utilities.jar with timestamp 1704420668075
24/01/05 02:11:08 INFO executor.Executor: Starting executor ID driver on host localhost
24/01/05 02:11:08 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45245.
24/01/05 02:11:08 INFO netty.NettyBlockTransferService: Server created on adhoc-1:45245
24/01/05 02:11:08 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
24/01/05 02:11:08 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, adhoc-1, 45245, None)
24/01/05 02:11:08 INFO storage.BlockManagerMasterEndpoint: Registering block manager adhoc-1:45245 with 366.3 MB RAM, BlockManagerId(driver, adhoc-1, 45245, None)
24/01/05 02:11:08 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, adhoc-1, 45245, None)
24/01/05 02:11:08 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, adhoc-1, 45245, None)
24/01/05 02:11:08 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@62e6a3ec{/metrics/json,null,AVAILABLE,@Spark}
24/01/05 02:11:09 WARN config.DFSPropertiesConfiguration: Cannot find HUDI_CONF_DIR, please set it as the dir of hudi-defaults.conf
24/01/05 02:11:09 WARN config.DFSPropertiesConfiguration: Properties file file:/etc/hudi/conf/hudi-defaults.conf not found. Ignoring to load props file
24/01/05 02:11:09 INFO internal.SharedState: loading hive config file: file:/opt/hadoop-2.8.4/etc/hadoop/hive-site.xml
24/01/05 02:11:09 INFO internal.SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/opt/spark-warehouse').
24/01/05 02:11:09 INFO internal.SharedState: Warehouse path is 'file:/opt/spark-warehouse'.
24/01/05 02:11:09 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@423c5404{/SQL,null,AVAILABLE,@Spark}
24/01/05 02:11:09 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5853ca50{/SQL/json,null,AVAILABLE,@Spark}
24/01/05 02:11:09 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3c79088e{/SQL/execution,null,AVAILABLE,@Spark}
24/01/05 02:11:09 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4a37191a{/SQL/execution/json,null,AVAILABLE,@Spark}
24/01/05 02:11:09 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@24eb65e3{/static/sql,null,AVAILABLE,@Spark}
24/01/05 02:11:09 INFO state.StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
24/01/05 02:11:09 WARN sql.SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
24/01/05 02:11:09 INFO streamer.HoodieStreamer: Creating Hudi Streamer with configs:
auto.offset.reset: earliest
bootstrap.servers: kafkabroker:9092
hoodie.auto.adjust.lock.configs: true
hoodie.bulkinsert.shuffle.parallelism: 2
hoodie.compact.inline: false
hoodie.datasource.write.partitionpath.field: date
hoodie.datasource.write.reconcile.schema: false
hoodie.datasource.write.recordkey.field: key
hoodie.delete.shuffle.parallelism: 2
hoodie.embed.timeline.server: true
hoodie.filesystem.view.type: EMBEDDED_KV_STORE
hoodie.insert.shuffle.parallelism: 2
hoodie.streamer.schemaprovider.source.schema.file: /var/demo/config/schema.avsc
hoodie.streamer.schemaprovider.target.schema.file: /var/demo/config/schema.avsc
hoodie.streamer.source.kafka.topic: stock_ticks
hoodie.table.type: MERGE_ON_READ
hoodie.upsert.shuffle.parallelism: 2

24/01/05 02:11:09 INFO fs.FSUtils: Resolving file /var/demo/config/schema.avscto be a remote file.
24/01/05 02:11:09 INFO factory.HoodieSparkKeyGeneratorFactory: The value of hoodie.datasource.write.keygenerator.type is empty; inferred to be SIMPLE
24/01/05 02:11:09 INFO factory.HoodieSparkKeyGeneratorFactory: The value of hoodie.datasource.write.keygenerator.type is empty; inferred to be SIMPLE
24/01/05 02:11:09 INFO table.HoodieTableMetaClient: Initializing /user/hive/warehouse/stock_ticks_mor as hoodie table /user/hive/warehouse/stock_ticks_mor
24/01/05 02:11:10 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:11:10 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:11:10 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:11:10 INFO table.HoodieTableMetaClient: Finished initializing Table of type MERGE_ON_READ from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:11:10 INFO config.HoodieWriteConfig: Automatically set hoodie.write.lock.provider=org.apache.hudi.client.transaction.lock.InProcessLockProvider since user has not set the lock provider for single writer with async table services
24/01/05 02:11:10 INFO config.HoodieWriteConfig: Automatically set hoodie.write.lock.provider=org.apache.hudi.client.transaction.lock.InProcessLockProvider since user has not set the lock provider for single writer with async table services
24/01/05 02:11:10 INFO ingestion.HoodieIngestionService: Ingestion service starts running in run-once mode
24/01/05 02:11:10 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:11:10 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:11:10 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:11:10 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Optional.empty
24/01/05 02:11:10 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:11:10 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:11:10 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:11:10 INFO streamer.StreamSync: Checkpoint to resume from : Optional.empty
24/01/05 02:11:10 INFO consumer.ConsumerConfig: ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [kafkabroker:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

24/01/05 02:11:10 INFO utils.AppInfoParser: Kafka version : 2.0.0
24/01/05 02:11:10 INFO utils.AppInfoParser: Kafka commitId : 3402a8361b734732
24/01/05 02:11:10 INFO clients.Metadata: Cluster ID: OtZ13RgXQhuquRcIZizanw
24/01/05 02:11:10 INFO helpers.KafkaOffsetGen: SourceLimit not configured, set numEvents to default value : 5000000
24/01/05 02:11:10 INFO helpers.KafkaOffsetGen: getNextOffsetRanges set config hoodie.streamer.source.kafka.minPartitions to 0
24/01/05 02:11:10 INFO sources.KafkaSource: About to read 2611 from Kafka for topic :stock_ticks
24/01/05 02:11:10 WARN kafka010.KafkaUtils: overriding enable.auto.commit to false for executor
24/01/05 02:11:10 WARN kafka010.KafkaUtils: overriding auto.offset.reset to none for executor
24/01/05 02:11:10 ERROR kafka010.KafkaUtils: group.id is null, you should probably set it
24/01/05 02:11:10 WARN kafka010.KafkaUtils: overriding executor group.id to spark-executor-null
24/01/05 02:11:10 WARN kafka010.KafkaUtils: overriding receive.buffer.bytes to 65536 see KAFKA-3135
24/01/05 02:11:10 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Optional.empty
24/01/05 02:11:10 INFO factory.HoodieSparkKeyGeneratorFactory: The value of hoodie.datasource.write.keygenerator.type is empty; inferred to be SIMPLE
24/01/05 02:11:10 INFO streamer.StreamSync: Setting up new Hoodie Write Client
24/01/05 02:11:10 INFO config.HoodieWriteConfig: Automatically set hoodie.write.lock.provider=org.apache.hudi.client.transaction.lock.InProcessLockProvider since user has not set the lock provider for single writer with async table services
24/01/05 02:11:10 INFO embedded.EmbeddedTimelineService: Overriding hostIp to (adhoc-1) found in spark-conf. It was null
24/01/05 02:11:10 INFO view.FileSystemViewManager: Creating View Manager with storage type :EMBEDDED_KV_STORE
24/01/05 02:11:10 INFO view.FileSystemViewManager: Creating embedded rocks-db based Table View
24/01/05 02:11:10 INFO util.log: Logging initialized @4525ms to org.apache.hudi.org.eclipse.jetty.util.log.Slf4jLog
24/01/05 02:11:10 INFO javalin.Javalin: 
       __                      __ _            __ __
      / /____ _ _   __ ____ _ / /(_)____      / // /
 __  / // __ `/| | / // __ `// // // __ \    / // /_
/ /_/ // /_/ / | |/ // /_/ // // // / / /   /__  __/
\____/ \__,_/  |___/ \__,_//_//_//_/ /_/      /_/

          https://javalin.io/documentation

24/01/05 02:11:10 INFO javalin.Javalin: Starting Javalin ...
24/01/05 02:11:10 INFO javalin.Javalin: You are running Javalin 4.6.7 (released October 24, 2022. Your Javalin version is 437 days old. Consider checking for a newer version.).
24/01/05 02:11:11 INFO server.Server: jetty-9.4.48.v20220622; built: 2022-06-21T20:42:25.880Z; git: 6b67c5719d1f4371b33655ff2d047d24e171e49a; jvm 1.8.0_212-b04
24/01/05 02:11:11 INFO server.Server: Started @4909ms
24/01/05 02:11:11 INFO javalin.Javalin: Listening on http://localhost:33433/
24/01/05 02:11:11 INFO javalin.Javalin: Javalin started in 148ms \o/
24/01/05 02:11:11 INFO service.TimelineService: Starting Timeline server on port :33433
24/01/05 02:11:11 INFO embedded.EmbeddedTimelineService: Started embedded timeline server at adhoc-1:33433
24/01/05 02:11:11 INFO client.BaseHoodieClient: Timeline Server already running. Not restarting the service
24/01/05 02:11:11 INFO client.BaseHoodieClient: Timeline Server already running. Not restarting the service
24/01/05 02:11:11 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:11:11 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:11:11 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:11:11 INFO table.HoodieTableMetaClient: Loading Active commit timeline for /user/hive/warehouse/stock_ticks_mor
24/01/05 02:11:11 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Optional.empty
24/01/05 02:11:11 INFO util.CleanerUtils: Cleaned failed attempts if any
24/01/05 02:11:11 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:11:11 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:11:11 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:11:11 INFO table.HoodieTableMetaClient: Loading Active commit timeline for /user/hive/warehouse/stock_ticks_mor
24/01/05 02:11:11 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Optional.empty
24/01/05 02:11:11 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:11:11 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:11:11 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:11:11 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:11:11 INFO view.FileSystemViewManager: Creating View Manager with storage type :REMOTE_FIRST
24/01/05 02:11:11 INFO view.FileSystemViewManager: Creating remote first table view
24/01/05 02:11:11 INFO client.BaseHoodieWriteClient: Generate a new instant time: 20240105021110208 action: deltacommit
24/01/05 02:11:11 INFO timeline.HoodieActiveTimeline: Creating a new instant [==>20240105021110208__deltacommit__REQUESTED]
24/01/05 02:11:11 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:11:11 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:11:11 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:11:11 INFO table.HoodieTableMetaClient: Loading Active commit timeline for /user/hive/warehouse/stock_ticks_mor
24/01/05 02:11:11 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[==>20240105021110208__deltacommit__REQUESTED__20240105021111218]}
24/01/05 02:11:11 INFO transaction.TransactionManager: Transaction starting for Option{val=[==>20240105021110208__deltacommit__INFLIGHT]} with latest completed transaction instant Optional.empty
24/01/05 02:11:11 INFO lock.LockManager: LockProvider org.apache.hudi.client.transaction.lock.InProcessLockProvider
24/01/05 02:11:11 INFO lock.InProcessLockProvider: Base Path /user/hive/warehouse/stock_ticks_mor, Lock Instance java.util.concurrent.locks.ReentrantReadWriteLock@592a1882[Write locks = 0, Read locks = 0], Thread main, In-process lock state ACQUIRING
24/01/05 02:11:11 INFO lock.InProcessLockProvider: Base Path /user/hive/warehouse/stock_ticks_mor, Lock Instance java.util.concurrent.locks.ReentrantReadWriteLock@592a1882[Write locks = 1, Read locks = 0], Thread main, In-process lock state ACQUIRED
24/01/05 02:11:11 INFO transaction.TransactionManager: Transaction started for Option{val=[==>20240105021110208__deltacommit__INFLIGHT]} with latest completed transaction instant Optional.empty
24/01/05 02:11:11 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:11:11 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:11:11 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:11:11 INFO metadata.HoodieBackedTableMetadataWriter: Async metadata indexing disabled and following partitions already initialized: []
24/01/05 02:11:11 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[==>20240105021110208__deltacommit__REQUESTED__20240105021111218]}
24/01/05 02:11:11 INFO table.HoodieTableMetaClient: Initializing /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata as hoodie table /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:11:11 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:11:11 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:11:11 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:11:11 INFO table.HoodieTableMetaClient: Finished initializing Table of type MERGE_ON_READ from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:11:11 INFO spark.SparkContext: Starting job: collect at HoodieSparkEngineContext.java:116
24/01/05 02:11:11 INFO scheduler.DAGScheduler: Got job 0 (collect at HoodieSparkEngineContext.java:116) with 1 output partitions
24/01/05 02:11:11 INFO scheduler.DAGScheduler: Final stage: ResultStage 0 (collect at HoodieSparkEngineContext.java:116)
24/01/05 02:11:11 INFO scheduler.DAGScheduler: Parents of final stage: List()
24/01/05 02:11:11 INFO scheduler.DAGScheduler: Missing parents: List()
24/01/05 02:11:11 INFO scheduler.DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[8] at map at HoodieSparkEngineContext.java:116), which has no missing parents
24/01/05 02:11:11 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 72.2 KB, free 366.2 MB)
24/01/05 02:11:11 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 26.3 KB, free 366.2 MB)
24/01/05 02:11:11 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on adhoc-1:45245 (size: 26.3 KB, free: 366.3 MB)
24/01/05 02:11:11 INFO spark.SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1161
24/01/05 02:11:11 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[8] at map at HoodieSparkEngineContext.java:116) (first 15 tasks are for partitions Vector(0))
24/01/05 02:11:11 INFO scheduler.TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
24/01/05 02:11:11 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7842 bytes)
24/01/05 02:11:11 INFO executor.Executor: Running task 0.0 in stage 0.0 (TID 0)
24/01/05 02:11:11 INFO executor.Executor: Fetching spark://adhoc-1:38551/jars/hoodie-utilities.jar with timestamp 1704420668075
24/01/05 02:11:11 INFO client.TransportClientFactory: Successfully created connection to adhoc-1/172.20.0.16:38551 after 30 ms (0 ms spent in bootstraps)
24/01/05 02:11:11 INFO util.Utils: Fetching spark://adhoc-1:38551/jars/hoodie-utilities.jar to /tmp/spark-77e2afa3-ce3f-437c-a020-ebfc94bc90bd/userFiles-f7fcc832-c684-4fb2-8efe-8728d3bc0573/fetchFileTemp3984984135723149752.tmp
24/01/05 02:11:12 INFO executor.Executor: Adding file:/tmp/spark-77e2afa3-ce3f-437c-a020-ebfc94bc90bd/userFiles-f7fcc832-c684-4fb2-8efe-8728d3bc0573/hoodie-utilities.jar to class loader
24/01/05 02:11:12 INFO executor.Executor: Finished task 0.0 in stage 0.0 (TID 0). 876 bytes result sent to driver
24/01/05 02:11:12 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 652 ms on localhost (executor driver) (1/1)
24/01/05 02:11:12 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
24/01/05 02:11:12 INFO scheduler.DAGScheduler: ResultStage 0 (collect at HoodieSparkEngineContext.java:116) finished in 0.910 s
24/01/05 02:11:12 INFO scheduler.DAGScheduler: Job 0 finished: collect at HoodieSparkEngineContext.java:116, took 0.942015 s
24/01/05 02:11:12 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Optional.empty
24/01/05 02:11:12 INFO metadata.HoodieBackedTableMetadataWriter: Initializing MDT partition FILES at instant 00000000000000010
24/01/05 02:11:12 INFO metadata.HoodieBackedTableMetadataWriter: Committing total 0 partitions and 0 files to metadata
24/01/05 02:11:12 INFO spark.SparkContext: Starting job: count at HoodieJavaRDD.java:115
24/01/05 02:11:12 INFO scheduler.DAGScheduler: Got job 1 (count at HoodieJavaRDD.java:115) with 1 output partitions
24/01/05 02:11:12 INFO scheduler.DAGScheduler: Final stage: ResultStage 1 (count at HoodieJavaRDD.java:115)
24/01/05 02:11:12 INFO scheduler.DAGScheduler: Parents of final stage: List()
24/01/05 02:11:12 INFO scheduler.DAGScheduler: Missing parents: List()
24/01/05 02:11:12 INFO scheduler.DAGScheduler: Submitting ResultStage 1 (ParallelCollectionRDD[9] at parallelize at HoodieSparkEngineContext.java:111), which has no missing parents
24/01/05 02:11:12 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 1512.0 B, free 366.2 MB)
24/01/05 02:11:12 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 1046.0 B, free 366.2 MB)
24/01/05 02:11:12 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on adhoc-1:45245 (size: 1046.0 B, free: 366.3 MB)
24/01/05 02:11:12 INFO spark.SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1161
24/01/05 02:11:12 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (ParallelCollectionRDD[9] at parallelize at HoodieSparkEngineContext.java:111) (first 15 tasks are for partitions Vector(0))
24/01/05 02:11:12 INFO scheduler.TaskSchedulerImpl: Adding task set 1.0 with 1 tasks
24/01/05 02:11:12 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 7801 bytes)
24/01/05 02:11:12 INFO executor.Executor: Running task 0.0 in stage 1.0 (TID 1)
24/01/05 02:11:12 INFO executor.Executor: Finished task 0.0 in stage 1.0 (TID 1). 666 bytes result sent to driver
24/01/05 02:11:12 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 27 ms on localhost (executor driver) (1/1)
24/01/05 02:11:12 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
24/01/05 02:11:12 INFO scheduler.DAGScheduler: ResultStage 1 (count at HoodieJavaRDD.java:115) finished in 0.038 s
24/01/05 02:11:12 INFO scheduler.DAGScheduler: Job 1 finished: count at HoodieJavaRDD.java:115, took 0.041046 s
24/01/05 02:11:12 INFO metadata.HoodieBackedTableMetadataWriter: Initializing FILES index with 1 mappings and 1 file groups.
24/01/05 02:11:12 INFO metadata.HoodieBackedTableMetadataWriter: Creating 1 file groups for partition files with base fileId files- at instant time 00000000000000010
24/01/05 02:11:12 INFO spark.SparkContext: Starting job: foreach at HoodieSparkEngineContext.java:155
24/01/05 02:11:12 INFO scheduler.DAGScheduler: Got job 2 (foreach at HoodieSparkEngineContext.java:155) with 1 output partitions
24/01/05 02:11:12 INFO scheduler.DAGScheduler: Final stage: ResultStage 2 (foreach at HoodieSparkEngineContext.java:155)
24/01/05 02:11:12 INFO scheduler.DAGScheduler: Parents of final stage: List()
24/01/05 02:11:12 INFO scheduler.DAGScheduler: Missing parents: List()
24/01/05 02:11:12 INFO scheduler.DAGScheduler: Submitting ResultStage 2 (ParallelCollectionRDD[10] at parallelize at HoodieSparkEngineContext.java:155), which has no missing parents
24/01/05 02:11:12 INFO memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 296.9 KB, free 365.9 MB)
24/01/05 02:11:12 INFO memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 100.2 KB, free 365.8 MB)
24/01/05 02:11:12 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on adhoc-1:45245 (size: 100.2 KB, free: 366.2 MB)
24/01/05 02:11:12 INFO spark.SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1161
24/01/05 02:11:12 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (ParallelCollectionRDD[10] at parallelize at HoodieSparkEngineContext.java:155) (first 15 tasks are for partitions Vector(0))
24/01/05 02:11:12 INFO scheduler.TaskSchedulerImpl: Adding task set 2.0 with 1 tasks
24/01/05 02:11:12 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, PROCESS_LOCAL, 7737 bytes)
24/01/05 02:11:12 INFO executor.Executor: Running task 0.0 in stage 2.0 (TID 2)
24/01/05 02:11:12 INFO log.HoodieLogFormat$WriterBuilder: Building HoodieLogFormat Writer
24/01/05 02:11:12 INFO log.HoodieLogFormat$WriterBuilder: HoodieLogFile on path /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/files/.files-0000-0_00000000000000010.log.1_0-0-0
24/01/05 02:11:12 INFO log.HoodieLogFormatWriter: HoodieLogFile{pathStr='/user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/files/.files-0000-0_00000000000000010.log.1_0-0-0', fileLen=0} does not exist. Create a new file
24/01/05 02:11:12 INFO executor.Executor: Finished task 0.0 in stage 2.0 (TID 2). 666 bytes result sent to driver
24/01/05 02:11:12 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 96 ms on localhost (executor driver) (1/1)
24/01/05 02:11:12 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
24/01/05 02:11:12 INFO scheduler.DAGScheduler: ResultStage 2 (foreach at HoodieSparkEngineContext.java:155) finished in 0.126 s
24/01/05 02:11:12 INFO scheduler.DAGScheduler: Job 2 finished: foreach at HoodieSparkEngineContext.java:155, took 0.128473 s
24/01/05 02:11:12 INFO view.AbstractTableFileSystemView: Took 1 ms to read  0 instants, 0 replaced file groups
24/01/05 02:11:12 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:11:12 INFO metadata.HoodieTableMetadataUtil: Loading latest file slices for metadata table partition files
24/01/05 02:11:12 INFO view.AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
24/01/05 02:11:12 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:11:12 INFO view.AbstractTableFileSystemView: Building file system view for partition (files)
24/01/05 02:11:12 INFO client.BaseHoodieClient: Embedded Timeline Server is disabled. Not starting timeline service
24/01/05 02:11:12 INFO client.BaseHoodieClient: Embedded Timeline Server is disabled. Not starting timeline service
24/01/05 02:11:12 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:11:12 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:11:12 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:11:12 INFO table.HoodieTableMetaClient: Loading Active commit timeline for /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:11:12 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Optional.empty
24/01/05 02:11:12 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:11:12 INFO view.FileSystemViewManager: Creating View Manager with storage type :MEMORY
24/01/05 02:11:12 INFO view.FileSystemViewManager: Creating in-memory based Table View
24/01/05 02:11:12 INFO metadata.HoodieBackedTableMetadataWriter: New commit at 00000000000000010 being applied to MDT.
24/01/05 02:11:12 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:11:12 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:11:12 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:11:12 INFO table.HoodieTableMetaClient: Loading Active commit timeline for /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:11:12 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Optional.empty
24/01/05 02:11:12 INFO util.CleanerUtils: Cleaned failed attempts if any
24/01/05 02:11:12 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:11:12 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:11:12 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:11:12 INFO table.HoodieTableMetaClient: Loading Active commit timeline for /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:11:12 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Optional.empty
24/01/05 02:11:12 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:11:12 INFO view.FileSystemViewManager: Creating View Manager with storage type :MEMORY
24/01/05 02:11:12 INFO view.FileSystemViewManager: Creating in-memory based Table View
24/01/05 02:11:12 INFO client.BaseHoodieWriteClient: Generate a new instant time: 00000000000000010 action: deltacommit
24/01/05 02:11:12 INFO timeline.HoodieActiveTimeline: Creating a new instant [==>00000000000000010__deltacommit__REQUESTED]
24/01/05 02:11:12 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:11:12 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:11:12 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:11:12 INFO table.HoodieTableMetaClient: Loading Active commit timeline for /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:11:12 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[==>00000000000000010__deltacommit__REQUESTED__20240105021112766]}
24/01/05 02:11:12 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:11:12 INFO view.FileSystemViewManager: Creating View Manager with storage type :MEMORY
24/01/05 02:11:12 INFO view.FileSystemViewManager: Creating in-memory based Table View
24/01/05 02:11:12 INFO async.AsyncCleanerService: The HoodieWriteClient is not configured to auto & async clean. Async clean service will not start.
24/01/05 02:11:12 INFO async.AsyncArchiveService: The HoodieWriteClient is not configured to auto & async archive. Async archive service will not start.
24/01/05 02:11:12 INFO timeline.HoodieActiveTimeline: Checking for file exists ?/user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/00000000000000010.deltacommit.requested
24/01/05 02:11:12 INFO timeline.HoodieActiveTimeline: Create new file for toInstant ?/user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/00000000000000010.deltacommit.inflight
24/01/05 02:11:12 INFO spark.SparkContext: Starting job: collect at SparkHoodieMetadataBulkInsertPartitioner.java:95
24/01/05 02:11:12 INFO scheduler.DAGScheduler: Registering RDD 14 (keyBy at SparkHoodieMetadataBulkInsertPartitioner.java:74)
24/01/05 02:11:12 INFO scheduler.DAGScheduler: Got job 3 (collect at SparkHoodieMetadataBulkInsertPartitioner.java:95) with 1 output partitions
24/01/05 02:11:12 INFO scheduler.DAGScheduler: Final stage: ResultStage 4 (collect at SparkHoodieMetadataBulkInsertPartitioner.java:95)
24/01/05 02:11:12 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)
24/01/05 02:11:12 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 3)
24/01/05 02:11:12 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 3 (MapPartitionsRDD[14] at keyBy at SparkHoodieMetadataBulkInsertPartitioner.java:74), which has no missing parents
24/01/05 02:11:12 INFO memory.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 7.0 KB, free 365.8 MB)
24/01/05 02:11:12 INFO memory.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 4.0 KB, free 365.8 MB)
24/01/05 02:11:12 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on adhoc-1:45245 (size: 4.0 KB, free: 366.2 MB)
24/01/05 02:11:12 INFO spark.SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1161
24/01/05 02:11:12 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[14] at keyBy at SparkHoodieMetadataBulkInsertPartitioner.java:74) (first 15 tasks are for partitions Vector(0))
24/01/05 02:11:12 INFO scheduler.TaskSchedulerImpl: Adding task set 3.0 with 1 tasks
24/01/05 02:11:12 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3, localhost, executor driver, partition 0, PROCESS_LOCAL, 7899 bytes)
24/01/05 02:11:12 INFO executor.Executor: Running task 0.0 in stage 3.0 (TID 3)
24/01/05 02:11:12 INFO executor.Executor: Finished task 0.0 in stage 3.0 (TID 3). 821 bytes result sent to driver
24/01/05 02:11:12 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 44 ms on localhost (executor driver) (1/1)
24/01/05 02:11:12 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
24/01/05 02:11:12 INFO scheduler.DAGScheduler: ShuffleMapStage 3 (keyBy at SparkHoodieMetadataBulkInsertPartitioner.java:74) finished in 0.072 s
24/01/05 02:11:12 INFO scheduler.DAGScheduler: looking for newly runnable stages
24/01/05 02:11:12 INFO scheduler.DAGScheduler: running: Set()
24/01/05 02:11:12 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 4)
24/01/05 02:11:12 INFO scheduler.DAGScheduler: failed: Set()
24/01/05 02:11:12 INFO scheduler.DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[17] at mapPartitions at SparkHoodieMetadataBulkInsertPartitioner.java:81), which has no missing parents
24/01/05 02:11:12 INFO memory.MemoryStore: Block broadcast_4 stored as values in memory (estimated size 5.4 KB, free 365.8 MB)
24/01/05 02:11:12 INFO memory.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 3.0 KB, free 365.8 MB)
24/01/05 02:11:12 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on adhoc-1:45245 (size: 3.0 KB, free: 366.2 MB)
24/01/05 02:11:12 INFO spark.SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1161
24/01/05 02:11:12 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[17] at mapPartitions at SparkHoodieMetadataBulkInsertPartitioner.java:81) (first 15 tasks are for partitions Vector(0))
24/01/05 02:11:12 INFO scheduler.TaskSchedulerImpl: Adding task set 4.0 with 1 tasks
24/01/05 02:11:12 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4, localhost, executor driver, partition 0, ANY, 7662 bytes)
24/01/05 02:11:12 INFO executor.Executor: Running task 0.0 in stage 4.0 (TID 4)
24/01/05 02:11:12 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
24/01/05 02:11:12 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 9 ms
24/01/05 02:11:12 INFO executor.Executor: Finished task 0.0 in stage 4.0 (TID 4). 1110 bytes result sent to driver
24/01/05 02:11:12 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 67 ms on localhost (executor driver) (1/1)
24/01/05 02:11:12 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 
24/01/05 02:11:13 INFO scheduler.DAGScheduler: ResultStage 4 (collect at SparkHoodieMetadataBulkInsertPartitioner.java:95) finished in 0.079 s
24/01/05 02:11:13 INFO scheduler.DAGScheduler: Job 3 finished: collect at SparkHoodieMetadataBulkInsertPartitioner.java:95, took 0.173649 s
24/01/05 02:11:13 INFO commit.BaseSparkCommitActionExecutor: no validators configured.
24/01/05 02:11:13 INFO commit.BaseCommitActionExecutor: Auto commit enabled: Committing 00000000000000010
24/01/05 02:11:13 INFO spark.SparkContext: Starting job: collect at HoodieJavaRDD.java:177
24/01/05 02:11:13 INFO scheduler.DAGScheduler: Got job 4 (collect at HoodieJavaRDD.java:177) with 1 output partitions
24/01/05 02:11:13 INFO scheduler.DAGScheduler: Final stage: ResultStage 6 (collect at HoodieJavaRDD.java:177)
24/01/05 02:11:13 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 5)
24/01/05 02:11:13 INFO scheduler.DAGScheduler: Missing parents: List()
24/01/05 02:11:13 INFO scheduler.DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[20] at map at HoodieJavaRDD.java:125), which has no missing parents
24/01/05 02:11:13 INFO memory.MemoryStore: Block broadcast_5 stored as values in memory (estimated size 273.1 KB, free 365.5 MB)
24/01/05 02:11:13 INFO memory.MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 96.5 KB, free 365.4 MB)
24/01/05 02:11:13 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on adhoc-1:45245 (size: 96.5 KB, free: 366.1 MB)
24/01/05 02:11:13 INFO spark.SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1161
24/01/05 02:11:13 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[20] at map at HoodieJavaRDD.java:125) (first 15 tasks are for partitions Vector(0))
24/01/05 02:11:13 INFO scheduler.TaskSchedulerImpl: Adding task set 6.0 with 1 tasks
24/01/05 02:11:13 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 6.0 (TID 5, localhost, executor driver, partition 0, ANY, 7662 bytes)
24/01/05 02:11:13 INFO executor.Executor: Running task 0.0 in stage 6.0 (TID 5)
24/01/05 02:11:13 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
24/01/05 02:11:13 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
24/01/05 02:11:13 INFO queue.SimpleExecutor: Starting consumer, consuming records from the records iterator directly
24/01/05 02:11:13 INFO marker.DirectWriteMarkers: Creating Marker Path=/user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/.temp/00000000000000010/files/files-0000-0_0-6-5_00000000000000010.hfile.marker.CREATE
24/01/05 02:11:13 INFO marker.DirectWriteMarkers: [direct] Created marker file /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/.temp/00000000000000010/files/files-0000-0_0-6-5_00000000000000010.hfile.marker.CREATE in 6 ms
24/01/05 02:11:13 INFO spark.ContextCleaner: Cleaned accumulator 116
24/01/05 02:11:13 INFO spark.ContextCleaner: Cleaned accumulator 98
24/01/05 02:11:13 INFO spark.ContextCleaner: Cleaned accumulator 104
24/01/05 02:11:13 INFO spark.ContextCleaner: Cleaned accumulator 30
24/01/05 02:11:13 INFO spark.ContextCleaner: Cleaned accumulator 63
24/01/05 02:11:13 INFO spark.ContextCleaner: Cleaned accumulator 112
24/01/05 02:11:13 INFO spark.ContextCleaner: Cleaned accumulator 38
24/01/05 02:11:13 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on adhoc-1:45245 in memory (size: 3.0 KB, free: 366.1 MB)
24/01/05 02:11:13 INFO spark.ContextCleaner: Cleaned accumulator 33
24/01/05 02:11:13 INFO storage.BlockManagerInfo: Removed broadcast_2_piece0 on adhoc-1:45245 in memory (size: 100.2 KB, free: 366.2 MB)
24/01/05 02:11:13 INFO spark.ContextCleaner: Cleaned accumulator 120
24/01/05 02:11:13 INFO spark.ContextCleaner: Cleaned accumulator 103
24/01/05 02:11:13 INFO spark.ContextCleaner: Cleaned accumulator 110
24/01/05 02:11:13 INFO spark.ContextCleaner: Cleaned accumulator 100
24/01/05 02:11:13 INFO spark.ContextCleaner: Cleaned accumulator 109
24/01/05 02:11:13 INFO storage.BlockManagerInfo: Removed broadcast_0_piece0 on adhoc-1:45245 in memory (size: 26.3 KB, free: 366.2 MB)
24/01/05 02:11:13 INFO spark.ContextCleaner: Cleaned accumulator 92
24/01/05 02:11:13 INFO spark.ContextCleaner: Cleaned accumulator 97
24/01/05 02:11:13 INFO spark.ContextCleaner: Cleaned accumulator 71
24/01/05 02:11:13 INFO spark.ContextCleaner: Cleaned accumulator 54
24/01/05 02:11:13 INFO spark.ContextCleaner: Cleaned accumulator 64
24/01/05 02:11:13 INFO spark.ContextCleaner: Cleaned accumulator 67
24/01/05 02:11:13 INFO spark.ContextCleaner: Cleaned accumulator 114
24/01/05 02:11:13 INFO spark.ContextCleaner: Cleaned accumulator 85
24/01/05 02:11:13 INFO spark.ContextCleaner: Cleaned accumulator 34
24/01/05 02:11:13 INFO spark.ContextCleaner: Cleaned accumulator 48
24/01/05 02:11:13 INFO spark.ContextCleaner: Cleaned accumulator 49
24/01/05 02:11:13 INFO spark.ContextCleaner: Cleaned accumulator 66
24/01/05 02:11:13 INFO spark.ContextCleaner: Cleaned accumulator 58
24/01/05 02:11:13 INFO spark.ContextCleaner: Cleaned accumulator 99
24/01/05 02:11:13 INFO spark.ContextCleaner: Cleaned accumulator 108
24/01/05 02:11:13 INFO spark.ContextCleaner: Cleaned accumulator 42
24/01/05 02:11:13 INFO spark.ContextCleaner: Cleaned accumulator 86
24/01/05 02:11:13 INFO spark.ContextCleaner: Cleaned accumulator 90
24/01/05 02:11:13 INFO spark.ContextCleaner: Cleaned accumulator 121
24/01/05 02:11:13 INFO spark.ContextCleaner: Cleaned accumulator 102
24/01/05 02:11:13 INFO spark.ContextCleaner: Cleaned accumulator 45
24/01/05 02:11:13 INFO spark.ContextCleaner: Cleaned accumulator 46
24/01/05 02:11:13 INFO spark.ContextCleaner: Cleaned accumulator 78
24/01/05 02:11:13 INFO spark.ContextCleaner: Cleaned accumulator 117
24/01/05 02:11:13 INFO spark.ContextCleaner: Cleaned accumulator 41
24/01/05 02:11:13 INFO spark.ContextCleaner: Cleaned accumulator 84
24/01/05 02:11:13 INFO spark.ContextCleaner: Cleaned accumulator 32
24/01/05 02:11:13 INFO spark.ContextCleaner: Cleaned accumulator 122
24/01/05 02:11:13 INFO spark.ContextCleaner: Cleaned accumulator 47
24/01/05 02:11:13 INFO spark.ContextCleaner: Cleaned accumulator 70
24/01/05 02:11:13 INFO spark.ContextCleaner: Cleaned accumulator 119
24/01/05 02:11:13 INFO spark.ContextCleaner: Cleaned accumulator 73
24/01/05 02:11:13 INFO spark.ContextCleaner: Cleaned accumulator 95
24/01/05 02:11:13 INFO spark.ContextCleaner: Cleaned accumulator 68
24/01/05 02:11:13 INFO spark.ContextCleaner: Cleaned accumulator 55
24/01/05 02:11:13 INFO spark.ContextCleaner: Cleaned accumulator 88
24/01/05 02:11:13 INFO spark.ContextCleaner: Cleaned accumulator 77
24/01/05 02:11:13 INFO spark.ContextCleaner: Cleaned accumulator 25
24/01/05 02:11:13 INFO spark.ContextCleaner: Cleaned accumulator 69
24/01/05 02:11:13 INFO spark.ContextCleaner: Cleaned accumulator 118
24/01/05 02:11:13 INFO spark.ContextCleaner: Cleaned accumulator 124
24/01/05 02:11:13 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on adhoc-1:45245 in memory (size: 1046.0 B, free: 366.2 MB)
24/01/05 02:11:13 INFO spark.ContextCleaner: Cleaned accumulator 51
24/01/05 02:11:13 INFO spark.ContextCleaner: Cleaned accumulator 93
24/01/05 02:11:13 INFO spark.ContextCleaner: Cleaned accumulator 94
24/01/05 02:11:13 INFO spark.ContextCleaner: Cleaned accumulator 79
24/01/05 02:11:13 INFO spark.ContextCleaner: Cleaned accumulator 62
24/01/05 02:11:13 INFO spark.ContextCleaner: Cleaned accumulator 52
24/01/05 02:11:13 INFO spark.ContextCleaner: Cleaned accumulator 31
24/01/05 02:11:13 INFO spark.ContextCleaner: Cleaned accumulator 39
24/01/05 02:11:13 INFO spark.ContextCleaner: Cleaned accumulator 72
24/01/05 02:11:13 INFO spark.ContextCleaner: Cleaned accumulator 81
24/01/05 02:11:13 INFO spark.ContextCleaner: Cleaned accumulator 56
24/01/05 02:11:13 INFO spark.ContextCleaner: Cleaned accumulator 82
24/01/05 02:11:13 INFO spark.ContextCleaner: Cleaned accumulator 61
24/01/05 02:11:13 INFO spark.ContextCleaner: Cleaned accumulator 106
24/01/05 02:11:13 INFO spark.ContextCleaner: Cleaned accumulator 29
24/01/05 02:11:13 INFO spark.ContextCleaner: Cleaned accumulator 40
24/01/05 02:11:13 INFO spark.ContextCleaner: Cleaned accumulator 43
24/01/05 02:11:13 INFO spark.ContextCleaner: Cleaned accumulator 26
24/01/05 02:11:13 INFO spark.ContextCleaner: Cleaned accumulator 57
24/01/05 02:11:13 INFO spark.ContextCleaner: Cleaned accumulator 59
24/01/05 02:11:13 INFO spark.ContextCleaner: Cleaned accumulator 96
24/01/05 02:11:13 INFO spark.ContextCleaner: Cleaned accumulator 113
24/01/05 02:11:13 INFO spark.ContextCleaner: Cleaned accumulator 87
24/01/05 02:11:13 INFO spark.ContextCleaner: Cleaned accumulator 60
24/01/05 02:11:13 INFO spark.ContextCleaner: Cleaned accumulator 65
24/01/05 02:11:13 INFO spark.ContextCleaner: Cleaned accumulator 44
24/01/05 02:11:13 INFO spark.ContextCleaner: Cleaned accumulator 36
24/01/05 02:11:13 INFO spark.ContextCleaner: Cleaned accumulator 50
24/01/05 02:11:13 INFO spark.ContextCleaner: Cleaned accumulator 27
24/01/05 02:11:13 INFO spark.ContextCleaner: Cleaned accumulator 53
24/01/05 02:11:13 INFO spark.ContextCleaner: Cleaned accumulator 91
24/01/05 02:11:13 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on adhoc-1:45245 in memory (size: 4.0 KB, free: 366.2 MB)
24/01/05 02:11:13 INFO spark.ContextCleaner: Cleaned accumulator 83
24/01/05 02:11:13 INFO spark.ContextCleaner: Cleaned accumulator 75
24/01/05 02:11:13 INFO spark.ContextCleaner: Cleaned accumulator 105
24/01/05 02:11:13 INFO spark.ContextCleaner: Cleaned accumulator 101
24/01/05 02:11:13 INFO spark.ContextCleaner: Cleaned accumulator 89
24/01/05 02:11:13 INFO spark.ContextCleaner: Cleaned accumulator 115
24/01/05 02:11:13 INFO spark.ContextCleaner: Cleaned accumulator 74
24/01/05 02:11:13 INFO spark.ContextCleaner: Cleaned accumulator 37
24/01/05 02:11:13 INFO spark.ContextCleaner: Cleaned accumulator 107
24/01/05 02:11:13 INFO spark.ContextCleaner: Cleaned accumulator 80
24/01/05 02:11:13 INFO spark.ContextCleaner: Cleaned accumulator 111
24/01/05 02:11:13 INFO spark.ContextCleaner: Cleaned accumulator 76
24/01/05 02:11:13 INFO spark.ContextCleaner: Cleaned accumulator 123
24/01/05 02:11:13 INFO spark.ContextCleaner: Cleaned accumulator 28
24/01/05 02:11:13 INFO spark.ContextCleaner: Cleaned accumulator 35
24/01/05 02:11:13 INFO impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
24/01/05 02:11:13 INFO impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
24/01/05 02:11:13 INFO impl.MetricsSystemImpl: HBase metrics system started
24/01/05 02:11:13 INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.hudi.org.apache.hadoop.hbase.metrics.impl.MetricRegistriesImpl
24/01/05 02:11:13 INFO compress.CodecPool: Got brand-new compressor [.gz]
24/01/05 02:11:13 INFO compress.CodecPool: Got brand-new compressor [.gz]
24/01/05 02:11:13 INFO io.HoodieCreateHandle: New CreateHandle for partition :files with fileId files-0000-0
24/01/05 02:11:13 INFO io.HoodieCreateHandle: Closing the file files-0000-0 as we are done with all the records 1
24/01/05 02:11:13 INFO io.HoodieCreateHandle: CreateHandle for partitionPath files fileID files-0000-0, took 682 ms.
24/01/05 02:11:13 INFO memory.MemoryStore: Block rdd_19_0 stored as values in memory (estimated size 319.0 B, free 365.9 MB)
24/01/05 02:11:13 INFO storage.BlockManagerInfo: Added rdd_19_0 in memory on adhoc-1:45245 (size: 319.0 B, free: 366.2 MB)
24/01/05 02:11:13 INFO executor.Executor: Finished task 0.0 in stage 6.0 (TID 5). 1345 bytes result sent to driver
24/01/05 02:11:13 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 6.0 (TID 5) in 794 ms on localhost (executor driver) (1/1)
24/01/05 02:11:13 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool 
24/01/05 02:11:13 INFO scheduler.DAGScheduler: ResultStage 6 (collect at HoodieJavaRDD.java:177) finished in 0.828 s
24/01/05 02:11:13 INFO scheduler.DAGScheduler: Job 4 finished: collect at HoodieJavaRDD.java:177, took 0.835038 s
24/01/05 02:11:13 INFO util.CommitUtils: Creating  metadata for BULK_INSERT numWriteStats:1 numReplaceFileIds:0
24/01/05 02:11:13 INFO commit.BaseSparkCommitActionExecutor: Committing 00000000000000010, action Type deltacommit, operation Type BULK_INSERT
24/01/05 02:11:13 INFO spark.SparkContext: Starting job: collect at HoodieSparkEngineContext.java:150
24/01/05 02:11:13 INFO scheduler.DAGScheduler: Got job 5 (collect at HoodieSparkEngineContext.java:150) with 1 output partitions
24/01/05 02:11:13 INFO scheduler.DAGScheduler: Final stage: ResultStage 7 (collect at HoodieSparkEngineContext.java:150)
24/01/05 02:11:13 INFO scheduler.DAGScheduler: Parents of final stage: List()
24/01/05 02:11:13 INFO scheduler.DAGScheduler: Missing parents: List()
24/01/05 02:11:13 INFO scheduler.DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[22] at flatMap at HoodieSparkEngineContext.java:150), which has no missing parents
24/01/05 02:11:13 INFO memory.MemoryStore: Block broadcast_6 stored as values in memory (estimated size 72.5 KB, free 365.9 MB)
24/01/05 02:11:13 INFO memory.MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 26.7 KB, free 365.8 MB)
24/01/05 02:11:13 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on adhoc-1:45245 (size: 26.7 KB, free: 366.2 MB)
24/01/05 02:11:13 INFO spark.SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1161
24/01/05 02:11:13 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[22] at flatMap at HoodieSparkEngineContext.java:150) (first 15 tasks are for partitions Vector(0))
24/01/05 02:11:13 INFO scheduler.TaskSchedulerImpl: Adding task set 7.0 with 1 tasks
24/01/05 02:11:13 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 7.0 (TID 6, localhost, executor driver, partition 0, PROCESS_LOCAL, 7838 bytes)
24/01/05 02:11:13 INFO executor.Executor: Running task 0.0 in stage 7.0 (TID 6)
24/01/05 02:11:14 INFO executor.Executor: Finished task 0.0 in stage 7.0 (TID 6). 718 bytes result sent to driver
24/01/05 02:11:14 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 7.0 (TID 6) in 26 ms on localhost (executor driver) (1/1)
24/01/05 02:11:14 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool 
24/01/05 02:11:14 INFO scheduler.DAGScheduler: ResultStage 7 (collect at HoodieSparkEngineContext.java:150) finished in 0.040 s
24/01/05 02:11:14 INFO scheduler.DAGScheduler: Job 5 finished: collect at HoodieSparkEngineContext.java:150, took 0.042955 s
24/01/05 02:11:14 INFO timeline.HoodieActiveTimeline: Marking instant complete [==>00000000000000010__deltacommit__INFLIGHT]
24/01/05 02:11:14 INFO timeline.HoodieActiveTimeline: Checking for file exists ?/user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/00000000000000010.deltacommit.inflight
24/01/05 02:11:14 INFO timeline.HoodieActiveTimeline: Create new file for toInstant ?/user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/00000000000000010.deltacommit
24/01/05 02:11:14 INFO timeline.HoodieActiveTimeline: Completed [==>00000000000000010__deltacommit__INFLIGHT]
24/01/05 02:11:14 INFO commit.BaseSparkCommitActionExecutor: Committed 00000000000000010
24/01/05 02:11:14 INFO spark.SparkContext: Starting job: collectAsMap at HoodieSparkEngineContext.java:164
24/01/05 02:11:14 INFO scheduler.DAGScheduler: Got job 6 (collectAsMap at HoodieSparkEngineContext.java:164) with 1 output partitions
24/01/05 02:11:14 INFO scheduler.DAGScheduler: Final stage: ResultStage 8 (collectAsMap at HoodieSparkEngineContext.java:164)
24/01/05 02:11:14 INFO scheduler.DAGScheduler: Parents of final stage: List()
24/01/05 02:11:14 INFO scheduler.DAGScheduler: Missing parents: List()
24/01/05 02:11:14 INFO scheduler.DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[24] at mapToPair at HoodieSparkEngineContext.java:161), which has no missing parents
24/01/05 02:11:14 INFO memory.MemoryStore: Block broadcast_7 stored as values in memory (estimated size 72.8 KB, free 365.8 MB)
24/01/05 02:11:14 INFO memory.MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 26.8 KB, free 365.7 MB)
24/01/05 02:11:14 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on adhoc-1:45245 (size: 26.8 KB, free: 366.2 MB)
24/01/05 02:11:14 INFO spark.SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1161
24/01/05 02:11:14 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[24] at mapToPair at HoodieSparkEngineContext.java:161) (first 15 tasks are for partitions Vector(0))
24/01/05 02:11:14 INFO scheduler.TaskSchedulerImpl: Adding task set 8.0 with 1 tasks
24/01/05 02:11:14 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 8.0 (TID 7, localhost, executor driver, partition 0, PROCESS_LOCAL, 7838 bytes)
24/01/05 02:11:14 INFO executor.Executor: Running task 0.0 in stage 8.0 (TID 7)
24/01/05 02:11:14 INFO executor.Executor: Finished task 0.0 in stage 8.0 (TID 7). 787 bytes result sent to driver
24/01/05 02:11:14 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 8.0 (TID 7) in 23 ms on localhost (executor driver) (1/1)
24/01/05 02:11:14 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool 
24/01/05 02:11:14 INFO scheduler.DAGScheduler: ResultStage 8 (collectAsMap at HoodieSparkEngineContext.java:164) finished in 0.035 s
24/01/05 02:11:14 INFO scheduler.DAGScheduler: Job 6 finished: collectAsMap at HoodieSparkEngineContext.java:164, took 0.036799 s
24/01/05 02:11:14 INFO fs.FSUtils: Removed directory at /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/.temp/00000000000000010
24/01/05 02:11:14 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:11:14 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:11:14 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:11:14 INFO table.HoodieTableMetaClient: Loading Active commit timeline for /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:11:14 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20240105021114060]}
24/01/05 02:11:14 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:11:14 INFO view.FileSystemViewManager: Creating View Manager with storage type :MEMORY
24/01/05 02:11:14 INFO view.FileSystemViewManager: Creating in-memory based Table View
24/01/05 02:11:14 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20240105021114060]}
24/01/05 02:11:14 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20240105021114060]}
24/01/05 02:11:14 INFO table.HoodieTableConfig: MDT /user/hive/warehouse/stock_ticks_mor partition FILES has been enabled
24/01/05 02:11:14 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:11:14 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:11:14 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:11:14 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:11:14 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:11:14 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:11:14 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20240105021114060]}
24/01/05 02:11:14 INFO view.AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
24/01/05 02:11:14 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:11:14 INFO metadata.HoodieBackedTableMetadataWriter: Initializing FILES index in metadata table took 1735 in ms
24/01/05 02:11:14 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:11:14 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:11:14 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:11:14 INFO table.HoodieTableMetaClient: Loading Active commit timeline for /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:11:14 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20240105021114060]}
24/01/05 02:11:14 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:11:14 INFO view.FileSystemViewManager: Creating View Manager with storage type :MEMORY
24/01/05 02:11:14 INFO view.FileSystemViewManager: Creating in-memory based Table View
24/01/05 02:11:14 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:11:14 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:11:14 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:11:14 INFO table.HoodieTableMetaClient: Loading Active commit timeline for /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:11:14 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20240105021114060]}
24/01/05 02:11:14 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:11:14 INFO view.FileSystemViewManager: Creating View Manager with storage type :MEMORY
24/01/05 02:11:14 INFO view.FileSystemViewManager: Creating in-memory based Table View
24/01/05 02:11:14 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20240105021114060]}
24/01/05 02:11:14 INFO metadata.HoodieBackedTableMetadataWriter: Latest deltacommit time found is 00000000000000010, running clean operations.
24/01/05 02:11:14 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20240105021114060]}
24/01/05 02:11:14 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:11:14 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:11:14 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:11:14 INFO table.HoodieTableMetaClient: Loading Active commit timeline for /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:11:14 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20240105021114060]}
24/01/05 02:11:14 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:11:14 INFO view.FileSystemViewManager: Creating View Manager with storage type :MEMORY
24/01/05 02:11:14 INFO view.FileSystemViewManager: Creating in-memory based Table View
24/01/05 02:11:14 INFO client.BaseHoodieWriteClient: Cleaner started
24/01/05 02:11:14 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:11:14 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:11:14 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:11:14 INFO table.HoodieTableMetaClient: Loading Active commit timeline for /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:11:14 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20240105021114060]}
24/01/05 02:11:14 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:11:14 INFO view.FileSystemViewManager: Creating View Manager with storage type :MEMORY
24/01/05 02:11:14 INFO view.FileSystemViewManager: Creating in-memory based Table View
24/01/05 02:11:14 INFO client.BaseHoodieWriteClient: Scheduling cleaning at instant time :00000000000000010002
24/01/05 02:11:14 INFO view.FileSystemViewManager: Creating InMemory based view for basePath /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:11:14 INFO view.AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
24/01/05 02:11:14 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:11:14 INFO clean.CleanPlanner: No earliest commit to retain. No need to scan partitions !!
24/01/05 02:11:14 INFO clean.CleanPlanActionExecutor: Nothing to clean here. It is already clean
24/01/05 02:11:14 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20240105021114060]}
24/01/05 02:11:14 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:11:14 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:11:14 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:11:14 INFO table.HoodieTableMetaClient: Loading Active commit timeline for /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:11:14 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20240105021114060]}
24/01/05 02:11:14 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:11:14 INFO view.FileSystemViewManager: Creating View Manager with storage type :MEMORY
24/01/05 02:11:14 INFO view.FileSystemViewManager: Creating in-memory based Table View
24/01/05 02:11:14 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[==>20240105021110208__deltacommit__REQUESTED__20240105021111218]}
24/01/05 02:11:14 INFO client.BaseHoodieWriteClient: Scheduling table service COMPACT
24/01/05 02:11:14 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:11:14 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:11:14 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:11:14 INFO table.HoodieTableMetaClient: Loading Active commit timeline for /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:11:14 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20240105021114060]}
24/01/05 02:11:14 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:11:14 INFO view.FileSystemViewManager: Creating View Manager with storage type :MEMORY
24/01/05 02:11:14 INFO view.FileSystemViewManager: Creating in-memory based Table View
24/01/05 02:11:14 INFO client.BaseHoodieWriteClient: Scheduling compaction at instant time :00000000000000010001
24/01/05 02:11:14 INFO compact.ScheduleCompactionActionExecutor: Checking if compaction needs to be run on /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:11:14 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:11:14 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:11:14 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:11:14 INFO table.HoodieTableMetaClient: Loading Active commit timeline for /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:11:14 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20240105021114060]}
24/01/05 02:11:14 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:11:14 INFO view.FileSystemViewManager: Creating View Manager with storage type :MEMORY
24/01/05 02:11:14 INFO view.FileSystemViewManager: Creating in-memory based Table View
24/01/05 02:11:14 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20240105021114060]}
24/01/05 02:11:14 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:11:14 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:11:14 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:11:14 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[==>20240105021110208__deltacommit__REQUESTED__20240105021111218]}
24/01/05 02:11:14 INFO client.HoodieTimelineArchiver: No Instants to archive
24/01/05 02:11:14 INFO metadata.HoodieBackedTableMetadataWriter: All the table services operations on MDT completed successfully
24/01/05 02:11:14 INFO transaction.TransactionManager: Transaction ending with transaction owner Option{val=[==>20240105021110208__deltacommit__INFLIGHT]}
24/01/05 02:11:14 INFO lock.InProcessLockProvider: Base Path /user/hive/warehouse/stock_ticks_mor, Lock Instance java.util.concurrent.locks.ReentrantReadWriteLock@592a1882[Write locks = 1, Read locks = 0], Thread main, In-process lock state RELEASING
24/01/05 02:11:14 INFO lock.InProcessLockProvider: Base Path /user/hive/warehouse/stock_ticks_mor, Lock Instance java.util.concurrent.locks.ReentrantReadWriteLock@592a1882[Write locks = 0, Read locks = 0], Thread main, In-process lock state RELEASED
24/01/05 02:11:14 INFO lock.InProcessLockProvider: Base Path /user/hive/warehouse/stock_ticks_mor, Lock Instance java.util.concurrent.locks.ReentrantReadWriteLock@592a1882[Write locks = 0, Read locks = 0], Thread main, In-process lock state ALREADY_RELEASED
24/01/05 02:11:14 INFO lock.LockManager: Released connection created for acquiring lock
24/01/05 02:11:14 INFO transaction.TransactionManager: Transaction ended with transaction owner Option{val=[==>20240105021110208__deltacommit__INFLIGHT]}
24/01/05 02:11:14 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:11:14 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:11:14 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:11:14 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:11:14 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:11:14 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:11:14 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20240105021114060]}
24/01/05 02:11:14 INFO view.AbstractTableFileSystemView: Took 1 ms to read  0 instants, 0 replaced file groups
24/01/05 02:11:14 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:11:14 INFO view.FileSystemViewManager: Creating View Manager with storage type :REMOTE_FIRST
24/01/05 02:11:14 INFO view.FileSystemViewManager: Creating remote first table view
24/01/05 02:11:14 INFO async.AsyncCleanerService: The HoodieWriteClient is not configured to auto & async clean. Async clean service will not start.
24/01/05 02:11:14 INFO async.AsyncArchiveService: The HoodieWriteClient is not configured to auto & async archive. Async archive service will not start.
24/01/05 02:11:14 INFO spark.SparkContext: Starting job: collect at HoodieJavaRDD.java:177
24/01/05 02:11:14 INFO scheduler.DAGScheduler: Registering RDD 25 (mapToPair at HoodieJavaRDD.java:149)
24/01/05 02:11:14 INFO scheduler.DAGScheduler: Registering RDD 31 (distinct at HoodieJavaRDD.java:157)
24/01/05 02:11:14 INFO scheduler.DAGScheduler: Got job 7 (collect at HoodieJavaRDD.java:177) with 2 output partitions
24/01/05 02:11:14 INFO scheduler.DAGScheduler: Final stage: ResultStage 11 (collect at HoodieJavaRDD.java:177)
24/01/05 02:11:14 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 10)
24/01/05 02:11:14 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 10)
24/01/05 02:11:14 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 9 (MapPartitionsRDD[25] at mapToPair at HoodieJavaRDD.java:149), which has no missing parents
24/01/05 02:11:14 INFO memory.MemoryStore: Block broadcast_8 stored as values in memory (estimated size 29.5 KB, free 365.7 MB)
24/01/05 02:11:14 INFO memory.MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 13.9 KB, free 365.7 MB)
24/01/05 02:11:14 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on adhoc-1:45245 (size: 13.9 KB, free: 366.1 MB)
24/01/05 02:11:14 INFO spark.SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1161
24/01/05 02:11:14 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 9 (MapPartitionsRDD[25] at mapToPair at HoodieJavaRDD.java:149) (first 15 tasks are for partitions Vector(0))
24/01/05 02:11:14 INFO scheduler.TaskSchedulerImpl: Adding task set 9.0 with 1 tasks
24/01/05 02:11:14 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 9.0 (TID 8, localhost, executor driver, partition 0, PROCESS_LOCAL, 7739 bytes)
24/01/05 02:11:14 INFO executor.Executor: Running task 0.0 in stage 9.0 (TID 8)
24/01/05 02:11:14 INFO kafka010.KafkaRDD: Computing topic stock_ticks, partition 0 offsets 0 -> 2611
24/01/05 02:11:14 INFO kafka010.KafkaDataConsumer: Initializing cache 16 64 0.75
24/01/05 02:11:14 INFO consumer.ConsumerConfig: ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [kafkabroker:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-executor-null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

24/01/05 02:11:14 INFO utils.AppInfoParser: Kafka version : 2.0.0
24/01/05 02:11:14 INFO utils.AppInfoParser: Kafka commitId : 3402a8361b734732
24/01/05 02:11:14 INFO factory.HoodieSparkKeyGeneratorFactory: The value of hoodie.datasource.write.keygenerator.type is empty; inferred to be SIMPLE
24/01/05 02:11:14 INFO kafka010.InternalKafkaConsumer: Initial fetch for spark-executor-null stock_ticks-0 0
24/01/05 02:11:14 INFO clients.Metadata: Cluster ID: OtZ13RgXQhuquRcIZizanw
24/01/05 02:11:14 INFO executor.Executor: Finished task 0.0 in stage 9.0 (TID 8). 951 bytes result sent to driver
24/01/05 02:11:14 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 9.0 (TID 8) in 433 ms on localhost (executor driver) (1/1)
24/01/05 02:11:14 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool 
24/01/05 02:11:14 INFO scheduler.DAGScheduler: ShuffleMapStage 9 (mapToPair at HoodieJavaRDD.java:149) finished in 0.444 s
24/01/05 02:11:14 INFO scheduler.DAGScheduler: looking for newly runnable stages
24/01/05 02:11:14 INFO scheduler.DAGScheduler: running: Set()
24/01/05 02:11:14 INFO scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 10, ResultStage 11)
24/01/05 02:11:14 INFO scheduler.DAGScheduler: failed: Set()
24/01/05 02:11:14 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 10 (MapPartitionsRDD[31] at distinct at HoodieJavaRDD.java:157), which has no missing parents
24/01/05 02:11:14 INFO memory.MemoryStore: Block broadcast_9 stored as values in memory (estimated size 24.3 KB, free 365.7 MB)
24/01/05 02:11:14 INFO memory.MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 11.5 KB, free 365.7 MB)
24/01/05 02:11:14 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on adhoc-1:45245 (size: 11.5 KB, free: 366.1 MB)
24/01/05 02:11:14 INFO spark.SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1161
24/01/05 02:11:14 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 10 (MapPartitionsRDD[31] at distinct at HoodieJavaRDD.java:157) (first 15 tasks are for partitions Vector(0, 1))
24/01/05 02:11:14 INFO scheduler.TaskSchedulerImpl: Adding task set 10.0 with 2 tasks
24/01/05 02:11:14 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 10.0 (TID 9, localhost, executor driver, partition 0, ANY, 7651 bytes)
24/01/05 02:11:14 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 10.0 (TID 10, localhost, executor driver, partition 1, ANY, 7651 bytes)
24/01/05 02:11:14 INFO executor.Executor: Running task 0.0 in stage 10.0 (TID 9)
24/01/05 02:11:14 INFO executor.Executor: Running task 1.0 in stage 10.0 (TID 10)
24/01/05 02:11:14 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
24/01/05 02:11:14 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
24/01/05 02:11:14 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
24/01/05 02:11:14 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
24/01/05 02:11:14 INFO memory.MemoryStore: Block rdd_27_0 stored as values in memory (estimated size 8.8 KB, free 365.7 MB)
24/01/05 02:11:14 INFO storage.BlockManagerInfo: Added rdd_27_0 in memory on adhoc-1:45245 (size: 8.8 KB, free: 366.1 MB)
24/01/05 02:11:14 INFO memory.MemoryStore: Block rdd_27_1 stored as values in memory (estimated size 13.7 KB, free 365.6 MB)
24/01/05 02:11:14 INFO storage.BlockManagerInfo: Added rdd_27_1 in memory on adhoc-1:45245 (size: 13.7 KB, free: 366.1 MB)
24/01/05 02:11:14 INFO executor.Executor: Finished task 0.0 in stage 10.0 (TID 9). 1252 bytes result sent to driver
24/01/05 02:11:14 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 10.0 (TID 9) in 68 ms on localhost (executor driver) (1/2)
24/01/05 02:11:14 INFO executor.Executor: Finished task 1.0 in stage 10.0 (TID 10). 1252 bytes result sent to driver
24/01/05 02:11:14 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 10.0 (TID 10) in 70 ms on localhost (executor driver) (2/2)
24/01/05 02:11:14 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool 
24/01/05 02:11:14 INFO scheduler.DAGScheduler: ShuffleMapStage 10 (distinct at HoodieJavaRDD.java:157) finished in 0.080 s
24/01/05 02:11:14 INFO scheduler.DAGScheduler: looking for newly runnable stages
24/01/05 02:11:14 INFO scheduler.DAGScheduler: running: Set()
24/01/05 02:11:14 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 11)
24/01/05 02:11:14 INFO scheduler.DAGScheduler: failed: Set()
24/01/05 02:11:14 INFO scheduler.DAGScheduler: Submitting ResultStage 11 (MapPartitionsRDD[33] at distinct at HoodieJavaRDD.java:157), which has no missing parents
24/01/05 02:11:14 INFO memory.MemoryStore: Block broadcast_10 stored as values in memory (estimated size 4.1 KB, free 365.6 MB)
24/01/05 02:11:14 INFO memory.MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 2.4 KB, free 365.6 MB)
24/01/05 02:11:14 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on adhoc-1:45245 (size: 2.4 KB, free: 366.1 MB)
24/01/05 02:11:14 INFO spark.SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1161
24/01/05 02:11:14 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ResultStage 11 (MapPartitionsRDD[33] at distinct at HoodieJavaRDD.java:157) (first 15 tasks are for partitions Vector(0, 1))
24/01/05 02:11:14 INFO scheduler.TaskSchedulerImpl: Adding task set 11.0 with 2 tasks
24/01/05 02:11:14 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 11.0 (TID 11, localhost, executor driver, partition 0, PROCESS_LOCAL, 7662 bytes)
24/01/05 02:11:14 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 11.0 (TID 12, localhost, executor driver, partition 1, ANY, 7662 bytes)
24/01/05 02:11:14 INFO executor.Executor: Running task 0.0 in stage 11.0 (TID 11)
24/01/05 02:11:14 INFO executor.Executor: Running task 1.0 in stage 11.0 (TID 12)
24/01/05 02:11:14 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
24/01/05 02:11:14 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
24/01/05 02:11:14 INFO storage.ShuffleBlockFetcherIterator: Getting 2 non-empty blocks including 2 local blocks and 0 remote blocks
24/01/05 02:11:14 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
24/01/05 02:11:14 INFO executor.Executor: Finished task 0.0 in stage 11.0 (TID 11). 1098 bytes result sent to driver
24/01/05 02:11:14 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 11.0 (TID 11) in 10 ms on localhost (executor driver) (1/2)
24/01/05 02:11:14 INFO executor.Executor: Finished task 1.0 in stage 11.0 (TID 12). 1110 bytes result sent to driver
24/01/05 02:11:14 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 11.0 (TID 12) in 13 ms on localhost (executor driver) (2/2)
24/01/05 02:11:14 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool 
24/01/05 02:11:14 INFO scheduler.DAGScheduler: ResultStage 11 (collect at HoodieJavaRDD.java:177) finished in 0.021 s
24/01/05 02:11:14 INFO scheduler.DAGScheduler: Job 7 finished: collect at HoodieJavaRDD.java:177, took 0.563252 s
24/01/05 02:11:15 INFO spark.SparkContext: Starting job: collect at HoodieSparkEngineContext.java:150
24/01/05 02:11:15 INFO scheduler.DAGScheduler: Got job 8 (collect at HoodieSparkEngineContext.java:150) with 1 output partitions
24/01/05 02:11:15 INFO scheduler.DAGScheduler: Final stage: ResultStage 12 (collect at HoodieSparkEngineContext.java:150)
24/01/05 02:11:15 INFO scheduler.DAGScheduler: Parents of final stage: List()
24/01/05 02:11:15 INFO scheduler.DAGScheduler: Missing parents: List()
24/01/05 02:11:15 INFO scheduler.DAGScheduler: Submitting ResultStage 12 (MapPartitionsRDD[35] at flatMap at HoodieSparkEngineContext.java:150), which has no missing parents
24/01/05 02:11:15 INFO memory.MemoryStore: Block broadcast_11 stored as values in memory (estimated size 338.9 KB, free 365.3 MB)
24/01/05 02:11:15 INFO memory.MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 121.2 KB, free 365.2 MB)
24/01/05 02:11:15 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on adhoc-1:45245 (size: 121.2 KB, free: 366.0 MB)
24/01/05 02:11:15 INFO spark.SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1161
24/01/05 02:11:15 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 12 (MapPartitionsRDD[35] at flatMap at HoodieSparkEngineContext.java:150) (first 15 tasks are for partitions Vector(0))
24/01/05 02:11:15 INFO scheduler.TaskSchedulerImpl: Adding task set 12.0 with 1 tasks
24/01/05 02:11:15 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 12.0 (TID 13, localhost, executor driver, partition 0, PROCESS_LOCAL, 7735 bytes)
24/01/05 02:11:15 INFO executor.Executor: Running task 0.0 in stage 12.0 (TID 13)
24/01/05 02:11:15 INFO executor.Executor: Finished task 0.0 in stage 12.0 (TID 13). 668 bytes result sent to driver
24/01/05 02:11:15 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 12.0 (TID 13) in 32 ms on localhost (executor driver) (1/1)
24/01/05 02:11:15 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool 
24/01/05 02:11:15 INFO scheduler.DAGScheduler: ResultStage 12 (collect at HoodieSparkEngineContext.java:150) finished in 0.062 s
24/01/05 02:11:15 INFO scheduler.DAGScheduler: Job 8 finished: collect at HoodieSparkEngineContext.java:150, took 0.064242 s
24/01/05 02:11:15 INFO rdd.MapPartitionsRDD: Removing RDD 27 from persistence list
24/01/05 02:11:15 INFO storage.BlockManager: Removing RDD 27
24/01/05 02:11:15 INFO view.AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
24/01/05 02:11:15 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:11:15 INFO spark.SparkContext: Starting job: countByKey at HoodieJavaPairRDD.java:105
24/01/05 02:11:15 INFO scheduler.DAGScheduler: Registering RDD 28 (mapToPair at HoodieJavaRDD.java:149)
24/01/05 02:11:15 INFO scheduler.DAGScheduler: Registering RDD 38 (mapToPair at HoodieJavaRDD.java:149)
24/01/05 02:11:15 INFO scheduler.DAGScheduler: Registering RDD 46 (countByKey at HoodieJavaPairRDD.java:105)
24/01/05 02:11:15 INFO scheduler.DAGScheduler: Got job 9 (countByKey at HoodieJavaPairRDD.java:105) with 2 output partitions
24/01/05 02:11:15 INFO scheduler.DAGScheduler: Final stage: ResultStage 17 (countByKey at HoodieJavaPairRDD.java:105)
24/01/05 02:11:15 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 16)
24/01/05 02:11:15 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 16)
24/01/05 02:11:15 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 14 (MapPartitionsRDD[28] at mapToPair at HoodieJavaRDD.java:149), which has no missing parents
24/01/05 02:11:15 INFO memory.MemoryStore: Block broadcast_12 stored as values in memory (estimated size 23.7 KB, free 365.2 MB)
24/01/05 02:11:15 INFO memory.MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 11.4 KB, free 365.2 MB)
24/01/05 02:11:15 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on adhoc-1:45245 (size: 11.4 KB, free: 366.0 MB)
24/01/05 02:11:15 INFO spark.SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1161
24/01/05 02:11:15 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 14 (MapPartitionsRDD[28] at mapToPair at HoodieJavaRDD.java:149) (first 15 tasks are for partitions Vector(0, 1))
24/01/05 02:11:15 INFO scheduler.TaskSchedulerImpl: Adding task set 14.0 with 2 tasks
24/01/05 02:11:15 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 15 (MapPartitionsRDD[38] at mapToPair at HoodieJavaRDD.java:149), which has no missing parents
24/01/05 02:11:15 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 14.0 (TID 14, localhost, executor driver, partition 0, ANY, 7651 bytes)
24/01/05 02:11:15 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 14.0 (TID 15, localhost, executor driver, partition 1, ANY, 7651 bytes)
24/01/05 02:11:15 INFO executor.Executor: Running task 0.0 in stage 14.0 (TID 14)
24/01/05 02:11:15 INFO executor.Executor: Running task 1.0 in stage 14.0 (TID 15)
24/01/05 02:11:15 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
24/01/05 02:11:15 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
24/01/05 02:11:15 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
24/01/05 02:11:15 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
24/01/05 02:11:15 INFO executor.Executor: Finished task 0.0 in stage 14.0 (TID 14). 1252 bytes result sent to driver
24/01/05 02:11:15 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 14.0 (TID 14) in 92 ms on localhost (executor driver) (1/2)
24/01/05 02:11:15 INFO executor.Executor: Finished task 1.0 in stage 14.0 (TID 15). 1295 bytes result sent to driver
24/01/05 02:11:15 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 14.0 (TID 15) in 96 ms on localhost (executor driver) (2/2)
24/01/05 02:11:15 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 14.0, whose tasks have all completed, from pool 
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 152
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 264
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 180
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 158
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 182
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 194
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 253
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 266
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 262
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 213
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 156
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 173
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 202
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 225
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 259
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 215
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 246
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 287
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 160
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 292
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 296
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 153
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 190
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 250
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 127
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 222
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 165
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 291
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 162
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 142
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 207
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 209
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 269
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 147
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 125
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 255
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 247
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 241
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 137
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 148
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 243
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 201
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 226
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 232
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 134
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 171
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 136
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 181
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 203
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 188
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 139
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 277
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 210
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 220
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 150
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 192
24/01/05 02:11:15 INFO storage.BlockManagerInfo: Removed broadcast_9_piece0 on adhoc-1:45245 in memory (size: 11.5 KB, free: 366.0 MB)
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 281
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 227
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 169
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 159
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 289
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 239
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 144
24/01/05 02:11:15 INFO memory.MemoryStore: Block broadcast_13 stored as values in memory (estimated size 340.7 KB, free 364.9 MB)
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned shuffle 1
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 242
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 135
24/01/05 02:11:15 INFO storage.BlockManagerInfo: Removed broadcast_6_piece0 on adhoc-1:45245 in memory (size: 26.7 KB, free: 366.0 MB)
24/01/05 02:11:15 INFO memory.MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 122.1 KB, free 364.8 MB)
24/01/05 02:11:15 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on adhoc-1:45245 (size: 122.1 KB, free: 365.9 MB)
24/01/05 02:11:15 INFO spark.SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1161
24/01/05 02:11:15 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 15 (MapPartitionsRDD[38] at mapToPair at HoodieJavaRDD.java:149) (first 15 tasks are for partitions Vector(0))
24/01/05 02:11:15 INFO scheduler.TaskSchedulerImpl: Adding task set 15.0 with 1 tasks
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 297
24/01/05 02:11:15 INFO storage.BlockManager: Removing RDD 19
24/01/05 02:11:15 INFO scheduler.DAGScheduler: ShuffleMapStage 14 (mapToPair at HoodieJavaRDD.java:149) finished in 0.128 s
24/01/05 02:11:15 INFO scheduler.DAGScheduler: looking for newly runnable stages
24/01/05 02:11:15 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 15)
24/01/05 02:11:15 INFO scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 16, ResultStage 17)
24/01/05 02:11:15 INFO scheduler.DAGScheduler: failed: Set()
24/01/05 02:11:15 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 15.0 (TID 16, localhost, executor driver, partition 0, PROCESS_LOCAL, 7712 bytes)
24/01/05 02:11:15 INFO executor.Executor: Running task 0.0 in stage 15.0 (TID 16)
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned RDD 19
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 221
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 256
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 286
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 172
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 131
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 216
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 251
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 138
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 167
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 219
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 130
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 198
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 252
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 154
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 195
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 294
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 200
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 141
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 193
24/01/05 02:11:15 INFO storage.BlockManagerInfo: Removed broadcast_11_piece0 on adhoc-1:45245 in memory (size: 121.2 KB, free: 366.0 MB)
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 183
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 283
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 245
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 267
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 197
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 211
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 290
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 271
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 185
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 279
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 189
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 272
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 175
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 280
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 151
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 163
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 260
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 236
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 278
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 146
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 295
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 205
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 263
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 177
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 164
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 299
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 128
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 206
24/01/05 02:11:15 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on adhoc-1:45245 in memory (size: 96.5 KB, free: 366.1 MB)
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 133
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 155
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 170
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 218
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 184
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 143
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 285
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 230
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 235
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 186
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 284
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 274
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 275
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 276
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 179
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 282
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 157
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 129
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 145
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 223
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 176
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 268
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 273
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 149
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 231
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 228
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 237
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 196
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 166
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 174
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 199
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 233
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 187
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 132
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 257
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 293
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 161
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 270
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 244
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 140
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 168
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 217
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 229
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 240
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 191
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 261
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 288
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 234
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 265
24/01/05 02:11:15 INFO storage.BlockManagerInfo: Removed broadcast_10_piece0 on adhoc-1:45245 in memory (size: 2.4 KB, free: 366.1 MB)
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 178
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 208
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 248
24/01/05 02:11:15 INFO storage.BlockManagerInfo: Removed broadcast_7_piece0 on adhoc-1:45245 in memory (size: 26.8 KB, free: 366.2 MB)
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 214
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 238
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 258
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 212
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 204
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned shuffle 0
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 254
24/01/05 02:11:15 INFO storage.BlockManagerInfo: Removed broadcast_8_piece0 on adhoc-1:45245 in memory (size: 13.9 KB, free: 366.2 MB)
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 298
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 249
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 224
24/01/05 02:11:15 INFO spark.ContextCleaner: Cleaned accumulator 126
24/01/05 02:11:15 INFO executor.Executor: Finished task 0.0 in stage 15.0 (TID 16). 693 bytes result sent to driver
24/01/05 02:11:15 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 15.0 (TID 16) in 33 ms on localhost (executor driver) (1/1)
24/01/05 02:11:15 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 15.0, whose tasks have all completed, from pool 
24/01/05 02:11:15 INFO scheduler.DAGScheduler: ShuffleMapStage 15 (mapToPair at HoodieJavaRDD.java:149) finished in 0.148 s
24/01/05 02:11:15 INFO scheduler.DAGScheduler: looking for newly runnable stages
24/01/05 02:11:15 INFO scheduler.DAGScheduler: running: Set()
24/01/05 02:11:15 INFO scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 16, ResultStage 17)
24/01/05 02:11:15 INFO scheduler.DAGScheduler: failed: Set()
24/01/05 02:11:15 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 16 (MapPartitionsRDD[46] at countByKey at HoodieJavaPairRDD.java:105), which has no missing parents
24/01/05 02:11:15 INFO memory.MemoryStore: Block broadcast_14 stored as values in memory (estimated size 7.7 KB, free 365.8 MB)
24/01/05 02:11:15 INFO memory.MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 4.0 KB, free 365.8 MB)
24/01/05 02:11:15 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on adhoc-1:45245 (size: 4.0 KB, free: 366.2 MB)
24/01/05 02:11:15 INFO spark.SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1161
24/01/05 02:11:15 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 16 (MapPartitionsRDD[46] at countByKey at HoodieJavaPairRDD.java:105) (first 15 tasks are for partitions Vector(0, 1))
24/01/05 02:11:15 INFO scheduler.TaskSchedulerImpl: Adding task set 16.0 with 2 tasks
24/01/05 02:11:15 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 16.0 (TID 17, localhost, executor driver, partition 0, PROCESS_LOCAL, 7714 bytes)
24/01/05 02:11:15 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 16.0 (TID 18, localhost, executor driver, partition 1, PROCESS_LOCAL, 7714 bytes)
24/01/05 02:11:15 INFO executor.Executor: Running task 0.0 in stage 16.0 (TID 17)
24/01/05 02:11:15 INFO executor.Executor: Running task 1.0 in stage 16.0 (TID 18)
24/01/05 02:11:15 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
24/01/05 02:11:15 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
24/01/05 02:11:15 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
24/01/05 02:11:15 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
24/01/05 02:11:15 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
24/01/05 02:11:15 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
24/01/05 02:11:15 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
24/01/05 02:11:15 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
24/01/05 02:11:15 INFO memory.MemoryStore: Block rdd_44_0 stored as values in memory (estimated size 8.8 KB, free 365.8 MB)
24/01/05 02:11:15 INFO storage.BlockManagerInfo: Added rdd_44_0 in memory on adhoc-1:45245 (size: 8.8 KB, free: 366.2 MB)
24/01/05 02:11:15 INFO memory.MemoryStore: Block rdd_44_1 stored as values in memory (estimated size 13.7 KB, free 365.8 MB)
24/01/05 02:11:15 INFO storage.BlockManagerInfo: Added rdd_44_1 in memory on adhoc-1:45245 (size: 13.7 KB, free: 366.1 MB)
24/01/05 02:11:15 INFO executor.Executor: Finished task 1.0 in stage 16.0 (TID 18). 1252 bytes result sent to driver
24/01/05 02:11:15 INFO executor.Executor: Finished task 0.0 in stage 16.0 (TID 17). 1252 bytes result sent to driver
24/01/05 02:11:15 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 16.0 (TID 18) in 50 ms on localhost (executor driver) (1/2)
24/01/05 02:11:15 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 16.0 (TID 17) in 52 ms on localhost (executor driver) (2/2)
24/01/05 02:11:15 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 16.0, whose tasks have all completed, from pool 
24/01/05 02:11:15 INFO scheduler.DAGScheduler: ShuffleMapStage 16 (countByKey at HoodieJavaPairRDD.java:105) finished in 0.062 s
24/01/05 02:11:15 INFO scheduler.DAGScheduler: looking for newly runnable stages
24/01/05 02:11:15 INFO scheduler.DAGScheduler: running: Set()
24/01/05 02:11:15 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 17)
24/01/05 02:11:15 INFO scheduler.DAGScheduler: failed: Set()
24/01/05 02:11:15 INFO scheduler.DAGScheduler: Submitting ResultStage 17 (ShuffledRDD[47] at countByKey at HoodieJavaPairRDD.java:105), which has no missing parents
24/01/05 02:11:15 INFO memory.MemoryStore: Block broadcast_15 stored as values in memory (estimated size 3.6 KB, free 365.8 MB)
24/01/05 02:11:15 INFO memory.MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 2.1 KB, free 365.8 MB)
24/01/05 02:11:15 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on adhoc-1:45245 (size: 2.1 KB, free: 366.1 MB)
24/01/05 02:11:15 INFO spark.SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1161
24/01/05 02:11:15 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ResultStage 17 (ShuffledRDD[47] at countByKey at HoodieJavaPairRDD.java:105) (first 15 tasks are for partitions Vector(0, 1))
24/01/05 02:11:15 INFO scheduler.TaskSchedulerImpl: Adding task set 17.0 with 2 tasks
24/01/05 02:11:15 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 17.0 (TID 19, localhost, executor driver, partition 0, PROCESS_LOCAL, 7662 bytes)
24/01/05 02:11:15 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 17.0 (TID 20, localhost, executor driver, partition 1, ANY, 7662 bytes)
24/01/05 02:11:15 INFO executor.Executor: Running task 1.0 in stage 17.0 (TID 20)
24/01/05 02:11:15 INFO executor.Executor: Running task 0.0 in stage 17.0 (TID 19)
24/01/05 02:11:15 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
24/01/05 02:11:15 INFO storage.ShuffleBlockFetcherIterator: Getting 2 non-empty blocks including 2 local blocks and 0 remote blocks
24/01/05 02:11:15 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
24/01/05 02:11:15 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
24/01/05 02:11:15 INFO executor.Executor: Finished task 0.0 in stage 17.0 (TID 19). 1055 bytes result sent to driver
24/01/05 02:11:15 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 17.0 (TID 19) in 11 ms on localhost (executor driver) (1/2)
24/01/05 02:11:15 INFO executor.Executor: Finished task 1.0 in stage 17.0 (TID 20). 1112 bytes result sent to driver
24/01/05 02:11:15 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 17.0 (TID 20) in 13 ms on localhost (executor driver) (2/2)
24/01/05 02:11:15 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 17.0, whose tasks have all completed, from pool 
24/01/05 02:11:15 INFO scheduler.DAGScheduler: ResultStage 17 (countByKey at HoodieJavaPairRDD.java:105) finished in 0.020 s
24/01/05 02:11:15 INFO scheduler.DAGScheduler: Job 9 finished: countByKey at HoodieJavaPairRDD.java:105, took 0.251279 s
24/01/05 02:11:15 INFO commit.UpsertPartitioner: AvgRecordSize => 1024
24/01/05 02:11:15 INFO spark.SparkContext: Starting job: collectAsMap at UpsertPartitioner.java:282
24/01/05 02:11:15 INFO scheduler.DAGScheduler: Got job 10 (collectAsMap at UpsertPartitioner.java:282) with 1 output partitions
24/01/05 02:11:15 INFO scheduler.DAGScheduler: Final stage: ResultStage 18 (collectAsMap at UpsertPartitioner.java:282)
24/01/05 02:11:15 INFO scheduler.DAGScheduler: Parents of final stage: List()
24/01/05 02:11:15 INFO scheduler.DAGScheduler: Missing parents: List()
24/01/05 02:11:15 INFO scheduler.DAGScheduler: Submitting ResultStage 18 (MapPartitionsRDD[49] at mapToPair at UpsertPartitioner.java:281), which has no missing parents
24/01/05 02:11:15 INFO memory.MemoryStore: Block broadcast_16 stored as values in memory (estimated size 339.7 KB, free 365.4 MB)
24/01/05 02:11:15 INFO memory.MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 121.4 KB, free 365.3 MB)
24/01/05 02:11:15 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on adhoc-1:45245 (size: 121.4 KB, free: 366.0 MB)
24/01/05 02:11:15 INFO spark.SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1161
24/01/05 02:11:15 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 18 (MapPartitionsRDD[49] at mapToPair at UpsertPartitioner.java:281) (first 15 tasks are for partitions Vector(0))
24/01/05 02:11:15 INFO scheduler.TaskSchedulerImpl: Adding task set 18.0 with 1 tasks
24/01/05 02:11:15 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 18.0 (TID 21, localhost, executor driver, partition 0, PROCESS_LOCAL, 7735 bytes)
24/01/05 02:11:15 INFO executor.Executor: Running task 0.0 in stage 18.0 (TID 21)
24/01/05 02:11:15 INFO executor.Executor: Finished task 0.0 in stage 18.0 (TID 21). 759 bytes result sent to driver
24/01/05 02:11:15 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 18.0 (TID 21) in 47 ms on localhost (executor driver) (1/1)
24/01/05 02:11:15 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 18.0, whose tasks have all completed, from pool 
24/01/05 02:11:15 INFO scheduler.DAGScheduler: ResultStage 18 (collectAsMap at UpsertPartitioner.java:282) finished in 0.118 s
24/01/05 02:11:15 INFO scheduler.DAGScheduler: Job 10 finished: collectAsMap at UpsertPartitioner.java:282, took 0.119899 s
24/01/05 02:11:15 INFO view.AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
24/01/05 02:11:15 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:11:15 INFO commit.UpsertPartitioner: Total Buckets: 1
24/01/05 02:11:15 INFO timeline.HoodieActiveTimeline: Checking for file exists ?/user/hive/warehouse/stock_ticks_mor/.hoodie/20240105021110208.deltacommit.requested
24/01/05 02:11:15 INFO timeline.HoodieActiveTimeline: Create new file for toInstant ?/user/hive/warehouse/stock_ticks_mor/.hoodie/20240105021110208.deltacommit.inflight
24/01/05 02:11:15 INFO commit.BaseSparkCommitActionExecutor: no validators configured.
24/01/05 02:11:15 INFO commit.BaseCommitActionExecutor: Auto commit disabled for 20240105021110208
24/01/05 02:11:15 INFO spark.SparkContext: Starting job: sum at StreamSync.java:783
24/01/05 02:11:15 INFO scheduler.DAGScheduler: Registering RDD 50 (mapToPair at HoodieJavaRDD.java:149)
24/01/05 02:11:15 INFO scheduler.DAGScheduler: Got job 11 (sum at StreamSync.java:783) with 1 output partitions
24/01/05 02:11:15 INFO scheduler.DAGScheduler: Final stage: ResultStage 23 (sum at StreamSync.java:783)
24/01/05 02:11:15 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 22)
24/01/05 02:11:15 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 22)
24/01/05 02:11:15 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 22 (MapPartitionsRDD[50] at mapToPair at HoodieJavaRDD.java:149), which has no missing parents
24/01/05 02:11:15 INFO memory.MemoryStore: Block broadcast_17 stored as values in memory (estimated size 344.3 KB, free 365.0 MB)
24/01/05 02:11:15 INFO memory.MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 123.0 KB, free 364.9 MB)
24/01/05 02:11:15 INFO storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on adhoc-1:45245 (size: 123.0 KB, free: 365.9 MB)
24/01/05 02:11:15 INFO spark.SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1161
24/01/05 02:11:15 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 22 (MapPartitionsRDD[50] at mapToPair at HoodieJavaRDD.java:149) (first 15 tasks are for partitions Vector(0, 1))
24/01/05 02:11:15 INFO scheduler.TaskSchedulerImpl: Adding task set 22.0 with 2 tasks
24/01/05 02:11:15 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 22.0 (TID 22, localhost, executor driver, partition 0, PROCESS_LOCAL, 7714 bytes)
24/01/05 02:11:15 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 22.0 (TID 23, localhost, executor driver, partition 1, PROCESS_LOCAL, 7714 bytes)
24/01/05 02:11:15 INFO executor.Executor: Running task 0.0 in stage 22.0 (TID 22)
24/01/05 02:11:15 INFO executor.Executor: Running task 1.0 in stage 22.0 (TID 23)
24/01/05 02:11:15 INFO storage.BlockManager: Found block rdd_44_1 locally
24/01/05 02:11:15 INFO storage.BlockManager: Found block rdd_44_0 locally
24/01/05 02:11:15 INFO executor.Executor: Finished task 0.0 in stage 22.0 (TID 22). 907 bytes result sent to driver
24/01/05 02:11:15 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 22.0 (TID 22) in 39 ms on localhost (executor driver) (1/2)
24/01/05 02:11:15 INFO executor.Executor: Finished task 1.0 in stage 22.0 (TID 23). 907 bytes result sent to driver
24/01/05 02:11:15 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 22.0 (TID 23) in 41 ms on localhost (executor driver) (2/2)
24/01/05 02:11:15 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 22.0, whose tasks have all completed, from pool 
24/01/05 02:11:15 INFO scheduler.DAGScheduler: ShuffleMapStage 22 (mapToPair at HoodieJavaRDD.java:149) finished in 0.087 s
24/01/05 02:11:15 INFO scheduler.DAGScheduler: looking for newly runnable stages
24/01/05 02:11:15 INFO scheduler.DAGScheduler: running: Set()
24/01/05 02:11:15 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 23)
24/01/05 02:11:15 INFO scheduler.DAGScheduler: failed: Set()
24/01/05 02:11:15 INFO scheduler.DAGScheduler: Submitting ResultStage 23 (MapPartitionsRDD[55] at mapToDouble at StreamSync.java:783), which has no missing parents
24/01/05 02:11:15 INFO memory.MemoryStore: Block broadcast_18 stored as values in memory (estimated size 351.4 KB, free 364.5 MB)
24/01/05 02:11:16 INFO memory.MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 127.2 KB, free 364.4 MB)
24/01/05 02:11:16 INFO storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on adhoc-1:45245 (size: 127.2 KB, free: 365.8 MB)
24/01/05 02:11:16 INFO spark.SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1161
24/01/05 02:11:16 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 23 (MapPartitionsRDD[55] at mapToDouble at StreamSync.java:783) (first 15 tasks are for partitions Vector(0))
24/01/05 02:11:16 INFO scheduler.TaskSchedulerImpl: Adding task set 23.0 with 1 tasks
24/01/05 02:11:16 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 23.0 (TID 24, localhost, executor driver, partition 0, ANY, 7662 bytes)
24/01/05 02:11:16 INFO executor.Executor: Running task 0.0 in stage 23.0 (TID 24)
24/01/05 02:11:16 INFO storage.ShuffleBlockFetcherIterator: Getting 2 non-empty blocks including 2 local blocks and 0 remote blocks
24/01/05 02:11:16 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
24/01/05 02:11:16 INFO queue.SimpleExecutor: Starting consumer, consuming records from the records iterator directly
24/01/05 02:11:16 WARN marker.WriteMarkersFactory: Timeline-server-based markers are not supported for HDFS: base path /user/hive/warehouse/stock_ticks_mor.  Falling back to direct markers.
24/01/05 02:11:16 INFO marker.DirectWriteMarkers: Creating Marker Path=/user/hive/warehouse/stock_ticks_mor/.hoodie/.temp/20240105021110208/2018/08/31/63b57973-43fe-40da-bb65-66fab049ab22-0_0-23-24_20240105021110208.parquet.marker.CREATE
24/01/05 02:11:16 INFO marker.DirectWriteMarkers: [direct] Created marker file /user/hive/warehouse/stock_ticks_mor/.hoodie/.temp/20240105021110208/2018/08/31/63b57973-43fe-40da-bb65-66fab049ab22-0_0-23-24_20240105021110208.parquet.marker.CREATE in 7 ms
24/01/05 02:11:16 INFO compress.CodecPool: Got brand-new compressor [.gz]
24/01/05 02:11:16 INFO io.HoodieCreateHandle: New CreateHandle for partition :2018/08/31 with fileId 63b57973-43fe-40da-bb65-66fab049ab22-0
24/01/05 02:11:16 INFO io.HoodieCreateHandle: Closing the file 63b57973-43fe-40da-bb65-66fab049ab22-0 as we are done with all the records 146
24/01/05 02:11:16 INFO hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 27155
24/01/05 02:11:16 INFO io.HoodieCreateHandle: CreateHandle for partitionPath 2018/08/31 fileID 63b57973-43fe-40da-bb65-66fab049ab22-0, took 566 ms.
24/01/05 02:11:16 INFO memory.MemoryStore: Block rdd_54_0 stored as values in memory (estimated size 376.0 B, free 364.4 MB)
24/01/05 02:11:16 INFO storage.BlockManagerInfo: Added rdd_54_0 in memory on adhoc-1:45245 (size: 376.0 B, free: 365.8 MB)
24/01/05 02:11:16 INFO executor.Executor: Finished task 0.0 in stage 23.0 (TID 24). 974 bytes result sent to driver
24/01/05 02:11:16 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 23.0 (TID 24) in 600 ms on localhost (executor driver) (1/1)
24/01/05 02:11:16 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 23.0, whose tasks have all completed, from pool 
24/01/05 02:11:16 INFO scheduler.DAGScheduler: ResultStage 23 (sum at StreamSync.java:783) finished in 0.635 s
24/01/05 02:11:16 INFO scheduler.DAGScheduler: Job 11 finished: sum at StreamSync.java:783, took 0.729033 s
24/01/05 02:11:16 INFO spark.SparkContext: Starting job: sum at StreamSync.java:784
24/01/05 02:11:16 INFO scheduler.DAGScheduler: Got job 12 (sum at StreamSync.java:784) with 1 output partitions
24/01/05 02:11:16 INFO scheduler.DAGScheduler: Final stage: ResultStage 28 (sum at StreamSync.java:784)
24/01/05 02:11:16 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 27)
24/01/05 02:11:16 INFO scheduler.DAGScheduler: Missing parents: List()
24/01/05 02:11:16 INFO scheduler.DAGScheduler: Submitting ResultStage 28 (MapPartitionsRDD[57] at mapToDouble at StreamSync.java:784), which has no missing parents
24/01/05 02:11:16 INFO memory.MemoryStore: Block broadcast_19 stored as values in memory (estimated size 351.3 KB, free 364.1 MB)
24/01/05 02:11:16 INFO memory.MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 127.2 KB, free 363.9 MB)
24/01/05 02:11:16 INFO storage.BlockManagerInfo: Added broadcast_19_piece0 in memory on adhoc-1:45245 (size: 127.2 KB, free: 365.7 MB)
24/01/05 02:11:16 INFO spark.SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1161
24/01/05 02:11:16 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 28 (MapPartitionsRDD[57] at mapToDouble at StreamSync.java:784) (first 15 tasks are for partitions Vector(0))
24/01/05 02:11:16 INFO scheduler.TaskSchedulerImpl: Adding task set 28.0 with 1 tasks
24/01/05 02:11:16 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 28.0 (TID 25, localhost, executor driver, partition 0, PROCESS_LOCAL, 7662 bytes)
24/01/05 02:11:16 INFO executor.Executor: Running task 0.0 in stage 28.0 (TID 25)
24/01/05 02:11:16 INFO storage.BlockManager: Found block rdd_54_0 locally
24/01/05 02:11:16 INFO executor.Executor: Finished task 0.0 in stage 28.0 (TID 25). 759 bytes result sent to driver
24/01/05 02:11:16 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 28.0 (TID 25) in 54 ms on localhost (executor driver) (1/1)
24/01/05 02:11:16 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 28.0, whose tasks have all completed, from pool 
24/01/05 02:11:16 INFO scheduler.DAGScheduler: ResultStage 28 (sum at StreamSync.java:784) finished in 0.108 s
24/01/05 02:11:16 INFO scheduler.DAGScheduler: Job 12 finished: sum at StreamSync.java:784, took 0.112060 s
24/01/05 02:11:16 INFO streamer.StreamSync: instantTime=20240105021110208, totalRecords=146, totalErrorRecords=0, totalSuccessfulRecords=146
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 428
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 360
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 420
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 496
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 417
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 425
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 438
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 485
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 412
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 489
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 447
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 426
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 399
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 415
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 423
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 384
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 478
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 414
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 389
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 385
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 368
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 486
24/01/05 02:11:16 INFO storage.BlockManagerInfo: Removed broadcast_16_piece0 on adhoc-1:45245 in memory (size: 121.4 KB, free: 365.8 MB)
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 391
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 436
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 442
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 487
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 365
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 387
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 483
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 462
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 366
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 433
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 392
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 450
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 369
24/01/05 02:11:16 INFO storage.BlockManagerInfo: Removed broadcast_19_piece0 on adhoc-1:45245 in memory (size: 127.2 KB, free: 365.9 MB)
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 493
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 370
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 435
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 454
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 497
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 380
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 403
24/01/05 02:11:16 INFO storage.BlockManagerInfo: Removed broadcast_18_piece0 on adhoc-1:45245 in memory (size: 127.2 KB, free: 366.0 MB)
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 467
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 479
24/01/05 02:11:16 INFO storage.BlockManagerInfo: Removed broadcast_13_piece0 on adhoc-1:45245 in memory (size: 122.1 KB, free: 366.1 MB)
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 419
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 355
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 471
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 382
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 427
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 498
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 499
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 400
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 461
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 473
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 379
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 472
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 469
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 448
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 458
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 376
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 441
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 378
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 466
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 401
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 353
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 393
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 482
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 356
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 406
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 383
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 408
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 446
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 418
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 364
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 459
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 407
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 358
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 451
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 495
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 367
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 477
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 372
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 405
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 492
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 465
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 375
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 402
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 350
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 363
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 351
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 421
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 422
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 398
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 453
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 475
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 491
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 395
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 396
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 434
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 440
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 373
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 432
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 481
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 386
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 463
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 416
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 449
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 460
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 476
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 397
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 439
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 452
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 404
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 494
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 430
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 455
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 381
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 424
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 456
24/01/05 02:11:16 INFO storage.BlockManagerInfo: Removed broadcast_14_piece0 on adhoc-1:45245 in memory (size: 4.0 KB, free: 366.1 MB)
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 357
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 468
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 431
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 429
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 361
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 457
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 409
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 445
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 413
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 437
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 464
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 470
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 394
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 490
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 359
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 354
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 377
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 362
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 480
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 474
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 488
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 410
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 390
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 371
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 484
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 352
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 411
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 444
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 443
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 374
24/01/05 02:11:16 INFO storage.BlockManagerInfo: Removed broadcast_15_piece0 on adhoc-1:45245 in memory (size: 2.1 KB, free: 366.1 MB)
24/01/05 02:11:16 INFO spark.ContextCleaner: Cleaned accumulator 388
24/01/05 02:11:16 INFO storage.BlockManagerInfo: Removed broadcast_17_piece0 on adhoc-1:45245 in memory (size: 123.0 KB, free: 366.3 MB)
24/01/05 02:11:16 INFO spark.SparkContext: Starting job: collect at SparkRDDWriteClient.java:103
24/01/05 02:11:16 INFO scheduler.DAGScheduler: Got job 13 (collect at SparkRDDWriteClient.java:103) with 1 output partitions
24/01/05 02:11:16 INFO scheduler.DAGScheduler: Final stage: ResultStage 33 (collect at SparkRDDWriteClient.java:103)
24/01/05 02:11:16 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 32)
24/01/05 02:11:16 INFO scheduler.DAGScheduler: Missing parents: List()
24/01/05 02:11:16 INFO scheduler.DAGScheduler: Submitting ResultStage 33 (MapPartitionsRDD[59] at map at SparkRDDWriteClient.java:103), which has no missing parents
24/01/05 02:11:16 INFO memory.MemoryStore: Block broadcast_20 stored as values in memory (estimated size 351.6 KB, free 365.9 MB)
24/01/05 02:11:16 INFO memory.MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 127.3 KB, free 365.8 MB)
24/01/05 02:11:16 INFO storage.BlockManagerInfo: Added broadcast_20_piece0 in memory on adhoc-1:45245 (size: 127.3 KB, free: 366.1 MB)
24/01/05 02:11:16 INFO spark.SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:1161
24/01/05 02:11:16 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 33 (MapPartitionsRDD[59] at map at SparkRDDWriteClient.java:103) (first 15 tasks are for partitions Vector(0))
24/01/05 02:11:16 INFO scheduler.TaskSchedulerImpl: Adding task set 33.0 with 1 tasks
24/01/05 02:11:16 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 33.0 (TID 26, localhost, executor driver, partition 0, PROCESS_LOCAL, 7662 bytes)
24/01/05 02:11:16 INFO executor.Executor: Running task 0.0 in stage 33.0 (TID 26)
24/01/05 02:11:16 INFO storage.BlockManager: Found block rdd_54_0 locally
24/01/05 02:11:16 INFO executor.Executor: Finished task 0.0 in stage 33.0 (TID 26). 1033 bytes result sent to driver
24/01/05 02:11:16 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 33.0 (TID 26) in 23 ms on localhost (executor driver) (1/1)
24/01/05 02:11:16 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 33.0, whose tasks have all completed, from pool 
24/01/05 02:11:16 INFO scheduler.DAGScheduler: ResultStage 33 (collect at SparkRDDWriteClient.java:103) finished in 0.061 s
24/01/05 02:11:16 INFO scheduler.DAGScheduler: Job 13 finished: collect at SparkRDDWriteClient.java:103, took 0.063319 s
24/01/05 02:11:16 INFO client.BaseHoodieWriteClient: Committing 20240105021110208 action deltacommit
24/01/05 02:11:16 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:11:16 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:11:16 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:11:16 INFO table.HoodieTableMetaClient: Loading Active commit timeline for /user/hive/warehouse/stock_ticks_mor
24/01/05 02:11:16 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[==>20240105021110208__deltacommit__INFLIGHT__20240105021115773]}
24/01/05 02:11:16 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:11:16 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:11:16 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:11:16 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:11:16 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:11:16 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:11:16 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20240105021114060]}
24/01/05 02:11:16 INFO view.AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
24/01/05 02:11:16 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:11:16 INFO view.FileSystemViewManager: Creating View Manager with storage type :REMOTE_FIRST
24/01/05 02:11:16 INFO view.FileSystemViewManager: Creating remote first table view
24/01/05 02:11:16 INFO util.CommitUtils: Creating  metadata for UPSERT numWriteStats:1 numReplaceFileIds:0
24/01/05 02:11:16 INFO transaction.TransactionManager: Transaction starting for Option{val=[==>20240105021110208__deltacommit__INFLIGHT]} with latest completed transaction instant Optional.empty
24/01/05 02:11:16 INFO lock.LockManager: LockProvider org.apache.hudi.client.transaction.lock.InProcessLockProvider
24/01/05 02:11:16 INFO lock.InProcessLockProvider: Base Path /user/hive/warehouse/stock_ticks_mor, Lock Instance java.util.concurrent.locks.ReentrantReadWriteLock@592a1882[Write locks = 0, Read locks = 0], Thread main, In-process lock state ACQUIRING
24/01/05 02:11:16 INFO lock.InProcessLockProvider: Base Path /user/hive/warehouse/stock_ticks_mor, Lock Instance java.util.concurrent.locks.ReentrantReadWriteLock@592a1882[Write locks = 1, Read locks = 0], Thread main, In-process lock state ACQUIRED
24/01/05 02:11:16 INFO transaction.TransactionManager: Transaction started for Option{val=[==>20240105021110208__deltacommit__INFLIGHT]} with latest completed transaction instant Optional.empty
24/01/05 02:11:16 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:11:16 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:11:16 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:11:16 INFO table.HoodieTableMetaClient: Loading Active commit timeline for /user/hive/warehouse/stock_ticks_mor
24/01/05 02:11:16 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[==>20240105021110208__deltacommit__INFLIGHT__20240105021115773]}
24/01/05 02:11:16 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:11:16 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:11:16 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:11:16 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:11:16 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:11:16 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:11:16 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20240105021114060]}
24/01/05 02:11:16 INFO view.AbstractTableFileSystemView: Took 1 ms to read  0 instants, 0 replaced file groups
24/01/05 02:11:16 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:11:16 INFO view.FileSystemViewManager: Creating View Manager with storage type :REMOTE_FIRST
24/01/05 02:11:17 INFO view.FileSystemViewManager: Creating remote first table view
24/01/05 02:11:17 INFO client.BaseHoodieWriteClient: Committing 20240105021110208 action deltacommit
24/01/05 02:11:17 WARN marker.WriteMarkersFactory: Timeline-server-based markers are not supported for HDFS: base path /user/hive/warehouse/stock_ticks_mor.  Falling back to direct markers.
24/01/05 02:11:17 INFO spark.SparkContext: Starting job: collect at HoodieSparkEngineContext.java:150
24/01/05 02:11:17 INFO scheduler.DAGScheduler: Got job 14 (collect at HoodieSparkEngineContext.java:150) with 1 output partitions
24/01/05 02:11:17 INFO scheduler.DAGScheduler: Final stage: ResultStage 34 (collect at HoodieSparkEngineContext.java:150)
24/01/05 02:11:17 INFO scheduler.DAGScheduler: Parents of final stage: List()
24/01/05 02:11:17 INFO scheduler.DAGScheduler: Missing parents: List()
24/01/05 02:11:17 INFO scheduler.DAGScheduler: Submitting ResultStage 34 (MapPartitionsRDD[61] at flatMap at HoodieSparkEngineContext.java:150), which has no missing parents
24/01/05 02:11:17 INFO memory.MemoryStore: Block broadcast_21 stored as values in memory (estimated size 72.5 KB, free 365.7 MB)
24/01/05 02:11:17 INFO memory.MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 26.7 KB, free 365.7 MB)
24/01/05 02:11:17 INFO storage.BlockManagerInfo: Added broadcast_21_piece0 in memory on adhoc-1:45245 (size: 26.7 KB, free: 366.1 MB)
24/01/05 02:11:17 INFO spark.SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:1161
24/01/05 02:11:17 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 34 (MapPartitionsRDD[61] at flatMap at HoodieSparkEngineContext.java:150) (first 15 tasks are for partitions Vector(0))
24/01/05 02:11:17 INFO scheduler.TaskSchedulerImpl: Adding task set 34.0 with 1 tasks
24/01/05 02:11:17 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 34.0 (TID 27, localhost, executor driver, partition 0, PROCESS_LOCAL, 7820 bytes)
24/01/05 02:11:17 INFO executor.Executor: Running task 0.0 in stage 34.0 (TID 27)
24/01/05 02:11:17 INFO executor.Executor: Finished task 0.0 in stage 34.0 (TID 27). 755 bytes result sent to driver
24/01/05 02:11:17 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 34.0 (TID 27) in 15 ms on localhost (executor driver) (1/1)
24/01/05 02:11:17 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 34.0, whose tasks have all completed, from pool 
24/01/05 02:11:17 INFO scheduler.DAGScheduler: ResultStage 34 (collect at HoodieSparkEngineContext.java:150) finished in 0.029 s
24/01/05 02:11:17 INFO scheduler.DAGScheduler: Job 14 finished: collect at HoodieSparkEngineContext.java:150, took 0.029989 s
24/01/05 02:11:17 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:11:17 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:11:17 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:11:17 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:11:17 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:11:17 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:11:17 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20240105021114060]}
24/01/05 02:11:17 INFO metadata.HoodieBackedTableMetadataWriter: Async metadata indexing disabled and following partitions already initialized: [files]
24/01/05 02:11:17 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:11:17 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:11:17 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:11:17 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:11:17 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:11:17 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:11:17 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20240105021114060]}
24/01/05 02:11:17 INFO view.AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
24/01/05 02:11:17 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:11:17 INFO metadata.HoodieTableMetadataUtil: Updating at 20240105021110208 from Commit/UPSERT. #partitions_updated=2, #files_added=1
24/01/05 02:11:17 INFO view.AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
24/01/05 02:11:17 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:11:17 INFO metadata.HoodieTableMetadataUtil: Loading latest file slices for metadata table partition files
24/01/05 02:11:17 INFO view.AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
24/01/05 02:11:17 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:11:17 INFO view.AbstractTableFileSystemView: Building file system view for partition (files)
24/01/05 02:11:17 INFO client.BaseHoodieClient: Embedded Timeline Server is disabled. Not starting timeline service
24/01/05 02:11:17 INFO client.BaseHoodieClient: Embedded Timeline Server is disabled. Not starting timeline service
24/01/05 02:11:17 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:11:17 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:11:17 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:11:17 INFO table.HoodieTableMetaClient: Loading Active commit timeline for /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:11:17 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20240105021114060]}
24/01/05 02:11:17 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:11:17 INFO view.FileSystemViewManager: Creating View Manager with storage type :MEMORY
24/01/05 02:11:17 INFO view.FileSystemViewManager: Creating in-memory based Table View
24/01/05 02:11:17 INFO metadata.HoodieBackedTableMetadataWriter: New commit at 20240105021110208 being applied to MDT.
24/01/05 02:11:17 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:11:17 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:11:17 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:11:17 INFO table.HoodieTableMetaClient: Loading Active commit timeline for /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:11:17 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20240105021114060]}
24/01/05 02:11:17 INFO util.CleanerUtils: Cleaned failed attempts if any
24/01/05 02:11:17 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:11:17 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:11:17 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:11:17 INFO table.HoodieTableMetaClient: Loading Active commit timeline for /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:11:17 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20240105021114060]}
24/01/05 02:11:17 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:11:17 INFO view.FileSystemViewManager: Creating View Manager with storage type :MEMORY
24/01/05 02:11:17 INFO view.FileSystemViewManager: Creating in-memory based Table View
24/01/05 02:11:17 INFO client.BaseHoodieWriteClient: Generate a new instant time: 20240105021110208 action: deltacommit
24/01/05 02:11:17 INFO timeline.HoodieActiveTimeline: Creating a new instant [==>20240105021110208__deltacommit__REQUESTED]
24/01/05 02:11:17 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:11:17 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:11:17 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:11:17 INFO table.HoodieTableMetaClient: Loading Active commit timeline for /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:11:17 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[==>20240105021110208__deltacommit__REQUESTED__20240105021117178]}
24/01/05 02:11:17 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:11:17 INFO view.FileSystemViewManager: Creating View Manager with storage type :MEMORY
24/01/05 02:11:17 INFO view.FileSystemViewManager: Creating in-memory based Table View
24/01/05 02:11:17 INFO async.AsyncCleanerService: The HoodieWriteClient is not configured to auto & async clean. Async clean service will not start.
24/01/05 02:11:17 INFO async.AsyncArchiveService: The HoodieWriteClient is not configured to auto & async archive. Async archive service will not start.
24/01/05 02:11:17 INFO view.AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
24/01/05 02:11:17 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:11:17 INFO spark.SparkContext: Starting job: countByKey at HoodieJavaPairRDD.java:105
24/01/05 02:11:17 INFO scheduler.DAGScheduler: Registering RDD 70 (countByKey at HoodieJavaPairRDD.java:105)
24/01/05 02:11:17 INFO scheduler.DAGScheduler: Got job 15 (countByKey at HoodieJavaPairRDD.java:105) with 1 output partitions
24/01/05 02:11:17 INFO scheduler.DAGScheduler: Final stage: ResultStage 36 (countByKey at HoodieJavaPairRDD.java:105)
24/01/05 02:11:17 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 35)
24/01/05 02:11:17 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 35)
24/01/05 02:11:17 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 35 (MapPartitionsRDD[70] at countByKey at HoodieJavaPairRDD.java:105), which has no missing parents
24/01/05 02:11:17 INFO memory.MemoryStore: Block broadcast_22 stored as values in memory (estimated size 8.6 KB, free 365.7 MB)
24/01/05 02:11:17 INFO memory.MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 4.7 KB, free 365.7 MB)
24/01/05 02:11:17 INFO storage.BlockManagerInfo: Added broadcast_22_piece0 in memory on adhoc-1:45245 (size: 4.7 KB, free: 366.1 MB)
24/01/05 02:11:17 INFO spark.SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:1161
24/01/05 02:11:17 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 35 (MapPartitionsRDD[70] at countByKey at HoodieJavaPairRDD.java:105) (first 15 tasks are for partitions Vector(0))
24/01/05 02:11:17 INFO scheduler.TaskSchedulerImpl: Adding task set 35.0 with 1 tasks
24/01/05 02:11:17 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 35.0 (TID 28, localhost, executor driver, partition 0, PROCESS_LOCAL, 8086 bytes)
24/01/05 02:11:17 INFO executor.Executor: Running task 0.0 in stage 35.0 (TID 28)
24/01/05 02:11:17 INFO memory.MemoryStore: Block rdd_68_0 stored as values in memory (estimated size 398.0 B, free 365.7 MB)
24/01/05 02:11:17 INFO storage.BlockManagerInfo: Added rdd_68_0 in memory on adhoc-1:45245 (size: 398.0 B, free: 366.1 MB)
24/01/05 02:11:17 INFO executor.Executor: Finished task 0.0 in stage 35.0 (TID 28). 950 bytes result sent to driver
24/01/05 02:11:17 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 35.0 (TID 28) in 27 ms on localhost (executor driver) (1/1)
24/01/05 02:11:17 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 35.0, whose tasks have all completed, from pool 
24/01/05 02:11:17 INFO scheduler.DAGScheduler: ShuffleMapStage 35 (countByKey at HoodieJavaPairRDD.java:105) finished in 0.035 s
24/01/05 02:11:17 INFO scheduler.DAGScheduler: looking for newly runnable stages
24/01/05 02:11:17 INFO scheduler.DAGScheduler: running: Set()
24/01/05 02:11:17 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 36)
24/01/05 02:11:17 INFO scheduler.DAGScheduler: failed: Set()
24/01/05 02:11:17 INFO scheduler.DAGScheduler: Submitting ResultStage 36 (ShuffledRDD[71] at countByKey at HoodieJavaPairRDD.java:105), which has no missing parents
24/01/05 02:11:17 INFO memory.MemoryStore: Block broadcast_23 stored as values in memory (estimated size 3.6 KB, free 365.7 MB)
24/01/05 02:11:17 INFO memory.MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 2.1 KB, free 365.7 MB)
24/01/05 02:11:17 INFO storage.BlockManagerInfo: Added broadcast_23_piece0 in memory on adhoc-1:45245 (size: 2.1 KB, free: 366.1 MB)
24/01/05 02:11:17 INFO spark.SparkContext: Created broadcast 23 from broadcast at DAGScheduler.scala:1161
24/01/05 02:11:17 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 36 (ShuffledRDD[71] at countByKey at HoodieJavaPairRDD.java:105) (first 15 tasks are for partitions Vector(0))
24/01/05 02:11:17 INFO scheduler.TaskSchedulerImpl: Adding task set 36.0 with 1 tasks
24/01/05 02:11:17 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 36.0 (TID 29, localhost, executor driver, partition 0, ANY, 7662 bytes)
24/01/05 02:11:17 INFO executor.Executor: Running task 0.0 in stage 36.0 (TID 29)
24/01/05 02:11:17 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
24/01/05 02:11:17 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
24/01/05 02:11:17 INFO executor.Executor: Finished task 0.0 in stage 36.0 (TID 29). 1136 bytes result sent to driver
24/01/05 02:11:17 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 36.0 (TID 29) in 11 ms on localhost (executor driver) (1/1)
24/01/05 02:11:17 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 36.0, whose tasks have all completed, from pool 
24/01/05 02:11:17 INFO scheduler.DAGScheduler: ResultStage 36 (countByKey at HoodieJavaPairRDD.java:105) finished in 0.019 s
24/01/05 02:11:17 INFO scheduler.DAGScheduler: Job 15 finished: countByKey at HoodieJavaPairRDD.java:105, took 0.057950 s
24/01/05 02:11:17 INFO commit.UpsertPartitioner: AvgRecordSize => 1024
24/01/05 02:11:17 INFO spark.SparkContext: Starting job: collectAsMap at UpsertPartitioner.java:282
24/01/05 02:11:17 INFO scheduler.DAGScheduler: Got job 16 (collectAsMap at UpsertPartitioner.java:282) with 1 output partitions
24/01/05 02:11:17 INFO scheduler.DAGScheduler: Final stage: ResultStage 37 (collectAsMap at UpsertPartitioner.java:282)
24/01/05 02:11:17 INFO scheduler.DAGScheduler: Parents of final stage: List()
24/01/05 02:11:17 INFO scheduler.DAGScheduler: Missing parents: List()
24/01/05 02:11:17 INFO scheduler.DAGScheduler: Submitting ResultStage 37 (MapPartitionsRDD[73] at mapToPair at UpsertPartitioner.java:281), which has no missing parents
24/01/05 02:11:17 INFO memory.MemoryStore: Block broadcast_24 stored as values in memory (estimated size 268.9 KB, free 365.4 MB)
24/01/05 02:11:17 INFO memory.MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 95.8 KB, free 365.3 MB)
24/01/05 02:11:17 INFO storage.BlockManagerInfo: Added broadcast_24_piece0 in memory on adhoc-1:45245 (size: 95.8 KB, free: 366.0 MB)
24/01/05 02:11:17 INFO spark.SparkContext: Created broadcast 24 from broadcast at DAGScheduler.scala:1161
24/01/05 02:11:17 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 37 (MapPartitionsRDD[73] at mapToPair at UpsertPartitioner.java:281) (first 15 tasks are for partitions Vector(0))
24/01/05 02:11:17 INFO scheduler.TaskSchedulerImpl: Adding task set 37.0 with 1 tasks
24/01/05 02:11:17 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 37.0 (TID 30, localhost, executor driver, partition 0, PROCESS_LOCAL, 7730 bytes)
24/01/05 02:11:17 INFO executor.Executor: Running task 0.0 in stage 37.0 (TID 30)
24/01/05 02:11:17 INFO view.FileSystemViewManager: Creating View Manager with storage type :MEMORY
24/01/05 02:11:17 INFO view.FileSystemViewManager: Creating in-memory based Table View
24/01/05 02:11:17 INFO view.FileSystemViewManager: Creating InMemory based view for basePath /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:11:17 INFO view.AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
24/01/05 02:11:17 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:11:17 INFO view.AbstractTableFileSystemView: Building file system view for partition (files)
24/01/05 02:11:17 INFO executor.Executor: Finished task 0.0 in stage 37.0 (TID 30). 700 bytes result sent to driver
24/01/05 02:11:17 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 37.0 (TID 30) in 31 ms on localhost (executor driver) (1/1)
24/01/05 02:11:17 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 37.0, whose tasks have all completed, from pool 
24/01/05 02:11:17 INFO scheduler.DAGScheduler: ResultStage 37 (collectAsMap at UpsertPartitioner.java:282) finished in 0.059 s
24/01/05 02:11:17 INFO scheduler.DAGScheduler: Job 16 finished: collectAsMap at UpsertPartitioner.java:282, took 0.061176 s
24/01/05 02:11:17 INFO view.AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
24/01/05 02:11:17 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:11:17 INFO commit.UpsertPartitioner: Total Buckets: 1
24/01/05 02:11:17 INFO timeline.HoodieActiveTimeline: Checking for file exists ?/user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/20240105021110208.deltacommit.requested
24/01/05 02:11:17 INFO timeline.HoodieActiveTimeline: Create new file for toInstant ?/user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/20240105021110208.deltacommit.inflight
24/01/05 02:11:17 INFO commit.BaseSparkCommitActionExecutor: no validators configured.
24/01/05 02:11:17 INFO commit.BaseCommitActionExecutor: Auto commit enabled: Committing 20240105021110208
24/01/05 02:11:17 INFO spark.SparkContext: Starting job: collect at HoodieJavaRDD.java:177
24/01/05 02:11:17 INFO scheduler.DAGScheduler: Registering RDD 74 (mapToPair at HoodieJavaRDD.java:149)
24/01/05 02:11:17 INFO scheduler.DAGScheduler: Got job 17 (collect at HoodieJavaRDD.java:177) with 1 output partitions
24/01/05 02:11:17 INFO scheduler.DAGScheduler: Final stage: ResultStage 39 (collect at HoodieJavaRDD.java:177)
24/01/05 02:11:17 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 38)
24/01/05 02:11:17 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 38)
24/01/05 02:11:17 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 38 (MapPartitionsRDD[74] at mapToPair at HoodieJavaRDD.java:149), which has no missing parents
24/01/05 02:11:17 INFO memory.MemoryStore: Block broadcast_25 stored as values in memory (estimated size 273.6 KB, free 365.0 MB)
24/01/05 02:11:17 INFO memory.MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 95.5 KB, free 364.9 MB)
24/01/05 02:11:17 INFO storage.BlockManagerInfo: Added broadcast_25_piece0 in memory on adhoc-1:45245 (size: 95.5 KB, free: 365.9 MB)
24/01/05 02:11:17 INFO spark.SparkContext: Created broadcast 25 from broadcast at DAGScheduler.scala:1161
24/01/05 02:11:17 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 38 (MapPartitionsRDD[74] at mapToPair at HoodieJavaRDD.java:149) (first 15 tasks are for partitions Vector(0))
24/01/05 02:11:17 INFO scheduler.TaskSchedulerImpl: Adding task set 38.0 with 1 tasks
24/01/05 02:11:17 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 38.0 (TID 31, localhost, executor driver, partition 0, PROCESS_LOCAL, 8086 bytes)
24/01/05 02:11:17 INFO executor.Executor: Running task 0.0 in stage 38.0 (TID 31)
24/01/05 02:11:17 INFO storage.BlockManager: Found block rdd_68_0 locally
24/01/05 02:11:17 INFO executor.Executor: Finished task 0.0 in stage 38.0 (TID 31). 907 bytes result sent to driver
24/01/05 02:11:17 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 38.0 (TID 31) in 20 ms on localhost (executor driver) (1/1)
24/01/05 02:11:17 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 38.0, whose tasks have all completed, from pool 
24/01/05 02:11:17 INFO scheduler.DAGScheduler: ShuffleMapStage 38 (mapToPair at HoodieJavaRDD.java:149) finished in 0.044 s
24/01/05 02:11:17 INFO scheduler.DAGScheduler: looking for newly runnable stages
24/01/05 02:11:17 INFO scheduler.DAGScheduler: running: Set()
24/01/05 02:11:17 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 39)
24/01/05 02:11:17 INFO scheduler.DAGScheduler: failed: Set()
24/01/05 02:11:17 INFO scheduler.DAGScheduler: Submitting ResultStage 39 (MapPartitionsRDD[79] at map at HoodieJavaRDD.java:125), which has no missing parents
24/01/05 02:11:17 INFO memory.MemoryStore: Block broadcast_26 stored as values in memory (estimated size 348.5 KB, free 364.6 MB)
24/01/05 02:11:17 INFO memory.MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 126.8 KB, free 364.5 MB)
24/01/05 02:11:17 INFO storage.BlockManagerInfo: Added broadcast_26_piece0 in memory on adhoc-1:45245 (size: 126.8 KB, free: 365.8 MB)
24/01/05 02:11:17 INFO spark.SparkContext: Created broadcast 26 from broadcast at DAGScheduler.scala:1161
24/01/05 02:11:17 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 39 (MapPartitionsRDD[79] at map at HoodieJavaRDD.java:125) (first 15 tasks are for partitions Vector(0))
24/01/05 02:11:17 INFO scheduler.TaskSchedulerImpl: Adding task set 39.0 with 1 tasks
24/01/05 02:11:17 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 39.0 (TID 32, localhost, executor driver, partition 0, ANY, 7662 bytes)
24/01/05 02:11:17 INFO executor.Executor: Running task 0.0 in stage 39.0 (TID 32)
24/01/05 02:11:17 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
24/01/05 02:11:17 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
24/01/05 02:11:17 INFO deltacommit.BaseSparkDeltaCommitActionExecutor: Merging updates for commit 20240105021110208 for file files-0000-0
24/01/05 02:11:17 INFO view.FileSystemViewManager: Creating View Manager with storage type :MEMORY
24/01/05 02:11:17 INFO view.FileSystemViewManager: Creating in-memory based Table View
24/01/05 02:11:17 INFO view.FileSystemViewManager: Creating InMemory based view for basePath /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:11:17 INFO view.AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
24/01/05 02:11:17 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:11:17 INFO view.AbstractTableFileSystemView: Building file system view for partition (files)
# WARNING: Unable to attach Serviceability Agent. Unable to attach even with module exceptions: [org.apache.hudi.org.openjdk.jol.vm.sa.SASupportException: Sense failed., org.apache.hudi.org.openjdk.jol.vm.sa.SASupportException: Sense failed., org.apache.hudi.org.openjdk.jol.vm.sa.SASupportException: Sense failed.]
24/01/05 02:11:18 INFO marker.DirectWriteMarkers: Creating Marker Path=/user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/.temp/20240105021110208/files/files-0000-0_0-39-32_00000000000000010.hfile.marker.APPEND
24/01/05 02:11:18 INFO marker.DirectWriteMarkers: [direct] Created marker file /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/.temp/20240105021110208/files/files-0000-0_0-39-32_00000000000000010.hfile.marker.APPEND in 7 ms
24/01/05 02:11:18 INFO log.HoodieLogFormat$WriterBuilder: Building HoodieLogFormat Writer
24/01/05 02:11:18 INFO log.HoodieLogFormat$WriterBuilder: HoodieLogFile on path /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/files/.files-0000-0_00000000000000010.log.1_0-0-0
24/01/05 02:11:18 INFO log.HoodieLogFormatWriter: HoodieLogFile{pathStr='/user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/files/.files-0000-0_00000000000000010.log.1_0-0-0', fileLen=80} exists. Appending to existing file
24/01/05 02:11:18 INFO compress.CodecPool: Got brand-new compressor [.gz]
24/01/05 02:11:18 INFO compress.CodecPool: Got brand-new compressor [.gz]
24/01/05 02:11:18 INFO io.HoodieAppendHandle: AppendHandle for partitionPath files filePath files/.files-0000-0_00000000000000010.log.1_0-0-0, took 854 ms.
24/01/05 02:11:18 INFO memory.MemoryStore: Block rdd_78_0 stored as values in memory (estimated size 437.0 B, free 364.5 MB)
24/01/05 02:11:18 INFO storage.BlockManagerInfo: Added rdd_78_0 in memory on adhoc-1:45245 (size: 437.0 B, free: 365.8 MB)
24/01/05 02:11:18 INFO executor.Executor: Finished task 0.0 in stage 39.0 (TID 32). 1442 bytes result sent to driver
24/01/05 02:11:18 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 39.0 (TID 32) in 898 ms on localhost (executor driver) (1/1)
24/01/05 02:11:18 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 39.0, whose tasks have all completed, from pool 
24/01/05 02:11:18 INFO scheduler.DAGScheduler: ResultStage 39 (collect at HoodieJavaRDD.java:177) finished in 0.925 s
24/01/05 02:11:18 INFO scheduler.DAGScheduler: Job 17 finished: collect at HoodieJavaRDD.java:177, took 0.972677 s
24/01/05 02:11:18 INFO util.CommitUtils: Creating  metadata for UPSERT_PREPPED numWriteStats:1 numReplaceFileIds:0
24/01/05 02:11:18 INFO commit.BaseSparkCommitActionExecutor: Committing 20240105021110208, action Type deltacommit, operation Type UPSERT_PREPPED
24/01/05 02:11:18 INFO spark.SparkContext: Starting job: collect at HoodieSparkEngineContext.java:150
24/01/05 02:11:18 INFO scheduler.DAGScheduler: Got job 18 (collect at HoodieSparkEngineContext.java:150) with 1 output partitions
24/01/05 02:11:18 INFO scheduler.DAGScheduler: Final stage: ResultStage 40 (collect at HoodieSparkEngineContext.java:150)
24/01/05 02:11:18 INFO scheduler.DAGScheduler: Parents of final stage: List()
24/01/05 02:11:18 INFO scheduler.DAGScheduler: Missing parents: List()
24/01/05 02:11:18 INFO scheduler.DAGScheduler: Submitting ResultStage 40 (MapPartitionsRDD[81] at flatMap at HoodieSparkEngineContext.java:150), which has no missing parents
24/01/05 02:11:18 INFO memory.MemoryStore: Block broadcast_27 stored as values in memory (estimated size 72.5 KB, free 364.4 MB)
24/01/05 02:11:18 INFO memory.MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 26.7 KB, free 364.4 MB)
24/01/05 02:11:18 INFO storage.BlockManagerInfo: Added broadcast_27_piece0 in memory on adhoc-1:45245 (size: 26.7 KB, free: 365.8 MB)
24/01/05 02:11:18 INFO spark.SparkContext: Created broadcast 27 from broadcast at DAGScheduler.scala:1161
24/01/05 02:11:18 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 40 (MapPartitionsRDD[81] at flatMap at HoodieSparkEngineContext.java:150) (first 15 tasks are for partitions Vector(0))
24/01/05 02:11:18 INFO scheduler.TaskSchedulerImpl: Adding task set 40.0 with 1 tasks
24/01/05 02:11:18 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 40.0 (TID 33, localhost, executor driver, partition 0, PROCESS_LOCAL, 7838 bytes)
24/01/05 02:11:18 INFO executor.Executor: Running task 0.0 in stage 40.0 (TID 33)
24/01/05 02:11:18 INFO executor.Executor: Finished task 0.0 in stage 40.0 (TID 33). 668 bytes result sent to driver
24/01/05 02:11:18 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 40.0 (TID 33) in 11 ms on localhost (executor driver) (1/1)
24/01/05 02:11:18 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 40.0, whose tasks have all completed, from pool 
24/01/05 02:11:18 INFO scheduler.DAGScheduler: ResultStage 40 (collect at HoodieSparkEngineContext.java:150) finished in 0.026 s
24/01/05 02:11:18 INFO scheduler.DAGScheduler: Job 18 finished: collect at HoodieSparkEngineContext.java:150, took 0.026874 s
24/01/05 02:11:18 INFO timeline.HoodieActiveTimeline: Marking instant complete [==>20240105021110208__deltacommit__INFLIGHT]
24/01/05 02:11:18 INFO timeline.HoodieActiveTimeline: Checking for file exists ?/user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/20240105021110208.deltacommit.inflight
24/01/05 02:11:18 INFO timeline.HoodieActiveTimeline: Create new file for toInstant ?/user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/20240105021110208.deltacommit
24/01/05 02:11:18 INFO timeline.HoodieActiveTimeline: Completed [==>20240105021110208__deltacommit__INFLIGHT]
24/01/05 02:11:18 INFO commit.BaseSparkCommitActionExecutor: Committed 20240105021110208
24/01/05 02:11:18 INFO spark.SparkContext: Starting job: collectAsMap at HoodieSparkEngineContext.java:164
24/01/05 02:11:18 INFO scheduler.DAGScheduler: Got job 19 (collectAsMap at HoodieSparkEngineContext.java:164) with 1 output partitions
24/01/05 02:11:18 INFO scheduler.DAGScheduler: Final stage: ResultStage 41 (collectAsMap at HoodieSparkEngineContext.java:164)
24/01/05 02:11:18 INFO scheduler.DAGScheduler: Parents of final stage: List()
24/01/05 02:11:18 INFO scheduler.DAGScheduler: Missing parents: List()
24/01/05 02:11:18 INFO scheduler.DAGScheduler: Submitting ResultStage 41 (MapPartitionsRDD[83] at mapToPair at HoodieSparkEngineContext.java:161), which has no missing parents
24/01/05 02:11:18 INFO memory.MemoryStore: Block broadcast_28 stored as values in memory (estimated size 72.8 KB, free 364.3 MB)
24/01/05 02:11:18 INFO memory.MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 26.8 KB, free 364.3 MB)
24/01/05 02:11:18 INFO storage.BlockManagerInfo: Added broadcast_28_piece0 in memory on adhoc-1:45245 (size: 26.8 KB, free: 365.7 MB)
24/01/05 02:11:18 INFO spark.SparkContext: Created broadcast 28 from broadcast at DAGScheduler.scala:1161
24/01/05 02:11:18 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 41 (MapPartitionsRDD[83] at mapToPair at HoodieSparkEngineContext.java:161) (first 15 tasks are for partitions Vector(0))
24/01/05 02:11:18 INFO scheduler.TaskSchedulerImpl: Adding task set 41.0 with 1 tasks
24/01/05 02:11:18 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 41.0 (TID 34, localhost, executor driver, partition 0, PROCESS_LOCAL, 7838 bytes)
24/01/05 02:11:18 INFO executor.Executor: Running task 0.0 in stage 41.0 (TID 34)
24/01/05 02:11:18 INFO executor.Executor: Finished task 0.0 in stage 41.0 (TID 34). 787 bytes result sent to driver
24/01/05 02:11:18 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 41.0 (TID 34) in 11 ms on localhost (executor driver) (1/1)
24/01/05 02:11:18 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 41.0, whose tasks have all completed, from pool 
24/01/05 02:11:18 INFO scheduler.DAGScheduler: ResultStage 41 (collectAsMap at HoodieSparkEngineContext.java:164) finished in 0.024 s
24/01/05 02:11:18 INFO scheduler.DAGScheduler: Job 19 finished: collectAsMap at HoodieSparkEngineContext.java:164, took 0.026320 s
24/01/05 02:11:18 INFO fs.FSUtils: Removed directory at /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/.temp/20240105021110208
24/01/05 02:11:18 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:11:18 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:11:18 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:11:18 INFO table.HoodieTableMetaClient: Loading Active commit timeline for /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:11:18 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105021110208__deltacommit__COMPLETED__20240105021118540]}
24/01/05 02:11:18 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:11:18 INFO view.FileSystemViewManager: Creating View Manager with storage type :MEMORY
24/01/05 02:11:18 INFO view.FileSystemViewManager: Creating in-memory based Table View
24/01/05 02:11:18 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105021110208__deltacommit__COMPLETED__20240105021118540]}
24/01/05 02:11:18 INFO timeline.HoodieActiveTimeline: Marking instant complete [==>20240105021110208__deltacommit__INFLIGHT]
24/01/05 02:11:18 INFO timeline.HoodieActiveTimeline: Checking for file exists ?/user/hive/warehouse/stock_ticks_mor/.hoodie/20240105021110208.deltacommit.inflight
24/01/05 02:11:18 INFO timeline.HoodieActiveTimeline: Create new file for toInstant ?/user/hive/warehouse/stock_ticks_mor/.hoodie/20240105021110208.deltacommit
24/01/05 02:11:18 INFO timeline.HoodieActiveTimeline: Completed [==>20240105021110208__deltacommit__INFLIGHT]
24/01/05 02:11:18 WARN marker.WriteMarkersFactory: Timeline-server-based markers are not supported for HDFS: base path /user/hive/warehouse/stock_ticks_mor.  Falling back to direct markers.
24/01/05 02:11:18 INFO spark.SparkContext: Starting job: collectAsMap at HoodieSparkEngineContext.java:164
24/01/05 02:11:18 INFO scheduler.DAGScheduler: Got job 20 (collectAsMap at HoodieSparkEngineContext.java:164) with 1 output partitions
24/01/05 02:11:18 INFO scheduler.DAGScheduler: Final stage: ResultStage 42 (collectAsMap at HoodieSparkEngineContext.java:164)
24/01/05 02:11:18 INFO scheduler.DAGScheduler: Parents of final stage: List()
24/01/05 02:11:18 INFO scheduler.DAGScheduler: Missing parents: List()
24/01/05 02:11:18 INFO scheduler.DAGScheduler: Submitting ResultStage 42 (MapPartitionsRDD[85] at mapToPair at HoodieSparkEngineContext.java:161), which has no missing parents
24/01/05 02:11:18 INFO memory.MemoryStore: Block broadcast_29 stored as values in memory (estimated size 72.8 KB, free 364.2 MB)
24/01/05 02:11:18 INFO memory.MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 26.8 KB, free 364.2 MB)
24/01/05 02:11:18 INFO storage.BlockManagerInfo: Added broadcast_29_piece0 in memory on adhoc-1:45245 (size: 26.8 KB, free: 365.7 MB)
24/01/05 02:11:18 INFO spark.SparkContext: Created broadcast 29 from broadcast at DAGScheduler.scala:1161
24/01/05 02:11:18 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 42 (MapPartitionsRDD[85] at mapToPair at HoodieSparkEngineContext.java:161) (first 15 tasks are for partitions Vector(0))
24/01/05 02:11:18 INFO scheduler.TaskSchedulerImpl: Adding task set 42.0 with 1 tasks
24/01/05 02:11:18 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 42.0 (TID 35, localhost, executor driver, partition 0, PROCESS_LOCAL, 7820 bytes)
24/01/05 02:11:18 INFO executor.Executor: Running task 0.0 in stage 42.0 (TID 35)
24/01/05 02:11:18 INFO executor.Executor: Finished task 0.0 in stage 42.0 (TID 35). 769 bytes result sent to driver
24/01/05 02:11:18 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 42.0 (TID 35) in 14 ms on localhost (executor driver) (1/1)
24/01/05 02:11:18 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 42.0, whose tasks have all completed, from pool 
24/01/05 02:11:18 INFO scheduler.DAGScheduler: ResultStage 42 (collectAsMap at HoodieSparkEngineContext.java:164) finished in 0.027 s
24/01/05 02:11:18 INFO scheduler.DAGScheduler: Job 20 finished: collectAsMap at HoodieSparkEngineContext.java:164, took 0.029151 s
24/01/05 02:11:18 INFO fs.FSUtils: Removed directory at /user/hive/warehouse/stock_ticks_mor/.hoodie/.temp/20240105021110208
24/01/05 02:11:18 INFO client.BaseHoodieWriteClient: Committed 20240105021110208
24/01/05 02:11:18 INFO rdd.MapPartitionsRDD: Removing RDD 44 from persistence list
24/01/05 02:11:18 INFO storage.BlockManager: Removing RDD 44
24/01/05 02:11:18 INFO rdd.MapPartitionsRDD: Removing RDD 54 from persistence list
24/01/05 02:11:18 INFO storage.BlockManager: Removing RDD 54
24/01/05 02:11:18 INFO rdd.UnionRDD: Removing RDD 68 from persistence list
24/01/05 02:11:18 INFO storage.BlockManager: Removing RDD 68
24/01/05 02:11:18 INFO rdd.MapPartitionsRDD: Removing RDD 78 from persistence list
24/01/05 02:11:18 INFO storage.BlockManager: Removing RDD 78
24/01/05 02:11:18 INFO transaction.TransactionManager: Transaction ending with transaction owner Option{val=[==>20240105021110208__deltacommit__INFLIGHT]}
24/01/05 02:11:18 INFO lock.InProcessLockProvider: Base Path /user/hive/warehouse/stock_ticks_mor, Lock Instance java.util.concurrent.locks.ReentrantReadWriteLock@592a1882[Write locks = 1, Read locks = 0], Thread main, In-process lock state RELEASING
24/01/05 02:11:18 INFO lock.InProcessLockProvider: Base Path /user/hive/warehouse/stock_ticks_mor, Lock Instance java.util.concurrent.locks.ReentrantReadWriteLock@592a1882[Write locks = 0, Read locks = 0], Thread main, In-process lock state RELEASED
24/01/05 02:11:18 INFO lock.InProcessLockProvider: Base Path /user/hive/warehouse/stock_ticks_mor, Lock Instance java.util.concurrent.locks.ReentrantReadWriteLock@592a1882[Write locks = 0, Read locks = 0], Thread main, In-process lock state ALREADY_RELEASED
24/01/05 02:11:18 INFO lock.LockManager: Released connection created for acquiring lock
24/01/05 02:11:18 INFO transaction.TransactionManager: Transaction ended with transaction owner Option{val=[==>20240105021110208__deltacommit__INFLIGHT]}
24/01/05 02:11:18 INFO client.BaseHoodieWriteClient: Start to clean synchronously.
24/01/05 02:11:18 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:11:18 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:11:18 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:11:18 INFO table.HoodieTableMetaClient: Loading Active commit timeline for /user/hive/warehouse/stock_ticks_mor
24/01/05 02:11:18 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105021110208__deltacommit__COMPLETED__20240105021118626]}
24/01/05 02:11:18 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:11:18 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:11:18 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:11:18 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:11:18 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:11:18 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:11:18 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105021110208__deltacommit__COMPLETED__20240105021118540]}
24/01/05 02:11:18 INFO view.AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
24/01/05 02:11:18 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:11:18 INFO view.FileSystemViewManager: Creating View Manager with storage type :REMOTE_FIRST
24/01/05 02:11:18 INFO view.FileSystemViewManager: Creating remote first table view
24/01/05 02:11:18 INFO client.BaseHoodieWriteClient: Cleaner started
24/01/05 02:11:18 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:11:18 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:11:18 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:11:18 INFO table.HoodieTableMetaClient: Loading Active commit timeline for /user/hive/warehouse/stock_ticks_mor
24/01/05 02:11:18 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105021110208__deltacommit__COMPLETED__20240105021118626]}
24/01/05 02:11:18 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:11:18 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:11:18 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:11:18 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:11:18 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:11:18 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:11:18 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105021110208__deltacommit__COMPLETED__20240105021118540]}
24/01/05 02:11:18 INFO view.AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
24/01/05 02:11:18 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:11:18 INFO view.FileSystemViewManager: Creating View Manager with storage type :REMOTE_FIRST
24/01/05 02:11:18 INFO view.FileSystemViewManager: Creating remote first table view
24/01/05 02:11:18 INFO client.BaseHoodieWriteClient: Scheduling cleaning at instant time :20240105021118703
24/01/05 02:11:18 INFO view.FileSystemViewManager: Creating remote view for basePath /user/hive/warehouse/stock_ticks_mor. Server=adhoc-1:33433, Timeout=300
24/01/05 02:11:18 INFO view.FileSystemViewManager: Creating InMemory based view for basePath /user/hive/warehouse/stock_ticks_mor
24/01/05 02:11:18 INFO view.AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
24/01/05 02:11:18 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:11:18 INFO view.RemoteHoodieTableFileSystemView: Sending request : (http://adhoc-1:33433/v1/hoodie/view/compactions/pending/?basepath=%2Fuser%2Fhive%2Fwarehouse%2Fstock_ticks_mor&lastinstantts=20240105021110208&timelinehash=4edbe545d5e4877759b59ac9b4488385fad3f5cc412ad1a7af51246e9bb79357)
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 654
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 505
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 717
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 557
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 718
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 692
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 631
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 744
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 709
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 535
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 543
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 562
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 710
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 546
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 656
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 721
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 530
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 597
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 732
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 506
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 677
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 524
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 737
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 547
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 590
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 571
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 549
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 682
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 559
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 550
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 649
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 655
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 576
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 650
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 749
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 580
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 691
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 705
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 532
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 609
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 616
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 689
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 629
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 569
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 513
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 648
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 675
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 501
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 628
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 626
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 514
24/01/05 02:11:19 INFO storage.BlockManagerInfo: Removed broadcast_25_piece0 on adhoc-1:45245 in memory (size: 95.5 KB, free: 365.8 MB)
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 685
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 523
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 603
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 673
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 633
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 662
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 659
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 610
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 619
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 518
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 734
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 596
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 605
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 512
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 528
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 714
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 572
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 575
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 577
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 502
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 533
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 579
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 500
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 545
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 636
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 591
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 687
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 566
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 730
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 671
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 504
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 565
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 727
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 517
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 538
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 642
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 733
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 746
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 686
24/01/05 02:11:19 INFO storage.BlockManagerInfo: Removed broadcast_27_piece0 on adhoc-1:45245 in memory (size: 26.7 KB, free: 365.9 MB)
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 608
24/01/05 02:11:19 INFO storage.BlockManagerInfo: Removed broadcast_24_piece0 on adhoc-1:45245 in memory (size: 95.8 KB, free: 366.0 MB)
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 637
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 522
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 695
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 743
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 600
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 657
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 725
24/01/05 02:11:19 INFO storage.BlockManagerInfo: Removed broadcast_20_piece0 on adhoc-1:45245 in memory (size: 127.3 KB, free: 366.1 MB)
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 660
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 676
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 704
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 713
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 553
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 700
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 658
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 602
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 703
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 640
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 582
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 611
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 679
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 519
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 541
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 585
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 587
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 696
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 669
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 720
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 715
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned shuffle 8
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 708
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 711
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 651
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 688
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned shuffle 7
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 606
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 698
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 697
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 740
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 664
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 592
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 630
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 581
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 641
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 598
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 638
24/01/05 02:11:19 INFO storage.BlockManagerInfo: Removed broadcast_29_piece0 on adhoc-1:45245 in memory (size: 26.8 KB, free: 366.1 MB)
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 586
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 644
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 653
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 542
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 678
24/01/05 02:11:19 INFO storage.BlockManager: Removing RDD 68
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned RDD 68
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 536
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 544
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 594
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 724
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 515
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 510
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 539
24/01/05 02:11:19 INFO storage.BlockManagerInfo: Removed broadcast_23_piece0 on adhoc-1:45245 in memory (size: 2.1 KB, free: 366.1 MB)
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 680
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 617
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 674
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 560
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 570
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 723
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 729
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 735
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 706
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 625
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 699
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 738
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 613
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 526
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 552
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 540
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 741
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 701
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 634
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 534
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 614
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 736
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 623
24/01/05 02:11:19 INFO storage.BlockManager: Removing RDD 78
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned RDD 78
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 595
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 748
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 672
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 681
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 663
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 531
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 690
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 584
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 599
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 624
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 561
24/01/05 02:11:19 INFO storage.BlockManagerInfo: Removed broadcast_22_piece0 on adhoc-1:45245 in memory (size: 4.7 KB, free: 366.1 MB)
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 615
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 537
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 508
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 604
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 661
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 643
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 719
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 578
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 601
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 747
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 621
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 507
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 527
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 583
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 666
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 684
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 731
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 722
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 567
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 564
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 573
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 509
24/01/05 02:11:19 INFO storage.BlockManagerInfo: Removed broadcast_21_piece0 on adhoc-1:45245 in memory (size: 26.7 KB, free: 366.1 MB)
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 645
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 668
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 716
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 632
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 551
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 574
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 627
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 503
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 556
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 568
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 707
24/01/05 02:11:19 INFO storage.BlockManagerInfo: Removed broadcast_26_piece0 on adhoc-1:45245 in memory (size: 126.8 KB, free: 366.3 MB)
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 683
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 520
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 588
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 555
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 665
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 525
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 647
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 607
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 694
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 521
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 646
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 670
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 693
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 529
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 548
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 652
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 558
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 622
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 589
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 745
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 742
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 639
24/01/05 02:11:19 INFO storage.BlockManagerInfo: Removed broadcast_28_piece0 on adhoc-1:45245 in memory (size: 26.8 KB, free: 366.3 MB)
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 702
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 712
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 554
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 612
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 563
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 667
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 739
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 618
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 511
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 635
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 726
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 728
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 516
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 593
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 620
24/01/05 02:11:19 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:11:19 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:11:19 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:11:19 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105021110208__deltacommit__COMPLETED__20240105021118626]}
24/01/05 02:11:19 INFO collection.RocksDBDAO: DELETING RocksDB persisted at /tmp/hoodie_timeline_rocksdb/_user_hive_warehouse_stock_ticks_mor/493e7d31-b98a-412d-9e3c-a316483de02f
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 302
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 309
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 319
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 19
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 327
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 23
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 335
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 301
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 305
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 339
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 336
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 9
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 347
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 21
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 321
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 348
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 17
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 304
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 331
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 316
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 24
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 325
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 311
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 333
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 7
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 330
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 317
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 341
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 4
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 6
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 340
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 332
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 326
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 318
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 346
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 320
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 1
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 310
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 22
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 314
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 349
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 345
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 342
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 337
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 324
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 306
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 308
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 312
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned shuffle 5
24/01/05 02:11:19 INFO storage.BlockManagerInfo: Removed broadcast_12_piece0 on adhoc-1:45245 in memory (size: 11.4 KB, free: 366.3 MB)
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 13
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 315
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 307
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 12
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 303
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 11
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 8
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: No column family found. Loading default
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 329
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 15
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 18
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 10
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 344
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 5
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 323
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 343
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 322
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 14
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 2
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 328
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 334
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 20
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 16
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 300
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 313
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 338
24/01/05 02:11:19 INFO spark.ContextCleaner: Cleaned accumulator 3
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : RocksDB version: 7.5.3

24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Compile date 2022-09-02 09:50:20
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : DB SUMMARY

24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : DB Session ID:  YIICZ51MKI5OQCSSUA0A

24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : SST files in /tmp/hoodie_timeline_rocksdb/_user_hive_warehouse_stock_ticks_mor/493e7d31-b98a-412d-9e3c-a316483de02f dir, Total Num: 0, files: 

24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Write Ahead Log file in /tmp/hoodie_timeline_rocksdb/_user_hive_warehouse_stock_ticks_mor/493e7d31-b98a-412d-9e3c-a316483de02f: 

24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                         Options.error_if_exists: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                       Options.create_if_missing: 1
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                         Options.paranoid_checks: 1
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :             Options.flush_verify_memtable_count: 1
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                               Options.track_and_verify_wals_in_manifest: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :        Options.verify_sst_unique_id_in_manifest: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                                     Options.env: 0x7ff4da5ca5e0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                                      Options.fs: PosixFileSystem
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                                Options.info_log: 0x7ff4fc0972a8
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                Options.max_file_opening_threads: 16
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                              Options.statistics: 0x7ff4fc097320
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                               Options.use_fsync: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                       Options.max_log_file_size: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                  Options.max_manifest_file_size: 1073741824
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                   Options.log_file_time_to_roll: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                       Options.keep_log_file_num: 1000
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                    Options.recycle_log_file_num: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                         Options.allow_fallocate: 1
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                        Options.allow_mmap_reads: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                       Options.allow_mmap_writes: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                        Options.use_direct_reads: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                        Options.use_direct_io_for_flush_and_compaction: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :          Options.create_missing_column_families: 1
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                              Options.db_log_dir: 
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                                 Options.wal_dir: 
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                Options.table_cache_numshardbits: 6
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                         Options.WAL_ttl_seconds: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                       Options.WAL_size_limit_MB: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                        Options.max_write_batch_group_size_bytes: 1048576
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :             Options.manifest_preallocation_size: 4194304
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                     Options.is_fd_close_on_exec: 1
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                   Options.advise_random_on_open: 1
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                    Options.db_write_buffer_size: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                    Options.write_buffer_manager: 0x7ff4fc10d540
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :         Options.access_hint_on_compaction_start: 1
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :           Options.random_access_max_buffer_size: 1048576
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                      Options.use_adaptive_mutex: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                            Options.rate_limiter: (nil)
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :     Options.sst_file_manager.rate_bytes_per_sec: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                       Options.wal_recovery_mode: 2
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                  Options.enable_thread_tracking: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                  Options.enable_pipelined_write: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                  Options.unordered_write: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :         Options.allow_concurrent_memtable_write: 1
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :      Options.enable_write_thread_adaptive_yield: 1
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :             Options.write_thread_max_yield_usec: 100
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :            Options.write_thread_slow_yield_usec: 3
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                               Options.row_cache: None
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                              Options.wal_filter: None
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :             Options.avoid_flush_during_recovery: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :             Options.allow_ingest_behind: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :             Options.two_write_queues: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :             Options.manual_wal_flush: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :             Options.wal_compression: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :             Options.atomic_flush: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :             Options.avoid_unnecessary_blocking_io: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                 Options.persist_stats_to_disk: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                 Options.write_dbid_to_manifest: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                 Options.log_readahead_size: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                 Options.file_checksum_gen_factory: Unknown
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                 Options.best_efforts_recovery: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                Options.max_bgerror_resume_count: 2147483647
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :            Options.bgerror_resume_retry_interval: 1000000
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :             Options.allow_data_in_errors: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :             Options.db_host_id: __hostname__
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :             Options.enforce_single_del_contracts: true
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :             Options.max_background_jobs: 2
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :             Options.max_background_compactions: -1
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :             Options.max_subcompactions: 1
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :             Options.avoid_flush_during_shutdown: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :           Options.writable_file_max_buffer_size: 1048576
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :             Options.delayed_write_rate : 16777216
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :             Options.max_total_wal_size: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :             Options.delete_obsolete_files_period_micros: 21600000000
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                   Options.stats_dump_period_sec: 300
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                 Options.stats_persist_period_sec: 600
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                 Options.stats_history_buffer_size: 1048576
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                          Options.max_open_files: -1
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                          Options.bytes_per_sync: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                      Options.wal_bytes_per_sync: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                   Options.strict_bytes_per_sync: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :       Options.compaction_readahead_size: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                  Options.max_background_flushes: -1
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Compression algorithms supported:
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : 	kZSTDNotFinalCompression supported: 1
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : 	kZSTD supported: 1
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : 	kXpressCompression supported: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : 	kLZ4HCCompression supported: 1
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : 	kLZ4Compression supported: 1
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : 	kBZip2Compression supported: 1
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : 	kZlibCompression supported: 1
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : 	kSnappyCompression supported: 1
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Fast CRC32 supported: Not supported on x86
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : DMutex implementation: pthread_mutex_t
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : [/db_impl/db_impl_open.cc:313] Creating manifest 1 

24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : [/version_set.cc:4994] Recovering from manifest file: /tmp/hoodie_timeline_rocksdb/_user_hive_warehouse_stock_ticks_mor/493e7d31-b98a-412d-9e3c-a316483de02f/MANIFEST-000001

24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : [/column_family.cc:618] --------------- Options for column family [default]:

24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :               Options.comparator: leveldb.BytewiseComparator
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :           Options.merge_operator: None
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :        Options.compaction_filter: None
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :        Options.compaction_filter_factory: None
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :  Options.sst_partitioner_factory: None
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :         Options.memtable_factory: SkipListFactory
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :            Options.table_factory: BlockBasedTable
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :            table_factory options:   flush_block_policy_factory: FlushBlockBySizePolicyFactory (0x7ff4fc10b460)
  cache_index_and_filter_blocks: 0
  cache_index_and_filter_blocks_with_high_priority: 1
  pin_l0_filter_and_index_blocks_in_cache: 0
  pin_top_level_index_and_filter: 1
  index_type: 0
  data_block_index_type: 0
  index_shortening: 1
  data_block_hash_table_util_ratio: 0.750000
  checksum: 1
  no_block_cache: 0
  block_cache: 0x7ff4fc10b4b0
  block_cache_name: LRUCache
  block_cache_options:
    capacity : 8388608
    num_shard_bits : 4
    strict_capacity_limit : 0
    memory_allocator : None
    high_pri_pool_ratio: 0.000
  block_cache_compressed: (nil)
  persistent_cache: (nil)
  block_size: 4096
  block_size_deviation: 10
  block_restart_interval: 16
  index_block_restart_interval: 1
  metadata_block_size: 4096
  partition_filters: 0
  use_delta_encoding: 1
  filter_policy: nullptr
  whole_key_filtering: 1
  verify_compression: 0
  read_amp_bytes_per_bit: 0
  format_version: 5
  enable_index_compression: 1
  block_align: 0
  max_auto_readahead_size: 262144
  prepopulate_block_cache: 0
  initial_auto_readahead_size: 8192

24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :        Options.write_buffer_size: 67108864
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :  Options.max_write_buffer_number: 2
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :          Options.compression: Snappy
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression: Disabled
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :       Options.prefix_extractor: nullptr
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :   Options.memtable_insert_with_hint_prefix_extractor: nullptr
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :             Options.num_levels: 7
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :        Options.min_write_buffer_number_to_merge: 1
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :     Options.max_write_buffer_number_to_maintain: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :     Options.max_write_buffer_size_to_maintain: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :            Options.bottommost_compression_opts.window_bits: -14
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression_opts.level: 32767
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :               Options.bottommost_compression_opts.strategy: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.max_dict_bytes: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.zstd_max_train_bytes: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.parallel_threads: 1
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression_opts.enabled: false
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.max_dict_buffer_bytes: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.use_zstd_dict_trainer: true
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :            Options.compression_opts.window_bits: -14
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                  Options.compression_opts.level: 32767
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :               Options.compression_opts.strategy: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.max_dict_bytes: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.zstd_max_train_bytes: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.use_zstd_dict_trainer: true
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.parallel_threads: 1
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                  Options.compression_opts.enabled: false
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.max_dict_buffer_bytes: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :      Options.level0_file_num_compaction_trigger: 4
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :          Options.level0_slowdown_writes_trigger: 20
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :              Options.level0_stop_writes_trigger: 36
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                   Options.target_file_size_base: 67108864
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :             Options.target_file_size_multiplier: 1
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                Options.max_bytes_for_level_base: 268435456
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Options.level_compaction_dynamic_level_bytes: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :          Options.max_bytes_for_level_multiplier: 10.000000
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[0]: 1
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[1]: 1
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[2]: 1
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[3]: 1
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[4]: 1
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[5]: 1
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[6]: 1
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :       Options.max_sequential_skip_in_iterations: 8
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                    Options.max_compaction_bytes: 1677721600
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                        Options.arena_block_size: 1048576
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :   Options.soft_pending_compaction_bytes_limit: 68719476736
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :   Options.hard_pending_compaction_bytes_limit: 274877906944
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                Options.disable_auto_compactions: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                        Options.compaction_style: kCompactionStyleLevel
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                          Options.compaction_pri: kMinOverlappingRatio
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.size_ratio: 1
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.min_merge_width: 2
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.max_merge_width: 4294967295
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.max_size_amplification_percent: 200
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.compression_size_percent: -1
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.stop_style: kCompactionStopStyleTotalSize
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_fifo.max_table_files_size: 1073741824
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_fifo.allow_compaction: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                   Options.table_properties_collectors: 
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                   Options.inplace_update_support: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                 Options.inplace_update_num_locks: 10000
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :               Options.memtable_prefix_bloom_size_ratio: 0.000000
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :               Options.memtable_whole_key_filtering: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :   Options.memtable_huge_page_size: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                           Options.bloom_locality: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                    Options.max_successive_merges: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                Options.optimize_filters_for_hits: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                Options.paranoid_file_checks: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                Options.force_consistency_checks: 1
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                Options.report_bg_io_stats: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                               Options.ttl: 2592000
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :          Options.periodic_compaction_seconds: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :  Options.preclude_last_level_data_seconds: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                       Options.enable_blob_files: false
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                           Options.min_blob_size: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                          Options.blob_file_size: 268435456
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                   Options.blob_compression_type: NoCompression
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :          Options.enable_blob_garbage_collection: false
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :      Options.blob_garbage_collection_age_cutoff: 0.250000
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Options.blob_garbage_collection_force_threshold: 1.000000
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :          Options.blob_compaction_readahead_size: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                Options.blob_file_starting_level: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Options.experimental_mempurge_threshold: 0.000000
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : [/version_set.cc:5042] Recovered from manifest file:/tmp/hoodie_timeline_rocksdb/_user_hive_warehouse_stock_ticks_mor/493e7d31-b98a-412d-9e3c-a316483de02f/MANIFEST-000001 succeeded,manifest_file_number is 1, next_file_number is 3, last_sequence is 0, log_number is 0,prev_log_number is 0,max_column_family is 0,min_log_number_to_keep is 0

24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : [/version_set.cc:5051] Column family [default] (ID 0), log number is 0

24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : [/db_impl/db_impl_open.cc:532] DB ID: d524237b-58f7-4b0a-9243-f75513c47466

24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : [/version_set.cc:4513] Creating manifest 5

24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : [/db_impl/db_impl_open.cc:2024] SstFileManager instance 0x7ff4fc10dc50
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : DB pointer 0x7ff4fc10e340
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : [/db_impl/db_impl.cc:1070] ------- DUMPING STATS -------
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : [/db_impl/db_impl.cc:1071] 
** DB Stats **
Uptime(secs): 0.0 total, 0.0 interval
Cumulative writes: 0 writes, 0 keys, 0 commit groups, 0.0 writes per commit group, ingest: 0.00 GB, 0.00 MB/s
Cumulative WAL: 0 writes, 0 syncs, 0.00 writes per sync, written: 0.00 GB, 0.00 MB/s
Cumulative stall: 00:00:0.000 H:M:S, 0.0 percent
Interval writes: 0 writes, 0 keys, 0 commit groups, 0.0 writes per commit group, ingest: 0.00 MB, 0.00 MB/s
Interval WAL: 0 writes, 0 syncs, 0.00 writes per sync, written: 0.00 GB, 0.00 MB/s
Interval stall: 00:00:0.000 H:M:S, 0.0 percent

** Compaction Stats [default] **
Level    Files   Size     Score Read(GB)  Rn(GB) Rnp1(GB) Write(GB) Wnew(GB) Moved(GB) W-Amp Rd(MB/s) Wr(MB/s) Comp(sec) CompMergeCPU(sec) Comp(cnt) Avg(sec) KeyIn KeyDrop Rblob(GB) Wblob(GB)
------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
 Sum      0/0    0.00 KB   0.0      0.0     0.0      0.0       0.0      0.0       0.0   0.0      0.0      0.0      0.00              0.00         0    0.000       0      0       0.0       0.0
 Int      0/0    0.00 KB   0.0      0.0     0.0      0.0       0.0      0.0       0.0   0.0      0.0      0.0      0.00              0.00         0    0.000       0      0       0.0       0.0

** Compaction Stats [default] **
Priority    Files   Size     Score Read(GB)  Rn(GB) Rnp1(GB) Write(GB) Wnew(GB) Moved(GB) W-Amp Rd(MB/s) Wr(MB/s) Comp(sec) CompMergeCPU(sec) Comp(cnt) Avg(sec) KeyIn KeyDrop Rblob(GB) Wblob(GB)
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Blob file count: 0, total size: 0.0 GB, garbage size: 0.0 GB, space amp: 0.0

Uptime(secs): 0.0 total, 0.0 interval
Flush(GB): cumulative 0.000, interval 0.000
AddFile(GB): cumulative 0.000, interval 0.000
AddFile(Total Files): cumulative 0, interval 0
AddFile(L0 Files): cumulative 0, interval 0
AddFile(Keys): cumulative 0, interval 0
Cumulative compaction: 0.00 GB write, 0.00 MB/s write, 0.00 GB read, 0.00 MB/s read, 0.0 seconds
Interval compaction: 0.00 GB write, 0.00 MB/s write, 0.00 GB read, 0.00 MB/s read, 0.0 seconds
Stalls(count): 0 level0_slowdown, 0 level0_slowdown_with_compaction, 0 level0_numfiles, 0 level0_numfiles_with_compaction, 0 stop for pending_compaction_bytes, 0 slowdown for pending_compaction_bytes, 0 memtable_compaction, 0 memtable_slowdown, interval 0 total count
Block cache LRUCache@0x7ff4fc10b4b0#599 capacity: 8.00 MB collections: 1 last_copies: 0 last_secs: 8.1e-05 secs_since: 0
Block cache entry stats(count,size,portion): Misc(1,0.00 KB,0%)

** File Read Latency Histogram By Level [default] **

24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : [/db_impl/db_impl.cc:762] STATISTICS:
 rocksdb.block.cache.miss COUNT : 0
rocksdb.block.cache.hit COUNT : 0
rocksdb.block.cache.add COUNT : 0
rocksdb.block.cache.add.failures COUNT : 0
rocksdb.block.cache.index.miss COUNT : 0
rocksdb.block.cache.index.hit COUNT : 0
rocksdb.block.cache.index.add COUNT : 0
rocksdb.block.cache.index.bytes.insert COUNT : 0
rocksdb.block.cache.index.bytes.evict COUNT : 0
rocksdb.block.cache.filter.miss COUNT : 0
rocksdb.block.cache.filter.hit COUNT : 0
rocksdb.block.cache.filter.add COUNT : 0
rocksdb.block.cache.filter.bytes.insert COUNT : 0
rocksdb.block.cache.filter.bytes.evict COUNT : 0
rocksdb.block.cache.data.miss COUNT : 0
rocksdb.block.cache.data.hit COUNT : 0
rocksdb.block.cache.data.add COUNT : 0
rocksdb.block.cache.data.bytes.insert COUNT : 0
rocksdb.block.cache.bytes.read COUNT : 0
rocksdb.block.cache.bytes.write COUNT : 0
rocksdb.bloom.filter.useful COUNT : 0
rocksdb.bloom.filter.full.positive COUNT : 0
rocksdb.bloom.filter.full.true.positive COUNT : 0
rocksdb.bloom.filter.micros COUNT : 0
rocksdb.persistent.cache.hit COUNT : 0
rocksdb.persistent.cache.miss COUNT : 0
rocksdb.sim.block.cache.hit COUNT : 0
rocksdb.sim.block.cache.miss COUNT : 0
rocksdb.memtable.hit COUNT : 0
rocksdb.memtable.miss COUNT : 0
rocksdb.l0.hit COUNT : 0
rocksdb.l1.hit COUNT : 0
rocksdb.l2andup.hit COUNT : 0
rocksdb.compaction.key.drop.new COUNT : 0
rocksdb.compaction.key.drop.obsolete COUNT : 0
rocksdb.compaction.key.drop.range_del COUNT : 0
rocksdb.compaction.key.drop.user COUNT : 0
rocksdb.compaction.range_del.drop.obsolete COUNT : 0
rocksdb.compaction.optimized.del.drop.obsolete COUNT : 0
rocksdb.compaction.cancelled COUNT : 0
rocksdb.number.keys.written COUNT : 0
rocksdb.number.keys.read COUNT : 0
rocksdb.number.keys.updated COUNT : 0
rocksdb.bytes.written COUNT : 0
rocksdb.bytes.read COUNT : 0
rocksdb.number.db.seek COUNT : 0
rocksdb.number.db.next COUNT : 0
rocksdb.number.db.prev COUNT : 0
rocksdb.number.db.seek.found COUNT : 0
rocksdb.number.db.next.found COUNT : 0
rocksdb.number.db.prev.found COUNT : 0
rocksdb.db.iter.bytes.read COUNT : 0
rocksdb.no.file.closes COUNT : 0
rocksdb.no.file.opens COUNT : 0
rocksdb.no.file.errors COUNT : 0
rocksdb.l0.slowdown.micros COUNT : 0
rocksdb.memtable.compaction.micros COUNT : 0
rocksdb.l0.num.files.stall.micros COUNT : 0
rocksdb.stall.micros COUNT : 0
rocksdb.db.mutex.wait.micros COUNT : 0
rocksdb.rate.limit.delay.millis COUNT : 0
rocksdb.num.iterators COUNT : 0
rocksdb.number.multiget.get COUNT : 0
rocksdb.number.multiget.keys.read COUNT : 0
rocksdb.number.multiget.bytes.read COUNT : 0
rocksdb.number.deletes.filtered COUNT : 0
rocksdb.number.merge.failures COUNT : 0
rocksdb.bloom.filter.prefix.checked COUNT : 0
rocksdb.bloom.filter.prefix.useful COUNT : 0
rocksdb.number.reseeks.iteration COUNT : 0
rocksdb.getupdatessince.calls COUNT : 0
rocksdb.block.cachecompressed.miss COUNT : 0
rocksdb.block.cachecompressed.hit COUNT : 0
rocksdb.block.cachecompressed.add COUNT : 0
rocksdb.block.cachecompressed.add.failures COUNT : 0
rocksdb.wal.synced COUNT : 0
rocksdb.wal.bytes COUNT : 0
rocksdb.write.self COUNT : 0
rocksdb.write.other COUNT : 0
rocksdb.write.timeout COUNT : 0
rocksdb.write.wal COUNT : 0
rocksdb.compact.read.bytes COUNT : 0
rocksdb.compact.write.bytes COUNT : 0
rocksdb.flush.write.bytes COUNT : 0
rocksdb.compact.read.marked.bytes COUNT : 0
rocksdb.compact.read.periodic.bytes COUNT : 0
rocksdb.compact.read.ttl.bytes COUNT : 0
rocksdb.compact.write.marked.bytes COUNT : 0
rocksdb.compact.write.periodic.bytes COUNT : 0
rocksdb.compact.write.ttl.bytes COUNT : 0
rocksdb.number.direct.load.table.properties COUNT : 0
rocksdb.number.superversion_acquires COUNT : 0
rocksdb.number.superversion_releases COUNT : 0
rocksdb.number.superversion_cleanups COUNT : 0
rocksdb.number.block.compressed COUNT : 0
rocksdb.number.block.decompressed COUNT : 0
rocksdb.number.block.not_compressed COUNT : 0
rocksdb.merge.operation.time.nanos COUNT : 0
rocksdb.filter.operation.time.nanos COUNT : 0
rocksdb.row.cache.hit COUNT : 0
rocksdb.row.cache.miss COUNT : 0
rocksdb.read.amp.estimate.useful.bytes COUNT : 0
rocksdb.read.amp.total.read.bytes COUNT : 0
rocksdb.number.rate_limiter.drains COUNT : 0
rocksdb.number.iter.skip COUNT : 0
rocksdb.blobdb.num.put COUNT : 0
rocksdb.blobdb.num.write COUNT : 0
rocksdb.blobdb.num.get COUNT : 0
rocksdb.blobdb.num.multiget COUNT : 0
rocksdb.blobdb.num.seek COUNT : 0
rocksdb.blobdb.num.next COUNT : 0
rocksdb.blobdb.num.prev COUNT : 0
rocksdb.blobdb.num.keys.written COUNT : 0
rocksdb.blobdb.num.keys.read COUNT : 0
rocksdb.blobdb.bytes.written COUNT : 0
rocksdb.blobdb.bytes.read COUNT : 0
rocksdb.blobdb.write.inlined COUNT : 0
rocksdb.blobdb.write.inlined.ttl COUNT : 0
rocksdb.blobdb.write.blob COUNT : 0
rocksdb.blobdb.write.blob.ttl COUNT : 0
rocksdb.blobdb.blob.file.bytes.written COUNT : 0
rocksdb.blobdb.blob.file.bytes.read COUNT : 0
rocksdb.blobdb.blob.file.synced COUNT : 0
rocksdb.blobdb.blob.index.expired.count COUNT : 0
rocksdb.blobdb.blob.index.expired.size COUNT : 0
rocksdb.blobdb.blob.index.evicted.count COUNT : 0
rocksdb.blobdb.blob.index.evicted.size COUNT : 0
rocksdb.blobdb.gc.num.files COUNT : 0
rocksdb.blobdb.gc.num.new.files COUNT : 0
rocksdb.blobdb.gc.failures COUNT : 0
rocksdb.blobdb.gc.num.keys.overwritten COUNT : 0
rocksdb.blobdb.gc.num.keys.expired COUNT : 0
rocksdb.blobdb.gc.num.keys.relocated COUNT : 0
rocksdb.blobdb.gc.bytes.overwritten COUNT : 0
rocksdb.blobdb.gc.bytes.expired COUNT : 0
rocksdb.blobdb.gc.bytes.relocated COUNT : 0
rocksdb.blobdb.fifo.num.files.evicted COUNT : 0
rocksdb.blobdb.fifo.num.keys.evicted COUNT : 0
rocksdb.blobdb.fifo.bytes.evicted COUNT : 0
rocksdb.txn.overhead.mutex.prepare COUNT : 0
rocksdb.txn.overhead.mutex.old.commit.map COUNT : 0
rocksdb.txn.overhead.duplicate.key COUNT : 0
rocksdb.txn.overhead.mutex.snapshot COUNT : 0
rocksdb.txn.get.tryagain COUNT : 0
rocksdb.number.multiget.keys.found COUNT : 0
rocksdb.num.iterator.created COUNT : 0
rocksdb.num.iterator.deleted COUNT : 0
rocksdb.block.cache.compression.dict.miss COUNT : 0
rocksdb.block.cache.compression.dict.hit COUNT : 0
rocksdb.block.cache.compression.dict.add COUNT : 0
rocksdb.block.cache.compression.dict.bytes.insert COUNT : 0
rocksdb.block.cache.compression.dict.bytes.evict COUNT : 0
rocksdb.block.cache.add.redundant COUNT : 0
rocksdb.block.cache.index.add.redundant COUNT : 0
rocksdb.block.cache.filter.add.redundant COUNT : 0
rocksdb.block.cache.data.add.redundant COUNT : 0
rocksdb.block.cache.compression.dict.add.redundant COUNT : 0
rocksdb.files.marked.trash COUNT : 0
rocksdb.files.deleted.immediately COUNT : 0
rocksdb.error.handler.bg.errro.count COUNT : 0
rocksdb.error.handler.bg.io.errro.count COUNT : 0
rocksdb.error.handler.bg.retryable.io.errro.count COUNT : 0
rocksdb.error.handler.autoresume.count COUNT : 0
rocksdb.error.handler.autoresume.retry.total.count COUNT : 0
rocksdb.error.handler.autoresume.success.count COUNT : 0
rocksdb.memtable.payload.bytes.at.flush COUNT : 0
rocksdb.memtable.garbage.bytes.at.flush COUNT : 0
rocksdb.secondary.cache.hits COUNT : 0
rocksdb.verify_checksum.read.bytes COUNT : 0
rocksdb.backup.read.bytes COUNT : 0
rocksdb.backup.write.bytes COUNT : 0
rocksdb.remote.compact.read.bytes COUNT : 0
rocksdb.remote.compact.write.bytes COUNT : 0
rocksdb.hot.file.read.bytes COUNT : 0
rocksdb.warm.file.read.bytes COUNT : 0
rocksdb.cold.file.read.bytes COUNT : 0
rocksdb.hot.file.read.count COUNT : 0
rocksdb.warm.file.read.count COUNT : 0
rocksdb.cold.file.read.count COUNT : 0
rocksdb.last.level.read.bytes COUNT : 0
rocksdb.last.level.read.count COUNT : 0
rocksdb.non.last.level.read.bytes COUNT : 0
rocksdb.non.last.level.read.count COUNT : 0
rocksdb.block.checksum.compute.count COUNT : 0
rocksdb.multiget.coroutine.count COUNT : 0
rocksdb.blobdb.cache.miss COUNT : 0
rocksdb.blobdb.cache.hit COUNT : 0
rocksdb.blobdb.cache.add COUNT : 0
rocksdb.blobdb.cache.add.failures COUNT : 0
rocksdb.blobdb.cache.bytes.read COUNT : 0
rocksdb.blobdb.cache.bytes.write COUNT : 0
rocksdb.db.get.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.db.write.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.compaction.times.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.compaction.times.cpu_micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.subcompaction.setup.times.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.table.sync.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.compaction.outfile.sync.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.wal.file.sync.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.manifest.file.sync.micros P50 : 794.000000 P95 : 827.000000 P99 : 827.000000 P100 : 827.000000 COUNT : 2 SUM : 1621
rocksdb.table.open.io.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.db.multiget.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.read.block.compaction.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.read.block.get.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.write.raw.block.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.l0.slowdown.count P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.memtable.compaction.count P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.num.files.stall.count P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.hard.rate.limit.delay.count P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.soft.rate.limit.delay.count P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.numfiles.in.singlecompaction P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.db.seek.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.db.write.stall P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.sst.read.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.num.subcompactions.scheduled P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.bytes.per.read P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.bytes.per.write P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.bytes.per.multiget P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.bytes.compressed P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.bytes.decompressed P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.compression.times.nanos P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.decompression.times.nanos P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.read.num.merge_operands P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.blobdb.key.size P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.blobdb.value.size P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.blobdb.write.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.blobdb.get.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.blobdb.multiget.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.blobdb.seek.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.blobdb.next.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.blobdb.prev.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.blobdb.blob.file.write.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.blobdb.blob.file.read.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.blobdb.blob.file.sync.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.blobdb.gc.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.blobdb.compression.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.blobdb.decompression.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.db.flush.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.sst.batch.size P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.num.index.and.filter.blocks.read.per.level P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.num.data.blocks.read.per.level P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.num.sst.read.per.level P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.error.handler.autoresume.retry.count P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.async.read.bytes P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.poll.wait.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.prefetched.bytes.discarded P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.multiget.io.batch.size P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.num.level.read.per.multiget P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0

24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : [/column_family.cc:618] --------------- Options for column family [hudi_view__user_hive_warehouse_stock_ticks_mor]:

24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :               Options.comparator: leveldb.BytewiseComparator
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :           Options.merge_operator: None
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :        Options.compaction_filter: None
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :        Options.compaction_filter_factory: None
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :  Options.sst_partitioner_factory: None
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :         Options.memtable_factory: SkipListFactory
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :            Options.table_factory: BlockBasedTable
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :            table_factory options:   flush_block_policy_factory: FlushBlockBySizePolicyFactory (0x7ff4fc12c260)
  cache_index_and_filter_blocks: 0
  cache_index_and_filter_blocks_with_high_priority: 1
  pin_l0_filter_and_index_blocks_in_cache: 0
  pin_top_level_index_and_filter: 1
  index_type: 0
  data_block_index_type: 0
  index_shortening: 1
  data_block_hash_table_util_ratio: 0.750000
  checksum: 1
  no_block_cache: 0
  block_cache: 0x7ff4fc135450
  block_cache_name: LRUCache
  block_cache_options:
    capacity : 8388608
    num_shard_bits : 4
    strict_capacity_limit : 0
    memory_allocator : None
    high_pri_pool_ratio: 0.000
  block_cache_compressed: (nil)
  persistent_cache: (nil)
  block_size: 4096
  block_size_deviation: 10
  block_restart_interval: 16
  index_block_restart_interval: 1
  metadata_block_size: 4096
  partition_filters: 0
  use_delta_encoding: 1
  filter_policy: nullptr
  whole_key_filtering: 1
  verify_compression: 0
  read_amp_bytes_per_bit: 0
  format_version: 5
  enable_index_compression: 1
  block_align: 0
  max_auto_readahead_size: 262144
  prepopulate_block_cache: 0
  initial_auto_readahead_size: 8192

24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :        Options.write_buffer_size: 67108864
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :  Options.max_write_buffer_number: 2
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :          Options.compression: Snappy
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression: Disabled
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :       Options.prefix_extractor: nullptr
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :   Options.memtable_insert_with_hint_prefix_extractor: nullptr
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :             Options.num_levels: 7
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :        Options.min_write_buffer_number_to_merge: 1
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :     Options.max_write_buffer_number_to_maintain: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :     Options.max_write_buffer_size_to_maintain: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :            Options.bottommost_compression_opts.window_bits: -14
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression_opts.level: 32767
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :               Options.bottommost_compression_opts.strategy: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.max_dict_bytes: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.zstd_max_train_bytes: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.parallel_threads: 1
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression_opts.enabled: false
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.max_dict_buffer_bytes: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.use_zstd_dict_trainer: true
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :            Options.compression_opts.window_bits: -14
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                  Options.compression_opts.level: 32767
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :               Options.compression_opts.strategy: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.max_dict_bytes: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.zstd_max_train_bytes: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.use_zstd_dict_trainer: true
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.parallel_threads: 1
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                  Options.compression_opts.enabled: false
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.max_dict_buffer_bytes: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :      Options.level0_file_num_compaction_trigger: 4
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :          Options.level0_slowdown_writes_trigger: 20
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :              Options.level0_stop_writes_trigger: 36
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                   Options.target_file_size_base: 67108864
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :             Options.target_file_size_multiplier: 1
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                Options.max_bytes_for_level_base: 268435456
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Options.level_compaction_dynamic_level_bytes: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :          Options.max_bytes_for_level_multiplier: 10.000000
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[0]: 1
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[1]: 1
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[2]: 1
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[3]: 1
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[4]: 1
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[5]: 1
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[6]: 1
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :       Options.max_sequential_skip_in_iterations: 8
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                    Options.max_compaction_bytes: 1677721600
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                        Options.arena_block_size: 1048576
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :   Options.soft_pending_compaction_bytes_limit: 68719476736
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :   Options.hard_pending_compaction_bytes_limit: 274877906944
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                Options.disable_auto_compactions: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                        Options.compaction_style: kCompactionStyleLevel
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                          Options.compaction_pri: kMinOverlappingRatio
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.size_ratio: 1
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.min_merge_width: 2
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.max_merge_width: 4294967295
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.max_size_amplification_percent: 200
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.compression_size_percent: -1
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.stop_style: kCompactionStopStyleTotalSize
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_fifo.max_table_files_size: 1073741824
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_fifo.allow_compaction: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                   Options.table_properties_collectors: 
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                   Options.inplace_update_support: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                 Options.inplace_update_num_locks: 10000
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :               Options.memtable_prefix_bloom_size_ratio: 0.000000
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :               Options.memtable_whole_key_filtering: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :   Options.memtable_huge_page_size: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                           Options.bloom_locality: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                    Options.max_successive_merges: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                Options.optimize_filters_for_hits: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                Options.paranoid_file_checks: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                Options.force_consistency_checks: 1
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                Options.report_bg_io_stats: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                               Options.ttl: 2592000
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :          Options.periodic_compaction_seconds: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :  Options.preclude_last_level_data_seconds: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                       Options.enable_blob_files: false
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                           Options.min_blob_size: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                          Options.blob_file_size: 268435456
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                   Options.blob_compression_type: NoCompression
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :          Options.enable_blob_garbage_collection: false
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :      Options.blob_garbage_collection_age_cutoff: 0.250000
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Options.blob_garbage_collection_force_threshold: 1.000000
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :          Options.blob_compaction_readahead_size: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                Options.blob_file_starting_level: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Options.experimental_mempurge_threshold: 0.000000
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : [/db_impl/db_impl.cc:2861] Created column family [hudi_view__user_hive_warehouse_stock_ticks_mor] (ID 1)
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : [/column_family.cc:618] --------------- Options for column family [hudi_pending_compaction__user_hive_warehouse_stock_ticks_mor]:

24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :               Options.comparator: leveldb.BytewiseComparator
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :           Options.merge_operator: None
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :        Options.compaction_filter: None
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :        Options.compaction_filter_factory: None
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :  Options.sst_partitioner_factory: None
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :         Options.memtable_factory: SkipListFactory
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :            Options.table_factory: BlockBasedTable
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :            table_factory options:   flush_block_policy_factory: FlushBlockBySizePolicyFactory (0x7ff4fc1f5290)
  cache_index_and_filter_blocks: 0
  cache_index_and_filter_blocks_with_high_priority: 1
  pin_l0_filter_and_index_blocks_in_cache: 0
  pin_top_level_index_and_filter: 1
  index_type: 0
  data_block_index_type: 0
  index_shortening: 1
  data_block_hash_table_util_ratio: 0.750000
  checksum: 1
  no_block_cache: 0
  block_cache: 0x7ff4fc1364a0
  block_cache_name: LRUCache
  block_cache_options:
    capacity : 8388608
    num_shard_bits : 4
    strict_capacity_limit : 0
    memory_allocator : None
    high_pri_pool_ratio: 0.000
  block_cache_compressed: (nil)
  persistent_cache: (nil)
  block_size: 4096
  block_size_deviation: 10
  block_restart_interval: 16
  index_block_restart_interval: 1
  metadata_block_size: 4096
  partition_filters: 0
  use_delta_encoding: 1
  filter_policy: nullptr
  whole_key_filtering: 1
  verify_compression: 0
  read_amp_bytes_per_bit: 0
  format_version: 5
  enable_index_compression: 1
  block_align: 0
  max_auto_readahead_size: 262144
  prepopulate_block_cache: 0
  initial_auto_readahead_size: 8192

24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :        Options.write_buffer_size: 67108864
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :  Options.max_write_buffer_number: 2
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :          Options.compression: Snappy
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression: Disabled
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :       Options.prefix_extractor: nullptr
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :   Options.memtable_insert_with_hint_prefix_extractor: nullptr
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :             Options.num_levels: 7
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :        Options.min_write_buffer_number_to_merge: 1
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :     Options.max_write_buffer_number_to_maintain: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :     Options.max_write_buffer_size_to_maintain: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :            Options.bottommost_compression_opts.window_bits: -14
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression_opts.level: 32767
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :               Options.bottommost_compression_opts.strategy: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.max_dict_bytes: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.zstd_max_train_bytes: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.parallel_threads: 1
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression_opts.enabled: false
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.max_dict_buffer_bytes: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.use_zstd_dict_trainer: true
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :            Options.compression_opts.window_bits: -14
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                  Options.compression_opts.level: 32767
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :               Options.compression_opts.strategy: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.max_dict_bytes: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.zstd_max_train_bytes: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.use_zstd_dict_trainer: true
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.parallel_threads: 1
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                  Options.compression_opts.enabled: false
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.max_dict_buffer_bytes: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :      Options.level0_file_num_compaction_trigger: 4
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :          Options.level0_slowdown_writes_trigger: 20
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :              Options.level0_stop_writes_trigger: 36
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                   Options.target_file_size_base: 67108864
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :             Options.target_file_size_multiplier: 1
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                Options.max_bytes_for_level_base: 268435456
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Options.level_compaction_dynamic_level_bytes: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :          Options.max_bytes_for_level_multiplier: 10.000000
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[0]: 1
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[1]: 1
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[2]: 1
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[3]: 1
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[4]: 1
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[5]: 1
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[6]: 1
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :       Options.max_sequential_skip_in_iterations: 8
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                    Options.max_compaction_bytes: 1677721600
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                        Options.arena_block_size: 1048576
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :   Options.soft_pending_compaction_bytes_limit: 68719476736
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :   Options.hard_pending_compaction_bytes_limit: 274877906944
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                Options.disable_auto_compactions: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                        Options.compaction_style: kCompactionStyleLevel
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                          Options.compaction_pri: kMinOverlappingRatio
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.size_ratio: 1
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.min_merge_width: 2
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.max_merge_width: 4294967295
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.max_size_amplification_percent: 200
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.compression_size_percent: -1
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.stop_style: kCompactionStopStyleTotalSize
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_fifo.max_table_files_size: 1073741824
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_fifo.allow_compaction: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                   Options.table_properties_collectors: 
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                   Options.inplace_update_support: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                 Options.inplace_update_num_locks: 10000
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :               Options.memtable_prefix_bloom_size_ratio: 0.000000
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :               Options.memtable_whole_key_filtering: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :   Options.memtable_huge_page_size: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                           Options.bloom_locality: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                    Options.max_successive_merges: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                Options.optimize_filters_for_hits: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                Options.paranoid_file_checks: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                Options.force_consistency_checks: 1
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                Options.report_bg_io_stats: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                               Options.ttl: 2592000
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :          Options.periodic_compaction_seconds: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :  Options.preclude_last_level_data_seconds: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                       Options.enable_blob_files: false
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                           Options.min_blob_size: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                          Options.blob_file_size: 268435456
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                   Options.blob_compression_type: NoCompression
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :          Options.enable_blob_garbage_collection: false
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :      Options.blob_garbage_collection_age_cutoff: 0.250000
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Options.blob_garbage_collection_force_threshold: 1.000000
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :          Options.blob_compaction_readahead_size: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                Options.blob_file_starting_level: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Options.experimental_mempurge_threshold: 0.000000
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : [/db_impl/db_impl.cc:2861] Created column family [hudi_pending_compaction__user_hive_warehouse_stock_ticks_mor] (ID 2)
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : [/column_family.cc:618] --------------- Options for column family [hudi_pending_log_compaction__user_hive_warehouse_stock_ticks_mor]:

24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :               Options.comparator: leveldb.BytewiseComparator
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :           Options.merge_operator: None
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :        Options.compaction_filter: None
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :        Options.compaction_filter_factory: None
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :  Options.sst_partitioner_factory: None
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :         Options.memtable_factory: SkipListFactory
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :            Options.table_factory: BlockBasedTable
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :            table_factory options:   flush_block_policy_factory: FlushBlockBySizePolicyFactory (0x7ff4fc1277b0)
  cache_index_and_filter_blocks: 0
  cache_index_and_filter_blocks_with_high_priority: 1
  pin_l0_filter_and_index_blocks_in_cache: 0
  pin_top_level_index_and_filter: 1
  index_type: 0
  data_block_index_type: 0
  index_shortening: 1
  data_block_hash_table_util_ratio: 0.750000
  checksum: 1
  no_block_cache: 0
  block_cache: 0x7ff4fc127800
  block_cache_name: LRUCache
  block_cache_options:
    capacity : 8388608
    num_shard_bits : 4
    strict_capacity_limit : 0
    memory_allocator : None
    high_pri_pool_ratio: 0.000
  block_cache_compressed: (nil)
  persistent_cache: (nil)
  block_size: 4096
  block_size_deviation: 10
  block_restart_interval: 16
  index_block_restart_interval: 1
  metadata_block_size: 4096
  partition_filters: 0
  use_delta_encoding: 1
  filter_policy: nullptr
  whole_key_filtering: 1
  verify_compression: 0
  read_amp_bytes_per_bit: 0
  format_version: 5
  enable_index_compression: 1
  block_align: 0
  max_auto_readahead_size: 262144
  prepopulate_block_cache: 0
  initial_auto_readahead_size: 8192

24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :        Options.write_buffer_size: 67108864
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :  Options.max_write_buffer_number: 2
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :          Options.compression: Snappy
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression: Disabled
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :       Options.prefix_extractor: nullptr
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :   Options.memtable_insert_with_hint_prefix_extractor: nullptr
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :             Options.num_levels: 7
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :        Options.min_write_buffer_number_to_merge: 1
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :     Options.max_write_buffer_number_to_maintain: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :     Options.max_write_buffer_size_to_maintain: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :            Options.bottommost_compression_opts.window_bits: -14
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression_opts.level: 32767
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :               Options.bottommost_compression_opts.strategy: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.max_dict_bytes: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.zstd_max_train_bytes: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.parallel_threads: 1
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression_opts.enabled: false
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.max_dict_buffer_bytes: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.use_zstd_dict_trainer: true
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :            Options.compression_opts.window_bits: -14
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                  Options.compression_opts.level: 32767
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :               Options.compression_opts.strategy: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.max_dict_bytes: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.zstd_max_train_bytes: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.use_zstd_dict_trainer: true
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.parallel_threads: 1
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                  Options.compression_opts.enabled: false
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.max_dict_buffer_bytes: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :      Options.level0_file_num_compaction_trigger: 4
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :          Options.level0_slowdown_writes_trigger: 20
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :              Options.level0_stop_writes_trigger: 36
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                   Options.target_file_size_base: 67108864
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :             Options.target_file_size_multiplier: 1
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                Options.max_bytes_for_level_base: 268435456
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Options.level_compaction_dynamic_level_bytes: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :          Options.max_bytes_for_level_multiplier: 10.000000
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[0]: 1
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[1]: 1
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[2]: 1
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[3]: 1
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[4]: 1
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[5]: 1
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[6]: 1
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :       Options.max_sequential_skip_in_iterations: 8
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                    Options.max_compaction_bytes: 1677721600
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                        Options.arena_block_size: 1048576
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :   Options.soft_pending_compaction_bytes_limit: 68719476736
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :   Options.hard_pending_compaction_bytes_limit: 274877906944
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                Options.disable_auto_compactions: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                        Options.compaction_style: kCompactionStyleLevel
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                          Options.compaction_pri: kMinOverlappingRatio
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.size_ratio: 1
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.min_merge_width: 2
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.max_merge_width: 4294967295
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.max_size_amplification_percent: 200
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.compression_size_percent: -1
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.stop_style: kCompactionStopStyleTotalSize
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_fifo.max_table_files_size: 1073741824
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_fifo.allow_compaction: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                   Options.table_properties_collectors: 
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                   Options.inplace_update_support: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                 Options.inplace_update_num_locks: 10000
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :               Options.memtable_prefix_bloom_size_ratio: 0.000000
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :               Options.memtable_whole_key_filtering: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :   Options.memtable_huge_page_size: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                           Options.bloom_locality: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                    Options.max_successive_merges: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                Options.optimize_filters_for_hits: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                Options.paranoid_file_checks: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                Options.force_consistency_checks: 1
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                Options.report_bg_io_stats: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                               Options.ttl: 2592000
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :          Options.periodic_compaction_seconds: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :  Options.preclude_last_level_data_seconds: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                       Options.enable_blob_files: false
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                           Options.min_blob_size: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                          Options.blob_file_size: 268435456
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                   Options.blob_compression_type: NoCompression
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :          Options.enable_blob_garbage_collection: false
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :      Options.blob_garbage_collection_age_cutoff: 0.250000
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Options.blob_garbage_collection_force_threshold: 1.000000
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :          Options.blob_compaction_readahead_size: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                Options.blob_file_starting_level: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Options.experimental_mempurge_threshold: 0.000000
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : [/db_impl/db_impl.cc:2861] Created column family [hudi_pending_log_compaction__user_hive_warehouse_stock_ticks_mor] (ID 3)
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : [/column_family.cc:618] --------------- Options for column family [hudi_bootstrap_basefile__user_hive_warehouse_stock_ticks_mor]:

24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :               Options.comparator: leveldb.BytewiseComparator
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :           Options.merge_operator: None
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :        Options.compaction_filter: None
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :        Options.compaction_filter_factory: None
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :  Options.sst_partitioner_factory: None
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :         Options.memtable_factory: SkipListFactory
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :            Options.table_factory: BlockBasedTable
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :            table_factory options:   flush_block_policy_factory: FlushBlockBySizePolicyFactory (0x7ff4fc177b10)
  cache_index_and_filter_blocks: 0
  cache_index_and_filter_blocks_with_high_priority: 1
  pin_l0_filter_and_index_blocks_in_cache: 0
  pin_top_level_index_and_filter: 1
  index_type: 0
  data_block_index_type: 0
  index_shortening: 1
  data_block_hash_table_util_ratio: 0.750000
  checksum: 1
  no_block_cache: 0
  block_cache: 0x7ff4fc177b60
  block_cache_name: LRUCache
  block_cache_options:
    capacity : 8388608
    num_shard_bits : 4
    strict_capacity_limit : 0
    memory_allocator : None
    high_pri_pool_ratio: 0.000
  block_cache_compressed: (nil)
  persistent_cache: (nil)
  block_size: 4096
  block_size_deviation: 10
  block_restart_interval: 16
  index_block_restart_interval: 1
  metadata_block_size: 4096
  partition_filters: 0
  use_delta_encoding: 1
  filter_policy: nullptr
  whole_key_filtering: 1
  verify_compression: 0
  read_amp_bytes_per_bit: 0
  format_version: 5
  enable_index_compression: 1
  block_align: 0
  max_auto_readahead_size: 262144
  prepopulate_block_cache: 0
  initial_auto_readahead_size: 8192

24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :        Options.write_buffer_size: 67108864
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :  Options.max_write_buffer_number: 2
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :          Options.compression: Snappy
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression: Disabled
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :       Options.prefix_extractor: nullptr
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :   Options.memtable_insert_with_hint_prefix_extractor: nullptr
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :             Options.num_levels: 7
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :        Options.min_write_buffer_number_to_merge: 1
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :     Options.max_write_buffer_number_to_maintain: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :     Options.max_write_buffer_size_to_maintain: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :            Options.bottommost_compression_opts.window_bits: -14
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression_opts.level: 32767
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :               Options.bottommost_compression_opts.strategy: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.max_dict_bytes: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.zstd_max_train_bytes: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.parallel_threads: 1
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression_opts.enabled: false
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.max_dict_buffer_bytes: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.use_zstd_dict_trainer: true
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :            Options.compression_opts.window_bits: -14
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                  Options.compression_opts.level: 32767
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :               Options.compression_opts.strategy: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.max_dict_bytes: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.zstd_max_train_bytes: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.use_zstd_dict_trainer: true
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.parallel_threads: 1
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                  Options.compression_opts.enabled: false
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.max_dict_buffer_bytes: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :      Options.level0_file_num_compaction_trigger: 4
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :          Options.level0_slowdown_writes_trigger: 20
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :              Options.level0_stop_writes_trigger: 36
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                   Options.target_file_size_base: 67108864
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :             Options.target_file_size_multiplier: 1
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                Options.max_bytes_for_level_base: 268435456
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Options.level_compaction_dynamic_level_bytes: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :          Options.max_bytes_for_level_multiplier: 10.000000
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[0]: 1
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[1]: 1
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[2]: 1
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[3]: 1
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[4]: 1
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[5]: 1
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[6]: 1
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :       Options.max_sequential_skip_in_iterations: 8
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                    Options.max_compaction_bytes: 1677721600
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                        Options.arena_block_size: 1048576
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :   Options.soft_pending_compaction_bytes_limit: 68719476736
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :   Options.hard_pending_compaction_bytes_limit: 274877906944
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                Options.disable_auto_compactions: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                        Options.compaction_style: kCompactionStyleLevel
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                          Options.compaction_pri: kMinOverlappingRatio
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.size_ratio: 1
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.min_merge_width: 2
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.max_merge_width: 4294967295
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.max_size_amplification_percent: 200
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.compression_size_percent: -1
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.stop_style: kCompactionStopStyleTotalSize
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_fifo.max_table_files_size: 1073741824
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_fifo.allow_compaction: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                   Options.table_properties_collectors: 
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                   Options.inplace_update_support: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                 Options.inplace_update_num_locks: 10000
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :               Options.memtable_prefix_bloom_size_ratio: 0.000000
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :               Options.memtable_whole_key_filtering: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :   Options.memtable_huge_page_size: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                           Options.bloom_locality: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                    Options.max_successive_merges: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                Options.optimize_filters_for_hits: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                Options.paranoid_file_checks: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                Options.force_consistency_checks: 1
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                Options.report_bg_io_stats: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                               Options.ttl: 2592000
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :          Options.periodic_compaction_seconds: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :  Options.preclude_last_level_data_seconds: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                       Options.enable_blob_files: false
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                           Options.min_blob_size: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                          Options.blob_file_size: 268435456
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                   Options.blob_compression_type: NoCompression
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :          Options.enable_blob_garbage_collection: false
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :      Options.blob_garbage_collection_age_cutoff: 0.250000
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Options.blob_garbage_collection_force_threshold: 1.000000
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :          Options.blob_compaction_readahead_size: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                Options.blob_file_starting_level: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Options.experimental_mempurge_threshold: 0.000000
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : [/db_impl/db_impl.cc:2861] Created column family [hudi_bootstrap_basefile__user_hive_warehouse_stock_ticks_mor] (ID 4)
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : [/column_family.cc:618] --------------- Options for column family [hudi_partitions__user_hive_warehouse_stock_ticks_mor]:

24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :               Options.comparator: leveldb.BytewiseComparator
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :           Options.merge_operator: None
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :        Options.compaction_filter: None
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :        Options.compaction_filter_factory: None
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :  Options.sst_partitioner_factory: None
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :         Options.memtable_factory: SkipListFactory
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :            Options.table_factory: BlockBasedTable
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :            table_factory options:   flush_block_policy_factory: FlushBlockBySizePolicyFactory (0x7ff4fc185520)
  cache_index_and_filter_blocks: 0
  cache_index_and_filter_blocks_with_high_priority: 1
  pin_l0_filter_and_index_blocks_in_cache: 0
  pin_top_level_index_and_filter: 1
  index_type: 0
  data_block_index_type: 0
  index_shortening: 1
  data_block_hash_table_util_ratio: 0.750000
  checksum: 1
  no_block_cache: 0
  block_cache: 0x7ff4fc185570
  block_cache_name: LRUCache
  block_cache_options:
    capacity : 8388608
    num_shard_bits : 4
    strict_capacity_limit : 0
    memory_allocator : None
    high_pri_pool_ratio: 0.000
  block_cache_compressed: (nil)
  persistent_cache: (nil)
  block_size: 4096
  block_size_deviation: 10
  block_restart_interval: 16
  index_block_restart_interval: 1
  metadata_block_size: 4096
  partition_filters: 0
  use_delta_encoding: 1
  filter_policy: nullptr
  whole_key_filtering: 1
  verify_compression: 0
  read_amp_bytes_per_bit: 0
  format_version: 5
  enable_index_compression: 1
  block_align: 0
  max_auto_readahead_size: 262144
  prepopulate_block_cache: 0
  initial_auto_readahead_size: 8192

24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :        Options.write_buffer_size: 67108864
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :  Options.max_write_buffer_number: 2
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :          Options.compression: Snappy
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression: Disabled
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :       Options.prefix_extractor: nullptr
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :   Options.memtable_insert_with_hint_prefix_extractor: nullptr
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :             Options.num_levels: 7
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :        Options.min_write_buffer_number_to_merge: 1
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :     Options.max_write_buffer_number_to_maintain: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :     Options.max_write_buffer_size_to_maintain: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :            Options.bottommost_compression_opts.window_bits: -14
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression_opts.level: 32767
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :               Options.bottommost_compression_opts.strategy: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.max_dict_bytes: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.zstd_max_train_bytes: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.parallel_threads: 1
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression_opts.enabled: false
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.max_dict_buffer_bytes: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.use_zstd_dict_trainer: true
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :            Options.compression_opts.window_bits: -14
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                  Options.compression_opts.level: 32767
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :               Options.compression_opts.strategy: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.max_dict_bytes: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.zstd_max_train_bytes: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.use_zstd_dict_trainer: true
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.parallel_threads: 1
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                  Options.compression_opts.enabled: false
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.max_dict_buffer_bytes: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :      Options.level0_file_num_compaction_trigger: 4
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :          Options.level0_slowdown_writes_trigger: 20
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :              Options.level0_stop_writes_trigger: 36
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                   Options.target_file_size_base: 67108864
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :             Options.target_file_size_multiplier: 1
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                Options.max_bytes_for_level_base: 268435456
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Options.level_compaction_dynamic_level_bytes: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :          Options.max_bytes_for_level_multiplier: 10.000000
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[0]: 1
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[1]: 1
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[2]: 1
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[3]: 1
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[4]: 1
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[5]: 1
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[6]: 1
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :       Options.max_sequential_skip_in_iterations: 8
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                    Options.max_compaction_bytes: 1677721600
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                        Options.arena_block_size: 1048576
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :   Options.soft_pending_compaction_bytes_limit: 68719476736
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :   Options.hard_pending_compaction_bytes_limit: 274877906944
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                Options.disable_auto_compactions: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                        Options.compaction_style: kCompactionStyleLevel
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                          Options.compaction_pri: kMinOverlappingRatio
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.size_ratio: 1
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.min_merge_width: 2
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.max_merge_width: 4294967295
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.max_size_amplification_percent: 200
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.compression_size_percent: -1
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.stop_style: kCompactionStopStyleTotalSize
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_fifo.max_table_files_size: 1073741824
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_fifo.allow_compaction: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                   Options.table_properties_collectors: 
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                   Options.inplace_update_support: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                 Options.inplace_update_num_locks: 10000
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :               Options.memtable_prefix_bloom_size_ratio: 0.000000
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :               Options.memtable_whole_key_filtering: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :   Options.memtable_huge_page_size: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                           Options.bloom_locality: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                    Options.max_successive_merges: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                Options.optimize_filters_for_hits: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                Options.paranoid_file_checks: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                Options.force_consistency_checks: 1
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                Options.report_bg_io_stats: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                               Options.ttl: 2592000
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :          Options.periodic_compaction_seconds: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :  Options.preclude_last_level_data_seconds: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                       Options.enable_blob_files: false
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                           Options.min_blob_size: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                          Options.blob_file_size: 268435456
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                   Options.blob_compression_type: NoCompression
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :          Options.enable_blob_garbage_collection: false
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :      Options.blob_garbage_collection_age_cutoff: 0.250000
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Options.blob_garbage_collection_force_threshold: 1.000000
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :          Options.blob_compaction_readahead_size: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                Options.blob_file_starting_level: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Options.experimental_mempurge_threshold: 0.000000
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : [/db_impl/db_impl.cc:2861] Created column family [hudi_partitions__user_hive_warehouse_stock_ticks_mor] (ID 5)
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : [/column_family.cc:618] --------------- Options for column family [hudi_replaced_fg_user_hive_warehouse_stock_ticks_mor]:

24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :               Options.comparator: leveldb.BytewiseComparator
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :           Options.merge_operator: None
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :        Options.compaction_filter: None
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :        Options.compaction_filter_factory: None
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :  Options.sst_partitioner_factory: None
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :         Options.memtable_factory: SkipListFactory
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :            Options.table_factory: BlockBasedTable
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :            table_factory options:   flush_block_policy_factory: FlushBlockBySizePolicyFactory (0x7ff4fc192ed0)
  cache_index_and_filter_blocks: 0
  cache_index_and_filter_blocks_with_high_priority: 1
  pin_l0_filter_and_index_blocks_in_cache: 0
  pin_top_level_index_and_filter: 1
  index_type: 0
  data_block_index_type: 0
  index_shortening: 1
  data_block_hash_table_util_ratio: 0.750000
  checksum: 1
  no_block_cache: 0
  block_cache: 0x7ff4fc192f20
  block_cache_name: LRUCache
  block_cache_options:
    capacity : 8388608
    num_shard_bits : 4
    strict_capacity_limit : 0
    memory_allocator : None
    high_pri_pool_ratio: 0.000
  block_cache_compressed: (nil)
  persistent_cache: (nil)
  block_size: 4096
  block_size_deviation: 10
  block_restart_interval: 16
  index_block_restart_interval: 1
  metadata_block_size: 4096
  partition_filters: 0
  use_delta_encoding: 1
  filter_policy: nullptr
  whole_key_filtering: 1
  verify_compression: 0
  read_amp_bytes_per_bit: 0
  format_version: 5
  enable_index_compression: 1
  block_align: 0
  max_auto_readahead_size: 262144
  prepopulate_block_cache: 0
  initial_auto_readahead_size: 8192

24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :        Options.write_buffer_size: 67108864
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :  Options.max_write_buffer_number: 2
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :          Options.compression: Snappy
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression: Disabled
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :       Options.prefix_extractor: nullptr
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :   Options.memtable_insert_with_hint_prefix_extractor: nullptr
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :             Options.num_levels: 7
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :        Options.min_write_buffer_number_to_merge: 1
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :     Options.max_write_buffer_number_to_maintain: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :     Options.max_write_buffer_size_to_maintain: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :            Options.bottommost_compression_opts.window_bits: -14
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression_opts.level: 32767
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :               Options.bottommost_compression_opts.strategy: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.max_dict_bytes: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.zstd_max_train_bytes: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.parallel_threads: 1
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression_opts.enabled: false
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.max_dict_buffer_bytes: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.use_zstd_dict_trainer: true
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :            Options.compression_opts.window_bits: -14
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                  Options.compression_opts.level: 32767
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :               Options.compression_opts.strategy: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.max_dict_bytes: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.zstd_max_train_bytes: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.use_zstd_dict_trainer: true
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.parallel_threads: 1
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                  Options.compression_opts.enabled: false
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.max_dict_buffer_bytes: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :      Options.level0_file_num_compaction_trigger: 4
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :          Options.level0_slowdown_writes_trigger: 20
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :              Options.level0_stop_writes_trigger: 36
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                   Options.target_file_size_base: 67108864
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :             Options.target_file_size_multiplier: 1
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                Options.max_bytes_for_level_base: 268435456
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Options.level_compaction_dynamic_level_bytes: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :          Options.max_bytes_for_level_multiplier: 10.000000
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[0]: 1
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[1]: 1
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[2]: 1
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[3]: 1
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[4]: 1
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[5]: 1
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[6]: 1
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :       Options.max_sequential_skip_in_iterations: 8
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                    Options.max_compaction_bytes: 1677721600
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                        Options.arena_block_size: 1048576
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :   Options.soft_pending_compaction_bytes_limit: 68719476736
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :   Options.hard_pending_compaction_bytes_limit: 274877906944
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                Options.disable_auto_compactions: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                        Options.compaction_style: kCompactionStyleLevel
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                          Options.compaction_pri: kMinOverlappingRatio
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.size_ratio: 1
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.min_merge_width: 2
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.max_merge_width: 4294967295
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.max_size_amplification_percent: 200
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.compression_size_percent: -1
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.stop_style: kCompactionStopStyleTotalSize
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_fifo.max_table_files_size: 1073741824
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_fifo.allow_compaction: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                   Options.table_properties_collectors: 
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                   Options.inplace_update_support: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                 Options.inplace_update_num_locks: 10000
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :               Options.memtable_prefix_bloom_size_ratio: 0.000000
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :               Options.memtable_whole_key_filtering: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :   Options.memtable_huge_page_size: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                           Options.bloom_locality: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                    Options.max_successive_merges: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                Options.optimize_filters_for_hits: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                Options.paranoid_file_checks: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                Options.force_consistency_checks: 1
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                Options.report_bg_io_stats: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                               Options.ttl: 2592000
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :          Options.periodic_compaction_seconds: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :  Options.preclude_last_level_data_seconds: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                       Options.enable_blob_files: false
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                           Options.min_blob_size: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                          Options.blob_file_size: 268435456
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                   Options.blob_compression_type: NoCompression
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :          Options.enable_blob_garbage_collection: false
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :      Options.blob_garbage_collection_age_cutoff: 0.250000
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Options.blob_garbage_collection_force_threshold: 1.000000
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :          Options.blob_compaction_readahead_size: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                Options.blob_file_starting_level: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Options.experimental_mempurge_threshold: 0.000000
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : [/db_impl/db_impl.cc:2861] Created column family [hudi_replaced_fg_user_hive_warehouse_stock_ticks_mor] (ID 6)
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : [/column_family.cc:618] --------------- Options for column family [hudi_pending_clustering_fg_user_hive_warehouse_stock_ticks_mor]:

24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :               Options.comparator: leveldb.BytewiseComparator
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :           Options.merge_operator: None
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :        Options.compaction_filter: None
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :        Options.compaction_filter_factory: None
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :  Options.sst_partitioner_factory: None
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :         Options.memtable_factory: SkipListFactory
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :            Options.table_factory: BlockBasedTable
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :            table_factory options:   flush_block_policy_factory: FlushBlockBySizePolicyFactory (0x7ff4fc1a0930)
  cache_index_and_filter_blocks: 0
  cache_index_and_filter_blocks_with_high_priority: 1
  pin_l0_filter_and_index_blocks_in_cache: 0
  pin_top_level_index_and_filter: 1
  index_type: 0
  data_block_index_type: 0
  index_shortening: 1
  data_block_hash_table_util_ratio: 0.750000
  checksum: 1
  no_block_cache: 0
  block_cache: 0x7ff4fc1a0980
  block_cache_name: LRUCache
  block_cache_options:
    capacity : 8388608
    num_shard_bits : 4
    strict_capacity_limit : 0
    memory_allocator : None
    high_pri_pool_ratio: 0.000
  block_cache_compressed: (nil)
  persistent_cache: (nil)
  block_size: 4096
  block_size_deviation: 10
  block_restart_interval: 16
  index_block_restart_interval: 1
  metadata_block_size: 4096
  partition_filters: 0
  use_delta_encoding: 1
  filter_policy: nullptr
  whole_key_filtering: 1
  verify_compression: 0
  read_amp_bytes_per_bit: 0
  format_version: 5
  enable_index_compression: 1
  block_align: 0
  max_auto_readahead_size: 262144
  prepopulate_block_cache: 0
  initial_auto_readahead_size: 8192

24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :        Options.write_buffer_size: 67108864
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :  Options.max_write_buffer_number: 2
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :          Options.compression: Snappy
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression: Disabled
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :       Options.prefix_extractor: nullptr
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :   Options.memtable_insert_with_hint_prefix_extractor: nullptr
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :             Options.num_levels: 7
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :        Options.min_write_buffer_number_to_merge: 1
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :     Options.max_write_buffer_number_to_maintain: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :     Options.max_write_buffer_size_to_maintain: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :            Options.bottommost_compression_opts.window_bits: -14
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression_opts.level: 32767
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :               Options.bottommost_compression_opts.strategy: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.max_dict_bytes: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.zstd_max_train_bytes: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.parallel_threads: 1
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression_opts.enabled: false
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.max_dict_buffer_bytes: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.use_zstd_dict_trainer: true
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :            Options.compression_opts.window_bits: -14
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                  Options.compression_opts.level: 32767
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :               Options.compression_opts.strategy: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.max_dict_bytes: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.zstd_max_train_bytes: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.use_zstd_dict_trainer: true
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.parallel_threads: 1
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                  Options.compression_opts.enabled: false
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.max_dict_buffer_bytes: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :      Options.level0_file_num_compaction_trigger: 4
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :          Options.level0_slowdown_writes_trigger: 20
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :              Options.level0_stop_writes_trigger: 36
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                   Options.target_file_size_base: 67108864
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :             Options.target_file_size_multiplier: 1
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                Options.max_bytes_for_level_base: 268435456
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Options.level_compaction_dynamic_level_bytes: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :          Options.max_bytes_for_level_multiplier: 10.000000
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[0]: 1
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[1]: 1
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[2]: 1
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[3]: 1
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[4]: 1
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[5]: 1
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[6]: 1
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :       Options.max_sequential_skip_in_iterations: 8
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                    Options.max_compaction_bytes: 1677721600
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                        Options.arena_block_size: 1048576
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :   Options.soft_pending_compaction_bytes_limit: 68719476736
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :   Options.hard_pending_compaction_bytes_limit: 274877906944
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                Options.disable_auto_compactions: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                        Options.compaction_style: kCompactionStyleLevel
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                          Options.compaction_pri: kMinOverlappingRatio
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.size_ratio: 1
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.min_merge_width: 2
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.max_merge_width: 4294967295
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.max_size_amplification_percent: 200
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.compression_size_percent: -1
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.stop_style: kCompactionStopStyleTotalSize
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_fifo.max_table_files_size: 1073741824
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_fifo.allow_compaction: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                   Options.table_properties_collectors: 
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                   Options.inplace_update_support: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                 Options.inplace_update_num_locks: 10000
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :               Options.memtable_prefix_bloom_size_ratio: 0.000000
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :               Options.memtable_whole_key_filtering: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :   Options.memtable_huge_page_size: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                           Options.bloom_locality: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                    Options.max_successive_merges: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                Options.optimize_filters_for_hits: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                Options.paranoid_file_checks: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                Options.force_consistency_checks: 1
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                Options.report_bg_io_stats: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                               Options.ttl: 2592000
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :          Options.periodic_compaction_seconds: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :  Options.preclude_last_level_data_seconds: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                       Options.enable_blob_files: false
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                           Options.min_blob_size: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                          Options.blob_file_size: 268435456
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                   Options.blob_compression_type: NoCompression
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :          Options.enable_blob_garbage_collection: false
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :      Options.blob_garbage_collection_age_cutoff: 0.250000
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Options.blob_garbage_collection_force_threshold: 1.000000
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :          Options.blob_compaction_readahead_size: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB :                Options.blob_file_starting_level: 0
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : Options.experimental_mempurge_threshold: 0.000000
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : [/db_impl/db_impl.cc:2861] Created column family [hudi_pending_clustering_fg_user_hive_warehouse_stock_ticks_mor] (ID 7)
24/01/05 02:11:19 INFO view.RocksDbBasedFileSystemView: Resetting replacedFileGroups to ROCKSDB based file-system view at /tmp/hoodie_timeline_rocksdb, Total file-groups=0
24/01/05 02:11:19 INFO collection.RocksDBDAO: Prefix DELETE (query=part=) on hudi_replaced_fg_user_hive_warehouse_stock_ticks_mor
24/01/05 02:11:19 INFO view.RocksDbBasedFileSystemView: Resetting replacedFileGroups to ROCKSDB based file-system view complete
24/01/05 02:11:19 INFO view.AbstractTableFileSystemView: Took 3 ms to read  0 instants, 0 replaced file groups
24/01/05 02:11:19 INFO view.RocksDbBasedFileSystemView: Initializing pending compaction operations. Count=0
24/01/05 02:11:19 INFO view.RocksDbBasedFileSystemView: Initializing pending Log compaction operations. Count=0
24/01/05 02:11:19 INFO view.RocksDbBasedFileSystemView: Initializing external data file mapping. Count=0
24/01/05 02:11:19 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:11:19 INFO view.RocksDbBasedFileSystemView: Resetting file groups in pending clustering to ROCKSDB based file-system view at /tmp/hoodie_timeline_rocksdb, Total file-groups=0
24/01/05 02:11:19 INFO collection.RocksDBDAO: Prefix DELETE (query=part=) on hudi_pending_clustering_fg_user_hive_warehouse_stock_ticks_mor
24/01/05 02:11:19 INFO view.RocksDbBasedFileSystemView: Resetting replacedFileGroups to ROCKSDB based file-system view complete
24/01/05 02:11:19 INFO view.RocksDbBasedFileSystemView: Created ROCKSDB based file-system view at /tmp/hoodie_timeline_rocksdb
24/01/05 02:11:19 INFO collection.RocksDBDAO: Prefix Search for (query=) on hudi_pending_compaction__user_hive_warehouse_stock_ticks_mor. Total Time Taken (msec)=0. Serialization Time taken(micro)=0, num entries=0
24/01/05 02:11:19 INFO view.RemoteHoodieTableFileSystemView: Sending request : (http://adhoc-1:33433/v1/hoodie/view/logcompactions/pending/?basepath=%2Fuser%2Fhive%2Fwarehouse%2Fstock_ticks_mor&lastinstantts=20240105021110208&timelinehash=4edbe545d5e4877759b59ac9b4488385fad3f5cc412ad1a7af51246e9bb79357)
24/01/05 02:11:19 INFO collection.RocksDBDAO: Prefix Search for (query=) on hudi_pending_log_compaction__user_hive_warehouse_stock_ticks_mor. Total Time Taken (msec)=0. Serialization Time taken(micro)=0, num entries=0
24/01/05 02:11:19 INFO clean.CleanPlanner: No earliest commit to retain. No need to scan partitions !!
24/01/05 02:11:19 INFO clean.CleanPlanActionExecutor: Nothing to clean here. It is already clean
24/01/05 02:11:19 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105021110208__deltacommit__COMPLETED__20240105021118626]}
24/01/05 02:11:19 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:11:19 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:11:19 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:11:19 INFO table.HoodieTableMetaClient: Loading Active commit timeline for /user/hive/warehouse/stock_ticks_mor
24/01/05 02:11:19 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105021110208__deltacommit__COMPLETED__20240105021118626]}
24/01/05 02:11:19 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:11:19 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:11:19 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:11:19 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:11:19 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:11:19 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:11:19 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105021110208__deltacommit__COMPLETED__20240105021118540]}
24/01/05 02:11:19 INFO view.AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
24/01/05 02:11:19 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:11:19 INFO view.FileSystemViewManager: Creating View Manager with storage type :REMOTE_FIRST
24/01/05 02:11:19 INFO view.FileSystemViewManager: Creating remote first table view
24/01/05 02:11:19 INFO client.BaseHoodieWriteClient: Start to archive synchronously.
24/01/05 02:11:19 INFO transaction.TransactionManager: Transaction starting for Optional.empty with latest completed transaction instant Optional.empty
24/01/05 02:11:19 INFO lock.LockManager: LockProvider org.apache.hudi.client.transaction.lock.InProcessLockProvider
24/01/05 02:11:19 INFO lock.InProcessLockProvider: Base Path /user/hive/warehouse/stock_ticks_mor, Lock Instance java.util.concurrent.locks.ReentrantReadWriteLock@592a1882[Write locks = 0, Read locks = 0], Thread main, In-process lock state ACQUIRING
24/01/05 02:11:19 INFO lock.InProcessLockProvider: Base Path /user/hive/warehouse/stock_ticks_mor, Lock Instance java.util.concurrent.locks.ReentrantReadWriteLock@592a1882[Write locks = 1, Read locks = 0], Thread main, In-process lock state ACQUIRED
24/01/05 02:11:19 INFO transaction.TransactionManager: Transaction started for Optional.empty with latest completed transaction instant Optional.empty
24/01/05 02:11:19 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105021110208__deltacommit__COMPLETED__20240105021118626]}
24/01/05 02:11:19 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:11:19 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:11:19 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:11:19 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:11:19 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:11:19 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:11:19 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105021110208__deltacommit__COMPLETED__20240105021118540]}
24/01/05 02:11:19 INFO view.AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
24/01/05 02:11:19 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:11:19 INFO client.HoodieTimelineArchiver: Not archiving as there is no compaction yet on the metadata table
24/01/05 02:11:19 INFO client.HoodieTimelineArchiver: No Instants to archive
24/01/05 02:11:19 INFO transaction.TransactionManager: Transaction ending with transaction owner Optional.empty
24/01/05 02:11:19 INFO lock.InProcessLockProvider: Base Path /user/hive/warehouse/stock_ticks_mor, Lock Instance java.util.concurrent.locks.ReentrantReadWriteLock@592a1882[Write locks = 1, Read locks = 0], Thread main, In-process lock state RELEASING
24/01/05 02:11:19 INFO lock.InProcessLockProvider: Base Path /user/hive/warehouse/stock_ticks_mor, Lock Instance java.util.concurrent.locks.ReentrantReadWriteLock@592a1882[Write locks = 0, Read locks = 0], Thread main, In-process lock state RELEASED
24/01/05 02:11:19 INFO lock.InProcessLockProvider: Base Path /user/hive/warehouse/stock_ticks_mor, Lock Instance java.util.concurrent.locks.ReentrantReadWriteLock@592a1882[Write locks = 0, Read locks = 0], Thread main, In-process lock state ALREADY_RELEASED
24/01/05 02:11:19 INFO lock.LockManager: Released connection created for acquiring lock
24/01/05 02:11:19 INFO transaction.TransactionManager: Transaction ended with transaction owner Optional.empty
24/01/05 02:11:19 INFO view.FileSystemViewManager: Creating remote view for basePath /user/hive/warehouse/stock_ticks_mor. Server=adhoc-1:33433, Timeout=300
24/01/05 02:11:19 INFO view.FileSystemViewManager: Creating InMemory based view for basePath /user/hive/warehouse/stock_ticks_mor
24/01/05 02:11:19 INFO view.AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
24/01/05 02:11:19 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:11:19 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105021110208__deltacommit__COMPLETED__20240105021118626]}
24/01/05 02:11:19 INFO view.RemoteHoodieTableFileSystemView: Sending request : (http://adhoc-1:33433/v1/hoodie/view/refresh/?basepath=%2Fuser%2Fhive%2Fwarehouse%2Fstock_ticks_mor&lastinstantts=20240105021110208&timelinehash=4edbe545d5e4877759b59ac9b4488385fad3f5cc412ad1a7af51246e9bb79357)
24/01/05 02:11:19 INFO view.RocksDbBasedFileSystemView: Closing Rocksdb !!
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : [/db_impl/db_impl.cc:478] Shutdown: canceling all background work
24/01/05 02:11:19 INFO collection.RocksDBDAO: From Rocks DB : [/db_impl/db_impl.cc:677] Shutdown complete
24/01/05 02:11:19 INFO view.RocksDbBasedFileSystemView: Closed Rocksdb !!
24/01/05 02:11:19 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105021110208__deltacommit__COMPLETED__20240105021118626]}
24/01/05 02:11:19 INFO view.AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
24/01/05 02:11:19 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:11:19 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105021110208__deltacommit__COMPLETED__20240105021118626]}
24/01/05 02:11:19 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105021110208__deltacommit__COMPLETED__20240105021118540]}
24/01/05 02:11:19 INFO view.AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
24/01/05 02:11:19 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:11:19 INFO streamer.StreamSync: Commit 20240105021110208 successful!
24/01/05 02:11:19 INFO transaction.TransactionManager: Transaction manager closed
24/01/05 02:11:19 INFO transaction.TransactionManager: Transaction manager closed
24/01/05 02:11:19 INFO streamer.StreamSync: Shutting down embedded timeline server
24/01/05 02:11:19 INFO embedded.EmbeddedTimelineService: Closing Timeline server
24/01/05 02:11:19 INFO service.TimelineService: Closing Timeline Service
24/01/05 02:11:19 INFO javalin.Javalin: Stopping Javalin ...
24/01/05 02:11:19 INFO javalin.Javalin: Javalin has stopped
24/01/05 02:11:19 INFO service.TimelineService: Closed Timeline Service
24/01/05 02:11:19 INFO embedded.EmbeddedTimelineService: Closed Timeline server
24/01/05 02:11:19 INFO ingestion.HoodieIngestionService: Ingestion service (run-once mode) has been shut down.
24/01/05 02:11:19 INFO server.AbstractConnector: Stopped Spark@61019f59{HTTP/1.1,[http/1.1]}{0.0.0.0:8090}
24/01/05 02:11:19 INFO ui.SparkUI: Stopped Spark web UI at http://adhoc-1:8090
24/01/05 02:11:19 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
24/01/05 02:11:19 INFO memory.MemoryStore: MemoryStore cleared
24/01/05 02:11:19 INFO storage.BlockManager: BlockManager stopped
24/01/05 02:11:19 INFO storage.BlockManagerMaster: BlockManagerMaster stopped
24/01/05 02:11:19 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
24/01/05 02:11:19 INFO spark.SparkContext: Successfully stopped SparkContext
24/01/05 02:11:19 INFO util.ShutdownHookManager: Shutdown hook called
24/01/05 02:11:19 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-e6bd6a70-117b-4d11-a8cf-ab90129461f1
24/01/05 02:11:19 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-77e2afa3-ce3f-437c-a020-ebfc94bc90bd
+ /var/hoodie/ws/hudi-sync/hudi-hive-sync/run_sync_tool.sh --jdbc-url jdbc:hive2://hiveserver:10000 --user hive --pass hive --partitioned-by dt --base-path /user/hive/warehouse/stock_ticks_mor --database default --table stock_ticks_mor --partition-value-extractor org.apache.hudi.hive.SlashEncodedDayPartitionValueExtractor
Running Command : java -cp /opt/hive/lib/hive-metastore-2.3.3.jar::/opt/hive/lib/hive-service-2.3.3.jar::/opt/hive/lib/hive-exec-2.3.3.jar::/opt/hive/lib/hive-jdbc-2.3.3.jar:/opt/hive/lib/hive-jdbc-handler-2.3.3.jar::/opt/hive/lib/jackson-annotations-2.6.0.jar:/opt/hive/lib/jackson-core-2.6.5.jar:/opt/hive/lib/jackson-databind-2.6.5.jar:/opt/hive/lib/jackson-dataformat-smile-2.4.6.jar:/opt/hive/lib/jackson-datatype-guava-2.4.6.jar:/opt/hive/lib/jackson-datatype-joda-2.4.6.jar:/opt/hive/lib/jackson-jaxrs-1.9.13.jar:/opt/hive/lib/jackson-jaxrs-base-2.4.6.jar:/opt/hive/lib/jackson-jaxrs-json-provider-2.4.6.jar:/opt/hive/lib/jackson-jaxrs-smile-provider-2.4.6.jar:/opt/hive/lib/jackson-module-jaxb-annotations-2.4.6.jar:/opt/hive/lib/jackson-xc-1.9.13.jar::/opt/hadoop-2.8.4/share/hadoop/common/*:/opt/hadoop-2.8.4/share/hadoop/mapreduce/*:/opt/hadoop-2.8.4/share/hadoop/hdfs/*:/opt/hadoop-2.8.4/share/hadoop/common/lib/*:/opt/hadoop-2.8.4/share/hadoop/hdfs/lib/*:/etc/hadoop:/var/hoodie/ws/hudi-sync/hudi-hive-sync/../../packaging/hudi-hive-sync-bundle/target/hudi-hive-sync-bundle-0.14.1-rc2.jar org.apache.hudi.hive.HiveSyncTool --jdbc-url jdbc:hive2://hiveserver:10000 --user hive --pass hive --partitioned-by dt --base-path /user/hive/warehouse/stock_ticks_mor --database default --table stock_ticks_mor --partition-value-extractor org.apache.hudi.hive.SlashEncodedDayPartitionValueExtractor
2024-01-05 02:11:20,889 INFO  [main] conf.HiveConf (HiveConf.java:findConfigFile(181)) - Found configuration file file:/opt/hadoop-2.8.4/etc/hadoop/hive-site.xml
2024-01-05 02:11:21,459 WARN  [main] util.NativeCodeLoader (NativeCodeLoader.java:<clinit>(62)) - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2024-01-05 02:11:21,538 INFO  [main] table.HoodieTableMetaClient (HoodieTableMetaClient.java:<init>(133)) - Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
2024-01-05 02:11:22,005 INFO  [main] table.HoodieTableConfig (HoodieTableConfig.java:<init>(276)) - Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
2024-01-05 02:11:22,066 INFO  [main] table.HoodieTableMetaClient (HoodieTableMetaClient.java:<init>(152)) - Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
2024-01-05 02:11:22,066 INFO  [main] table.HoodieTableMetaClient (HoodieTableMetaClient.java:<init>(155)) - Loading Active commit timeline for /user/hive/warehouse/stock_ticks_mor
2024-01-05 02:11:22,089 INFO  [main] timeline.HoodieActiveTimeline (HoodieActiveTimeline.java:<init>(172)) - Loaded instants upto : Option{val=[20240105021110208__deltacommit__COMPLETED__20240105021118626]}
2024-01-05 02:11:22,347 INFO  [main] hive.metastore (HiveMetaStoreClient.java:open(407)) - Trying to connect to metastore with URI thrift://hivemetastore:9083
2024-01-05 02:11:22,365 INFO  [main] hive.metastore (HiveMetaStoreClient.java:open(481)) - Opened a connection to metastore, current connections: 1
2024-01-05 02:11:22,381 INFO  [main] hive.metastore (HiveMetaStoreClient.java:open(534)) - Connected to metastore.
2024-01-05 02:11:22,516 INFO  [main] jdbc.Utils (Utils.java:parseURL(325)) - Supplied authorities: hiveserver:10000
2024-01-05 02:11:22,516 INFO  [main] jdbc.Utils (Utils.java:parseURL(444)) - Resolved authority: hiveserver:10000
2024-01-05 02:11:22,916 INFO  [main] ddl.QueryBasedDDLExecutor (JDBCExecutor.java:createHiveConnection(105)) - Successfully established Hive connection to  jdbc:hive2://hiveserver:10000
2024-01-05 02:11:22,917 INFO  [main] hive.HiveSyncTool (HiveSyncTool.java:syncHoodieTable(162)) - Syncing target hoodie table with hive table(default.stock_ticks_mor). Hive metastore URL from HiveConf:thrift://hivemetastore:9083). Hive metastore URL from HiveSyncConfig:null, basePath :/user/hive/warehouse/stock_ticks_mor
2024-01-05 02:11:22,917 INFO  [main] hive.HiveSyncTool (HiveSyncTool.java:syncHoodieTable(224)) - Trying to sync hoodie table stock_ticks_mor_ro with base path /user/hive/warehouse/stock_ticks_mor of type MERGE_ON_READ
2024-01-05 02:11:23,271 INFO  [main] table.TableSchemaResolver (TableSchemaResolver.java:readSchemaFromParquetBaseFile(329)) - Reading schema from /user/hive/warehouse/stock_ticks_mor/2018/08/31/63b57973-43fe-40da-bb65-66fab049ab22-0_0-23-24_20240105021110208.parquet
SLF4J: Failed to load class "org.slf4j.impl.StaticLoggerBinder".
SLF4J: Defaulting to no-operation (NOP) logger implementation
SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.
2024-01-05 02:11:23,504 INFO  [main] hive.HiveSyncTool (HiveSyncTool.java:syncFirstTime(321)) - Sync table stock_ticks_mor_ro for the first time.
2024-01-05 02:11:23,514 INFO  [main] ddl.QueryBasedDDLExecutor (QueryBasedDDLExecutor.java:createTable(91)) - Creating table with CREATE EXTERNAL TABLE IF NOT EXISTS `default`.`stock_ticks_mor_ro`( `_hoodie_commit_time` string, `_hoodie_commit_seqno` string, `_hoodie_record_key` string, `_hoodie_partition_path` string, `_hoodie_file_name` string, `volume` bigint, `ts` string, `symbol` string, `year` int, `month` string, `high` double, `low` double, `key` string, `date` string, `close` double, `open` double, `day` string) PARTITIONED BY (`dt` string) ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe' WITH SERDEPROPERTIES ('hoodie.query.as.ro.table'='true','path'='/user/hive/warehouse/stock_ticks_mor') STORED AS INPUTFORMAT 'org.apache.hudi.hadoop.HoodieParquetInputFormat' OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat' LOCATION '/user/hive/warehouse/stock_ticks_mor' TBLPROPERTIES('spark.sql.sources.schema.partCol.0'='dt','spark.sql.sources.schema.numParts'='1','spark.sql.sources.schema.numPartCols'='1','spark.sql.sources.provider'='hudi','spark.sql.sources.schema.part.0'='{"type":"struct","fields":[{"name":"_hoodie_commit_time","type":"string","nullable":true,"metadata":{}},{"name":"_hoodie_commit_seqno","type":"string","nullable":true,"metadata":{}},{"name":"_hoodie_record_key","type":"string","nullable":true,"metadata":{}},{"name":"_hoodie_partition_path","type":"string","nullable":true,"metadata":{}},{"name":"_hoodie_file_name","type":"string","nullable":true,"metadata":{}},{"name":"volume","type":"long","nullable":false,"metadata":{}},{"name":"ts","type":"string","nullable":false,"metadata":{}},{"name":"symbol","type":"string","nullable":false,"metadata":{}},{"name":"year","type":"integer","nullable":false,"metadata":{}},{"name":"month","type":"string","nullable":false,"metadata":{}},{"name":"high","type":"double","nullable":false,"metadata":{}},{"name":"low","type":"double","nullable":false,"metadata":{}},{"name":"key","type":"string","nullable":false,"metadata":{}},{"name":"date","type":"string","nullable":false,"metadata":{}},{"name":"close","type":"double","nullable":false,"metadata":{}},{"name":"open","type":"double","nullable":false,"metadata":{}},{"name":"day","type":"string","nullable":false,"metadata":{}},{"name":"dt","type":"string","nullable":false,"metadata":{}}]}')
2024-01-05 02:11:23,516 INFO  [main] ddl.QueryBasedDDLExecutor (JDBCExecutor.java:runSQL(67)) - Executing SQL CREATE EXTERNAL TABLE IF NOT EXISTS `default`.`stock_ticks_mor_ro`( `_hoodie_commit_time` string, `_hoodie_commit_seqno` string, `_hoodie_record_key` string, `_hoodie_partition_path` string, `_hoodie_file_name` string, `volume` bigint, `ts` string, `symbol` string, `year` int, `month` string, `high` double, `low` double, `key` string, `date` string, `close` double, `open` double, `day` string) PARTITIONED BY (`dt` string) ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe' WITH SERDEPROPERTIES ('hoodie.query.as.ro.table'='true','path'='/user/hive/warehouse/stock_ticks_mor') STORED AS INPUTFORMAT 'org.apache.hudi.hadoop.HoodieParquetInputFormat' OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat' LOCATION '/user/hive/warehouse/stock_ticks_mor' TBLPROPERTIES('spark.sql.sources.schema.partCol.0'='dt','spark.sql.sources.schema.numParts'='1','spark.sql.sources.schema.numPartCols'='1','spark.sql.sources.provider'='hudi','spark.sql.sources.schema.part.0'='{"type":"struct","fields":[{"name":"_hoodie_commit_time","type":"string","nullable":true,"metadata":{}},{"name":"_hoodie_commit_seqno","type":"string","nullable":true,"metadata":{}},{"name":"_hoodie_record_key","type":"string","nullable":true,"metadata":{}},{"name":"_hoodie_partition_path","type":"string","nullable":true,"metadata":{}},{"name":"_hoodie_file_name","type":"string","nullable":true,"metadata":{}},{"name":"volume","type":"long","nullable":false,"metadata":{}},{"name":"ts","type":"string","nullable":false,"metadata":{}},{"name":"symbol","type":"string","nullable":false,"metadata":{}},{"name":"year","type":"integer","nullable":false,"metadata":{}},{"name":"month","type":"string","nullable":false,"metadata":{}},{"name":"high","type":"double","nullable":false,"metadata":{}},{"name":"low","type":"double","nullable":false,"metadata":{}},{"name":"key","type":"string","nullable":false,"metadata":{}},{"name":"date","type":"string","nullable":false,"metadata":{}},{"name":"close","type":"double","nullable":false,"metadata":{}},{"name":"open","type":"double","nullable":false,"metadata":{}},{"name":"day","type":"string","nullable":false,"metadata":{}},{"name":"dt","type":"string","nullable":false,"metadata":{}}]}')
2024-01-05 02:11:24,466 INFO  [main] hive.HiveSyncTool (HiveSyncTool.java:syncHoodieTable(265)) - Last commit time synced was found to be null, last commit completion time is found to be null
2024-01-05 02:11:24,467 INFO  [main] hive.HiveSyncTool (HiveSyncTool.java:syncHoodieTable(279)) - Sync all partitions given the last commit time synced is empty or before the start of the active timeline. Listing all partitions in /user/hive/warehouse/stock_ticks_mor, file system: DFS[DFSClient[clientName=DFSClient_NONMAPREDUCE_-1299929014_1, ugi=root (auth:SIMPLE)]]
2024-01-05 02:11:24,516 INFO  [main] table.HoodieTableConfig (HoodieTableConfig.java:<init>(276)) - Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
2024-01-05 02:11:24,541 INFO  [main] hive.HiveSyncTool (HiveSyncTool.java:syncPartitions(459)) - New Partitions [2018/08/31]
2024-01-05 02:11:24,541 INFO  [main] ddl.QueryBasedDDLExecutor (QueryBasedDDLExecutor.java:addPartitionsToTable(122)) - Adding partitions 1 to table stock_ticks_mor_ro
2024-01-05 02:11:24,542 INFO  [main] ddl.QueryBasedDDLExecutor (JDBCExecutor.java:runSQL(67)) - Executing SQL ALTER TABLE `default`.`stock_ticks_mor_ro` ADD IF NOT EXISTS   PARTITION (`dt`='2018-08-31') LOCATION '/user/hive/warehouse/stock_ticks_mor/2018/08/31' 
2024-01-05 02:11:24,789 INFO  [main] hive.HiveSyncTool (HiveSyncTool.java:syncHoodieTable(298)) - Sync complete for stock_ticks_mor_ro
2024-01-05 02:11:24,789 INFO  [main] hive.HiveSyncTool (HiveSyncTool.java:syncHoodieTable(224)) - Trying to sync hoodie table stock_ticks_mor_rt with base path /user/hive/warehouse/stock_ticks_mor of type MERGE_ON_READ
2024-01-05 02:11:24,802 INFO  [main] table.TableSchemaResolver (TableSchemaResolver.java:readSchemaFromParquetBaseFile(329)) - Reading schema from /user/hive/warehouse/stock_ticks_mor/2018/08/31/63b57973-43fe-40da-bb65-66fab049ab22-0_0-23-24_20240105021110208.parquet
2024-01-05 02:11:24,818 INFO  [main] hive.HiveSyncTool (HiveSyncTool.java:syncFirstTime(321)) - Sync table stock_ticks_mor_rt for the first time.
2024-01-05 02:11:24,819 INFO  [main] ddl.QueryBasedDDLExecutor (QueryBasedDDLExecutor.java:createTable(91)) - Creating table with CREATE EXTERNAL TABLE IF NOT EXISTS `default`.`stock_ticks_mor_rt`( `_hoodie_commit_time` string, `_hoodie_commit_seqno` string, `_hoodie_record_key` string, `_hoodie_partition_path` string, `_hoodie_file_name` string, `volume` bigint, `ts` string, `symbol` string, `year` int, `month` string, `high` double, `low` double, `key` string, `date` string, `close` double, `open` double, `day` string) PARTITIONED BY (`dt` string) ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe' WITH SERDEPROPERTIES ('hoodie.query.as.ro.table'='false','path'='/user/hive/warehouse/stock_ticks_mor') STORED AS INPUTFORMAT 'org.apache.hudi.hadoop.realtime.HoodieParquetRealtimeInputFormat' OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat' LOCATION '/user/hive/warehouse/stock_ticks_mor' TBLPROPERTIES('spark.sql.sources.schema.partCol.0'='dt','spark.sql.sources.schema.numParts'='1','spark.sql.sources.schema.numPartCols'='1','spark.sql.sources.provider'='hudi','spark.sql.sources.schema.part.0'='{"type":"struct","fields":[{"name":"_hoodie_commit_time","type":"string","nullable":true,"metadata":{}},{"name":"_hoodie_commit_seqno","type":"string","nullable":true,"metadata":{}},{"name":"_hoodie_record_key","type":"string","nullable":true,"metadata":{}},{"name":"_hoodie_partition_path","type":"string","nullable":true,"metadata":{}},{"name":"_hoodie_file_name","type":"string","nullable":true,"metadata":{}},{"name":"volume","type":"long","nullable":false,"metadata":{}},{"name":"ts","type":"string","nullable":false,"metadata":{}},{"name":"symbol","type":"string","nullable":false,"metadata":{}},{"name":"year","type":"integer","nullable":false,"metadata":{}},{"name":"month","type":"string","nullable":false,"metadata":{}},{"name":"high","type":"double","nullable":false,"metadata":{}},{"name":"low","type":"double","nullable":false,"metadata":{}},{"name":"key","type":"string","nullable":false,"metadata":{}},{"name":"date","type":"string","nullable":false,"metadata":{}},{"name":"close","type":"double","nullable":false,"metadata":{}},{"name":"open","type":"double","nullable":false,"metadata":{}},{"name":"day","type":"string","nullable":false,"metadata":{}},{"name":"dt","type":"string","nullable":false,"metadata":{}}]}')
2024-01-05 02:11:24,819 INFO  [main] ddl.QueryBasedDDLExecutor (JDBCExecutor.java:runSQL(67)) - Executing SQL CREATE EXTERNAL TABLE IF NOT EXISTS `default`.`stock_ticks_mor_rt`( `_hoodie_commit_time` string, `_hoodie_commit_seqno` string, `_hoodie_record_key` string, `_hoodie_partition_path` string, `_hoodie_file_name` string, `volume` bigint, `ts` string, `symbol` string, `year` int, `month` string, `high` double, `low` double, `key` string, `date` string, `close` double, `open` double, `day` string) PARTITIONED BY (`dt` string) ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe' WITH SERDEPROPERTIES ('hoodie.query.as.ro.table'='false','path'='/user/hive/warehouse/stock_ticks_mor') STORED AS INPUTFORMAT 'org.apache.hudi.hadoop.realtime.HoodieParquetRealtimeInputFormat' OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat' LOCATION '/user/hive/warehouse/stock_ticks_mor' TBLPROPERTIES('spark.sql.sources.schema.partCol.0'='dt','spark.sql.sources.schema.numParts'='1','spark.sql.sources.schema.numPartCols'='1','spark.sql.sources.provider'='hudi','spark.sql.sources.schema.part.0'='{"type":"struct","fields":[{"name":"_hoodie_commit_time","type":"string","nullable":true,"metadata":{}},{"name":"_hoodie_commit_seqno","type":"string","nullable":true,"metadata":{}},{"name":"_hoodie_record_key","type":"string","nullable":true,"metadata":{}},{"name":"_hoodie_partition_path","type":"string","nullable":true,"metadata":{}},{"name":"_hoodie_file_name","type":"string","nullable":true,"metadata":{}},{"name":"volume","type":"long","nullable":false,"metadata":{}},{"name":"ts","type":"string","nullable":false,"metadata":{}},{"name":"symbol","type":"string","nullable":false,"metadata":{}},{"name":"year","type":"integer","nullable":false,"metadata":{}},{"name":"month","type":"string","nullable":false,"metadata":{}},{"name":"high","type":"double","nullable":false,"metadata":{}},{"name":"low","type":"double","nullable":false,"metadata":{}},{"name":"key","type":"string","nullable":false,"metadata":{}},{"name":"date","type":"string","nullable":false,"metadata":{}},{"name":"close","type":"double","nullable":false,"metadata":{}},{"name":"open","type":"double","nullable":false,"metadata":{}},{"name":"day","type":"string","nullable":false,"metadata":{}},{"name":"dt","type":"string","nullable":false,"metadata":{}}]}')
2024-01-05 02:11:24,878 INFO  [main] hive.HiveSyncTool (HiveSyncTool.java:syncHoodieTable(265)) - Last commit time synced was found to be null, last commit completion time is found to be null
2024-01-05 02:11:24,878 INFO  [main] hive.HiveSyncTool (HiveSyncTool.java:syncHoodieTable(279)) - Sync all partitions given the last commit time synced is empty or before the start of the active timeline. Listing all partitions in /user/hive/warehouse/stock_ticks_mor, file system: DFS[DFSClient[clientName=DFSClient_NONMAPREDUCE_-1299929014_1, ugi=root (auth:SIMPLE)]]
2024-01-05 02:11:24,898 INFO  [main] table.HoodieTableConfig (HoodieTableConfig.java:<init>(276)) - Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
2024-01-05 02:11:24,919 INFO  [main] hive.HiveSyncTool (HiveSyncTool.java:syncPartitions(459)) - New Partitions [2018/08/31]
2024-01-05 02:11:24,919 INFO  [main] ddl.QueryBasedDDLExecutor (QueryBasedDDLExecutor.java:addPartitionsToTable(122)) - Adding partitions 1 to table stock_ticks_mor_rt
2024-01-05 02:11:24,920 INFO  [main] ddl.QueryBasedDDLExecutor (JDBCExecutor.java:runSQL(67)) - Executing SQL ALTER TABLE `default`.`stock_ticks_mor_rt` ADD IF NOT EXISTS   PARTITION (`dt`='2018-08-31') LOCATION '/user/hive/warehouse/stock_ticks_mor/2018/08/31' 
2024-01-05 02:11:25,131 INFO  [main] hive.HiveSyncTool (HiveSyncTool.java:syncHoodieTable(298)) - Sync complete for stock_ticks_mor_rt
2024-01-05 02:11:25,174 INFO  [main] hive.metastore (HiveMetaStoreClient.java:close(564)) - Closed a connection to metastore, current connections: 0
+ cat /home/alex/github/alexttx/hudi/docker/demo/data/batch_2.json
+ head -n100
+ kcat -b kafkabroker -t stock_ticks -P
+ docker exec -i adhoc-2 /bin/bash -x
+ spark-submit --class org.apache.hudi.utilities.streamer.HoodieStreamer /var/hoodie/ws/docker/hoodie/hadoop/hive_base/target/hoodie-utilities.jar --table-type MERGE_ON_READ --source-class org.apache.hudi.utilities.sources.JsonKafkaSource --source-ordering-field ts --target-base-path /user/hive/warehouse/stock_ticks_mor --target-table stock_ticks_mor --props /var/demo/config/kafka-source.properties --schemaprovider-class org.apache.hudi.utilities.schema.FilebasedSchemaProvider --disable-compaction
24/01/05 02:11:26 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
24/01/05 02:11:27 WARN streamer.SchedulerConfGenerator: Job Scheduling Configs will not be in effect as spark.scheduler.mode is not set to FAIR at instantiation time. Continuing without scheduling configs
24/01/05 02:11:27 INFO spark.SparkContext: Running Spark version 2.4.4
24/01/05 02:11:27 INFO spark.SparkContext: Submitted application: streamer-stock_ticks_mor
24/01/05 02:11:27 INFO spark.SecurityManager: Changing view acls to: root
24/01/05 02:11:27 INFO spark.SecurityManager: Changing modify acls to: root
24/01/05 02:11:27 INFO spark.SecurityManager: Changing view acls groups to: 
24/01/05 02:11:27 INFO spark.SecurityManager: Changing modify acls groups to: 
24/01/05 02:11:27 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
24/01/05 02:11:27 INFO Configuration.deprecation: mapred.output.compression.codec is deprecated. Instead, use mapreduce.output.fileoutputformat.compress.codec
24/01/05 02:11:27 INFO Configuration.deprecation: mapred.output.compress is deprecated. Instead, use mapreduce.output.fileoutputformat.compress
24/01/05 02:11:27 INFO Configuration.deprecation: mapred.output.compression.type is deprecated. Instead, use mapreduce.output.fileoutputformat.compress.type
24/01/05 02:11:27 INFO util.Utils: Successfully started service 'sparkDriver' on port 34903.
24/01/05 02:11:27 INFO spark.SparkEnv: Registering MapOutputTracker
24/01/05 02:11:27 INFO spark.SparkEnv: Registering BlockManagerMaster
24/01/05 02:11:27 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
24/01/05 02:11:27 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
24/01/05 02:11:27 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-4e9cae05-56f5-4d76-880f-81068a081063
24/01/05 02:11:27 INFO memory.MemoryStore: MemoryStore started with capacity 366.3 MB
24/01/05 02:11:27 INFO spark.SparkEnv: Registering OutputCommitCoordinator
24/01/05 02:11:27 INFO util.log: Logging initialized @2039ms
24/01/05 02:11:27 INFO server.Server: jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
24/01/05 02:11:27 INFO server.Server: Started @2118ms
24/01/05 02:11:27 INFO server.AbstractConnector: Started ServerConnector@26f3d90c{HTTP/1.1,[http/1.1]}{0.0.0.0:8090}
24/01/05 02:11:27 INFO util.Utils: Successfully started service 'SparkUI' on port 8090.
24/01/05 02:11:27 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6c2f1700{/jobs,null,AVAILABLE,@Spark}
24/01/05 02:11:27 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7d2a6eac{/jobs/json,null,AVAILABLE,@Spark}
24/01/05 02:11:27 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@18ca3c62{/jobs/job,null,AVAILABLE,@Spark}
24/01/05 02:11:27 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@44d70181{/jobs/job/json,null,AVAILABLE,@Spark}
24/01/05 02:11:27 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6aa648b9{/stages,null,AVAILABLE,@Spark}
24/01/05 02:11:27 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@23c650a3{/stages/json,null,AVAILABLE,@Spark}
24/01/05 02:11:27 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@742d4e15{/stages/stage,null,AVAILABLE,@Spark}
24/01/05 02:11:27 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4163f1cd{/stages/stage/json,null,AVAILABLE,@Spark}
24/01/05 02:11:27 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5fa05212{/stages/pool,null,AVAILABLE,@Spark}
24/01/05 02:11:27 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3e681bc{/stages/pool/json,null,AVAILABLE,@Spark}
24/01/05 02:11:27 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5c09d180{/storage,null,AVAILABLE,@Spark}
24/01/05 02:11:27 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@23aae55{/storage/json,null,AVAILABLE,@Spark}
24/01/05 02:11:27 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5f574cc2{/storage/rdd,null,AVAILABLE,@Spark}
24/01/05 02:11:27 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@680bddf5{/storage/rdd/json,null,AVAILABLE,@Spark}
24/01/05 02:11:27 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7a9c84a5{/environment,null,AVAILABLE,@Spark}
24/01/05 02:11:27 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2d83c5a5{/environment/json,null,AVAILABLE,@Spark}
24/01/05 02:11:27 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@48d7ad8b{/executors,null,AVAILABLE,@Spark}
24/01/05 02:11:27 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7e053511{/executors/json,null,AVAILABLE,@Spark}
24/01/05 02:11:27 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@60222fd8{/executors/threadDump,null,AVAILABLE,@Spark}
24/01/05 02:11:27 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@53bf7094{/executors/threadDump/json,null,AVAILABLE,@Spark}
24/01/05 02:11:27 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@26f1249d{/static,null,AVAILABLE,@Spark}
24/01/05 02:11:27 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7af707e0{/,null,AVAILABLE,@Spark}
24/01/05 02:11:27 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@737edcfa{/api,null,AVAILABLE,@Spark}
24/01/05 02:11:27 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3359c978{/jobs/job/kill,null,AVAILABLE,@Spark}
24/01/05 02:11:27 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7ba63fe5{/stages/stage/kill,null,AVAILABLE,@Spark}
24/01/05 02:11:27 INFO ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://adhoc-2:8090
24/01/05 02:11:27 INFO spark.SparkContext: Added JAR file:/var/hoodie/ws/docker/hoodie/hadoop/hive_base/target/hoodie-utilities.jar at spark://adhoc-2:34903/jars/hoodie-utilities.jar with timestamp 1704420687947
24/01/05 02:11:28 INFO executor.Executor: Starting executor ID driver on host localhost
24/01/05 02:11:28 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38731.
24/01/05 02:11:28 INFO netty.NettyBlockTransferService: Server created on adhoc-2:38731
24/01/05 02:11:28 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
24/01/05 02:11:28 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, adhoc-2, 38731, None)
24/01/05 02:11:28 INFO storage.BlockManagerMasterEndpoint: Registering block manager adhoc-2:38731 with 366.3 MB RAM, BlockManagerId(driver, adhoc-2, 38731, None)
24/01/05 02:11:28 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, adhoc-2, 38731, None)
24/01/05 02:11:28 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, adhoc-2, 38731, None)
24/01/05 02:11:28 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@31ee96f4{/metrics/json,null,AVAILABLE,@Spark}
24/01/05 02:11:29 WARN config.DFSPropertiesConfiguration: Cannot find HUDI_CONF_DIR, please set it as the dir of hudi-defaults.conf
24/01/05 02:11:29 WARN config.DFSPropertiesConfiguration: Properties file file:/etc/hudi/conf/hudi-defaults.conf not found. Ignoring to load props file
24/01/05 02:11:29 INFO internal.SharedState: loading hive config file: file:/opt/hadoop-2.8.4/etc/hadoop/hive-site.xml
24/01/05 02:11:29 INFO internal.SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/opt/spark-warehouse').
24/01/05 02:11:29 INFO internal.SharedState: Warehouse path is 'file:/opt/spark-warehouse'.
24/01/05 02:11:29 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1a0d96a5{/SQL,null,AVAILABLE,@Spark}
24/01/05 02:11:29 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5a02bfe3{/SQL/json,null,AVAILABLE,@Spark}
24/01/05 02:11:29 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5854a18{/SQL/execution,null,AVAILABLE,@Spark}
24/01/05 02:11:29 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@d5556bf{/SQL/execution/json,null,AVAILABLE,@Spark}
24/01/05 02:11:29 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1abebef3{/static/sql,null,AVAILABLE,@Spark}
24/01/05 02:11:29 INFO state.StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
24/01/05 02:11:29 WARN sql.SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
24/01/05 02:11:29 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:11:29 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:11:29 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:11:29 INFO factory.HoodieSparkKeyGeneratorFactory: The value of hoodie.datasource.write.keygenerator.type is empty; inferred to be SIMPLE
24/01/05 02:11:29 INFO streamer.HoodieStreamer: Creating Hudi Streamer with configs:
auto.offset.reset: earliest
bootstrap.servers: kafkabroker:9092
hoodie.auto.adjust.lock.configs: true
hoodie.bulkinsert.shuffle.parallelism: 2
hoodie.compact.inline: false
hoodie.datasource.write.partitionpath.field: date
hoodie.datasource.write.reconcile.schema: false
hoodie.datasource.write.recordkey.field: key
hoodie.delete.shuffle.parallelism: 2
hoodie.embed.timeline.server: true
hoodie.filesystem.view.type: EMBEDDED_KV_STORE
hoodie.insert.shuffle.parallelism: 2
hoodie.streamer.schemaprovider.source.schema.file: /var/demo/config/schema.avsc
hoodie.streamer.schemaprovider.target.schema.file: /var/demo/config/schema.avsc
hoodie.streamer.source.kafka.topic: stock_ticks
hoodie.table.type: MERGE_ON_READ
hoodie.upsert.shuffle.parallelism: 2

24/01/05 02:11:29 INFO fs.FSUtils: Resolving file /var/demo/config/schema.avscto be a remote file.
24/01/05 02:11:29 INFO factory.HoodieSparkKeyGeneratorFactory: The value of hoodie.datasource.write.keygenerator.type is empty; inferred to be SIMPLE
24/01/05 02:11:29 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:11:29 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:11:29 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:11:29 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105021110208__deltacommit__COMPLETED__20240105021118626]}
24/01/05 02:11:30 INFO config.HoodieWriteConfig: Automatically set hoodie.write.lock.provider=org.apache.hudi.client.transaction.lock.InProcessLockProvider since user has not set the lock provider for single writer with async table services
24/01/05 02:11:30 INFO config.HoodieWriteConfig: Automatically set hoodie.write.lock.provider=org.apache.hudi.client.transaction.lock.InProcessLockProvider since user has not set the lock provider for single writer with async table services
24/01/05 02:11:30 INFO ingestion.HoodieIngestionService: Ingestion service starts running in run-once mode
24/01/05 02:11:30 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:11:30 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:11:30 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:11:30 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105021110208__deltacommit__COMPLETED__20240105021118626]}
24/01/05 02:11:30 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:11:30 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:11:30 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:11:30 INFO streamer.StreamSync: Checkpoint to resume from : Option{val=stock_ticks,0:2611}
24/01/05 02:11:30 INFO consumer.ConsumerConfig: ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [kafkabroker:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

24/01/05 02:11:30 INFO utils.AppInfoParser: Kafka version : 2.0.0
24/01/05 02:11:30 INFO utils.AppInfoParser: Kafka commitId : 3402a8361b734732
24/01/05 02:11:30 INFO clients.Metadata: Cluster ID: OtZ13RgXQhuquRcIZizanw
24/01/05 02:11:30 INFO helpers.KafkaOffsetGen: SourceLimit not configured, set numEvents to default value : 5000000
24/01/05 02:11:30 INFO helpers.KafkaOffsetGen: getNextOffsetRanges set config hoodie.streamer.source.kafka.minPartitions to 0
24/01/05 02:11:30 INFO sources.KafkaSource: About to read 100 from Kafka for topic :stock_ticks
24/01/05 02:11:30 WARN kafka010.KafkaUtils: overriding enable.auto.commit to false for executor
24/01/05 02:11:30 WARN kafka010.KafkaUtils: overriding auto.offset.reset to none for executor
24/01/05 02:11:30 ERROR kafka010.KafkaUtils: group.id is null, you should probably set it
24/01/05 02:11:30 WARN kafka010.KafkaUtils: overriding executor group.id to spark-executor-null
24/01/05 02:11:30 WARN kafka010.KafkaUtils: overriding receive.buffer.bytes to 65536 see KAFKA-3135
24/01/05 02:11:30 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105021110208__deltacommit__COMPLETED__20240105021118626]}
24/01/05 02:11:30 INFO factory.HoodieSparkKeyGeneratorFactory: The value of hoodie.datasource.write.keygenerator.type is empty; inferred to be SIMPLE
24/01/05 02:11:30 INFO streamer.StreamSync: Setting up new Hoodie Write Client
24/01/05 02:11:30 INFO config.HoodieWriteConfig: Automatically set hoodie.write.lock.provider=org.apache.hudi.client.transaction.lock.InProcessLockProvider since user has not set the lock provider for single writer with async table services
24/01/05 02:11:30 INFO embedded.EmbeddedTimelineService: Overriding hostIp to (adhoc-2) found in spark-conf. It was null
24/01/05 02:11:30 INFO view.FileSystemViewManager: Creating View Manager with storage type :EMBEDDED_KV_STORE
24/01/05 02:11:30 INFO view.FileSystemViewManager: Creating embedded rocks-db based Table View
24/01/05 02:11:30 INFO util.log: Logging initialized @5081ms to org.apache.hudi.org.eclipse.jetty.util.log.Slf4jLog
24/01/05 02:11:31 INFO javalin.Javalin: 
       __                      __ _            __ __
      / /____ _ _   __ ____ _ / /(_)____      / // /
 __  / // __ `/| | / // __ `// // // __ \    / // /_
/ /_/ // /_/ / | |/ // /_/ // // // / / /   /__  __/
\____/ \__,_/  |___/ \__,_//_//_//_/ /_/      /_/

          https://javalin.io/documentation

24/01/05 02:11:31 INFO javalin.Javalin: Starting Javalin ...
24/01/05 02:11:31 INFO javalin.Javalin: You are running Javalin 4.6.7 (released October 24, 2022. Your Javalin version is 437 days old. Consider checking for a newer version.).
24/01/05 02:11:31 INFO server.Server: jetty-9.4.48.v20220622; built: 2022-06-21T20:42:25.880Z; git: 6b67c5719d1f4371b33655ff2d047d24e171e49a; jvm 1.8.0_212-b04
24/01/05 02:11:31 INFO server.Server: Started @5675ms
24/01/05 02:11:31 INFO javalin.Javalin: Listening on http://localhost:35479/
24/01/05 02:11:31 INFO javalin.Javalin: Javalin started in 259ms \o/
24/01/05 02:11:31 INFO service.TimelineService: Starting Timeline server on port :35479
24/01/05 02:11:31 INFO embedded.EmbeddedTimelineService: Started embedded timeline server at adhoc-2:35479
24/01/05 02:11:31 INFO client.BaseHoodieClient: Timeline Server already running. Not restarting the service
24/01/05 02:11:31 INFO client.BaseHoodieClient: Timeline Server already running. Not restarting the service
24/01/05 02:11:31 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:11:31 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:11:31 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:11:31 INFO table.HoodieTableMetaClient: Loading Active commit timeline for /user/hive/warehouse/stock_ticks_mor
24/01/05 02:11:31 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105021110208__deltacommit__COMPLETED__20240105021118626]}
24/01/05 02:11:31 INFO util.CleanerUtils: Cleaned failed attempts if any
24/01/05 02:11:31 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:11:31 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:11:31 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:11:31 INFO table.HoodieTableMetaClient: Loading Active commit timeline for /user/hive/warehouse/stock_ticks_mor
24/01/05 02:11:31 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105021110208__deltacommit__COMPLETED__20240105021118626]}
24/01/05 02:11:31 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:11:31 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:11:31 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:11:31 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:11:31 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:11:31 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:11:31 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105021110208__deltacommit__COMPLETED__20240105021118540]}
24/01/05 02:11:31 INFO view.AbstractTableFileSystemView: Took 3 ms to read  0 instants, 0 replaced file groups
24/01/05 02:11:31 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:11:31 INFO view.FileSystemViewManager: Creating View Manager with storage type :REMOTE_FIRST
24/01/05 02:11:31 INFO view.FileSystemViewManager: Creating remote first table view
24/01/05 02:11:31 INFO client.BaseHoodieWriteClient: Generate a new instant time: 20240105021130096 action: deltacommit
24/01/05 02:11:31 INFO timeline.HoodieActiveTimeline: Creating a new instant [==>20240105021130096__deltacommit__REQUESTED]
24/01/05 02:11:31 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:11:31 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:11:31 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:11:31 INFO table.HoodieTableMetaClient: Loading Active commit timeline for /user/hive/warehouse/stock_ticks_mor
24/01/05 02:11:31 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[==>20240105021130096__deltacommit__REQUESTED__20240105021131599]}
24/01/05 02:11:31 INFO transaction.TransactionManager: Transaction starting for Option{val=[==>20240105021130096__deltacommit__INFLIGHT]} with latest completed transaction instant Optional.empty
24/01/05 02:11:31 INFO lock.LockManager: LockProvider org.apache.hudi.client.transaction.lock.InProcessLockProvider
24/01/05 02:11:31 INFO lock.InProcessLockProvider: Base Path /user/hive/warehouse/stock_ticks_mor, Lock Instance java.util.concurrent.locks.ReentrantReadWriteLock@2e204155[Write locks = 0, Read locks = 0], Thread main, In-process lock state ACQUIRING
24/01/05 02:11:31 INFO lock.InProcessLockProvider: Base Path /user/hive/warehouse/stock_ticks_mor, Lock Instance java.util.concurrent.locks.ReentrantReadWriteLock@2e204155[Write locks = 1, Read locks = 0], Thread main, In-process lock state ACQUIRED
24/01/05 02:11:31 INFO transaction.TransactionManager: Transaction started for Option{val=[==>20240105021130096__deltacommit__INFLIGHT]} with latest completed transaction instant Optional.empty
24/01/05 02:11:31 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:11:31 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:11:31 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:11:31 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:11:31 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:11:31 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:11:31 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105021110208__deltacommit__COMPLETED__20240105021118540]}
24/01/05 02:11:31 INFO metadata.HoodieBackedTableMetadataWriter: Async metadata indexing disabled and following partitions already initialized: [files]
24/01/05 02:11:31 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:11:31 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:11:31 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:11:31 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:11:31 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:11:31 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:11:31 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105021110208__deltacommit__COMPLETED__20240105021118540]}
24/01/05 02:11:31 INFO view.AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
24/01/05 02:11:31 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:11:31 INFO client.BaseHoodieClient: Embedded Timeline Server is disabled. Not starting timeline service
24/01/05 02:11:31 INFO client.BaseHoodieClient: Embedded Timeline Server is disabled. Not starting timeline service
24/01/05 02:11:31 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:11:31 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:11:31 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:11:31 INFO table.HoodieTableMetaClient: Loading Active commit timeline for /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:11:31 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105021110208__deltacommit__COMPLETED__20240105021118540]}
24/01/05 02:11:31 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:11:31 INFO view.FileSystemViewManager: Creating View Manager with storage type :MEMORY
24/01/05 02:11:31 INFO view.FileSystemViewManager: Creating in-memory based Table View
24/01/05 02:11:31 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:11:31 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:11:31 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:11:31 INFO table.HoodieTableMetaClient: Loading Active commit timeline for /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:11:31 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105021110208__deltacommit__COMPLETED__20240105021118540]}
24/01/05 02:11:31 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:11:31 INFO view.FileSystemViewManager: Creating View Manager with storage type :MEMORY
24/01/05 02:11:31 INFO view.FileSystemViewManager: Creating in-memory based Table View
24/01/05 02:11:31 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105021110208__deltacommit__COMPLETED__20240105021118540]}
24/01/05 02:11:31 INFO metadata.HoodieBackedTableMetadataWriter: Latest deltacommit time found is 20240105021110208, running clean operations.
24/01/05 02:11:31 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105021110208__deltacommit__COMPLETED__20240105021118540]}
24/01/05 02:11:31 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:11:31 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:11:31 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:11:31 INFO table.HoodieTableMetaClient: Loading Active commit timeline for /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:11:31 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105021110208__deltacommit__COMPLETED__20240105021118540]}
24/01/05 02:11:31 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:11:31 INFO view.FileSystemViewManager: Creating View Manager with storage type :MEMORY
24/01/05 02:11:31 INFO view.FileSystemViewManager: Creating in-memory based Table View
24/01/05 02:11:31 INFO client.BaseHoodieWriteClient: Cleaner started
24/01/05 02:11:31 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:11:31 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:11:31 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:11:31 INFO table.HoodieTableMetaClient: Loading Active commit timeline for /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:11:31 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105021110208__deltacommit__COMPLETED__20240105021118540]}
24/01/05 02:11:31 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:11:31 INFO view.FileSystemViewManager: Creating View Manager with storage type :MEMORY
24/01/05 02:11:31 INFO view.FileSystemViewManager: Creating in-memory based Table View
24/01/05 02:11:31 INFO client.BaseHoodieWriteClient: Scheduling cleaning at instant time :20240105021110208002
24/01/05 02:11:31 INFO view.FileSystemViewManager: Creating InMemory based view for basePath /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:11:31 INFO view.AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
24/01/05 02:11:31 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:11:31 INFO clean.CleanPlanner: No earliest commit to retain. No need to scan partitions !!
24/01/05 02:11:31 INFO clean.CleanPlanActionExecutor: Nothing to clean here. It is already clean
24/01/05 02:11:31 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105021110208__deltacommit__COMPLETED__20240105021118540]}
24/01/05 02:11:31 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:11:31 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:11:31 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:11:31 INFO table.HoodieTableMetaClient: Loading Active commit timeline for /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:11:31 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105021110208__deltacommit__COMPLETED__20240105021118540]}
24/01/05 02:11:31 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:11:31 INFO view.FileSystemViewManager: Creating View Manager with storage type :MEMORY
24/01/05 02:11:31 INFO view.FileSystemViewManager: Creating in-memory based Table View
24/01/05 02:11:31 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[==>20240105021130096__deltacommit__REQUESTED__20240105021131599]}
24/01/05 02:11:31 INFO client.BaseHoodieWriteClient: Scheduling table service COMPACT
24/01/05 02:11:31 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:11:31 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:11:31 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:11:31 INFO table.HoodieTableMetaClient: Loading Active commit timeline for /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:11:31 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105021110208__deltacommit__COMPLETED__20240105021118540]}
24/01/05 02:11:31 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:11:31 INFO view.FileSystemViewManager: Creating View Manager with storage type :MEMORY
24/01/05 02:11:31 INFO view.FileSystemViewManager: Creating in-memory based Table View
24/01/05 02:11:31 INFO client.BaseHoodieWriteClient: Scheduling compaction at instant time :20240105021110208001
24/01/05 02:11:31 INFO compact.ScheduleCompactionActionExecutor: Checking if compaction needs to be run on /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:11:31 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:11:31 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:11:31 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:11:31 INFO table.HoodieTableMetaClient: Loading Active commit timeline for /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:11:31 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105021110208__deltacommit__COMPLETED__20240105021118540]}
24/01/05 02:11:31 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:11:31 INFO view.FileSystemViewManager: Creating View Manager with storage type :MEMORY
24/01/05 02:11:31 INFO view.FileSystemViewManager: Creating in-memory based Table View
24/01/05 02:11:31 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105021110208__deltacommit__COMPLETED__20240105021118540]}
24/01/05 02:11:31 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:11:31 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:11:31 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:11:31 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[==>20240105021130096__deltacommit__REQUESTED__20240105021131599]}
24/01/05 02:11:31 INFO client.HoodieTimelineArchiver: No Instants to archive
24/01/05 02:11:31 INFO metadata.HoodieBackedTableMetadataWriter: All the table services operations on MDT completed successfully
24/01/05 02:11:31 INFO transaction.TransactionManager: Transaction ending with transaction owner Option{val=[==>20240105021130096__deltacommit__INFLIGHT]}
24/01/05 02:11:31 INFO lock.InProcessLockProvider: Base Path /user/hive/warehouse/stock_ticks_mor, Lock Instance java.util.concurrent.locks.ReentrantReadWriteLock@2e204155[Write locks = 1, Read locks = 0], Thread main, In-process lock state RELEASING
24/01/05 02:11:31 INFO lock.InProcessLockProvider: Base Path /user/hive/warehouse/stock_ticks_mor, Lock Instance java.util.concurrent.locks.ReentrantReadWriteLock@2e204155[Write locks = 0, Read locks = 0], Thread main, In-process lock state RELEASED
24/01/05 02:11:31 INFO lock.InProcessLockProvider: Base Path /user/hive/warehouse/stock_ticks_mor, Lock Instance java.util.concurrent.locks.ReentrantReadWriteLock@2e204155[Write locks = 0, Read locks = 0], Thread main, In-process lock state ALREADY_RELEASED
24/01/05 02:11:31 INFO lock.LockManager: Released connection created for acquiring lock
24/01/05 02:11:31 INFO transaction.TransactionManager: Transaction ended with transaction owner Option{val=[==>20240105021130096__deltacommit__INFLIGHT]}
24/01/05 02:11:31 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:11:31 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:11:31 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:11:31 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:11:31 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:11:31 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:11:31 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105021110208__deltacommit__COMPLETED__20240105021118540]}
24/01/05 02:11:31 INFO view.AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
24/01/05 02:11:31 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:11:31 INFO view.FileSystemViewManager: Creating View Manager with storage type :REMOTE_FIRST
24/01/05 02:11:31 INFO view.FileSystemViewManager: Creating remote first table view
24/01/05 02:11:31 INFO async.AsyncCleanerService: The HoodieWriteClient is not configured to auto & async clean. Async clean service will not start.
24/01/05 02:11:31 INFO async.AsyncArchiveService: The HoodieWriteClient is not configured to auto & async archive. Async archive service will not start.
24/01/05 02:11:32 INFO spark.SparkContext: Starting job: collect at HoodieJavaRDD.java:177
24/01/05 02:11:32 INFO scheduler.DAGScheduler: Registering RDD 7 (mapToPair at HoodieJavaRDD.java:149)
24/01/05 02:11:32 INFO scheduler.DAGScheduler: Registering RDD 13 (distinct at HoodieJavaRDD.java:157)
24/01/05 02:11:32 INFO scheduler.DAGScheduler: Got job 0 (collect at HoodieJavaRDD.java:177) with 2 output partitions
24/01/05 02:11:32 INFO scheduler.DAGScheduler: Final stage: ResultStage 2 (collect at HoodieJavaRDD.java:177)
24/01/05 02:11:32 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 1)
24/01/05 02:11:32 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 1)
24/01/05 02:11:32 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[7] at mapToPair at HoodieJavaRDD.java:149), which has no missing parents
24/01/05 02:11:32 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 29.5 KB, free 366.3 MB)
24/01/05 02:11:32 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 14.0 KB, free 366.3 MB)
24/01/05 02:11:32 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on adhoc-2:38731 (size: 14.0 KB, free: 366.3 MB)
24/01/05 02:11:32 INFO spark.SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1161
24/01/05 02:11:32 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[7] at mapToPair at HoodieJavaRDD.java:149) (first 15 tasks are for partitions Vector(0))
24/01/05 02:11:32 INFO scheduler.TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
24/01/05 02:11:32 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7739 bytes)
24/01/05 02:11:32 INFO executor.Executor: Running task 0.0 in stage 0.0 (TID 0)
24/01/05 02:11:32 INFO executor.Executor: Fetching spark://adhoc-2:34903/jars/hoodie-utilities.jar with timestamp 1704420687947
24/01/05 02:11:32 INFO client.TransportClientFactory: Successfully created connection to adhoc-2/172.20.0.17:34903 after 46 ms (0 ms spent in bootstraps)
24/01/05 02:11:32 INFO util.Utils: Fetching spark://adhoc-2:34903/jars/hoodie-utilities.jar to /tmp/spark-f96a29b7-0950-41a6-b903-4c5d5734b32c/userFiles-17f63b04-0742-4ba4-9c96-5f9842403fd2/fetchFileTemp1605082592724685909.tmp
24/01/05 02:11:32 INFO executor.Executor: Adding file:/tmp/spark-f96a29b7-0950-41a6-b903-4c5d5734b32c/userFiles-17f63b04-0742-4ba4-9c96-5f9842403fd2/hoodie-utilities.jar to class loader
24/01/05 02:11:33 INFO kafka010.KafkaRDD: Computing topic stock_ticks, partition 0 offsets 2611 -> 2711
24/01/05 02:11:33 INFO kafka010.KafkaDataConsumer: Initializing cache 16 64 0.75
24/01/05 02:11:33 INFO consumer.ConsumerConfig: ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [kafkabroker:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-executor-null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

24/01/05 02:11:33 INFO utils.AppInfoParser: Kafka version : 2.0.0
24/01/05 02:11:33 INFO utils.AppInfoParser: Kafka commitId : 3402a8361b734732
24/01/05 02:11:33 INFO factory.HoodieSparkKeyGeneratorFactory: The value of hoodie.datasource.write.keygenerator.type is empty; inferred to be SIMPLE
24/01/05 02:11:33 INFO kafka010.InternalKafkaConsumer: Initial fetch for spark-executor-null stock_ticks-0 2611
24/01/05 02:11:33 INFO clients.Metadata: Cluster ID: OtZ13RgXQhuquRcIZizanw
24/01/05 02:11:33 INFO executor.Executor: Finished task 0.0 in stage 0.0 (TID 0). 1037 bytes result sent to driver
24/01/05 02:11:33 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 737 ms on localhost (executor driver) (1/1)
24/01/05 02:11:33 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
24/01/05 02:11:33 INFO scheduler.DAGScheduler: ShuffleMapStage 0 (mapToPair at HoodieJavaRDD.java:149) finished in 0.873 s
24/01/05 02:11:33 INFO scheduler.DAGScheduler: looking for newly runnable stages
24/01/05 02:11:33 INFO scheduler.DAGScheduler: running: Set()
24/01/05 02:11:33 INFO scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 1, ResultStage 2)
24/01/05 02:11:33 INFO scheduler.DAGScheduler: failed: Set()
24/01/05 02:11:33 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 1 (MapPartitionsRDD[13] at distinct at HoodieJavaRDD.java:157), which has no missing parents
24/01/05 02:11:33 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 24.3 KB, free 366.2 MB)
24/01/05 02:11:33 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 11.6 KB, free 366.2 MB)
24/01/05 02:11:33 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on adhoc-2:38731 (size: 11.6 KB, free: 366.3 MB)
24/01/05 02:11:33 INFO spark.SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1161
24/01/05 02:11:33 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[13] at distinct at HoodieJavaRDD.java:157) (first 15 tasks are for partitions Vector(0, 1))
24/01/05 02:11:33 INFO scheduler.TaskSchedulerImpl: Adding task set 1.0 with 2 tasks
24/01/05 02:11:33 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 7651 bytes)
24/01/05 02:11:33 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 1.0 (TID 2, localhost, executor driver, partition 1, ANY, 7651 bytes)
24/01/05 02:11:33 INFO executor.Executor: Running task 0.0 in stage 1.0 (TID 1)
24/01/05 02:11:33 INFO executor.Executor: Running task 1.0 in stage 1.0 (TID 2)
24/01/05 02:11:33 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
24/01/05 02:11:33 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
24/01/05 02:11:33 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 7 ms
24/01/05 02:11:33 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 7 ms
24/01/05 02:11:33 INFO memory.MemoryStore: Block rdd_9_0 stored as values in memory (estimated size 0.0 B, free 366.2 MB)
24/01/05 02:11:33 INFO storage.BlockManagerInfo: Added rdd_9_0 in memory on adhoc-2:38731 (size: 0.0 B, free: 366.3 MB)
24/01/05 02:11:33 INFO memory.MemoryStore: Block rdd_9_1 stored as values in memory (estimated size 634.0 B, free 366.2 MB)
24/01/05 02:11:33 INFO storage.BlockManagerInfo: Added rdd_9_1 in memory on adhoc-2:38731 (size: 634.0 B, free: 366.3 MB)
24/01/05 02:11:33 INFO executor.Executor: Finished task 0.0 in stage 1.0 (TID 1). 1166 bytes result sent to driver
24/01/05 02:11:33 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 94 ms on localhost (executor driver) (1/2)
24/01/05 02:11:33 INFO executor.Executor: Finished task 1.0 in stage 1.0 (TID 2). 1252 bytes result sent to driver
24/01/05 02:11:33 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 1.0 (TID 2) in 102 ms on localhost (executor driver) (2/2)
24/01/05 02:11:33 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
24/01/05 02:11:33 INFO scheduler.DAGScheduler: ShuffleMapStage 1 (distinct at HoodieJavaRDD.java:157) finished in 0.119 s
24/01/05 02:11:33 INFO scheduler.DAGScheduler: looking for newly runnable stages
24/01/05 02:11:33 INFO scheduler.DAGScheduler: running: Set()
24/01/05 02:11:33 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 2)
24/01/05 02:11:33 INFO scheduler.DAGScheduler: failed: Set()
24/01/05 02:11:33 INFO scheduler.DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[15] at distinct at HoodieJavaRDD.java:157), which has no missing parents
24/01/05 02:11:33 INFO memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 4.1 KB, free 366.2 MB)
24/01/05 02:11:33 INFO memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.4 KB, free 366.2 MB)
24/01/05 02:11:33 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on adhoc-2:38731 (size: 2.4 KB, free: 366.3 MB)
24/01/05 02:11:33 INFO spark.SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1161
24/01/05 02:11:33 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ResultStage 2 (MapPartitionsRDD[15] at distinct at HoodieJavaRDD.java:157) (first 15 tasks are for partitions Vector(0, 1))
24/01/05 02:11:33 INFO scheduler.TaskSchedulerImpl: Adding task set 2.0 with 2 tasks
24/01/05 02:11:33 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 2.0 (TID 3, localhost, executor driver, partition 0, PROCESS_LOCAL, 7662 bytes)
24/01/05 02:11:33 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 2.0 (TID 4, localhost, executor driver, partition 1, ANY, 7662 bytes)
24/01/05 02:11:33 INFO executor.Executor: Running task 1.0 in stage 2.0 (TID 4)
24/01/05 02:11:33 INFO executor.Executor: Running task 0.0 in stage 2.0 (TID 3)
24/01/05 02:11:33 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
24/01/05 02:11:33 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
24/01/05 02:11:33 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
24/01/05 02:11:33 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
24/01/05 02:11:33 INFO executor.Executor: Finished task 0.0 in stage 2.0 (TID 3). 1098 bytes result sent to driver
24/01/05 02:11:33 INFO executor.Executor: Finished task 1.0 in stage 2.0 (TID 4). 1110 bytes result sent to driver
24/01/05 02:11:33 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 2.0 (TID 4) in 24 ms on localhost (executor driver) (1/2)
24/01/05 02:11:33 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 2.0 (TID 3) in 30 ms on localhost (executor driver) (2/2)
24/01/05 02:11:33 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
24/01/05 02:11:33 INFO scheduler.DAGScheduler: ResultStage 2 (collect at HoodieJavaRDD.java:177) finished in 0.043 s
24/01/05 02:11:33 INFO scheduler.DAGScheduler: Job 0 finished: collect at HoodieJavaRDD.java:177, took 1.334830 s
24/01/05 02:11:33 INFO spark.SparkContext: Starting job: collect at HoodieSparkEngineContext.java:150
24/01/05 02:11:33 INFO scheduler.DAGScheduler: Got job 1 (collect at HoodieSparkEngineContext.java:150) with 1 output partitions
24/01/05 02:11:33 INFO scheduler.DAGScheduler: Final stage: ResultStage 3 (collect at HoodieSparkEngineContext.java:150)
24/01/05 02:11:33 INFO scheduler.DAGScheduler: Parents of final stage: List()
24/01/05 02:11:33 INFO scheduler.DAGScheduler: Missing parents: List()
24/01/05 02:11:33 INFO scheduler.DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[17] at flatMap at HoodieSparkEngineContext.java:150), which has no missing parents
24/01/05 02:11:33 INFO memory.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 339.1 KB, free 365.9 MB)
24/01/05 02:11:33 INFO memory.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 121.3 KB, free 365.8 MB)
24/01/05 02:11:33 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on adhoc-2:38731 (size: 121.3 KB, free: 366.2 MB)
24/01/05 02:11:33 INFO spark.SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1161
24/01/05 02:11:33 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[17] at flatMap at HoodieSparkEngineContext.java:150) (first 15 tasks are for partitions Vector(0))
24/01/05 02:11:33 INFO scheduler.TaskSchedulerImpl: Adding task set 3.0 with 1 tasks
24/01/05 02:11:33 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 3.0 (TID 5, localhost, executor driver, partition 0, PROCESS_LOCAL, 7735 bytes)
24/01/05 02:11:33 INFO executor.Executor: Running task 0.0 in stage 3.0 (TID 5)
24/01/05 02:11:33 INFO view.FileSystemViewManager: Creating View Manager with storage type :REMOTE_FIRST
24/01/05 02:11:33 INFO view.FileSystemViewManager: Creating remote first table view
24/01/05 02:11:33 INFO view.FileSystemViewManager: Creating remote view for basePath /user/hive/warehouse/stock_ticks_mor. Server=adhoc-2:35479, Timeout=300
24/01/05 02:11:33 INFO view.FileSystemViewManager: Creating InMemory based view for basePath /user/hive/warehouse/stock_ticks_mor
24/01/05 02:11:33 INFO view.AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
24/01/05 02:11:33 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:11:33 INFO view.RemoteHoodieTableFileSystemView: Sending request : (http://adhoc-2:35479/v1/hoodie/view/datafiles/beforeoron/latest/?partition=2018%2F08%2F31&maxinstant=20240105021110208&basepath=%2Fuser%2Fhive%2Fwarehouse%2Fstock_ticks_mor&lastinstantts=20240105021110208&timelinehash=4edbe545d5e4877759b59ac9b4488385fad3f5cc412ad1a7af51246e9bb79357)
24/01/05 02:11:34 INFO spark.ContextCleaner: Cleaned accumulator 45
24/01/05 02:11:34 INFO spark.ContextCleaner: Cleaned accumulator 59
24/01/05 02:11:34 INFO spark.ContextCleaner: Cleaned accumulator 13
24/01/05 02:11:34 INFO spark.ContextCleaner: Cleaned accumulator 71
24/01/05 02:11:34 INFO spark.ContextCleaner: Cleaned accumulator 74
24/01/05 02:11:34 INFO spark.ContextCleaner: Cleaned accumulator 8
24/01/05 02:11:34 INFO spark.ContextCleaner: Cleaned accumulator 4
24/01/05 02:11:34 INFO spark.ContextCleaner: Cleaned accumulator 38
24/01/05 02:11:34 INFO spark.ContextCleaner: Cleaned accumulator 65
24/01/05 02:11:34 INFO spark.ContextCleaner: Cleaned accumulator 43
24/01/05 02:11:34 INFO spark.ContextCleaner: Cleaned accumulator 29
24/01/05 02:11:34 INFO spark.ContextCleaner: Cleaned accumulator 9
24/01/05 02:11:34 INFO spark.ContextCleaner: Cleaned accumulator 24
24/01/05 02:11:34 INFO spark.ContextCleaner: Cleaned accumulator 70
24/01/05 02:11:34 INFO spark.ContextCleaner: Cleaned accumulator 22
24/01/05 02:11:34 INFO spark.ContextCleaner: Cleaned accumulator 31
24/01/05 02:11:34 INFO spark.ContextCleaner: Cleaned accumulator 39
24/01/05 02:11:34 INFO storage.BlockManagerInfo: Removed broadcast_0_piece0 on adhoc-2:38731 in memory (size: 14.0 KB, free: 366.2 MB)
24/01/05 02:11:34 INFO spark.ContextCleaner: Cleaned accumulator 58
24/01/05 02:11:34 INFO spark.ContextCleaner: Cleaned accumulator 64
24/01/05 02:11:34 INFO spark.ContextCleaner: Cleaned accumulator 56
24/01/05 02:11:34 INFO spark.ContextCleaner: Cleaned accumulator 6
24/01/05 02:11:34 INFO spark.ContextCleaner: Cleaned accumulator 30
24/01/05 02:11:34 INFO spark.ContextCleaner: Cleaned accumulator 3
24/01/05 02:11:34 INFO spark.ContextCleaner: Cleaned accumulator 61
24/01/05 02:11:34 INFO spark.ContextCleaner: Cleaned accumulator 15
24/01/05 02:11:34 INFO spark.ContextCleaner: Cleaned accumulator 0
24/01/05 02:11:34 INFO spark.ContextCleaner: Cleaned accumulator 23
24/01/05 02:11:34 INFO spark.ContextCleaner: Cleaned accumulator 51
24/01/05 02:11:34 INFO spark.ContextCleaner: Cleaned accumulator 12
24/01/05 02:11:34 INFO spark.ContextCleaner: Cleaned accumulator 28
24/01/05 02:11:34 INFO spark.ContextCleaner: Cleaned accumulator 1
24/01/05 02:11:34 INFO spark.ContextCleaner: Cleaned accumulator 14
24/01/05 02:11:34 INFO spark.ContextCleaner: Cleaned accumulator 44
24/01/05 02:11:34 INFO spark.ContextCleaner: Cleaned accumulator 46
24/01/05 02:11:34 INFO spark.ContextCleaner: Cleaned accumulator 17
24/01/05 02:11:34 INFO spark.ContextCleaner: Cleaned accumulator 73
24/01/05 02:11:34 INFO spark.ContextCleaner: Cleaned accumulator 60
24/01/05 02:11:34 INFO spark.ContextCleaner: Cleaned accumulator 21
24/01/05 02:11:34 INFO spark.ContextCleaner: Cleaned accumulator 68
24/01/05 02:11:34 INFO spark.ContextCleaner: Cleaned accumulator 66
24/01/05 02:11:34 INFO spark.ContextCleaner: Cleaned accumulator 62
24/01/05 02:11:34 INFO spark.ContextCleaner: Cleaned accumulator 5
24/01/05 02:11:34 INFO spark.ContextCleaner: Cleaned accumulator 26
24/01/05 02:11:34 INFO spark.ContextCleaner: Cleaned accumulator 48
24/01/05 02:11:34 INFO spark.ContextCleaner: Cleaned accumulator 47
24/01/05 02:11:34 INFO spark.ContextCleaner: Cleaned accumulator 50
24/01/05 02:11:34 INFO spark.ContextCleaner: Cleaned accumulator 34
24/01/05 02:11:34 INFO spark.ContextCleaner: Cleaned accumulator 25
24/01/05 02:11:34 INFO spark.ContextCleaner: Cleaned accumulator 18
24/01/05 02:11:34 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on adhoc-2:38731 in memory (size: 11.6 KB, free: 366.2 MB)
24/01/05 02:11:34 INFO spark.ContextCleaner: Cleaned accumulator 20
24/01/05 02:11:34 INFO spark.ContextCleaner: Cleaned accumulator 67
24/01/05 02:11:34 INFO spark.ContextCleaner: Cleaned accumulator 7
24/01/05 02:11:34 INFO spark.ContextCleaner: Cleaned accumulator 32
24/01/05 02:11:34 INFO spark.ContextCleaner: Cleaned accumulator 36
24/01/05 02:11:34 INFO spark.ContextCleaner: Cleaned accumulator 37
24/01/05 02:11:34 INFO spark.ContextCleaner: Cleaned accumulator 10
24/01/05 02:11:34 INFO spark.ContextCleaner: Cleaned accumulator 27
24/01/05 02:11:34 INFO spark.ContextCleaner: Cleaned accumulator 49
24/01/05 02:11:34 INFO spark.ContextCleaner: Cleaned accumulator 69
24/01/05 02:11:34 INFO spark.ContextCleaner: Cleaned accumulator 19
24/01/05 02:11:34 INFO spark.ContextCleaner: Cleaned accumulator 72
24/01/05 02:11:34 INFO spark.ContextCleaner: Cleaned accumulator 33
24/01/05 02:11:34 INFO spark.ContextCleaner: Cleaned accumulator 11
24/01/05 02:11:34 INFO spark.ContextCleaner: Cleaned accumulator 42
24/01/05 02:11:34 INFO storage.BlockManagerInfo: Removed broadcast_2_piece0 on adhoc-2:38731 in memory (size: 2.4 KB, free: 366.2 MB)
24/01/05 02:11:34 INFO spark.ContextCleaner: Cleaned accumulator 41
24/01/05 02:11:34 INFO spark.ContextCleaner: Cleaned accumulator 52
24/01/05 02:11:34 INFO spark.ContextCleaner: Cleaned accumulator 57
24/01/05 02:11:34 INFO spark.ContextCleaner: Cleaned accumulator 55
24/01/05 02:11:34 INFO spark.ContextCleaner: Cleaned accumulator 2
24/01/05 02:11:34 INFO spark.ContextCleaner: Cleaned accumulator 35
24/01/05 02:11:34 INFO spark.ContextCleaner: Cleaned accumulator 63
24/01/05 02:11:34 INFO spark.ContextCleaner: Cleaned accumulator 54
24/01/05 02:11:34 INFO spark.ContextCleaner: Cleaned accumulator 40
24/01/05 02:11:34 INFO spark.ContextCleaner: Cleaned accumulator 53
24/01/05 02:11:34 INFO spark.ContextCleaner: Cleaned shuffle 0
24/01/05 02:11:34 INFO spark.ContextCleaner: Cleaned accumulator 16
24/01/05 02:11:34 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:11:34 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:11:34 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:11:34 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[==>20240105021130096__deltacommit__REQUESTED__20240105021131599]}
24/01/05 02:11:34 INFO collection.RocksDBDAO: DELETING RocksDB persisted at /tmp/hoodie_timeline_rocksdb/_user_hive_warehouse_stock_ticks_mor/11876f22-8a15-45c0-8e34-faeacee05ad4
24/01/05 02:11:34 INFO collection.RocksDBDAO: No column family found. Loading default
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : RocksDB version: 7.5.3

24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Compile date 2022-09-02 09:50:20
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : DB SUMMARY

24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : DB Session ID:  W9H7ZZFBC6YV5VWRYXRN

24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : SST files in /tmp/hoodie_timeline_rocksdb/_user_hive_warehouse_stock_ticks_mor/11876f22-8a15-45c0-8e34-faeacee05ad4 dir, Total Num: 0, files: 

24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Write Ahead Log file in /tmp/hoodie_timeline_rocksdb/_user_hive_warehouse_stock_ticks_mor/11876f22-8a15-45c0-8e34-faeacee05ad4: 

24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                         Options.error_if_exists: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                       Options.create_if_missing: 1
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                         Options.paranoid_checks: 1
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :             Options.flush_verify_memtable_count: 1
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                               Options.track_and_verify_wals_in_manifest: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :        Options.verify_sst_unique_id_in_manifest: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                                     Options.env: 0x7f822f3f15e0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                                      Options.fs: PosixFileSystem
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                                Options.info_log: 0x7f81c80840d8
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                Options.max_file_opening_threads: 16
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                              Options.statistics: 0x7f81c8083fd0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                               Options.use_fsync: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                       Options.max_log_file_size: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                  Options.max_manifest_file_size: 1073741824
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                   Options.log_file_time_to_roll: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                       Options.keep_log_file_num: 1000
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                    Options.recycle_log_file_num: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                         Options.allow_fallocate: 1
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                        Options.allow_mmap_reads: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                       Options.allow_mmap_writes: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                        Options.use_direct_reads: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                        Options.use_direct_io_for_flush_and_compaction: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :          Options.create_missing_column_families: 1
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                              Options.db_log_dir: 
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                                 Options.wal_dir: 
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                Options.table_cache_numshardbits: 6
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                         Options.WAL_ttl_seconds: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                       Options.WAL_size_limit_MB: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                        Options.max_write_batch_group_size_bytes: 1048576
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :             Options.manifest_preallocation_size: 4194304
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                     Options.is_fd_close_on_exec: 1
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                   Options.advise_random_on_open: 1
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                    Options.db_write_buffer_size: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                    Options.write_buffer_manager: 0x7f81c80fa700
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :         Options.access_hint_on_compaction_start: 1
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :           Options.random_access_max_buffer_size: 1048576
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                      Options.use_adaptive_mutex: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                            Options.rate_limiter: (nil)
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :     Options.sst_file_manager.rate_bytes_per_sec: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                       Options.wal_recovery_mode: 2
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                  Options.enable_thread_tracking: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                  Options.enable_pipelined_write: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                  Options.unordered_write: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :         Options.allow_concurrent_memtable_write: 1
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :      Options.enable_write_thread_adaptive_yield: 1
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :             Options.write_thread_max_yield_usec: 100
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :            Options.write_thread_slow_yield_usec: 3
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                               Options.row_cache: None
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                              Options.wal_filter: None
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :             Options.avoid_flush_during_recovery: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :             Options.allow_ingest_behind: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :             Options.two_write_queues: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :             Options.manual_wal_flush: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :             Options.wal_compression: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :             Options.atomic_flush: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :             Options.avoid_unnecessary_blocking_io: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                 Options.persist_stats_to_disk: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                 Options.write_dbid_to_manifest: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                 Options.log_readahead_size: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                 Options.file_checksum_gen_factory: Unknown
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                 Options.best_efforts_recovery: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                Options.max_bgerror_resume_count: 2147483647
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :            Options.bgerror_resume_retry_interval: 1000000
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :             Options.allow_data_in_errors: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :             Options.db_host_id: __hostname__
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :             Options.enforce_single_del_contracts: true
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :             Options.max_background_jobs: 2
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :             Options.max_background_compactions: -1
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :             Options.max_subcompactions: 1
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :             Options.avoid_flush_during_shutdown: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :           Options.writable_file_max_buffer_size: 1048576
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :             Options.delayed_write_rate : 16777216
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :             Options.max_total_wal_size: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :             Options.delete_obsolete_files_period_micros: 21600000000
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                   Options.stats_dump_period_sec: 300
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                 Options.stats_persist_period_sec: 600
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                 Options.stats_history_buffer_size: 1048576
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                          Options.max_open_files: -1
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                          Options.bytes_per_sync: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                      Options.wal_bytes_per_sync: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                   Options.strict_bytes_per_sync: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :       Options.compaction_readahead_size: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                  Options.max_background_flushes: -1
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Compression algorithms supported:
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : 	kZSTDNotFinalCompression supported: 1
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : 	kZSTD supported: 1
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : 	kXpressCompression supported: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : 	kLZ4HCCompression supported: 1
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : 	kLZ4Compression supported: 1
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : 	kBZip2Compression supported: 1
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : 	kZlibCompression supported: 1
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : 	kSnappyCompression supported: 1
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Fast CRC32 supported: Not supported on x86
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : DMutex implementation: pthread_mutex_t
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : [/db_impl/db_impl_open.cc:313] Creating manifest 1 

24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : [/version_set.cc:4994] Recovering from manifest file: /tmp/hoodie_timeline_rocksdb/_user_hive_warehouse_stock_ticks_mor/11876f22-8a15-45c0-8e34-faeacee05ad4/MANIFEST-000001

24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : [/column_family.cc:618] --------------- Options for column family [default]:

24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :               Options.comparator: leveldb.BytewiseComparator
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :           Options.merge_operator: None
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :        Options.compaction_filter: None
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :        Options.compaction_filter_factory: None
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :  Options.sst_partitioner_factory: None
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :         Options.memtable_factory: SkipListFactory
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :            Options.table_factory: BlockBasedTable
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :            table_factory options:   flush_block_policy_factory: FlushBlockBySizePolicyFactory (0x7f81c80f8610)
  cache_index_and_filter_blocks: 0
  cache_index_and_filter_blocks_with_high_priority: 1
  pin_l0_filter_and_index_blocks_in_cache: 0
  pin_top_level_index_and_filter: 1
  index_type: 0
  data_block_index_type: 0
  index_shortening: 1
  data_block_hash_table_util_ratio: 0.750000
  checksum: 1
  no_block_cache: 0
  block_cache: 0x7f81c80f8660
  block_cache_name: LRUCache
  block_cache_options:
    capacity : 8388608
    num_shard_bits : 4
    strict_capacity_limit : 0
    memory_allocator : None
    high_pri_pool_ratio: 0.000
  block_cache_compressed: (nil)
  persistent_cache: (nil)
  block_size: 4096
  block_size_deviation: 10
  block_restart_interval: 16
  index_block_restart_interval: 1
  metadata_block_size: 4096
  partition_filters: 0
  use_delta_encoding: 1
  filter_policy: nullptr
  whole_key_filtering: 1
  verify_compression: 0
  read_amp_bytes_per_bit: 0
  format_version: 5
  enable_index_compression: 1
  block_align: 0
  max_auto_readahead_size: 262144
  prepopulate_block_cache: 0
  initial_auto_readahead_size: 8192

24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :        Options.write_buffer_size: 67108864
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :  Options.max_write_buffer_number: 2
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :          Options.compression: Snappy
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression: Disabled
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :       Options.prefix_extractor: nullptr
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :   Options.memtable_insert_with_hint_prefix_extractor: nullptr
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :             Options.num_levels: 7
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :        Options.min_write_buffer_number_to_merge: 1
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :     Options.max_write_buffer_number_to_maintain: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :     Options.max_write_buffer_size_to_maintain: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :            Options.bottommost_compression_opts.window_bits: -14
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression_opts.level: 32767
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :               Options.bottommost_compression_opts.strategy: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.max_dict_bytes: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.zstd_max_train_bytes: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.parallel_threads: 1
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression_opts.enabled: false
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.max_dict_buffer_bytes: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.use_zstd_dict_trainer: true
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :            Options.compression_opts.window_bits: -14
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                  Options.compression_opts.level: 32767
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :               Options.compression_opts.strategy: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.max_dict_bytes: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.zstd_max_train_bytes: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.use_zstd_dict_trainer: true
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.parallel_threads: 1
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                  Options.compression_opts.enabled: false
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.max_dict_buffer_bytes: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :      Options.level0_file_num_compaction_trigger: 4
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :          Options.level0_slowdown_writes_trigger: 20
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :              Options.level0_stop_writes_trigger: 36
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                   Options.target_file_size_base: 67108864
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :             Options.target_file_size_multiplier: 1
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                Options.max_bytes_for_level_base: 268435456
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Options.level_compaction_dynamic_level_bytes: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :          Options.max_bytes_for_level_multiplier: 10.000000
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[0]: 1
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[1]: 1
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[2]: 1
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[3]: 1
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[4]: 1
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[5]: 1
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[6]: 1
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :       Options.max_sequential_skip_in_iterations: 8
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                    Options.max_compaction_bytes: 1677721600
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                        Options.arena_block_size: 1048576
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :   Options.soft_pending_compaction_bytes_limit: 68719476736
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :   Options.hard_pending_compaction_bytes_limit: 274877906944
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                Options.disable_auto_compactions: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                        Options.compaction_style: kCompactionStyleLevel
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                          Options.compaction_pri: kMinOverlappingRatio
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.size_ratio: 1
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.min_merge_width: 2
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.max_merge_width: 4294967295
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.max_size_amplification_percent: 200
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.compression_size_percent: -1
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.stop_style: kCompactionStopStyleTotalSize
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_fifo.max_table_files_size: 1073741824
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_fifo.allow_compaction: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                   Options.table_properties_collectors: 
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                   Options.inplace_update_support: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                 Options.inplace_update_num_locks: 10000
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :               Options.memtable_prefix_bloom_size_ratio: 0.000000
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :               Options.memtable_whole_key_filtering: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :   Options.memtable_huge_page_size: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                           Options.bloom_locality: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                    Options.max_successive_merges: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                Options.optimize_filters_for_hits: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                Options.paranoid_file_checks: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                Options.force_consistency_checks: 1
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                Options.report_bg_io_stats: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                               Options.ttl: 2592000
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :          Options.periodic_compaction_seconds: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :  Options.preclude_last_level_data_seconds: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                       Options.enable_blob_files: false
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                           Options.min_blob_size: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                          Options.blob_file_size: 268435456
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                   Options.blob_compression_type: NoCompression
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :          Options.enable_blob_garbage_collection: false
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :      Options.blob_garbage_collection_age_cutoff: 0.250000
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Options.blob_garbage_collection_force_threshold: 1.000000
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :          Options.blob_compaction_readahead_size: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                Options.blob_file_starting_level: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Options.experimental_mempurge_threshold: 0.000000
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : [/version_set.cc:5042] Recovered from manifest file:/tmp/hoodie_timeline_rocksdb/_user_hive_warehouse_stock_ticks_mor/11876f22-8a15-45c0-8e34-faeacee05ad4/MANIFEST-000001 succeeded,manifest_file_number is 1, next_file_number is 3, last_sequence is 0, log_number is 0,prev_log_number is 0,max_column_family is 0,min_log_number_to_keep is 0

24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : [/version_set.cc:5051] Column family [default] (ID 0), log number is 0

24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : [/db_impl/db_impl_open.cc:532] DB ID: f561e19f-4a1e-403e-a2f3-95f7042953ad

24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : [/version_set.cc:4513] Creating manifest 5

24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : [/db_impl/db_impl_open.cc:2024] SstFileManager instance 0x7f81c80faac0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : DB pointer 0x7f81c80fb500
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : [/db_impl/db_impl.cc:1070] ------- DUMPING STATS -------
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : [/db_impl/db_impl.cc:1071] 
** DB Stats **
Uptime(secs): 0.0 total, 0.0 interval
Cumulative writes: 0 writes, 0 keys, 0 commit groups, 0.0 writes per commit group, ingest: 0.00 GB, 0.00 MB/s
Cumulative WAL: 0 writes, 0 syncs, 0.00 writes per sync, written: 0.00 GB, 0.00 MB/s
Cumulative stall: 00:00:0.000 H:M:S, 0.0 percent
Interval writes: 0 writes, 0 keys, 0 commit groups, 0.0 writes per commit group, ingest: 0.00 MB, 0.00 MB/s
Interval WAL: 0 writes, 0 syncs, 0.00 writes per sync, written: 0.00 GB, 0.00 MB/s
Interval stall: 00:00:0.000 H:M:S, 0.0 percent

** Compaction Stats [default] **
Level    Files   Size     Score Read(GB)  Rn(GB) Rnp1(GB) Write(GB) Wnew(GB) Moved(GB) W-Amp Rd(MB/s) Wr(MB/s) Comp(sec) CompMergeCPU(sec) Comp(cnt) Avg(sec) KeyIn KeyDrop Rblob(GB) Wblob(GB)
------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
 Sum      0/0    0.00 KB   0.0      0.0     0.0      0.0       0.0      0.0       0.0   0.0      0.0      0.0      0.00              0.00         0    0.000       0      0       0.0       0.0
 Int      0/0    0.00 KB   0.0      0.0     0.0      0.0       0.0      0.0       0.0   0.0      0.0      0.0      0.00              0.00         0    0.000       0      0       0.0       0.0

** Compaction Stats [default] **
Priority    Files   Size     Score Read(GB)  Rn(GB) Rnp1(GB) Write(GB) Wnew(GB) Moved(GB) W-Amp Rd(MB/s) Wr(MB/s) Comp(sec) CompMergeCPU(sec) Comp(cnt) Avg(sec) KeyIn KeyDrop Rblob(GB) Wblob(GB)
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Blob file count: 0, total size: 0.0 GB, garbage size: 0.0 GB, space amp: 0.0

Uptime(secs): 0.0 total, 0.0 interval
Flush(GB): cumulative 0.000, interval 0.000
AddFile(GB): cumulative 0.000, interval 0.000
AddFile(Total Files): cumulative 0, interval 0
AddFile(L0 Files): cumulative 0, interval 0
AddFile(Keys): cumulative 0, interval 0
Cumulative compaction: 0.00 GB write, 0.00 MB/s write, 0.00 GB read, 0.00 MB/s read, 0.0 seconds
Interval compaction: 0.00 GB write, 0.00 MB/s write, 0.00 GB read, 0.00 MB/s read, 0.0 seconds
Stalls(count): 0 level0_slowdown, 0 level0_slowdown_with_compaction, 0 level0_numfiles, 0 level0_numfiles_with_compaction, 0 stop for pending_compaction_bytes, 0 slowdown for pending_compaction_bytes, 0 memtable_compaction, 0 memtable_slowdown, interval 0 total count
Block cache LRUCache@0x7f81c80f8660#600 capacity: 8.00 MB collections: 1 last_copies: 0 last_secs: 5.7e-05 secs_since: 0
Block cache entry stats(count,size,portion): Misc(1,0.00 KB,0%)

** File Read Latency Histogram By Level [default] **

24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : [/db_impl/db_impl.cc:762] STATISTICS:
 rocksdb.block.cache.miss COUNT : 0
rocksdb.block.cache.hit COUNT : 0
rocksdb.block.cache.add COUNT : 0
rocksdb.block.cache.add.failures COUNT : 0
rocksdb.block.cache.index.miss COUNT : 0
rocksdb.block.cache.index.hit COUNT : 0
rocksdb.block.cache.index.add COUNT : 0
rocksdb.block.cache.index.bytes.insert COUNT : 0
rocksdb.block.cache.index.bytes.evict COUNT : 0
rocksdb.block.cache.filter.miss COUNT : 0
rocksdb.block.cache.filter.hit COUNT : 0
rocksdb.block.cache.filter.add COUNT : 0
rocksdb.block.cache.filter.bytes.insert COUNT : 0
rocksdb.block.cache.filter.bytes.evict COUNT : 0
rocksdb.block.cache.data.miss COUNT : 0
rocksdb.block.cache.data.hit COUNT : 0
rocksdb.block.cache.data.add COUNT : 0
rocksdb.block.cache.data.bytes.insert COUNT : 0
rocksdb.block.cache.bytes.read COUNT : 0
rocksdb.block.cache.bytes.write COUNT : 0
rocksdb.bloom.filter.useful COUNT : 0
rocksdb.bloom.filter.full.positive COUNT : 0
rocksdb.bloom.filter.full.true.positive COUNT : 0
rocksdb.bloom.filter.micros COUNT : 0
rocksdb.persistent.cache.hit COUNT : 0
rocksdb.persistent.cache.miss COUNT : 0
rocksdb.sim.block.cache.hit COUNT : 0
rocksdb.sim.block.cache.miss COUNT : 0
rocksdb.memtable.hit COUNT : 0
rocksdb.memtable.miss COUNT : 0
rocksdb.l0.hit COUNT : 0
rocksdb.l1.hit COUNT : 0
rocksdb.l2andup.hit COUNT : 0
rocksdb.compaction.key.drop.new COUNT : 0
rocksdb.compaction.key.drop.obsolete COUNT : 0
rocksdb.compaction.key.drop.range_del COUNT : 0
rocksdb.compaction.key.drop.user COUNT : 0
rocksdb.compaction.range_del.drop.obsolete COUNT : 0
rocksdb.compaction.optimized.del.drop.obsolete COUNT : 0
rocksdb.compaction.cancelled COUNT : 0
rocksdb.number.keys.written COUNT : 0
rocksdb.number.keys.read COUNT : 0
rocksdb.number.keys.updated COUNT : 0
rocksdb.bytes.written COUNT : 0
rocksdb.bytes.read COUNT : 0
rocksdb.number.db.seek COUNT : 0
rocksdb.number.db.next COUNT : 0
rocksdb.number.db.prev COUNT : 0
rocksdb.number.db.seek.found COUNT : 0
rocksdb.number.db.next.found COUNT : 0
rocksdb.number.db.prev.found COUNT : 0
rocksdb.db.iter.bytes.read COUNT : 0
rocksdb.no.file.closes COUNT : 0
rocksdb.no.file.opens COUNT : 0
rocksdb.no.file.errors COUNT : 0
rocksdb.l0.slowdown.micros COUNT : 0
rocksdb.memtable.compaction.micros COUNT : 0
rocksdb.l0.num.files.stall.micros COUNT : 0
rocksdb.stall.micros COUNT : 0
rocksdb.db.mutex.wait.micros COUNT : 0
rocksdb.rate.limit.delay.millis COUNT : 0
rocksdb.num.iterators COUNT : 0
rocksdb.number.multiget.get COUNT : 0
rocksdb.number.multiget.keys.read COUNT : 0
rocksdb.number.multiget.bytes.read COUNT : 0
rocksdb.number.deletes.filtered COUNT : 0
rocksdb.number.merge.failures COUNT : 0
rocksdb.bloom.filter.prefix.checked COUNT : 0
rocksdb.bloom.filter.prefix.useful COUNT : 0
rocksdb.number.reseeks.iteration COUNT : 0
rocksdb.getupdatessince.calls COUNT : 0
rocksdb.block.cachecompressed.miss COUNT : 0
rocksdb.block.cachecompressed.hit COUNT : 0
rocksdb.block.cachecompressed.add COUNT : 0
rocksdb.block.cachecompressed.add.failures COUNT : 0
rocksdb.wal.synced COUNT : 0
rocksdb.wal.bytes COUNT : 0
rocksdb.write.self COUNT : 0
rocksdb.write.other COUNT : 0
rocksdb.write.timeout COUNT : 0
rocksdb.write.wal COUNT : 0
rocksdb.compact.read.bytes COUNT : 0
rocksdb.compact.write.bytes COUNT : 0
rocksdb.flush.write.bytes COUNT : 0
rocksdb.compact.read.marked.bytes COUNT : 0
rocksdb.compact.read.periodic.bytes COUNT : 0
rocksdb.compact.read.ttl.bytes COUNT : 0
rocksdb.compact.write.marked.bytes COUNT : 0
rocksdb.compact.write.periodic.bytes COUNT : 0
rocksdb.compact.write.ttl.bytes COUNT : 0
rocksdb.number.direct.load.table.properties COUNT : 0
rocksdb.number.superversion_acquires COUNT : 0
rocksdb.number.superversion_releases COUNT : 0
rocksdb.number.superversion_cleanups COUNT : 0
rocksdb.number.block.compressed COUNT : 0
rocksdb.number.block.decompressed COUNT : 0
rocksdb.number.block.not_compressed COUNT : 0
rocksdb.merge.operation.time.nanos COUNT : 0
rocksdb.filter.operation.time.nanos COUNT : 0
rocksdb.row.cache.hit COUNT : 0
rocksdb.row.cache.miss COUNT : 0
rocksdb.read.amp.estimate.useful.bytes COUNT : 0
rocksdb.read.amp.total.read.bytes COUNT : 0
rocksdb.number.rate_limiter.drains COUNT : 0
rocksdb.number.iter.skip COUNT : 0
rocksdb.blobdb.num.put COUNT : 0
rocksdb.blobdb.num.write COUNT : 0
rocksdb.blobdb.num.get COUNT : 0
rocksdb.blobdb.num.multiget COUNT : 0
rocksdb.blobdb.num.seek COUNT : 0
rocksdb.blobdb.num.next COUNT : 0
rocksdb.blobdb.num.prev COUNT : 0
rocksdb.blobdb.num.keys.written COUNT : 0
rocksdb.blobdb.num.keys.read COUNT : 0
rocksdb.blobdb.bytes.written COUNT : 0
rocksdb.blobdb.bytes.read COUNT : 0
rocksdb.blobdb.write.inlined COUNT : 0
rocksdb.blobdb.write.inlined.ttl COUNT : 0
rocksdb.blobdb.write.blob COUNT : 0
rocksdb.blobdb.write.blob.ttl COUNT : 0
rocksdb.blobdb.blob.file.bytes.written COUNT : 0
rocksdb.blobdb.blob.file.bytes.read COUNT : 0
rocksdb.blobdb.blob.file.synced COUNT : 0
rocksdb.blobdb.blob.index.expired.count COUNT : 0
rocksdb.blobdb.blob.index.expired.size COUNT : 0
rocksdb.blobdb.blob.index.evicted.count COUNT : 0
rocksdb.blobdb.blob.index.evicted.size COUNT : 0
rocksdb.blobdb.gc.num.files COUNT : 0
rocksdb.blobdb.gc.num.new.files COUNT : 0
rocksdb.blobdb.gc.failures COUNT : 0
rocksdb.blobdb.gc.num.keys.overwritten COUNT : 0
rocksdb.blobdb.gc.num.keys.expired COUNT : 0
rocksdb.blobdb.gc.num.keys.relocated COUNT : 0
rocksdb.blobdb.gc.bytes.overwritten COUNT : 0
rocksdb.blobdb.gc.bytes.expired COUNT : 0
rocksdb.blobdb.gc.bytes.relocated COUNT : 0
rocksdb.blobdb.fifo.num.files.evicted COUNT : 0
rocksdb.blobdb.fifo.num.keys.evicted COUNT : 0
rocksdb.blobdb.fifo.bytes.evicted COUNT : 0
rocksdb.txn.overhead.mutex.prepare COUNT : 0
rocksdb.txn.overhead.mutex.old.commit.map COUNT : 0
rocksdb.txn.overhead.duplicate.key COUNT : 0
rocksdb.txn.overhead.mutex.snapshot COUNT : 0
rocksdb.txn.get.tryagain COUNT : 0
rocksdb.number.multiget.keys.found COUNT : 0
rocksdb.num.iterator.created COUNT : 0
rocksdb.num.iterator.deleted COUNT : 0
rocksdb.block.cache.compression.dict.miss COUNT : 0
rocksdb.block.cache.compression.dict.hit COUNT : 0
rocksdb.block.cache.compression.dict.add COUNT : 0
rocksdb.block.cache.compression.dict.bytes.insert COUNT : 0
rocksdb.block.cache.compression.dict.bytes.evict COUNT : 0
rocksdb.block.cache.add.redundant COUNT : 0
rocksdb.block.cache.index.add.redundant COUNT : 0
rocksdb.block.cache.filter.add.redundant COUNT : 0
rocksdb.block.cache.data.add.redundant COUNT : 0
rocksdb.block.cache.compression.dict.add.redundant COUNT : 0
rocksdb.files.marked.trash COUNT : 0
rocksdb.files.deleted.immediately COUNT : 0
rocksdb.error.handler.bg.errro.count COUNT : 0
rocksdb.error.handler.bg.io.errro.count COUNT : 0
rocksdb.error.handler.bg.retryable.io.errro.count COUNT : 0
rocksdb.error.handler.autoresume.count COUNT : 0
rocksdb.error.handler.autoresume.retry.total.count COUNT : 0
rocksdb.error.handler.autoresume.success.count COUNT : 0
rocksdb.memtable.payload.bytes.at.flush COUNT : 0
rocksdb.memtable.garbage.bytes.at.flush COUNT : 0
rocksdb.secondary.cache.hits COUNT : 0
rocksdb.verify_checksum.read.bytes COUNT : 0
rocksdb.backup.read.bytes COUNT : 0
rocksdb.backup.write.bytes COUNT : 0
rocksdb.remote.compact.read.bytes COUNT : 0
rocksdb.remote.compact.write.bytes COUNT : 0
rocksdb.hot.file.read.bytes COUNT : 0
rocksdb.warm.file.read.bytes COUNT : 0
rocksdb.cold.file.read.bytes COUNT : 0
rocksdb.hot.file.read.count COUNT : 0
rocksdb.warm.file.read.count COUNT : 0
rocksdb.cold.file.read.count COUNT : 0
rocksdb.last.level.read.bytes COUNT : 0
rocksdb.last.level.read.count COUNT : 0
rocksdb.non.last.level.read.bytes COUNT : 0
rocksdb.non.last.level.read.count COUNT : 0
rocksdb.block.checksum.compute.count COUNT : 0
rocksdb.multiget.coroutine.count COUNT : 0
rocksdb.blobdb.cache.miss COUNT : 0
rocksdb.blobdb.cache.hit COUNT : 0
rocksdb.blobdb.cache.add COUNT : 0
rocksdb.blobdb.cache.add.failures COUNT : 0
rocksdb.blobdb.cache.bytes.read COUNT : 0
rocksdb.blobdb.cache.bytes.write COUNT : 0
rocksdb.db.get.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.db.write.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.compaction.times.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.compaction.times.cpu_micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.subcompaction.setup.times.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.table.sync.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.compaction.outfile.sync.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.wal.file.sync.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.manifest.file.sync.micros P50 : 870.000000 P95 : 879.000000 P99 : 879.000000 P100 : 879.000000 COUNT : 2 SUM : 1667
rocksdb.table.open.io.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.db.multiget.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.read.block.compaction.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.read.block.get.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.write.raw.block.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.l0.slowdown.count P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.memtable.compaction.count P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.num.files.stall.count P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.hard.rate.limit.delay.count P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.soft.rate.limit.delay.count P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.numfiles.in.singlecompaction P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.db.seek.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.db.write.stall P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.sst.read.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.num.subcompactions.scheduled P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.bytes.per.read P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.bytes.per.write P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.bytes.per.multiget P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.bytes.compressed P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.bytes.decompressed P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.compression.times.nanos P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.decompression.times.nanos P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.read.num.merge_operands P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.blobdb.key.size P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.blobdb.value.size P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.blobdb.write.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.blobdb.get.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.blobdb.multiget.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.blobdb.seek.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.blobdb.next.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.blobdb.prev.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.blobdb.blob.file.write.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.blobdb.blob.file.read.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.blobdb.blob.file.sync.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.blobdb.gc.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.blobdb.compression.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.blobdb.decompression.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.db.flush.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.sst.batch.size P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.num.index.and.filter.blocks.read.per.level P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.num.data.blocks.read.per.level P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.num.sst.read.per.level P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.error.handler.autoresume.retry.count P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.async.read.bytes P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.poll.wait.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.prefetched.bytes.discarded P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.multiget.io.batch.size P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.num.level.read.per.multiget P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0

24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : [/column_family.cc:618] --------------- Options for column family [hudi_view__user_hive_warehouse_stock_ticks_mor]:

24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :               Options.comparator: leveldb.BytewiseComparator
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :           Options.merge_operator: None
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :        Options.compaction_filter: None
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :        Options.compaction_filter_factory: None
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :  Options.sst_partitioner_factory: None
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :         Options.memtable_factory: SkipListFactory
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :            Options.table_factory: BlockBasedTable
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :            table_factory options:   flush_block_policy_factory: FlushBlockBySizePolicyFactory (0x7f81c81dfb60)
  cache_index_and_filter_blocks: 0
  cache_index_and_filter_blocks_with_high_priority: 1
  pin_l0_filter_and_index_blocks_in_cache: 0
  pin_top_level_index_and_filter: 1
  index_type: 0
  data_block_index_type: 0
  index_shortening: 1
  data_block_hash_table_util_ratio: 0.750000
  checksum: 1
  no_block_cache: 0
  block_cache: 0x7f81c8124c40
  block_cache_name: LRUCache
  block_cache_options:
    capacity : 8388608
    num_shard_bits : 4
    strict_capacity_limit : 0
    memory_allocator : None
    high_pri_pool_ratio: 0.000
  block_cache_compressed: (nil)
  persistent_cache: (nil)
  block_size: 4096
  block_size_deviation: 10
  block_restart_interval: 16
  index_block_restart_interval: 1
  metadata_block_size: 4096
  partition_filters: 0
  use_delta_encoding: 1
  filter_policy: nullptr
  whole_key_filtering: 1
  verify_compression: 0
  read_amp_bytes_per_bit: 0
  format_version: 5
  enable_index_compression: 1
  block_align: 0
  max_auto_readahead_size: 262144
  prepopulate_block_cache: 0
  initial_auto_readahead_size: 8192

24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :        Options.write_buffer_size: 67108864
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :  Options.max_write_buffer_number: 2
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :          Options.compression: Snappy
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression: Disabled
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :       Options.prefix_extractor: nullptr
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :   Options.memtable_insert_with_hint_prefix_extractor: nullptr
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :             Options.num_levels: 7
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :        Options.min_write_buffer_number_to_merge: 1
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :     Options.max_write_buffer_number_to_maintain: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :     Options.max_write_buffer_size_to_maintain: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :            Options.bottommost_compression_opts.window_bits: -14
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression_opts.level: 32767
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :               Options.bottommost_compression_opts.strategy: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.max_dict_bytes: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.zstd_max_train_bytes: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.parallel_threads: 1
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression_opts.enabled: false
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.max_dict_buffer_bytes: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.use_zstd_dict_trainer: true
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :            Options.compression_opts.window_bits: -14
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                  Options.compression_opts.level: 32767
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :               Options.compression_opts.strategy: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.max_dict_bytes: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.zstd_max_train_bytes: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.use_zstd_dict_trainer: true
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.parallel_threads: 1
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                  Options.compression_opts.enabled: false
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.max_dict_buffer_bytes: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :      Options.level0_file_num_compaction_trigger: 4
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :          Options.level0_slowdown_writes_trigger: 20
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :              Options.level0_stop_writes_trigger: 36
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                   Options.target_file_size_base: 67108864
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :             Options.target_file_size_multiplier: 1
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                Options.max_bytes_for_level_base: 268435456
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Options.level_compaction_dynamic_level_bytes: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :          Options.max_bytes_for_level_multiplier: 10.000000
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[0]: 1
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[1]: 1
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[2]: 1
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[3]: 1
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[4]: 1
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[5]: 1
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[6]: 1
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :       Options.max_sequential_skip_in_iterations: 8
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                    Options.max_compaction_bytes: 1677721600
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                        Options.arena_block_size: 1048576
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :   Options.soft_pending_compaction_bytes_limit: 68719476736
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :   Options.hard_pending_compaction_bytes_limit: 274877906944
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                Options.disable_auto_compactions: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                        Options.compaction_style: kCompactionStyleLevel
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                          Options.compaction_pri: kMinOverlappingRatio
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.size_ratio: 1
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.min_merge_width: 2
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.max_merge_width: 4294967295
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.max_size_amplification_percent: 200
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.compression_size_percent: -1
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.stop_style: kCompactionStopStyleTotalSize
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_fifo.max_table_files_size: 1073741824
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_fifo.allow_compaction: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                   Options.table_properties_collectors: 
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                   Options.inplace_update_support: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                 Options.inplace_update_num_locks: 10000
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :               Options.memtable_prefix_bloom_size_ratio: 0.000000
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :               Options.memtable_whole_key_filtering: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :   Options.memtable_huge_page_size: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                           Options.bloom_locality: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                    Options.max_successive_merges: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                Options.optimize_filters_for_hits: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                Options.paranoid_file_checks: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                Options.force_consistency_checks: 1
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                Options.report_bg_io_stats: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                               Options.ttl: 2592000
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :          Options.periodic_compaction_seconds: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :  Options.preclude_last_level_data_seconds: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                       Options.enable_blob_files: false
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                           Options.min_blob_size: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                          Options.blob_file_size: 268435456
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                   Options.blob_compression_type: NoCompression
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :          Options.enable_blob_garbage_collection: false
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :      Options.blob_garbage_collection_age_cutoff: 0.250000
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Options.blob_garbage_collection_force_threshold: 1.000000
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :          Options.blob_compaction_readahead_size: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                Options.blob_file_starting_level: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Options.experimental_mempurge_threshold: 0.000000
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : [/db_impl/db_impl.cc:2861] Created column family [hudi_view__user_hive_warehouse_stock_ticks_mor] (ID 1)
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : [/column_family.cc:618] --------------- Options for column family [hudi_pending_compaction__user_hive_warehouse_stock_ticks_mor]:

24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :               Options.comparator: leveldb.BytewiseComparator
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :           Options.merge_operator: None
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :        Options.compaction_filter: None
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :        Options.compaction_filter_factory: None
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :  Options.sst_partitioner_factory: None
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :         Options.memtable_factory: SkipListFactory
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :            Options.table_factory: BlockBasedTable
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :            table_factory options:   flush_block_policy_factory: FlushBlockBySizePolicyFactory (0x7f81c81123c0)
  cache_index_and_filter_blocks: 0
  cache_index_and_filter_blocks_with_high_priority: 1
  pin_l0_filter_and_index_blocks_in_cache: 0
  pin_top_level_index_and_filter: 1
  index_type: 0
  data_block_index_type: 0
  index_shortening: 1
  data_block_hash_table_util_ratio: 0.750000
  checksum: 1
  no_block_cache: 0
  block_cache: 0x7f81c81259b0
  block_cache_name: LRUCache
  block_cache_options:
    capacity : 8388608
    num_shard_bits : 4
    strict_capacity_limit : 0
    memory_allocator : None
    high_pri_pool_ratio: 0.000
  block_cache_compressed: (nil)
  persistent_cache: (nil)
  block_size: 4096
  block_size_deviation: 10
  block_restart_interval: 16
  index_block_restart_interval: 1
  metadata_block_size: 4096
  partition_filters: 0
  use_delta_encoding: 1
  filter_policy: nullptr
  whole_key_filtering: 1
  verify_compression: 0
  read_amp_bytes_per_bit: 0
  format_version: 5
  enable_index_compression: 1
  block_align: 0
  max_auto_readahead_size: 262144
  prepopulate_block_cache: 0
  initial_auto_readahead_size: 8192

24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :        Options.write_buffer_size: 67108864
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :  Options.max_write_buffer_number: 2
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :          Options.compression: Snappy
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression: Disabled
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :       Options.prefix_extractor: nullptr
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :   Options.memtable_insert_with_hint_prefix_extractor: nullptr
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :             Options.num_levels: 7
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :        Options.min_write_buffer_number_to_merge: 1
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :     Options.max_write_buffer_number_to_maintain: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :     Options.max_write_buffer_size_to_maintain: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :            Options.bottommost_compression_opts.window_bits: -14
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression_opts.level: 32767
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :               Options.bottommost_compression_opts.strategy: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.max_dict_bytes: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.zstd_max_train_bytes: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.parallel_threads: 1
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression_opts.enabled: false
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.max_dict_buffer_bytes: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.use_zstd_dict_trainer: true
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :            Options.compression_opts.window_bits: -14
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                  Options.compression_opts.level: 32767
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :               Options.compression_opts.strategy: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.max_dict_bytes: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.zstd_max_train_bytes: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.use_zstd_dict_trainer: true
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.parallel_threads: 1
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                  Options.compression_opts.enabled: false
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.max_dict_buffer_bytes: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :      Options.level0_file_num_compaction_trigger: 4
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :          Options.level0_slowdown_writes_trigger: 20
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :              Options.level0_stop_writes_trigger: 36
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                   Options.target_file_size_base: 67108864
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :             Options.target_file_size_multiplier: 1
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                Options.max_bytes_for_level_base: 268435456
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Options.level_compaction_dynamic_level_bytes: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :          Options.max_bytes_for_level_multiplier: 10.000000
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[0]: 1
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[1]: 1
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[2]: 1
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[3]: 1
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[4]: 1
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[5]: 1
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[6]: 1
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :       Options.max_sequential_skip_in_iterations: 8
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                    Options.max_compaction_bytes: 1677721600
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                        Options.arena_block_size: 1048576
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :   Options.soft_pending_compaction_bytes_limit: 68719476736
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :   Options.hard_pending_compaction_bytes_limit: 274877906944
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                Options.disable_auto_compactions: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                        Options.compaction_style: kCompactionStyleLevel
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                          Options.compaction_pri: kMinOverlappingRatio
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.size_ratio: 1
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.min_merge_width: 2
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.max_merge_width: 4294967295
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.max_size_amplification_percent: 200
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.compression_size_percent: -1
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.stop_style: kCompactionStopStyleTotalSize
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_fifo.max_table_files_size: 1073741824
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_fifo.allow_compaction: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                   Options.table_properties_collectors: 
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                   Options.inplace_update_support: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                 Options.inplace_update_num_locks: 10000
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :               Options.memtable_prefix_bloom_size_ratio: 0.000000
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :               Options.memtable_whole_key_filtering: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :   Options.memtable_huge_page_size: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                           Options.bloom_locality: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                    Options.max_successive_merges: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                Options.optimize_filters_for_hits: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                Options.paranoid_file_checks: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                Options.force_consistency_checks: 1
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                Options.report_bg_io_stats: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                               Options.ttl: 2592000
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :          Options.periodic_compaction_seconds: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :  Options.preclude_last_level_data_seconds: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                       Options.enable_blob_files: false
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                           Options.min_blob_size: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                          Options.blob_file_size: 268435456
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                   Options.blob_compression_type: NoCompression
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :          Options.enable_blob_garbage_collection: false
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :      Options.blob_garbage_collection_age_cutoff: 0.250000
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Options.blob_garbage_collection_force_threshold: 1.000000
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :          Options.blob_compaction_readahead_size: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                Options.blob_file_starting_level: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Options.experimental_mempurge_threshold: 0.000000
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : [/db_impl/db_impl.cc:2861] Created column family [hudi_pending_compaction__user_hive_warehouse_stock_ticks_mor] (ID 2)
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : [/column_family.cc:618] --------------- Options for column family [hudi_pending_log_compaction__user_hive_warehouse_stock_ticks_mor]:

24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :               Options.comparator: leveldb.BytewiseComparator
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :           Options.merge_operator: None
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :        Options.compaction_filter: None
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :        Options.compaction_filter_factory: None
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :  Options.sst_partitioner_factory: None
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :         Options.memtable_factory: SkipListFactory
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :            Options.table_factory: BlockBasedTable
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :            table_factory options:   flush_block_policy_factory: FlushBlockBySizePolicyFactory (0x7f81c8154ff0)
  cache_index_and_filter_blocks: 0
  cache_index_and_filter_blocks_with_high_priority: 1
  pin_l0_filter_and_index_blocks_in_cache: 0
  pin_top_level_index_and_filter: 1
  index_type: 0
  data_block_index_type: 0
  index_shortening: 1
  data_block_hash_table_util_ratio: 0.750000
  checksum: 1
  no_block_cache: 0
  block_cache: 0x7f81c8155040
  block_cache_name: LRUCache
  block_cache_options:
    capacity : 8388608
    num_shard_bits : 4
    strict_capacity_limit : 0
    memory_allocator : None
    high_pri_pool_ratio: 0.000
  block_cache_compressed: (nil)
  persistent_cache: (nil)
  block_size: 4096
  block_size_deviation: 10
  block_restart_interval: 16
  index_block_restart_interval: 1
  metadata_block_size: 4096
  partition_filters: 0
  use_delta_encoding: 1
  filter_policy: nullptr
  whole_key_filtering: 1
  verify_compression: 0
  read_amp_bytes_per_bit: 0
  format_version: 5
  enable_index_compression: 1
  block_align: 0
  max_auto_readahead_size: 262144
  prepopulate_block_cache: 0
  initial_auto_readahead_size: 8192

24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :        Options.write_buffer_size: 67108864
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :  Options.max_write_buffer_number: 2
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :          Options.compression: Snappy
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression: Disabled
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :       Options.prefix_extractor: nullptr
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :   Options.memtable_insert_with_hint_prefix_extractor: nullptr
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :             Options.num_levels: 7
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :        Options.min_write_buffer_number_to_merge: 1
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :     Options.max_write_buffer_number_to_maintain: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :     Options.max_write_buffer_size_to_maintain: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :            Options.bottommost_compression_opts.window_bits: -14
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression_opts.level: 32767
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :               Options.bottommost_compression_opts.strategy: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.max_dict_bytes: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.zstd_max_train_bytes: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.parallel_threads: 1
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression_opts.enabled: false
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.max_dict_buffer_bytes: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.use_zstd_dict_trainer: true
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :            Options.compression_opts.window_bits: -14
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                  Options.compression_opts.level: 32767
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :               Options.compression_opts.strategy: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.max_dict_bytes: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.zstd_max_train_bytes: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.use_zstd_dict_trainer: true
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.parallel_threads: 1
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                  Options.compression_opts.enabled: false
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.max_dict_buffer_bytes: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :      Options.level0_file_num_compaction_trigger: 4
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :          Options.level0_slowdown_writes_trigger: 20
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :              Options.level0_stop_writes_trigger: 36
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                   Options.target_file_size_base: 67108864
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :             Options.target_file_size_multiplier: 1
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                Options.max_bytes_for_level_base: 268435456
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Options.level_compaction_dynamic_level_bytes: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :          Options.max_bytes_for_level_multiplier: 10.000000
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[0]: 1
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[1]: 1
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[2]: 1
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[3]: 1
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[4]: 1
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[5]: 1
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[6]: 1
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :       Options.max_sequential_skip_in_iterations: 8
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                    Options.max_compaction_bytes: 1677721600
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                        Options.arena_block_size: 1048576
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :   Options.soft_pending_compaction_bytes_limit: 68719476736
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :   Options.hard_pending_compaction_bytes_limit: 274877906944
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                Options.disable_auto_compactions: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                        Options.compaction_style: kCompactionStyleLevel
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                          Options.compaction_pri: kMinOverlappingRatio
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.size_ratio: 1
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.min_merge_width: 2
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.max_merge_width: 4294967295
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.max_size_amplification_percent: 200
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.compression_size_percent: -1
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.stop_style: kCompactionStopStyleTotalSize
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_fifo.max_table_files_size: 1073741824
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_fifo.allow_compaction: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                   Options.table_properties_collectors: 
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                   Options.inplace_update_support: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                 Options.inplace_update_num_locks: 10000
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :               Options.memtable_prefix_bloom_size_ratio: 0.000000
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :               Options.memtable_whole_key_filtering: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :   Options.memtable_huge_page_size: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                           Options.bloom_locality: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                    Options.max_successive_merges: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                Options.optimize_filters_for_hits: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                Options.paranoid_file_checks: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                Options.force_consistency_checks: 1
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                Options.report_bg_io_stats: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                               Options.ttl: 2592000
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :          Options.periodic_compaction_seconds: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :  Options.preclude_last_level_data_seconds: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                       Options.enable_blob_files: false
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                           Options.min_blob_size: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                          Options.blob_file_size: 268435456
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                   Options.blob_compression_type: NoCompression
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :          Options.enable_blob_garbage_collection: false
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :      Options.blob_garbage_collection_age_cutoff: 0.250000
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Options.blob_garbage_collection_force_threshold: 1.000000
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :          Options.blob_compaction_readahead_size: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                Options.blob_file_starting_level: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Options.experimental_mempurge_threshold: 0.000000
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : [/db_impl/db_impl.cc:2861] Created column family [hudi_pending_log_compaction__user_hive_warehouse_stock_ticks_mor] (ID 3)
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : [/column_family.cc:618] --------------- Options for column family [hudi_bootstrap_basefile__user_hive_warehouse_stock_ticks_mor]:

24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :               Options.comparator: leveldb.BytewiseComparator
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :           Options.merge_operator: None
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :        Options.compaction_filter: None
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :        Options.compaction_filter_factory: None
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :  Options.sst_partitioner_factory: None
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :         Options.memtable_factory: SkipListFactory
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :            Options.table_factory: BlockBasedTable
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :            table_factory options:   flush_block_policy_factory: FlushBlockBySizePolicyFactory (0x7f81c8034310)
  cache_index_and_filter_blocks: 0
  cache_index_and_filter_blocks_with_high_priority: 1
  pin_l0_filter_and_index_blocks_in_cache: 0
  pin_top_level_index_and_filter: 1
  index_type: 0
  data_block_index_type: 0
  index_shortening: 1
  data_block_hash_table_util_ratio: 0.750000
  checksum: 1
  no_block_cache: 0
  block_cache: 0x7f81c8034360
  block_cache_name: LRUCache
  block_cache_options:
    capacity : 8388608
    num_shard_bits : 4
    strict_capacity_limit : 0
    memory_allocator : None
    high_pri_pool_ratio: 0.000
  block_cache_compressed: (nil)
  persistent_cache: (nil)
  block_size: 4096
  block_size_deviation: 10
  block_restart_interval: 16
  index_block_restart_interval: 1
  metadata_block_size: 4096
  partition_filters: 0
  use_delta_encoding: 1
  filter_policy: nullptr
  whole_key_filtering: 1
  verify_compression: 0
  read_amp_bytes_per_bit: 0
  format_version: 5
  enable_index_compression: 1
  block_align: 0
  max_auto_readahead_size: 262144
  prepopulate_block_cache: 0
  initial_auto_readahead_size: 8192

24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :        Options.write_buffer_size: 67108864
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :  Options.max_write_buffer_number: 2
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :          Options.compression: Snappy
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression: Disabled
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :       Options.prefix_extractor: nullptr
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :   Options.memtable_insert_with_hint_prefix_extractor: nullptr
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :             Options.num_levels: 7
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :        Options.min_write_buffer_number_to_merge: 1
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :     Options.max_write_buffer_number_to_maintain: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :     Options.max_write_buffer_size_to_maintain: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :            Options.bottommost_compression_opts.window_bits: -14
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression_opts.level: 32767
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :               Options.bottommost_compression_opts.strategy: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.max_dict_bytes: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.zstd_max_train_bytes: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.parallel_threads: 1
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression_opts.enabled: false
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.max_dict_buffer_bytes: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.use_zstd_dict_trainer: true
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :            Options.compression_opts.window_bits: -14
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                  Options.compression_opts.level: 32767
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :               Options.compression_opts.strategy: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.max_dict_bytes: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.zstd_max_train_bytes: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.use_zstd_dict_trainer: true
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.parallel_threads: 1
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                  Options.compression_opts.enabled: false
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.max_dict_buffer_bytes: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :      Options.level0_file_num_compaction_trigger: 4
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :          Options.level0_slowdown_writes_trigger: 20
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :              Options.level0_stop_writes_trigger: 36
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                   Options.target_file_size_base: 67108864
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :             Options.target_file_size_multiplier: 1
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                Options.max_bytes_for_level_base: 268435456
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Options.level_compaction_dynamic_level_bytes: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :          Options.max_bytes_for_level_multiplier: 10.000000
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[0]: 1
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[1]: 1
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[2]: 1
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[3]: 1
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[4]: 1
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[5]: 1
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[6]: 1
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :       Options.max_sequential_skip_in_iterations: 8
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                    Options.max_compaction_bytes: 1677721600
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                        Options.arena_block_size: 1048576
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :   Options.soft_pending_compaction_bytes_limit: 68719476736
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :   Options.hard_pending_compaction_bytes_limit: 274877906944
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                Options.disable_auto_compactions: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                        Options.compaction_style: kCompactionStyleLevel
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                          Options.compaction_pri: kMinOverlappingRatio
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.size_ratio: 1
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.min_merge_width: 2
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.max_merge_width: 4294967295
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.max_size_amplification_percent: 200
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.compression_size_percent: -1
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.stop_style: kCompactionStopStyleTotalSize
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_fifo.max_table_files_size: 1073741824
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_fifo.allow_compaction: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                   Options.table_properties_collectors: 
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                   Options.inplace_update_support: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                 Options.inplace_update_num_locks: 10000
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :               Options.memtable_prefix_bloom_size_ratio: 0.000000
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :               Options.memtable_whole_key_filtering: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :   Options.memtable_huge_page_size: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                           Options.bloom_locality: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                    Options.max_successive_merges: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                Options.optimize_filters_for_hits: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                Options.paranoid_file_checks: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                Options.force_consistency_checks: 1
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                Options.report_bg_io_stats: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                               Options.ttl: 2592000
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :          Options.periodic_compaction_seconds: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :  Options.preclude_last_level_data_seconds: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                       Options.enable_blob_files: false
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                           Options.min_blob_size: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                          Options.blob_file_size: 268435456
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                   Options.blob_compression_type: NoCompression
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :          Options.enable_blob_garbage_collection: false
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :      Options.blob_garbage_collection_age_cutoff: 0.250000
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Options.blob_garbage_collection_force_threshold: 1.000000
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :          Options.blob_compaction_readahead_size: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                Options.blob_file_starting_level: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Options.experimental_mempurge_threshold: 0.000000
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : [/db_impl/db_impl.cc:2861] Created column family [hudi_bootstrap_basefile__user_hive_warehouse_stock_ticks_mor] (ID 4)
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : [/column_family.cc:618] --------------- Options for column family [hudi_partitions__user_hive_warehouse_stock_ticks_mor]:

24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :               Options.comparator: leveldb.BytewiseComparator
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :           Options.merge_operator: None
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :        Options.compaction_filter: None
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :        Options.compaction_filter_factory: None
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :  Options.sst_partitioner_factory: None
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :         Options.memtable_factory: SkipListFactory
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :            Options.table_factory: BlockBasedTable
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :            table_factory options:   flush_block_policy_factory: FlushBlockBySizePolicyFactory (0x7f81c8164de0)
  cache_index_and_filter_blocks: 0
  cache_index_and_filter_blocks_with_high_priority: 1
  pin_l0_filter_and_index_blocks_in_cache: 0
  pin_top_level_index_and_filter: 1
  index_type: 0
  data_block_index_type: 0
  index_shortening: 1
  data_block_hash_table_util_ratio: 0.750000
  checksum: 1
  no_block_cache: 0
  block_cache: 0x7f81c8164bb0
  block_cache_name: LRUCache
  block_cache_options:
    capacity : 8388608
    num_shard_bits : 4
    strict_capacity_limit : 0
    memory_allocator : None
    high_pri_pool_ratio: 0.000
  block_cache_compressed: (nil)
  persistent_cache: (nil)
  block_size: 4096
  block_size_deviation: 10
  block_restart_interval: 16
  index_block_restart_interval: 1
  metadata_block_size: 4096
  partition_filters: 0
  use_delta_encoding: 1
  filter_policy: nullptr
  whole_key_filtering: 1
  verify_compression: 0
  read_amp_bytes_per_bit: 0
  format_version: 5
  enable_index_compression: 1
  block_align: 0
  max_auto_readahead_size: 262144
  prepopulate_block_cache: 0
  initial_auto_readahead_size: 8192

24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :        Options.write_buffer_size: 67108864
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :  Options.max_write_buffer_number: 2
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :          Options.compression: Snappy
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression: Disabled
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :       Options.prefix_extractor: nullptr
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :   Options.memtable_insert_with_hint_prefix_extractor: nullptr
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :             Options.num_levels: 7
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :        Options.min_write_buffer_number_to_merge: 1
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :     Options.max_write_buffer_number_to_maintain: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :     Options.max_write_buffer_size_to_maintain: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :            Options.bottommost_compression_opts.window_bits: -14
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression_opts.level: 32767
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :               Options.bottommost_compression_opts.strategy: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.max_dict_bytes: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.zstd_max_train_bytes: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.parallel_threads: 1
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression_opts.enabled: false
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.max_dict_buffer_bytes: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.use_zstd_dict_trainer: true
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :            Options.compression_opts.window_bits: -14
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                  Options.compression_opts.level: 32767
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :               Options.compression_opts.strategy: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.max_dict_bytes: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.zstd_max_train_bytes: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.use_zstd_dict_trainer: true
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.parallel_threads: 1
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                  Options.compression_opts.enabled: false
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.max_dict_buffer_bytes: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :      Options.level0_file_num_compaction_trigger: 4
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :          Options.level0_slowdown_writes_trigger: 20
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :              Options.level0_stop_writes_trigger: 36
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                   Options.target_file_size_base: 67108864
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :             Options.target_file_size_multiplier: 1
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                Options.max_bytes_for_level_base: 268435456
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Options.level_compaction_dynamic_level_bytes: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :          Options.max_bytes_for_level_multiplier: 10.000000
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[0]: 1
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[1]: 1
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[2]: 1
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[3]: 1
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[4]: 1
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[5]: 1
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[6]: 1
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :       Options.max_sequential_skip_in_iterations: 8
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                    Options.max_compaction_bytes: 1677721600
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                        Options.arena_block_size: 1048576
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :   Options.soft_pending_compaction_bytes_limit: 68719476736
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :   Options.hard_pending_compaction_bytes_limit: 274877906944
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                Options.disable_auto_compactions: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                        Options.compaction_style: kCompactionStyleLevel
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                          Options.compaction_pri: kMinOverlappingRatio
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.size_ratio: 1
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.min_merge_width: 2
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.max_merge_width: 4294967295
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.max_size_amplification_percent: 200
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.compression_size_percent: -1
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.stop_style: kCompactionStopStyleTotalSize
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_fifo.max_table_files_size: 1073741824
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_fifo.allow_compaction: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                   Options.table_properties_collectors: 
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                   Options.inplace_update_support: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                 Options.inplace_update_num_locks: 10000
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :               Options.memtable_prefix_bloom_size_ratio: 0.000000
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :               Options.memtable_whole_key_filtering: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :   Options.memtable_huge_page_size: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                           Options.bloom_locality: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                    Options.max_successive_merges: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                Options.optimize_filters_for_hits: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                Options.paranoid_file_checks: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                Options.force_consistency_checks: 1
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                Options.report_bg_io_stats: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                               Options.ttl: 2592000
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :          Options.periodic_compaction_seconds: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :  Options.preclude_last_level_data_seconds: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                       Options.enable_blob_files: false
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                           Options.min_blob_size: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                          Options.blob_file_size: 268435456
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                   Options.blob_compression_type: NoCompression
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :          Options.enable_blob_garbage_collection: false
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :      Options.blob_garbage_collection_age_cutoff: 0.250000
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Options.blob_garbage_collection_force_threshold: 1.000000
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :          Options.blob_compaction_readahead_size: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                Options.blob_file_starting_level: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Options.experimental_mempurge_threshold: 0.000000
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : [/db_impl/db_impl.cc:2861] Created column family [hudi_partitions__user_hive_warehouse_stock_ticks_mor] (ID 5)
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : [/column_family.cc:618] --------------- Options for column family [hudi_replaced_fg_user_hive_warehouse_stock_ticks_mor]:

24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :               Options.comparator: leveldb.BytewiseComparator
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :           Options.merge_operator: None
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :        Options.compaction_filter: None
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :        Options.compaction_filter_factory: None
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :  Options.sst_partitioner_factory: None
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :         Options.memtable_factory: SkipListFactory
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :            Options.table_factory: BlockBasedTable
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :            table_factory options:   flush_block_policy_factory: FlushBlockBySizePolicyFactory (0x7f81c816f160)
  cache_index_and_filter_blocks: 0
  cache_index_and_filter_blocks_with_high_priority: 1
  pin_l0_filter_and_index_blocks_in_cache: 0
  pin_top_level_index_and_filter: 1
  index_type: 0
  data_block_index_type: 0
  index_shortening: 1
  data_block_hash_table_util_ratio: 0.750000
  checksum: 1
  no_block_cache: 0
  block_cache: 0x7f81c816f1b0
  block_cache_name: LRUCache
  block_cache_options:
    capacity : 8388608
    num_shard_bits : 4
    strict_capacity_limit : 0
    memory_allocator : None
    high_pri_pool_ratio: 0.000
  block_cache_compressed: (nil)
  persistent_cache: (nil)
  block_size: 4096
  block_size_deviation: 10
  block_restart_interval: 16
  index_block_restart_interval: 1
  metadata_block_size: 4096
  partition_filters: 0
  use_delta_encoding: 1
  filter_policy: nullptr
  whole_key_filtering: 1
  verify_compression: 0
  read_amp_bytes_per_bit: 0
  format_version: 5
  enable_index_compression: 1
  block_align: 0
  max_auto_readahead_size: 262144
  prepopulate_block_cache: 0
  initial_auto_readahead_size: 8192

24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :        Options.write_buffer_size: 67108864
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :  Options.max_write_buffer_number: 2
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :          Options.compression: Snappy
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression: Disabled
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :       Options.prefix_extractor: nullptr
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :   Options.memtable_insert_with_hint_prefix_extractor: nullptr
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :             Options.num_levels: 7
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :        Options.min_write_buffer_number_to_merge: 1
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :     Options.max_write_buffer_number_to_maintain: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :     Options.max_write_buffer_size_to_maintain: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :            Options.bottommost_compression_opts.window_bits: -14
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression_opts.level: 32767
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :               Options.bottommost_compression_opts.strategy: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.max_dict_bytes: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.zstd_max_train_bytes: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.parallel_threads: 1
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression_opts.enabled: false
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.max_dict_buffer_bytes: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.use_zstd_dict_trainer: true
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :            Options.compression_opts.window_bits: -14
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                  Options.compression_opts.level: 32767
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :               Options.compression_opts.strategy: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.max_dict_bytes: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.zstd_max_train_bytes: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.use_zstd_dict_trainer: true
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.parallel_threads: 1
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                  Options.compression_opts.enabled: false
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.max_dict_buffer_bytes: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :      Options.level0_file_num_compaction_trigger: 4
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :          Options.level0_slowdown_writes_trigger: 20
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :              Options.level0_stop_writes_trigger: 36
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                   Options.target_file_size_base: 67108864
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :             Options.target_file_size_multiplier: 1
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                Options.max_bytes_for_level_base: 268435456
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Options.level_compaction_dynamic_level_bytes: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :          Options.max_bytes_for_level_multiplier: 10.000000
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[0]: 1
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[1]: 1
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[2]: 1
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[3]: 1
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[4]: 1
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[5]: 1
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[6]: 1
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :       Options.max_sequential_skip_in_iterations: 8
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                    Options.max_compaction_bytes: 1677721600
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                        Options.arena_block_size: 1048576
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :   Options.soft_pending_compaction_bytes_limit: 68719476736
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :   Options.hard_pending_compaction_bytes_limit: 274877906944
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                Options.disable_auto_compactions: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                        Options.compaction_style: kCompactionStyleLevel
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                          Options.compaction_pri: kMinOverlappingRatio
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.size_ratio: 1
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.min_merge_width: 2
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.max_merge_width: 4294967295
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.max_size_amplification_percent: 200
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.compression_size_percent: -1
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.stop_style: kCompactionStopStyleTotalSize
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_fifo.max_table_files_size: 1073741824
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_fifo.allow_compaction: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                   Options.table_properties_collectors: 
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                   Options.inplace_update_support: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                 Options.inplace_update_num_locks: 10000
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :               Options.memtable_prefix_bloom_size_ratio: 0.000000
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :               Options.memtable_whole_key_filtering: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :   Options.memtable_huge_page_size: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                           Options.bloom_locality: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                    Options.max_successive_merges: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                Options.optimize_filters_for_hits: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                Options.paranoid_file_checks: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                Options.force_consistency_checks: 1
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                Options.report_bg_io_stats: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                               Options.ttl: 2592000
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :          Options.periodic_compaction_seconds: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :  Options.preclude_last_level_data_seconds: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                       Options.enable_blob_files: false
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                           Options.min_blob_size: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                          Options.blob_file_size: 268435456
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                   Options.blob_compression_type: NoCompression
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :          Options.enable_blob_garbage_collection: false
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :      Options.blob_garbage_collection_age_cutoff: 0.250000
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Options.blob_garbage_collection_force_threshold: 1.000000
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :          Options.blob_compaction_readahead_size: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                Options.blob_file_starting_level: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Options.experimental_mempurge_threshold: 0.000000
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : [/db_impl/db_impl.cc:2861] Created column family [hudi_replaced_fg_user_hive_warehouse_stock_ticks_mor] (ID 6)
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : [/column_family.cc:618] --------------- Options for column family [hudi_pending_clustering_fg_user_hive_warehouse_stock_ticks_mor]:

24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :               Options.comparator: leveldb.BytewiseComparator
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :           Options.merge_operator: None
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :        Options.compaction_filter: None
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :        Options.compaction_filter_factory: None
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :  Options.sst_partitioner_factory: None
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :         Options.memtable_factory: SkipListFactory
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :            Options.table_factory: BlockBasedTable
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :            table_factory options:   flush_block_policy_factory: FlushBlockBySizePolicyFactory (0x7f81c818b830)
  cache_index_and_filter_blocks: 0
  cache_index_and_filter_blocks_with_high_priority: 1
  pin_l0_filter_and_index_blocks_in_cache: 0
  pin_top_level_index_and_filter: 1
  index_type: 0
  data_block_index_type: 0
  index_shortening: 1
  data_block_hash_table_util_ratio: 0.750000
  checksum: 1
  no_block_cache: 0
  block_cache: 0x7f81c818b880
  block_cache_name: LRUCache
  block_cache_options:
    capacity : 8388608
    num_shard_bits : 4
    strict_capacity_limit : 0
    memory_allocator : None
    high_pri_pool_ratio: 0.000
  block_cache_compressed: (nil)
  persistent_cache: (nil)
  block_size: 4096
  block_size_deviation: 10
  block_restart_interval: 16
  index_block_restart_interval: 1
  metadata_block_size: 4096
  partition_filters: 0
  use_delta_encoding: 1
  filter_policy: nullptr
  whole_key_filtering: 1
  verify_compression: 0
  read_amp_bytes_per_bit: 0
  format_version: 5
  enable_index_compression: 1
  block_align: 0
  max_auto_readahead_size: 262144
  prepopulate_block_cache: 0
  initial_auto_readahead_size: 8192

24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :        Options.write_buffer_size: 67108864
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :  Options.max_write_buffer_number: 2
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :          Options.compression: Snappy
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression: Disabled
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :       Options.prefix_extractor: nullptr
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :   Options.memtable_insert_with_hint_prefix_extractor: nullptr
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :             Options.num_levels: 7
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :        Options.min_write_buffer_number_to_merge: 1
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :     Options.max_write_buffer_number_to_maintain: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :     Options.max_write_buffer_size_to_maintain: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :            Options.bottommost_compression_opts.window_bits: -14
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression_opts.level: 32767
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :               Options.bottommost_compression_opts.strategy: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.max_dict_bytes: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.zstd_max_train_bytes: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.parallel_threads: 1
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression_opts.enabled: false
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.max_dict_buffer_bytes: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.use_zstd_dict_trainer: true
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :            Options.compression_opts.window_bits: -14
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                  Options.compression_opts.level: 32767
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :               Options.compression_opts.strategy: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.max_dict_bytes: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.zstd_max_train_bytes: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.use_zstd_dict_trainer: true
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.parallel_threads: 1
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                  Options.compression_opts.enabled: false
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.max_dict_buffer_bytes: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :      Options.level0_file_num_compaction_trigger: 4
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :          Options.level0_slowdown_writes_trigger: 20
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :              Options.level0_stop_writes_trigger: 36
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                   Options.target_file_size_base: 67108864
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :             Options.target_file_size_multiplier: 1
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                Options.max_bytes_for_level_base: 268435456
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Options.level_compaction_dynamic_level_bytes: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :          Options.max_bytes_for_level_multiplier: 10.000000
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[0]: 1
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[1]: 1
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[2]: 1
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[3]: 1
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[4]: 1
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[5]: 1
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[6]: 1
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :       Options.max_sequential_skip_in_iterations: 8
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                    Options.max_compaction_bytes: 1677721600
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                        Options.arena_block_size: 1048576
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :   Options.soft_pending_compaction_bytes_limit: 68719476736
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :   Options.hard_pending_compaction_bytes_limit: 274877906944
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                Options.disable_auto_compactions: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                        Options.compaction_style: kCompactionStyleLevel
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                          Options.compaction_pri: kMinOverlappingRatio
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.size_ratio: 1
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.min_merge_width: 2
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.max_merge_width: 4294967295
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.max_size_amplification_percent: 200
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.compression_size_percent: -1
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.stop_style: kCompactionStopStyleTotalSize
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_fifo.max_table_files_size: 1073741824
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_fifo.allow_compaction: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                   Options.table_properties_collectors: 
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                   Options.inplace_update_support: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                 Options.inplace_update_num_locks: 10000
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :               Options.memtable_prefix_bloom_size_ratio: 0.000000
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :               Options.memtable_whole_key_filtering: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :   Options.memtable_huge_page_size: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                           Options.bloom_locality: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                    Options.max_successive_merges: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                Options.optimize_filters_for_hits: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                Options.paranoid_file_checks: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                Options.force_consistency_checks: 1
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                Options.report_bg_io_stats: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                               Options.ttl: 2592000
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :          Options.periodic_compaction_seconds: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :  Options.preclude_last_level_data_seconds: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                       Options.enable_blob_files: false
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                           Options.min_blob_size: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                          Options.blob_file_size: 268435456
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                   Options.blob_compression_type: NoCompression
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :          Options.enable_blob_garbage_collection: false
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :      Options.blob_garbage_collection_age_cutoff: 0.250000
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Options.blob_garbage_collection_force_threshold: 1.000000
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :          Options.blob_compaction_readahead_size: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB :                Options.blob_file_starting_level: 0
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : Options.experimental_mempurge_threshold: 0.000000
24/01/05 02:11:34 INFO collection.RocksDBDAO: From Rocks DB : [/db_impl/db_impl.cc:2861] Created column family [hudi_pending_clustering_fg_user_hive_warehouse_stock_ticks_mor] (ID 7)
24/01/05 02:11:34 INFO view.RocksDbBasedFileSystemView: Resetting replacedFileGroups to ROCKSDB based file-system view at /tmp/hoodie_timeline_rocksdb, Total file-groups=0
24/01/05 02:11:34 INFO collection.RocksDBDAO: Prefix DELETE (query=part=) on hudi_replaced_fg_user_hive_warehouse_stock_ticks_mor
24/01/05 02:11:34 INFO view.RocksDbBasedFileSystemView: Resetting replacedFileGroups to ROCKSDB based file-system view complete
24/01/05 02:11:34 INFO view.AbstractTableFileSystemView: Took 7 ms to read  0 instants, 0 replaced file groups
24/01/05 02:11:34 INFO view.RocksDbBasedFileSystemView: Initializing pending compaction operations. Count=0
24/01/05 02:11:34 INFO view.RocksDbBasedFileSystemView: Initializing pending Log compaction operations. Count=0
24/01/05 02:11:34 INFO view.RocksDbBasedFileSystemView: Initializing external data file mapping. Count=0
24/01/05 02:11:34 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:11:34 INFO view.RocksDbBasedFileSystemView: Resetting file groups in pending clustering to ROCKSDB based file-system view at /tmp/hoodie_timeline_rocksdb, Total file-groups=0
24/01/05 02:11:34 INFO collection.RocksDBDAO: Prefix DELETE (query=part=) on hudi_pending_clustering_fg_user_hive_warehouse_stock_ticks_mor
24/01/05 02:11:34 INFO view.RocksDbBasedFileSystemView: Resetting replacedFileGroups to ROCKSDB based file-system view complete
24/01/05 02:11:34 INFO view.RocksDbBasedFileSystemView: Created ROCKSDB based file-system view at /tmp/hoodie_timeline_rocksdb
24/01/05 02:11:34 INFO view.AbstractTableFileSystemView: Building file system view for partition (2018/08/31)
24/01/05 02:11:34 INFO view.RocksDbBasedFileSystemView: Resetting and adding new partition (2018/08/31) to ROCKSDB based file-system view at /tmp/hoodie_timeline_rocksdb, Total file-groups=1
24/01/05 02:11:34 INFO collection.RocksDBDAO: Prefix DELETE (query=type=slice,part=2018/08/31,id=) on hudi_view__user_hive_warehouse_stock_ticks_mor
24/01/05 02:11:34 INFO collection.RocksDBDAO: Prefix DELETE (query=type=df,part=2018/08/31,id=) on hudi_view__user_hive_warehouse_stock_ticks_mor
24/01/05 02:11:34 INFO view.RocksDbBasedFileSystemView: Finished adding new partition (2018/08/31) to ROCKSDB based file-system view at /tmp/hoodie_timeline_rocksdb, Total file-groups=1
24/01/05 02:11:34 INFO collection.RocksDBDAO: Prefix Search for (query=type=slice,part=2018/08/31,id=) on hudi_view__user_hive_warehouse_stock_ticks_mor. Total Time Taken (msec)=13. Serialization Time taken(micro)=12515, num entries=1
24/01/05 02:11:34 INFO executor.Executor: Finished task 0.0 in stage 3.0 (TID 5). 1150 bytes result sent to driver
24/01/05 02:11:34 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 3.0 (TID 5) in 1072 ms on localhost (executor driver) (1/1)
24/01/05 02:11:34 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
24/01/05 02:11:34 INFO scheduler.DAGScheduler: ResultStage 3 (collect at HoodieSparkEngineContext.java:150) finished in 1.127 s
24/01/05 02:11:34 INFO scheduler.DAGScheduler: Job 1 finished: collect at HoodieSparkEngineContext.java:150, took 1.131966 s
24/01/05 02:11:34 INFO rdd.MapPartitionsRDD: Removing RDD 9 from persistence list
24/01/05 02:11:34 INFO storage.BlockManager: Removing RDD 9
24/01/05 02:11:34 INFO view.AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
24/01/05 02:11:34 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:11:34 INFO spark.SparkContext: Starting job: countByKey at HoodieJavaPairRDD.java:105
24/01/05 02:11:34 INFO scheduler.DAGScheduler: Registering RDD 10 (mapToPair at HoodieJavaRDD.java:149)
24/01/05 02:11:34 INFO scheduler.DAGScheduler: Registering RDD 20 (mapToPair at HoodieJavaRDD.java:149)
24/01/05 02:11:34 INFO scheduler.DAGScheduler: Registering RDD 28 (countByKey at HoodieJavaPairRDD.java:105)
24/01/05 02:11:34 INFO scheduler.DAGScheduler: Got job 2 (countByKey at HoodieJavaPairRDD.java:105) with 2 output partitions
24/01/05 02:11:34 INFO scheduler.DAGScheduler: Final stage: ResultStage 8 (countByKey at HoodieJavaPairRDD.java:105)
24/01/05 02:11:34 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 7)
24/01/05 02:11:34 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 7)
24/01/05 02:11:34 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 5 (MapPartitionsRDD[10] at mapToPair at HoodieJavaRDD.java:149), which has no missing parents
24/01/05 02:11:34 INFO memory.MemoryStore: Block broadcast_4 stored as values in memory (estimated size 23.7 KB, free 365.8 MB)
24/01/05 02:11:34 INFO memory.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 11.4 KB, free 365.8 MB)
24/01/05 02:11:34 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on adhoc-2:38731 (size: 11.4 KB, free: 366.2 MB)
24/01/05 02:11:34 INFO spark.SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1161
24/01/05 02:11:34 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 5 (MapPartitionsRDD[10] at mapToPair at HoodieJavaRDD.java:149) (first 15 tasks are for partitions Vector(0, 1))
24/01/05 02:11:34 INFO scheduler.TaskSchedulerImpl: Adding task set 5.0 with 2 tasks
24/01/05 02:11:34 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 6 (MapPartitionsRDD[20] at mapToPair at HoodieJavaRDD.java:149), which has no missing parents
24/01/05 02:11:34 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 5.0 (TID 6, localhost, executor driver, partition 0, PROCESS_LOCAL, 7651 bytes)
24/01/05 02:11:34 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 5.0 (TID 7, localhost, executor driver, partition 1, ANY, 7651 bytes)
24/01/05 02:11:34 INFO executor.Executor: Running task 0.0 in stage 5.0 (TID 6)
24/01/05 02:11:34 INFO executor.Executor: Running task 1.0 in stage 5.0 (TID 7)
24/01/05 02:11:34 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
24/01/05 02:11:34 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
24/01/05 02:11:34 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
24/01/05 02:11:34 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
24/01/05 02:11:34 INFO executor.Executor: Finished task 0.0 in stage 5.0 (TID 6). 1123 bytes result sent to driver
24/01/05 02:11:34 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 5.0 (TID 6) in 26 ms on localhost (executor driver) (1/2)
24/01/05 02:11:34 INFO executor.Executor: Finished task 1.0 in stage 5.0 (TID 7). 1252 bytes result sent to driver
24/01/05 02:11:34 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 5.0 (TID 7) in 35 ms on localhost (executor driver) (2/2)
24/01/05 02:11:34 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool 
24/01/05 02:11:34 INFO memory.MemoryStore: Block broadcast_5 stored as values in memory (estimated size 341.0 KB, free 365.5 MB)
24/01/05 02:11:34 INFO memory.MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 122.2 KB, free 365.4 MB)
24/01/05 02:11:34 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on adhoc-2:38731 (size: 122.2 KB, free: 366.1 MB)
24/01/05 02:11:34 INFO spark.SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1161
24/01/05 02:11:34 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[20] at mapToPair at HoodieJavaRDD.java:149) (first 15 tasks are for partitions Vector(0))
24/01/05 02:11:34 INFO scheduler.TaskSchedulerImpl: Adding task set 6.0 with 1 tasks
24/01/05 02:11:34 INFO scheduler.DAGScheduler: ShuffleMapStage 5 (mapToPair at HoodieJavaRDD.java:149) finished in 0.071 s
24/01/05 02:11:34 INFO scheduler.DAGScheduler: looking for newly runnable stages
24/01/05 02:11:34 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 6)
24/01/05 02:11:34 INFO scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 7, ResultStage 8)
24/01/05 02:11:34 INFO scheduler.DAGScheduler: failed: Set()
24/01/05 02:11:34 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 6.0 (TID 8, localhost, executor driver, partition 0, PROCESS_LOCAL, 8108 bytes)
24/01/05 02:11:34 INFO executor.Executor: Running task 0.0 in stage 6.0 (TID 8)
24/01/05 02:11:35 INFO hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 146 records.
24/01/05 02:11:35 INFO hadoop.InternalParquetRecordReader: at row 0. reading next block
24/01/05 02:11:35 INFO compress.CodecPool: Got brand-new decompressor [.gz]
24/01/05 02:11:35 INFO hadoop.InternalParquetRecordReader: block read in memory in 54 ms. row count = 146
24/01/05 02:11:35 INFO collection.RocksDBDAO: From Rocks DB : [/db_impl/db_impl.cc:896] ------- PERSISTING STATS -------
24/01/05 02:11:35 INFO collection.RocksDBDAO: From Rocks DB : [/db_impl/db_impl.cc:966] [Pre-GC] In-memory stats history size: 48 bytes, slice count: 0
24/01/05 02:11:35 INFO collection.RocksDBDAO: From Rocks DB : [/db_impl/db_impl.cc:975] [Post-GC] In-memory stats history size: 48 bytes, slice count: 0
24/01/05 02:11:35 INFO executor.Executor: Finished task 0.0 in stage 6.0 (TID 8). 822 bytes result sent to driver
24/01/05 02:11:35 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 6.0 (TID 8) in 645 ms on localhost (executor driver) (1/1)
24/01/05 02:11:35 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool 
24/01/05 02:11:35 INFO scheduler.DAGScheduler: ShuffleMapStage 6 (mapToPair at HoodieJavaRDD.java:149) finished in 0.701 s
24/01/05 02:11:35 INFO scheduler.DAGScheduler: looking for newly runnable stages
24/01/05 02:11:35 INFO scheduler.DAGScheduler: running: Set()
24/01/05 02:11:35 INFO scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 7, ResultStage 8)
24/01/05 02:11:35 INFO scheduler.DAGScheduler: failed: Set()
24/01/05 02:11:35 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 7 (MapPartitionsRDD[28] at countByKey at HoodieJavaPairRDD.java:105), which has no missing parents
24/01/05 02:11:35 INFO memory.MemoryStore: Block broadcast_6 stored as values in memory (estimated size 7.7 KB, free 365.4 MB)
24/01/05 02:11:35 INFO memory.MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 4.0 KB, free 365.4 MB)
24/01/05 02:11:35 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on adhoc-2:38731 (size: 4.0 KB, free: 366.0 MB)
24/01/05 02:11:35 INFO spark.SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1161
24/01/05 02:11:35 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 7 (MapPartitionsRDD[28] at countByKey at HoodieJavaPairRDD.java:105) (first 15 tasks are for partitions Vector(0, 1))
24/01/05 02:11:35 INFO scheduler.TaskSchedulerImpl: Adding task set 7.0 with 2 tasks
24/01/05 02:11:35 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 7.0 (TID 9, localhost, executor driver, partition 0, PROCESS_LOCAL, 7714 bytes)
24/01/05 02:11:35 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 7.0 (TID 10, localhost, executor driver, partition 1, PROCESS_LOCAL, 7714 bytes)
24/01/05 02:11:35 INFO executor.Executor: Running task 1.0 in stage 7.0 (TID 10)
24/01/05 02:11:35 INFO executor.Executor: Running task 0.0 in stage 7.0 (TID 9)
24/01/05 02:11:35 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
24/01/05 02:11:35 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
24/01/05 02:11:35 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
24/01/05 02:11:35 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
24/01/05 02:11:35 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
24/01/05 02:11:35 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
24/01/05 02:11:35 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
24/01/05 02:11:35 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
24/01/05 02:11:35 INFO memory.MemoryStore: Block rdd_26_0 stored as values in memory (estimated size 0.0 B, free 365.4 MB)
24/01/05 02:11:35 INFO storage.BlockManagerInfo: Added rdd_26_0 in memory on adhoc-2:38731 (size: 0.0 B, free: 366.0 MB)
24/01/05 02:11:35 INFO executor.Executor: Finished task 0.0 in stage 7.0 (TID 9). 1166 bytes result sent to driver
24/01/05 02:11:35 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 7.0 (TID 9) in 73 ms on localhost (executor driver) (1/2)
24/01/05 02:11:35 INFO memory.MemoryStore: Block rdd_26_1 stored as values in memory (estimated size 858.0 B, free 365.4 MB)
24/01/05 02:11:35 INFO storage.BlockManagerInfo: Added rdd_26_1 in memory on adhoc-2:38731 (size: 858.0 B, free: 366.0 MB)
24/01/05 02:11:35 INFO executor.Executor: Finished task 1.0 in stage 7.0 (TID 10). 1252 bytes result sent to driver
24/01/05 02:11:35 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 7.0 (TID 10) in 97 ms on localhost (executor driver) (2/2)
24/01/05 02:11:35 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool 
24/01/05 02:11:35 INFO scheduler.DAGScheduler: ShuffleMapStage 7 (countByKey at HoodieJavaPairRDD.java:105) finished in 0.112 s
24/01/05 02:11:35 INFO scheduler.DAGScheduler: looking for newly runnable stages
24/01/05 02:11:35 INFO scheduler.DAGScheduler: running: Set()
24/01/05 02:11:35 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 8)
24/01/05 02:11:35 INFO scheduler.DAGScheduler: failed: Set()
24/01/05 02:11:35 INFO scheduler.DAGScheduler: Submitting ResultStage 8 (ShuffledRDD[29] at countByKey at HoodieJavaPairRDD.java:105), which has no missing parents
24/01/05 02:11:35 INFO memory.MemoryStore: Block broadcast_7 stored as values in memory (estimated size 3.6 KB, free 365.3 MB)
24/01/05 02:11:35 INFO memory.MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 2.1 KB, free 365.3 MB)
24/01/05 02:11:35 INFO spark.ContextCleaner: Cleaned accumulator 78
24/01/05 02:11:35 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on adhoc-2:38731 (size: 2.1 KB, free: 366.0 MB)
24/01/05 02:11:35 INFO spark.SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1161
24/01/05 02:11:35 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ResultStage 8 (ShuffledRDD[29] at countByKey at HoodieJavaPairRDD.java:105) (first 15 tasks are for partitions Vector(0, 1))
24/01/05 02:11:35 INFO scheduler.TaskSchedulerImpl: Adding task set 8.0 with 2 tasks
24/01/05 02:11:35 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on adhoc-2:38731 in memory (size: 11.4 KB, free: 366.1 MB)
24/01/05 02:11:35 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 8.0 (TID 11, localhost, executor driver, partition 0, PROCESS_LOCAL, 7662 bytes)
24/01/05 02:11:35 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 8.0 (TID 12, localhost, executor driver, partition 1, ANY, 7662 bytes)
24/01/05 02:11:35 INFO executor.Executor: Running task 0.0 in stage 8.0 (TID 11)
24/01/05 02:11:35 INFO executor.Executor: Running task 1.0 in stage 8.0 (TID 12)
24/01/05 02:11:35 INFO spark.ContextCleaner: Cleaned accumulator 99
24/01/05 02:11:35 INFO spark.ContextCleaner: Cleaned accumulator 88
24/01/05 02:11:35 INFO spark.ContextCleaner: Cleaned accumulator 98
24/01/05 02:11:35 INFO spark.ContextCleaner: Cleaned accumulator 85
24/01/05 02:11:35 INFO spark.ContextCleaner: Cleaned accumulator 81
24/01/05 02:11:35 INFO spark.ContextCleaner: Cleaned accumulator 93
24/01/05 02:11:35 INFO spark.ContextCleaner: Cleaned accumulator 77
24/01/05 02:11:35 INFO spark.ContextCleaner: Cleaned accumulator 76
24/01/05 02:11:35 INFO spark.ContextCleaner: Cleaned accumulator 95
24/01/05 02:11:35 INFO spark.ContextCleaner: Cleaned accumulator 75
24/01/05 02:11:35 INFO spark.ContextCleaner: Cleaned accumulator 89
24/01/05 02:11:35 INFO spark.ContextCleaner: Cleaned accumulator 86
24/01/05 02:11:35 INFO spark.ContextCleaner: Cleaned accumulator 82
24/01/05 02:11:35 INFO spark.ContextCleaner: Cleaned accumulator 91
24/01/05 02:11:35 INFO spark.ContextCleaner: Cleaned accumulator 92
24/01/05 02:11:35 INFO spark.ContextCleaner: Cleaned accumulator 83
24/01/05 02:11:35 INFO spark.ContextCleaner: Cleaned accumulator 94
24/01/05 02:11:35 INFO spark.ContextCleaner: Cleaned accumulator 90
24/01/05 02:11:35 INFO spark.ContextCleaner: Cleaned accumulator 96
24/01/05 02:11:35 INFO spark.ContextCleaner: Cleaned accumulator 97
24/01/05 02:11:35 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
24/01/05 02:11:35 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
24/01/05 02:11:35 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
24/01/05 02:11:35 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
24/01/05 02:11:35 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on adhoc-2:38731 in memory (size: 121.3 KB, free: 366.2 MB)
24/01/05 02:11:35 INFO spark.ContextCleaner: Cleaned accumulator 84
24/01/05 02:11:35 INFO spark.ContextCleaner: Cleaned accumulator 87
24/01/05 02:11:35 INFO spark.ContextCleaner: Cleaned accumulator 79
24/01/05 02:11:35 INFO spark.ContextCleaner: Cleaned accumulator 80
24/01/05 02:11:35 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on adhoc-2:38731 in memory (size: 122.2 KB, free: 366.3 MB)
24/01/05 02:11:35 INFO executor.Executor: Finished task 0.0 in stage 8.0 (TID 11). 1098 bytes result sent to driver
24/01/05 02:11:35 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 8.0 (TID 11) in 22 ms on localhost (executor driver) (1/2)
24/01/05 02:11:35 INFO executor.Executor: Finished task 1.0 in stage 8.0 (TID 12). 1210 bytes result sent to driver
24/01/05 02:11:35 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 8.0 (TID 12) in 24 ms on localhost (executor driver) (2/2)
24/01/05 02:11:35 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool 
24/01/05 02:11:35 INFO scheduler.DAGScheduler: ResultStage 8 (countByKey at HoodieJavaPairRDD.java:105) finished in 0.068 s
24/01/05 02:11:35 INFO scheduler.DAGScheduler: Job 2 finished: countByKey at HoodieJavaPairRDD.java:105, took 0.905977 s
24/01/05 02:11:35 INFO commit.UpsertPartitioner: AvgRecordSize => 1024
24/01/05 02:11:35 INFO spark.SparkContext: Starting job: collectAsMap at UpsertPartitioner.java:282
24/01/05 02:11:35 INFO scheduler.DAGScheduler: Got job 3 (collectAsMap at UpsertPartitioner.java:282) with 1 output partitions
24/01/05 02:11:35 INFO scheduler.DAGScheduler: Final stage: ResultStage 9 (collectAsMap at UpsertPartitioner.java:282)
24/01/05 02:11:35 INFO scheduler.DAGScheduler: Parents of final stage: List()
24/01/05 02:11:35 INFO scheduler.DAGScheduler: Missing parents: List()
24/01/05 02:11:35 INFO scheduler.DAGScheduler: Submitting ResultStage 9 (MapPartitionsRDD[31] at mapToPair at UpsertPartitioner.java:281), which has no missing parents
24/01/05 02:11:35 INFO memory.MemoryStore: Block broadcast_8 stored as values in memory (estimated size 340.5 KB, free 365.9 MB)
24/01/05 02:11:35 INFO memory.MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 121.8 KB, free 365.8 MB)
24/01/05 02:11:35 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on adhoc-2:38731 (size: 121.8 KB, free: 366.2 MB)
24/01/05 02:11:35 INFO spark.SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1161
24/01/05 02:11:35 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[31] at mapToPair at UpsertPartitioner.java:281) (first 15 tasks are for partitions Vector(0))
24/01/05 02:11:35 INFO scheduler.TaskSchedulerImpl: Adding task set 9.0 with 1 tasks
24/01/05 02:11:35 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 9.0 (TID 13, localhost, executor driver, partition 0, PROCESS_LOCAL, 7735 bytes)
24/01/05 02:11:35 INFO executor.Executor: Running task 0.0 in stage 9.0 (TID 13)
24/01/05 02:11:35 INFO view.FileSystemViewManager: Creating View Manager with storage type :REMOTE_FIRST
24/01/05 02:11:35 INFO view.FileSystemViewManager: Creating remote first table view
24/01/05 02:11:35 INFO view.FileSystemViewManager: Creating remote view for basePath /user/hive/warehouse/stock_ticks_mor. Server=adhoc-2:35479, Timeout=300
24/01/05 02:11:35 INFO view.FileSystemViewManager: Creating InMemory based view for basePath /user/hive/warehouse/stock_ticks_mor
24/01/05 02:11:35 INFO view.AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
24/01/05 02:11:35 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:11:35 INFO view.RemoteHoodieTableFileSystemView: Sending request : (http://adhoc-2:35479/v1/hoodie/view/slices/beforeoron/latest/?partition=2018%2F08%2F31&maxinstant=20240105021110208&includependingcompaction=false&basepath=%2Fuser%2Fhive%2Fwarehouse%2Fstock_ticks_mor&lastinstantts=20240105021110208&timelinehash=4edbe545d5e4877759b59ac9b4488385fad3f5cc412ad1a7af51246e9bb79357)
24/01/05 02:11:36 INFO collection.RocksDBDAO: Prefix Search for (query=type=slice,part=2018/08/31,id=) on hudi_view__user_hive_warehouse_stock_ticks_mor. Total Time Taken (msec)=8. Serialization Time taken(micro)=7835, num entries=1
24/01/05 02:11:36 INFO executor.Executor: Finished task 0.0 in stage 9.0 (TID 13). 813 bytes result sent to driver
24/01/05 02:11:36 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 9.0 (TID 13) in 69 ms on localhost (executor driver) (1/1)
24/01/05 02:11:36 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool 
24/01/05 02:11:36 INFO scheduler.DAGScheduler: ResultStage 9 (collectAsMap at UpsertPartitioner.java:282) finished in 0.105 s
24/01/05 02:11:36 INFO scheduler.DAGScheduler: Job 3 finished: collectAsMap at UpsertPartitioner.java:282, took 0.107917 s
24/01/05 02:11:36 INFO view.AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
24/01/05 02:11:36 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:11:36 INFO commit.UpsertPartitioner: Total Buckets: 1
24/01/05 02:11:36 INFO timeline.HoodieActiveTimeline: Checking for file exists ?/user/hive/warehouse/stock_ticks_mor/.hoodie/20240105021130096.deltacommit.requested
24/01/05 02:11:36 INFO timeline.HoodieActiveTimeline: Create new file for toInstant ?/user/hive/warehouse/stock_ticks_mor/.hoodie/20240105021130096.deltacommit.inflight
24/01/05 02:11:36 INFO commit.BaseSparkCommitActionExecutor: no validators configured.
24/01/05 02:11:36 INFO commit.BaseCommitActionExecutor: Auto commit disabled for 20240105021130096
24/01/05 02:11:36 INFO spark.SparkContext: Starting job: sum at StreamSync.java:783
24/01/05 02:11:36 INFO scheduler.DAGScheduler: Registering RDD 32 (mapToPair at HoodieJavaRDD.java:149)
24/01/05 02:11:36 INFO scheduler.DAGScheduler: Got job 4 (sum at StreamSync.java:783) with 1 output partitions
24/01/05 02:11:36 INFO scheduler.DAGScheduler: Final stage: ResultStage 14 (sum at StreamSync.java:783)
24/01/05 02:11:36 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 13)
24/01/05 02:11:36 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 13)
24/01/05 02:11:36 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 13 (MapPartitionsRDD[32] at mapToPair at HoodieJavaRDD.java:149), which has no missing parents
24/01/05 02:11:36 INFO memory.MemoryStore: Block broadcast_9 stored as values in memory (estimated size 344.4 KB, free 365.5 MB)
24/01/05 02:11:36 INFO memory.MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 123.0 KB, free 365.4 MB)
24/01/05 02:11:36 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on adhoc-2:38731 (size: 123.0 KB, free: 366.1 MB)
24/01/05 02:11:36 INFO spark.SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1161
24/01/05 02:11:36 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 13 (MapPartitionsRDD[32] at mapToPair at HoodieJavaRDD.java:149) (first 15 tasks are for partitions Vector(0, 1))
24/01/05 02:11:36 INFO scheduler.TaskSchedulerImpl: Adding task set 13.0 with 2 tasks
24/01/05 02:11:36 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 13.0 (TID 14, localhost, executor driver, partition 0, PROCESS_LOCAL, 7714 bytes)
24/01/05 02:11:36 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 13.0 (TID 15, localhost, executor driver, partition 1, PROCESS_LOCAL, 7714 bytes)
24/01/05 02:11:36 INFO executor.Executor: Running task 0.0 in stage 13.0 (TID 14)
24/01/05 02:11:36 INFO executor.Executor: Running task 1.0 in stage 13.0 (TID 15)
24/01/05 02:11:36 INFO storage.BlockManager: Found block rdd_26_1 locally
24/01/05 02:11:36 INFO executor.Executor: Finished task 1.0 in stage 13.0 (TID 15). 950 bytes result sent to driver
24/01/05 02:11:36 INFO storage.BlockManager: Found block rdd_26_0 locally
24/01/05 02:11:36 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 13.0 (TID 15) in 64 ms on localhost (executor driver) (1/2)
24/01/05 02:11:36 INFO executor.Executor: Finished task 0.0 in stage 13.0 (TID 14). 735 bytes result sent to driver
24/01/05 02:11:36 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 13.0 (TID 14) in 76 ms on localhost (executor driver) (2/2)
24/01/05 02:11:36 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 13.0, whose tasks have all completed, from pool 
24/01/05 02:11:36 INFO scheduler.DAGScheduler: ShuffleMapStage 13 (mapToPair at HoodieJavaRDD.java:149) finished in 0.136 s
24/01/05 02:11:36 INFO scheduler.DAGScheduler: looking for newly runnable stages
24/01/05 02:11:36 INFO scheduler.DAGScheduler: running: Set()
24/01/05 02:11:36 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 14)
24/01/05 02:11:36 INFO scheduler.DAGScheduler: failed: Set()
24/01/05 02:11:36 INFO scheduler.DAGScheduler: Submitting ResultStage 14 (MapPartitionsRDD[37] at mapToDouble at StreamSync.java:783), which has no missing parents
24/01/05 02:11:36 INFO memory.MemoryStore: Block broadcast_10 stored as values in memory (estimated size 351.4 KB, free 365.0 MB)
24/01/05 02:11:36 INFO memory.MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 126.9 KB, free 364.9 MB)
24/01/05 02:11:36 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on adhoc-2:38731 (size: 126.9 KB, free: 365.9 MB)
24/01/05 02:11:36 INFO spark.SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1161
24/01/05 02:11:36 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 14 (MapPartitionsRDD[37] at mapToDouble at StreamSync.java:783) (first 15 tasks are for partitions Vector(0))
24/01/05 02:11:36 INFO scheduler.TaskSchedulerImpl: Adding task set 14.0 with 1 tasks
24/01/05 02:11:36 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 14.0 (TID 16, localhost, executor driver, partition 0, ANY, 7662 bytes)
24/01/05 02:11:36 INFO executor.Executor: Running task 0.0 in stage 14.0 (TID 16)
24/01/05 02:11:36 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
24/01/05 02:11:36 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
24/01/05 02:11:36 INFO deltacommit.BaseSparkDeltaCommitActionExecutor: Merging updates for commit 20240105021130096 for file 63b57973-43fe-40da-bb65-66fab049ab22-0
24/01/05 02:11:36 INFO view.FileSystemViewManager: Creating View Manager with storage type :REMOTE_FIRST
24/01/05 02:11:36 INFO view.FileSystemViewManager: Creating remote first table view
24/01/05 02:11:36 INFO view.FileSystemViewManager: Creating remote view for basePath /user/hive/warehouse/stock_ticks_mor. Server=adhoc-2:35479, Timeout=300
24/01/05 02:11:36 INFO view.FileSystemViewManager: Creating InMemory based view for basePath /user/hive/warehouse/stock_ticks_mor
24/01/05 02:11:36 INFO view.AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
24/01/05 02:11:36 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:11:36 INFO view.RemoteHoodieTableFileSystemView: Sending request : (http://adhoc-2:35479/v1/hoodie/view/slices/file/latest/?partition=2018%2F08%2F31&basepath=%2Fuser%2Fhive%2Fwarehouse%2Fstock_ticks_mor&fileid=63b57973-43fe-40da-bb65-66fab049ab22-0&lastinstantts=20240105021110208&timelinehash=4edbe545d5e4877759b59ac9b4488385fad3f5cc412ad1a7af51246e9bb79357)
24/01/05 02:11:36 INFO collection.RocksDBDAO: Prefix Search for (query=type=slice,part=2018/08/31,id=63b57973-43fe-40da-bb65-66fab049ab22-0,instant=) on hudi_view__user_hive_warehouse_stock_ticks_mor. Total Time Taken (msec)=7. Serialization Time taken(micro)=6213, num entries=1
# WARNING: Unable to attach Serviceability Agent. Unable to attach even with module exceptions: [org.apache.hudi.org.openjdk.jol.vm.sa.SASupportException: Sense failed., org.apache.hudi.org.openjdk.jol.vm.sa.SASupportException: Sense failed., org.apache.hudi.org.openjdk.jol.vm.sa.SASupportException: Sense failed.]
24/01/05 02:11:37 WARN marker.WriteMarkersFactory: Timeline-server-based markers are not supported for HDFS: base path /user/hive/warehouse/stock_ticks_mor.  Falling back to direct markers.
24/01/05 02:11:37 INFO marker.DirectWriteMarkers: Creating Marker Path=/user/hive/warehouse/stock_ticks_mor/.hoodie/.temp/20240105021130096/2018/08/31/63b57973-43fe-40da-bb65-66fab049ab22-0_0-14-16_20240105021110208.parquet.marker.APPEND
24/01/05 02:11:37 INFO marker.DirectWriteMarkers: [direct] Created marker file /user/hive/warehouse/stock_ticks_mor/.hoodie/.temp/20240105021130096/2018/08/31/63b57973-43fe-40da-bb65-66fab049ab22-0_0-14-16_20240105021110208.parquet.marker.APPEND in 12 ms
24/01/05 02:11:37 INFO log.HoodieLogFormat$WriterBuilder: Building HoodieLogFormat Writer
24/01/05 02:11:37 INFO log.HoodieLogFormat$WriterBuilder: HoodieLogFile on path /user/hive/warehouse/stock_ticks_mor/2018/08/31/.63b57973-43fe-40da-bb65-66fab049ab22-0_20240105021110208.log.1_0-14-16
24/01/05 02:11:37 INFO log.HoodieLogFormatWriter: HoodieLogFile{pathStr='/user/hive/warehouse/stock_ticks_mor/2018/08/31/.63b57973-43fe-40da-bb65-66fab049ab22-0_20240105021110208.log.1_0-14-16', fileLen=0} does not exist. Create a new file
24/01/05 02:11:37 INFO io.HoodieAppendHandle: AppendHandle for partitionPath 2018/08/31 filePath 2018/08/31/.63b57973-43fe-40da-bb65-66fab049ab22-0_20240105021110208.log.1_0-14-16, took 789 ms.
24/01/05 02:11:37 INFO memory.MemoryStore: Block rdd_36_0 stored as values in memory (estimated size 546.0 B, free 364.9 MB)
24/01/05 02:11:37 INFO storage.BlockManagerInfo: Added rdd_36_0 in memory on adhoc-2:38731 (size: 546.0 B, free: 365.9 MB)
24/01/05 02:11:37 INFO executor.Executor: Finished task 0.0 in stage 14.0 (TID 16). 974 bytes result sent to driver
24/01/05 02:11:37 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 14.0 (TID 16) in 869 ms on localhost (executor driver) (1/1)
24/01/05 02:11:37 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 14.0, whose tasks have all completed, from pool 
24/01/05 02:11:37 INFO scheduler.DAGScheduler: ResultStage 14 (sum at StreamSync.java:783) finished in 0.936 s
24/01/05 02:11:37 INFO scheduler.DAGScheduler: Job 4 finished: sum at StreamSync.java:783, took 1.084936 s
24/01/05 02:11:37 INFO spark.SparkContext: Starting job: sum at StreamSync.java:784
24/01/05 02:11:37 INFO scheduler.DAGScheduler: Got job 5 (sum at StreamSync.java:784) with 1 output partitions
24/01/05 02:11:37 INFO scheduler.DAGScheduler: Final stage: ResultStage 19 (sum at StreamSync.java:784)
24/01/05 02:11:37 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 18)
24/01/05 02:11:37 INFO scheduler.DAGScheduler: Missing parents: List()
24/01/05 02:11:37 INFO scheduler.DAGScheduler: Submitting ResultStage 19 (MapPartitionsRDD[39] at mapToDouble at StreamSync.java:784), which has no missing parents
24/01/05 02:11:37 INFO memory.MemoryStore: Block broadcast_11 stored as values in memory (estimated size 351.4 KB, free 364.6 MB)
24/01/05 02:11:37 INFO memory.MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 126.9 KB, free 364.4 MB)
24/01/05 02:11:37 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on adhoc-2:38731 (size: 126.9 KB, free: 365.8 MB)
24/01/05 02:11:37 INFO spark.SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1161
24/01/05 02:11:37 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 19 (MapPartitionsRDD[39] at mapToDouble at StreamSync.java:784) (first 15 tasks are for partitions Vector(0))
24/01/05 02:11:37 INFO scheduler.TaskSchedulerImpl: Adding task set 19.0 with 1 tasks
24/01/05 02:11:37 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 19.0 (TID 17, localhost, executor driver, partition 0, PROCESS_LOCAL, 7662 bytes)
24/01/05 02:11:37 INFO executor.Executor: Running task 0.0 in stage 19.0 (TID 17)
24/01/05 02:11:37 INFO storage.BlockManager: Found block rdd_36_0 locally
24/01/05 02:11:37 INFO executor.Executor: Finished task 0.0 in stage 19.0 (TID 17). 759 bytes result sent to driver
24/01/05 02:11:37 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 19.0 (TID 17) in 38 ms on localhost (executor driver) (1/1)
24/01/05 02:11:37 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 19.0, whose tasks have all completed, from pool 
24/01/05 02:11:37 INFO scheduler.DAGScheduler: ResultStage 19 (sum at StreamSync.java:784) finished in 0.088 s
24/01/05 02:11:37 INFO scheduler.DAGScheduler: Job 5 finished: sum at StreamSync.java:784, took 0.094133 s
24/01/05 02:11:37 INFO streamer.StreamSync: instantTime=20240105021130096, totalRecords=4, totalErrorRecords=0, totalSuccessfulRecords=4
24/01/05 02:11:37 INFO spark.SparkContext: Starting job: collect at SparkRDDWriteClient.java:103
24/01/05 02:11:37 INFO scheduler.DAGScheduler: Got job 6 (collect at SparkRDDWriteClient.java:103) with 1 output partitions
24/01/05 02:11:37 INFO scheduler.DAGScheduler: Final stage: ResultStage 24 (collect at SparkRDDWriteClient.java:103)
24/01/05 02:11:37 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 23)
24/01/05 02:11:37 INFO scheduler.DAGScheduler: Missing parents: List()
24/01/05 02:11:37 INFO scheduler.DAGScheduler: Submitting ResultStage 24 (MapPartitionsRDD[41] at map at SparkRDDWriteClient.java:103), which has no missing parents
24/01/05 02:11:37 INFO memory.MemoryStore: Block broadcast_12 stored as values in memory (estimated size 351.6 KB, free 364.1 MB)
24/01/05 02:11:37 INFO memory.MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 127.0 KB, free 364.0 MB)
24/01/05 02:11:37 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on adhoc-2:38731 (size: 127.0 KB, free: 365.7 MB)
24/01/05 02:11:37 INFO spark.SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1161
24/01/05 02:11:37 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 24 (MapPartitionsRDD[41] at map at SparkRDDWriteClient.java:103) (first 15 tasks are for partitions Vector(0))
24/01/05 02:11:37 INFO scheduler.TaskSchedulerImpl: Adding task set 24.0 with 1 tasks
24/01/05 02:11:37 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 24.0 (TID 18, localhost, executor driver, partition 0, PROCESS_LOCAL, 7662 bytes)
24/01/05 02:11:37 INFO executor.Executor: Running task 0.0 in stage 24.0 (TID 18)
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 148
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 276
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 286
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 187
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 285
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 101
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 250
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 224
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 165
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 115
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 159
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 110
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 199
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 242
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 195
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 239
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 147
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 178
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 123
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 236
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 251
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 138
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 212
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 141
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 190
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 297
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 232
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 177
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 289
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 248
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 223
24/01/05 02:11:37 INFO storage.BlockManagerInfo: Removed broadcast_8_piece0 on adhoc-2:38731 in memory (size: 121.8 KB, free: 365.8 MB)
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 227
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 233
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 152
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 247
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 267
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 270
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 203
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 208
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 274
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 290
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 210
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 264
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 245
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 186
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 133
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 293
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 299
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 262
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 144
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 121
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 296
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 284
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 255
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 288
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 157
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 249
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 291
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 149
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 130
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 295
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 127
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 161
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 164
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 179
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 205
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 153
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 238
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 112
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 184
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 211
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 162
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 131
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 207
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 231
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 241
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 117
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 155
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 105
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 118
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 114
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 254
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 116
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 174
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 275
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 136
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 166
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 244
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 279
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 214
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 171
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 229
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 257
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 271
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 258
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 259
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 263
24/01/05 02:11:37 INFO storage.BlockManagerInfo: Removed broadcast_10_piece0 on adhoc-2:38731 in memory (size: 126.9 KB, free: 365.9 MB)
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 192
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 100
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 228
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 202
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 246
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 273
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 298
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 209
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 280
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned shuffle 4
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 222
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 278
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 218
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 272
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 283
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 193
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 129
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 287
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 139
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 107
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 183
24/01/05 02:11:37 INFO storage.BlockManagerInfo: Removed broadcast_7_piece0 on adhoc-2:38731 in memory (size: 2.1 KB, free: 365.9 MB)
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 122
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 225
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 191
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 216
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 226
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 113
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 189
24/01/05 02:11:37 INFO storage.BlockManagerInfo: Removed broadcast_11_piece0 on adhoc-2:38731 in memory (size: 126.9 KB, free: 366.1 MB)
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 103
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 201
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 292
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 108
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 204
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 145
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 167
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 281
24/01/05 02:11:37 INFO storage.BlockManagerInfo: Removed broadcast_9_piece0 on adhoc-2:38731 in memory (size: 123.0 KB, free: 366.2 MB)
24/01/05 02:11:37 INFO storage.BlockManager: Found block rdd_36_0 locally
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 213
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 266
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 125
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 253
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 106
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 200
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 163
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 134
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 198
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 135
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 188
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 150
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 143
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 102
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 158
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 243
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 196
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 126
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 132
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 265
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 237
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 260
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 185
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 206
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 282
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 197
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 256
24/01/05 02:11:37 INFO executor.Executor: Finished task 0.0 in stage 24.0 (TID 18). 1223 bytes result sent to driver
24/01/05 02:11:37 INFO storage.BlockManagerInfo: Removed broadcast_6_piece0 on adhoc-2:38731 in memory (size: 4.0 KB, free: 366.2 MB)
24/01/05 02:11:37 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 24.0 (TID 18) in 53 ms on localhost (executor driver) (1/1)
24/01/05 02:11:37 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 24.0, whose tasks have all completed, from pool 
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 268
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 104
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 154
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 269
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 294
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 128
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 221
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 151
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 111
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 230
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 142
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 168
24/01/05 02:11:37 INFO scheduler.DAGScheduler: ResultStage 24 (collect at SparkRDDWriteClient.java:103) finished in 0.121 s
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 170
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 173
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 124
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 220
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 252
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 181
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 215
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 156
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 120
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 180
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 240
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 261
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 160
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 119
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 194
24/01/05 02:11:37 INFO scheduler.DAGScheduler: Job 6 finished: collect at SparkRDDWriteClient.java:103, took 0.124347 s
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 277
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 172
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 175
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 217
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 146
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 140
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 109
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 235
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 169
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 182
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 176
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 219
24/01/05 02:11:37 INFO client.BaseHoodieWriteClient: Committing 20240105021130096 action deltacommit
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 137
24/01/05 02:11:37 INFO spark.ContextCleaner: Cleaned accumulator 234
24/01/05 02:11:37 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:11:37 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:11:37 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:11:37 INFO table.HoodieTableMetaClient: Loading Active commit timeline for /user/hive/warehouse/stock_ticks_mor
24/01/05 02:11:37 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[==>20240105021130096__deltacommit__INFLIGHT__20240105021136166]}
24/01/05 02:11:37 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:11:37 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:11:37 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:11:37 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:11:37 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:11:37 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:11:37 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105021110208__deltacommit__COMPLETED__20240105021118540]}
24/01/05 02:11:37 INFO view.AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
24/01/05 02:11:37 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:11:37 INFO view.FileSystemViewManager: Creating View Manager with storage type :REMOTE_FIRST
24/01/05 02:11:37 INFO view.FileSystemViewManager: Creating remote first table view
24/01/05 02:11:37 INFO util.CommitUtils: Creating  metadata for UPSERT numWriteStats:1 numReplaceFileIds:0
24/01/05 02:11:37 INFO transaction.TransactionManager: Transaction starting for Option{val=[==>20240105021130096__deltacommit__INFLIGHT]} with latest completed transaction instant Option{val=[20240105021110208__deltacommit__COMPLETED__20240105021118626]}
24/01/05 02:11:37 INFO lock.LockManager: LockProvider org.apache.hudi.client.transaction.lock.InProcessLockProvider
24/01/05 02:11:37 INFO lock.InProcessLockProvider: Base Path /user/hive/warehouse/stock_ticks_mor, Lock Instance java.util.concurrent.locks.ReentrantReadWriteLock@2e204155[Write locks = 0, Read locks = 0], Thread main, In-process lock state ACQUIRING
24/01/05 02:11:37 INFO lock.InProcessLockProvider: Base Path /user/hive/warehouse/stock_ticks_mor, Lock Instance java.util.concurrent.locks.ReentrantReadWriteLock@2e204155[Write locks = 1, Read locks = 0], Thread main, In-process lock state ACQUIRED
24/01/05 02:11:37 INFO transaction.TransactionManager: Transaction started for Option{val=[==>20240105021130096__deltacommit__INFLIGHT]} with latest completed transaction instant Option{val=[20240105021110208__deltacommit__COMPLETED__20240105021118626]}
24/01/05 02:11:37 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:11:37 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:11:37 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:11:37 INFO table.HoodieTableMetaClient: Loading Active commit timeline for /user/hive/warehouse/stock_ticks_mor
24/01/05 02:11:37 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[==>20240105021130096__deltacommit__INFLIGHT__20240105021136166]}
24/01/05 02:11:37 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:11:37 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:11:37 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:11:37 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:11:37 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:11:37 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:11:37 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105021110208__deltacommit__COMPLETED__20240105021118540]}
24/01/05 02:11:37 INFO view.AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
24/01/05 02:11:37 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:11:37 INFO view.FileSystemViewManager: Creating View Manager with storage type :REMOTE_FIRST
24/01/05 02:11:37 INFO view.FileSystemViewManager: Creating remote first table view
24/01/05 02:11:37 INFO client.BaseHoodieWriteClient: Committing 20240105021130096 action deltacommit
24/01/05 02:11:37 WARN marker.WriteMarkersFactory: Timeline-server-based markers are not supported for HDFS: base path /user/hive/warehouse/stock_ticks_mor.  Falling back to direct markers.
24/01/05 02:11:37 INFO spark.SparkContext: Starting job: collect at HoodieSparkEngineContext.java:150
24/01/05 02:11:37 INFO scheduler.DAGScheduler: Got job 7 (collect at HoodieSparkEngineContext.java:150) with 1 output partitions
24/01/05 02:11:37 INFO scheduler.DAGScheduler: Final stage: ResultStage 25 (collect at HoodieSparkEngineContext.java:150)
24/01/05 02:11:37 INFO scheduler.DAGScheduler: Parents of final stage: List()
24/01/05 02:11:37 INFO scheduler.DAGScheduler: Missing parents: List()
24/01/05 02:11:37 INFO scheduler.DAGScheduler: Submitting ResultStage 25 (MapPartitionsRDD[43] at flatMap at HoodieSparkEngineContext.java:150), which has no missing parents
24/01/05 02:11:37 INFO memory.MemoryStore: Block broadcast_13 stored as values in memory (estimated size 72.5 KB, free 365.8 MB)
24/01/05 02:11:37 INFO memory.MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 26.7 KB, free 365.7 MB)
24/01/05 02:11:37 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on adhoc-2:38731 (size: 26.7 KB, free: 366.1 MB)
24/01/05 02:11:37 INFO spark.SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1161
24/01/05 02:11:37 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 25 (MapPartitionsRDD[43] at flatMap at HoodieSparkEngineContext.java:150) (first 15 tasks are for partitions Vector(0))
24/01/05 02:11:37 INFO scheduler.TaskSchedulerImpl: Adding task set 25.0 with 1 tasks
24/01/05 02:11:37 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 25.0 (TID 19, localhost, executor driver, partition 0, PROCESS_LOCAL, 7820 bytes)
24/01/05 02:11:37 INFO executor.Executor: Running task 0.0 in stage 25.0 (TID 19)
24/01/05 02:11:37 INFO executor.Executor: Finished task 0.0 in stage 25.0 (TID 19). 668 bytes result sent to driver
24/01/05 02:11:37 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 25.0 (TID 19) in 21 ms on localhost (executor driver) (1/1)
24/01/05 02:11:37 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 25.0, whose tasks have all completed, from pool 
24/01/05 02:11:37 INFO scheduler.DAGScheduler: ResultStage 25 (collect at HoodieSparkEngineContext.java:150) finished in 0.036 s
24/01/05 02:11:37 INFO scheduler.DAGScheduler: Job 7 finished: collect at HoodieSparkEngineContext.java:150, took 0.037852 s
24/01/05 02:11:37 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:11:37 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:11:37 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:11:37 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:11:37 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:11:37 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:11:37 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105021110208__deltacommit__COMPLETED__20240105021118540]}
24/01/05 02:11:37 INFO metadata.HoodieBackedTableMetadataWriter: Async metadata indexing disabled and following partitions already initialized: [files]
24/01/05 02:11:37 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:11:37 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:11:37 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:11:37 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:11:37 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:11:37 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:11:37 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105021110208__deltacommit__COMPLETED__20240105021118540]}
24/01/05 02:11:37 INFO view.AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
24/01/05 02:11:37 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:11:37 INFO metadata.HoodieTableMetadataUtil: Updating at 20240105021130096 from Commit/UPSERT. #partitions_updated=2, #files_added=1
24/01/05 02:11:37 INFO view.AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
24/01/05 02:11:37 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:11:37 INFO metadata.HoodieTableMetadataUtil: Loading latest file slices for metadata table partition files
24/01/05 02:11:37 INFO view.AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
24/01/05 02:11:37 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:11:37 INFO view.AbstractTableFileSystemView: Building file system view for partition (files)
24/01/05 02:11:37 INFO client.BaseHoodieClient: Embedded Timeline Server is disabled. Not starting timeline service
24/01/05 02:11:37 INFO client.BaseHoodieClient: Embedded Timeline Server is disabled. Not starting timeline service
24/01/05 02:11:37 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:11:37 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:11:37 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:11:37 INFO table.HoodieTableMetaClient: Loading Active commit timeline for /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:11:37 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105021110208__deltacommit__COMPLETED__20240105021118540]}
24/01/05 02:11:37 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:11:37 INFO view.FileSystemViewManager: Creating View Manager with storage type :MEMORY
24/01/05 02:11:37 INFO view.FileSystemViewManager: Creating in-memory based Table View
24/01/05 02:11:37 INFO metadata.HoodieBackedTableMetadataWriter: New commit at 20240105021130096 being applied to MDT.
24/01/05 02:11:37 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:11:37 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:11:37 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:11:37 INFO table.HoodieTableMetaClient: Loading Active commit timeline for /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:11:37 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105021110208__deltacommit__COMPLETED__20240105021118540]}
24/01/05 02:11:37 INFO util.CleanerUtils: Cleaned failed attempts if any
24/01/05 02:11:37 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:11:37 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:11:37 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:11:37 INFO table.HoodieTableMetaClient: Loading Active commit timeline for /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:11:37 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105021110208__deltacommit__COMPLETED__20240105021118540]}
24/01/05 02:11:37 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:11:38 INFO view.FileSystemViewManager: Creating View Manager with storage type :MEMORY
24/01/05 02:11:38 INFO view.FileSystemViewManager: Creating in-memory based Table View
24/01/05 02:11:38 INFO client.BaseHoodieWriteClient: Generate a new instant time: 20240105021130096 action: deltacommit
24/01/05 02:11:38 INFO timeline.HoodieActiveTimeline: Creating a new instant [==>20240105021130096__deltacommit__REQUESTED]
24/01/05 02:11:38 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:11:38 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:11:38 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:11:38 INFO table.HoodieTableMetaClient: Loading Active commit timeline for /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:11:38 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[==>20240105021130096__deltacommit__REQUESTED__20240105021138004]}
24/01/05 02:11:38 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:11:38 INFO view.FileSystemViewManager: Creating View Manager with storage type :MEMORY
24/01/05 02:11:38 INFO view.FileSystemViewManager: Creating in-memory based Table View
24/01/05 02:11:38 INFO async.AsyncCleanerService: The HoodieWriteClient is not configured to auto & async clean. Async clean service will not start.
24/01/05 02:11:38 INFO async.AsyncArchiveService: The HoodieWriteClient is not configured to auto & async archive. Async archive service will not start.
24/01/05 02:11:38 INFO view.AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
24/01/05 02:11:38 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:11:38 INFO spark.SparkContext: Starting job: countByKey at HoodieJavaPairRDD.java:105
24/01/05 02:11:38 INFO scheduler.DAGScheduler: Registering RDD 52 (countByKey at HoodieJavaPairRDD.java:105)
24/01/05 02:11:38 INFO scheduler.DAGScheduler: Got job 8 (countByKey at HoodieJavaPairRDD.java:105) with 1 output partitions
24/01/05 02:11:38 INFO scheduler.DAGScheduler: Final stage: ResultStage 27 (countByKey at HoodieJavaPairRDD.java:105)
24/01/05 02:11:38 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 26)
24/01/05 02:11:38 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 26)
24/01/05 02:11:38 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 26 (MapPartitionsRDD[52] at countByKey at HoodieJavaPairRDD.java:105), which has no missing parents
24/01/05 02:11:38 INFO memory.MemoryStore: Block broadcast_14 stored as values in memory (estimated size 8.6 KB, free 365.7 MB)
24/01/05 02:11:38 INFO memory.MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 4.7 KB, free 365.7 MB)
24/01/05 02:11:38 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on adhoc-2:38731 (size: 4.7 KB, free: 366.1 MB)
24/01/05 02:11:38 INFO spark.SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1161
24/01/05 02:11:38 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 26 (MapPartitionsRDD[52] at countByKey at HoodieJavaPairRDD.java:105) (first 15 tasks are for partitions Vector(0))
24/01/05 02:11:38 INFO scheduler.TaskSchedulerImpl: Adding task set 26.0 with 1 tasks
24/01/05 02:11:38 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 26.0 (TID 20, localhost, executor driver, partition 0, PROCESS_LOCAL, 8084 bytes)
24/01/05 02:11:38 INFO executor.Executor: Running task 0.0 in stage 26.0 (TID 20)
24/01/05 02:11:38 INFO memory.MemoryStore: Block rdd_50_0 stored as values in memory (estimated size 396.0 B, free 365.7 MB)
24/01/05 02:11:38 INFO storage.BlockManagerInfo: Added rdd_50_0 in memory on adhoc-2:38731 (size: 396.0 B, free: 366.1 MB)
24/01/05 02:11:38 INFO executor.Executor: Finished task 0.0 in stage 26.0 (TID 20). 950 bytes result sent to driver
24/01/05 02:11:38 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 26.0 (TID 20) in 26 ms on localhost (executor driver) (1/1)
24/01/05 02:11:38 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 26.0, whose tasks have all completed, from pool 
24/01/05 02:11:38 INFO scheduler.DAGScheduler: ShuffleMapStage 26 (countByKey at HoodieJavaPairRDD.java:105) finished in 0.044 s
24/01/05 02:11:38 INFO scheduler.DAGScheduler: looking for newly runnable stages
24/01/05 02:11:38 INFO scheduler.DAGScheduler: running: Set()
24/01/05 02:11:38 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 27)
24/01/05 02:11:38 INFO scheduler.DAGScheduler: failed: Set()
24/01/05 02:11:38 INFO scheduler.DAGScheduler: Submitting ResultStage 27 (ShuffledRDD[53] at countByKey at HoodieJavaPairRDD.java:105), which has no missing parents
24/01/05 02:11:38 INFO memory.MemoryStore: Block broadcast_15 stored as values in memory (estimated size 3.6 KB, free 365.7 MB)
24/01/05 02:11:38 INFO memory.MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 2.1 KB, free 365.7 MB)
24/01/05 02:11:38 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on adhoc-2:38731 (size: 2.1 KB, free: 366.1 MB)
24/01/05 02:11:38 INFO spark.SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1161
24/01/05 02:11:38 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 27 (ShuffledRDD[53] at countByKey at HoodieJavaPairRDD.java:105) (first 15 tasks are for partitions Vector(0))
24/01/05 02:11:38 INFO scheduler.TaskSchedulerImpl: Adding task set 27.0 with 1 tasks
24/01/05 02:11:38 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 27.0 (TID 21, localhost, executor driver, partition 0, ANY, 7662 bytes)
24/01/05 02:11:38 INFO executor.Executor: Running task 0.0 in stage 27.0 (TID 21)
24/01/05 02:11:38 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
24/01/05 02:11:38 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
24/01/05 02:11:38 INFO executor.Executor: Finished task 0.0 in stage 27.0 (TID 21). 1179 bytes result sent to driver
24/01/05 02:11:38 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 27.0 (TID 21) in 10 ms on localhost (executor driver) (1/1)
24/01/05 02:11:38 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 27.0, whose tasks have all completed, from pool 
24/01/05 02:11:38 INFO scheduler.DAGScheduler: ResultStage 27 (countByKey at HoodieJavaPairRDD.java:105) finished in 0.017 s
24/01/05 02:11:38 INFO scheduler.DAGScheduler: Job 8 finished: countByKey at HoodieJavaPairRDD.java:105, took 0.069460 s
24/01/05 02:11:38 INFO commit.UpsertPartitioner: AvgRecordSize => 1024
24/01/05 02:11:38 INFO spark.SparkContext: Starting job: collectAsMap at UpsertPartitioner.java:282
24/01/05 02:11:38 INFO scheduler.DAGScheduler: Got job 9 (collectAsMap at UpsertPartitioner.java:282) with 1 output partitions
24/01/05 02:11:38 INFO scheduler.DAGScheduler: Final stage: ResultStage 28 (collectAsMap at UpsertPartitioner.java:282)
24/01/05 02:11:38 INFO scheduler.DAGScheduler: Parents of final stage: List()
24/01/05 02:11:38 INFO scheduler.DAGScheduler: Missing parents: List()
24/01/05 02:11:38 INFO scheduler.DAGScheduler: Submitting ResultStage 28 (MapPartitionsRDD[55] at mapToPair at UpsertPartitioner.java:281), which has no missing parents
24/01/05 02:11:38 INFO memory.MemoryStore: Block broadcast_16 stored as values in memory (estimated size 268.9 KB, free 365.5 MB)
24/01/05 02:11:38 INFO memory.MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 95.8 KB, free 365.4 MB)
24/01/05 02:11:38 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on adhoc-2:38731 (size: 95.8 KB, free: 366.0 MB)
24/01/05 02:11:38 INFO spark.SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1161
24/01/05 02:11:38 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 28 (MapPartitionsRDD[55] at mapToPair at UpsertPartitioner.java:281) (first 15 tasks are for partitions Vector(0))
24/01/05 02:11:38 INFO scheduler.TaskSchedulerImpl: Adding task set 28.0 with 1 tasks
24/01/05 02:11:38 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 28.0 (TID 22, localhost, executor driver, partition 0, PROCESS_LOCAL, 7730 bytes)
24/01/05 02:11:38 INFO executor.Executor: Running task 0.0 in stage 28.0 (TID 22)
24/01/05 02:11:38 INFO view.FileSystemViewManager: Creating View Manager with storage type :MEMORY
24/01/05 02:11:38 INFO view.FileSystemViewManager: Creating in-memory based Table View
24/01/05 02:11:38 INFO view.FileSystemViewManager: Creating InMemory based view for basePath /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:11:38 INFO view.AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
24/01/05 02:11:38 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:11:38 INFO view.AbstractTableFileSystemView: Building file system view for partition (files)
24/01/05 02:11:38 INFO executor.Executor: Finished task 0.0 in stage 28.0 (TID 22). 700 bytes result sent to driver
24/01/05 02:11:38 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 28.0 (TID 22) in 27 ms on localhost (executor driver) (1/1)
24/01/05 02:11:38 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 28.0, whose tasks have all completed, from pool 
24/01/05 02:11:38 INFO scheduler.DAGScheduler: ResultStage 28 (collectAsMap at UpsertPartitioner.java:282) finished in 0.050 s
24/01/05 02:11:38 INFO scheduler.DAGScheduler: Job 9 finished: collectAsMap at UpsertPartitioner.java:282, took 0.051339 s
24/01/05 02:11:38 INFO view.AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
24/01/05 02:11:38 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:11:38 INFO commit.UpsertPartitioner: Total Buckets: 1
24/01/05 02:11:38 INFO timeline.HoodieActiveTimeline: Checking for file exists ?/user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/20240105021130096.deltacommit.requested
24/01/05 02:11:38 INFO timeline.HoodieActiveTimeline: Create new file for toInstant ?/user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/20240105021130096.deltacommit.inflight
24/01/05 02:11:38 INFO commit.BaseSparkCommitActionExecutor: no validators configured.
24/01/05 02:11:38 INFO commit.BaseCommitActionExecutor: Auto commit enabled: Committing 20240105021130096
24/01/05 02:11:38 INFO spark.SparkContext: Starting job: collect at HoodieJavaRDD.java:177
24/01/05 02:11:38 INFO scheduler.DAGScheduler: Registering RDD 56 (mapToPair at HoodieJavaRDD.java:149)
24/01/05 02:11:38 INFO scheduler.DAGScheduler: Got job 10 (collect at HoodieJavaRDD.java:177) with 1 output partitions
24/01/05 02:11:38 INFO scheduler.DAGScheduler: Final stage: ResultStage 30 (collect at HoodieJavaRDD.java:177)
24/01/05 02:11:38 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 29)
24/01/05 02:11:38 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 29)
24/01/05 02:11:38 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 29 (MapPartitionsRDD[56] at mapToPair at HoodieJavaRDD.java:149), which has no missing parents
24/01/05 02:11:38 INFO memory.MemoryStore: Block broadcast_17 stored as values in memory (estimated size 273.7 KB, free 365.1 MB)
24/01/05 02:11:38 INFO memory.MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 95.5 KB, free 365.0 MB)
24/01/05 02:11:38 INFO storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on adhoc-2:38731 (size: 95.5 KB, free: 366.0 MB)
24/01/05 02:11:38 INFO spark.SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1161
24/01/05 02:11:38 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 29 (MapPartitionsRDD[56] at mapToPair at HoodieJavaRDD.java:149) (first 15 tasks are for partitions Vector(0))
24/01/05 02:11:38 INFO scheduler.TaskSchedulerImpl: Adding task set 29.0 with 1 tasks
24/01/05 02:11:38 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 29.0 (TID 23, localhost, executor driver, partition 0, PROCESS_LOCAL, 8084 bytes)
24/01/05 02:11:38 INFO executor.Executor: Running task 0.0 in stage 29.0 (TID 23)
24/01/05 02:11:38 INFO storage.BlockManager: Found block rdd_50_0 locally
24/01/05 02:11:38 INFO executor.Executor: Finished task 0.0 in stage 29.0 (TID 23). 907 bytes result sent to driver
24/01/05 02:11:38 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 29.0 (TID 23) in 24 ms on localhost (executor driver) (1/1)
24/01/05 02:11:38 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 29.0, whose tasks have all completed, from pool 
24/01/05 02:11:38 INFO scheduler.DAGScheduler: ShuffleMapStage 29 (mapToPair at HoodieJavaRDD.java:149) finished in 0.044 s
24/01/05 02:11:38 INFO scheduler.DAGScheduler: looking for newly runnable stages
24/01/05 02:11:38 INFO scheduler.DAGScheduler: running: Set()
24/01/05 02:11:38 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 30)
24/01/05 02:11:38 INFO scheduler.DAGScheduler: failed: Set()
24/01/05 02:11:38 INFO scheduler.DAGScheduler: Submitting ResultStage 30 (MapPartitionsRDD[61] at map at HoodieJavaRDD.java:125), which has no missing parents
24/01/05 02:11:38 INFO memory.MemoryStore: Block broadcast_18 stored as values in memory (estimated size 348.5 KB, free 364.7 MB)
24/01/05 02:11:38 INFO memory.MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 126.9 KB, free 364.5 MB)
24/01/05 02:11:38 INFO storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on adhoc-2:38731 (size: 126.9 KB, free: 365.8 MB)
24/01/05 02:11:38 INFO spark.SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1161
24/01/05 02:11:38 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 30 (MapPartitionsRDD[61] at map at HoodieJavaRDD.java:125) (first 15 tasks are for partitions Vector(0))
24/01/05 02:11:38 INFO scheduler.TaskSchedulerImpl: Adding task set 30.0 with 1 tasks
24/01/05 02:11:38 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 30.0 (TID 24, localhost, executor driver, partition 0, ANY, 7662 bytes)
24/01/05 02:11:38 INFO executor.Executor: Running task 0.0 in stage 30.0 (TID 24)
24/01/05 02:11:38 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
24/01/05 02:11:38 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
24/01/05 02:11:38 INFO deltacommit.BaseSparkDeltaCommitActionExecutor: Merging updates for commit 20240105021130096 for file files-0000-0
24/01/05 02:11:38 INFO view.FileSystemViewManager: Creating View Manager with storage type :MEMORY
24/01/05 02:11:38 INFO view.FileSystemViewManager: Creating in-memory based Table View
24/01/05 02:11:38 INFO view.FileSystemViewManager: Creating InMemory based view for basePath /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:11:38 INFO view.AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
24/01/05 02:11:38 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:11:38 INFO view.AbstractTableFileSystemView: Building file system view for partition (files)
24/01/05 02:11:38 INFO marker.DirectWriteMarkers: Creating Marker Path=/user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/.temp/20240105021130096/files/files-0000-0_0-30-24_00000000000000010.hfile.marker.APPEND
24/01/05 02:11:38 INFO marker.DirectWriteMarkers: [direct] Created marker file /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/.temp/20240105021130096/files/files-0000-0_0-30-24_00000000000000010.hfile.marker.APPEND in 5 ms
24/01/05 02:11:38 INFO log.HoodieLogFormat$WriterBuilder: Building HoodieLogFormat Writer
24/01/05 02:11:38 INFO log.HoodieLogFormat$WriterBuilder: HoodieLogFile on path /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/files/.files-0000-0_00000000000000010.log.1_0-0-0
24/01/05 02:11:38 INFO log.HoodieLogFormatWriter: HoodieLogFile{pathStr='/user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/files/.files-0000-0_00000000000000010.log.1_0-0-0', fileLen=13458} exists. Appending to existing file
24/01/05 02:11:38 INFO impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
24/01/05 02:11:38 INFO impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
24/01/05 02:11:38 INFO impl.MetricsSystemImpl: HBase metrics system started
24/01/05 02:11:38 INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.hudi.org.apache.hadoop.hbase.metrics.impl.MetricRegistriesImpl
24/01/05 02:11:38 INFO compress.CodecPool: Got brand-new compressor [.gz]
24/01/05 02:11:38 INFO compress.CodecPool: Got brand-new compressor [.gz]
24/01/05 02:11:38 INFO spark.ContextCleaner: Cleaned accumulator 416
24/01/05 02:11:38 INFO spark.ContextCleaner: Cleaned accumulator 320
24/01/05 02:11:38 INFO spark.ContextCleaner: Cleaned accumulator 379
24/01/05 02:11:38 INFO spark.ContextCleaner: Cleaned accumulator 367
24/01/05 02:11:38 INFO spark.ContextCleaner: Cleaned accumulator 316
24/01/05 02:11:38 INFO spark.ContextCleaner: Cleaned accumulator 337
24/01/05 02:11:38 INFO spark.ContextCleaner: Cleaned accumulator 339
24/01/05 02:11:38 INFO spark.ContextCleaner: Cleaned accumulator 378
24/01/05 02:11:38 INFO spark.ContextCleaner: Cleaned accumulator 389
24/01/05 02:11:38 INFO spark.ContextCleaner: Cleaned accumulator 319
24/01/05 02:11:38 INFO spark.ContextCleaner: Cleaned accumulator 328
24/01/05 02:11:38 INFO spark.ContextCleaner: Cleaned accumulator 354
24/01/05 02:11:38 INFO spark.ContextCleaner: Cleaned accumulator 343
24/01/05 02:11:38 INFO spark.ContextCleaner: Cleaned accumulator 350
24/01/05 02:11:38 INFO storage.BlockManagerInfo: Removed broadcast_15_piece0 on adhoc-2:38731 in memory (size: 2.1 KB, free: 365.8 MB)
24/01/05 02:11:38 INFO spark.ContextCleaner: Cleaned accumulator 347
24/01/05 02:11:38 INFO spark.ContextCleaner: Cleaned accumulator 400
24/01/05 02:11:38 INFO spark.ContextCleaner: Cleaned accumulator 327
24/01/05 02:11:38 INFO spark.ContextCleaner: Cleaned accumulator 323
24/01/05 02:11:38 INFO spark.ContextCleaner: Cleaned accumulator 373
24/01/05 02:11:38 INFO spark.ContextCleaner: Cleaned accumulator 360
24/01/05 02:11:38 INFO spark.ContextCleaner: Cleaned accumulator 348
24/01/05 02:11:38 INFO spark.ContextCleaner: Cleaned accumulator 382
24/01/05 02:11:38 INFO spark.ContextCleaner: Cleaned accumulator 386
24/01/05 02:11:38 INFO spark.ContextCleaner: Cleaned accumulator 405
24/01/05 02:11:38 INFO spark.ContextCleaner: Cleaned accumulator 421
24/01/05 02:11:38 INFO storage.BlockManagerInfo: Removed broadcast_16_piece0 on adhoc-2:38731 in memory (size: 95.8 KB, free: 365.9 MB)
24/01/05 02:11:38 INFO spark.ContextCleaner: Cleaned accumulator 357
24/01/05 02:11:38 INFO spark.ContextCleaner: Cleaned accumulator 365
24/01/05 02:11:38 INFO spark.ContextCleaner: Cleaned accumulator 363
24/01/05 02:11:38 INFO spark.ContextCleaner: Cleaned accumulator 392
24/01/05 02:11:38 INFO spark.ContextCleaner: Cleaned accumulator 335
24/01/05 02:11:38 INFO spark.ContextCleaner: Cleaned shuffle 6
24/01/05 02:11:38 INFO spark.ContextCleaner: Cleaned accumulator 300
24/01/05 02:11:38 INFO spark.ContextCleaner: Cleaned accumulator 314
24/01/05 02:11:38 INFO spark.ContextCleaner: Cleaned accumulator 364
24/01/05 02:11:38 INFO spark.ContextCleaner: Cleaned accumulator 341
24/01/05 02:11:38 INFO spark.ContextCleaner: Cleaned accumulator 356
24/01/05 02:11:38 INFO spark.ContextCleaner: Cleaned accumulator 407
24/01/05 02:11:38 INFO spark.ContextCleaner: Cleaned accumulator 387
24/01/05 02:11:38 INFO storage.BlockManagerInfo: Removed broadcast_14_piece0 on adhoc-2:38731 in memory (size: 4.7 KB, free: 365.9 MB)
24/01/05 02:11:38 INFO spark.ContextCleaner: Cleaned accumulator 414
24/01/05 02:11:38 INFO spark.ContextCleaner: Cleaned accumulator 340
24/01/05 02:11:38 INFO spark.ContextCleaner: Cleaned accumulator 406
24/01/05 02:11:38 INFO spark.ContextCleaner: Cleaned accumulator 423
24/01/05 02:11:38 INFO spark.ContextCleaner: Cleaned accumulator 413
24/01/05 02:11:38 INFO spark.ContextCleaner: Cleaned accumulator 398
24/01/05 02:11:38 INFO spark.ContextCleaner: Cleaned accumulator 332
24/01/05 02:11:38 INFO spark.ContextCleaner: Cleaned accumulator 312
24/01/05 02:11:38 INFO spark.ContextCleaner: Cleaned accumulator 324
24/01/05 02:11:38 INFO spark.ContextCleaner: Cleaned accumulator 345
24/01/05 02:11:38 INFO spark.ContextCleaner: Cleaned accumulator 322
24/01/05 02:11:38 INFO spark.ContextCleaner: Cleaned accumulator 412
24/01/05 02:11:38 INFO spark.ContextCleaner: Cleaned accumulator 370
24/01/05 02:11:38 INFO spark.ContextCleaner: Cleaned accumulator 385
24/01/05 02:11:38 INFO spark.ContextCleaner: Cleaned accumulator 334
24/01/05 02:11:38 INFO spark.ContextCleaner: Cleaned accumulator 346
24/01/05 02:11:38 INFO spark.ContextCleaner: Cleaned accumulator 331
24/01/05 02:11:38 INFO spark.ContextCleaner: Cleaned accumulator 403
24/01/05 02:11:38 INFO spark.ContextCleaner: Cleaned accumulator 310
24/01/05 02:11:38 INFO spark.ContextCleaner: Cleaned accumulator 374
24/01/05 02:11:38 INFO spark.ContextCleaner: Cleaned accumulator 415
24/01/05 02:11:38 INFO spark.ContextCleaner: Cleaned accumulator 317
24/01/05 02:11:38 INFO storage.BlockManagerInfo: Removed broadcast_13_piece0 on adhoc-2:38731 in memory (size: 26.7 KB, free: 366.0 MB)
24/01/05 02:11:38 INFO spark.ContextCleaner: Cleaned accumulator 388
24/01/05 02:11:38 INFO spark.ContextCleaner: Cleaned accumulator 399
24/01/05 02:11:38 INFO spark.ContextCleaner: Cleaned accumulator 344
24/01/05 02:11:38 INFO spark.ContextCleaner: Cleaned accumulator 325
24/01/05 02:11:38 INFO storage.BlockManagerInfo: Removed broadcast_17_piece0 on adhoc-2:38731 in memory (size: 95.5 KB, free: 366.1 MB)
24/01/05 02:11:38 INFO spark.ContextCleaner: Cleaned accumulator 376
24/01/05 02:11:38 INFO spark.ContextCleaner: Cleaned accumulator 380
24/01/05 02:11:38 INFO spark.ContextCleaner: Cleaned accumulator 402
24/01/05 02:11:38 INFO spark.ContextCleaner: Cleaned accumulator 391
24/01/05 02:11:38 INFO spark.ContextCleaner: Cleaned accumulator 307
24/01/05 02:11:38 INFO spark.ContextCleaner: Cleaned accumulator 302
24/01/05 02:11:38 INFO spark.ContextCleaner: Cleaned accumulator 342
24/01/05 02:11:38 INFO spark.ContextCleaner: Cleaned accumulator 352
24/01/05 02:11:38 INFO spark.ContextCleaner: Cleaned accumulator 351
24/01/05 02:11:38 INFO spark.ContextCleaner: Cleaned accumulator 404
24/01/05 02:11:38 INFO spark.ContextCleaner: Cleaned accumulator 418
24/01/05 02:11:38 INFO spark.ContextCleaner: Cleaned accumulator 338
24/01/05 02:11:38 INFO spark.ContextCleaner: Cleaned accumulator 422
24/01/05 02:11:38 INFO spark.ContextCleaner: Cleaned accumulator 358
24/01/05 02:11:38 INFO spark.ContextCleaner: Cleaned accumulator 409
24/01/05 02:11:38 INFO spark.ContextCleaner: Cleaned accumulator 411
24/01/05 02:11:38 INFO spark.ContextCleaner: Cleaned accumulator 303
24/01/05 02:11:38 INFO spark.ContextCleaner: Cleaned accumulator 393
24/01/05 02:11:38 INFO spark.ContextCleaner: Cleaned accumulator 311
24/01/05 02:11:38 INFO spark.ContextCleaner: Cleaned accumulator 309
24/01/05 02:11:38 INFO spark.ContextCleaner: Cleaned accumulator 375
24/01/05 02:11:38 INFO spark.ContextCleaner: Cleaned accumulator 362
24/01/05 02:11:38 INFO spark.ContextCleaner: Cleaned accumulator 349
24/01/05 02:11:38 INFO spark.ContextCleaner: Cleaned accumulator 329
24/01/05 02:11:38 INFO spark.ContextCleaner: Cleaned accumulator 330
24/01/05 02:11:38 INFO spark.ContextCleaner: Cleaned accumulator 313
24/01/05 02:11:38 INFO spark.ContextCleaner: Cleaned accumulator 419
24/01/05 02:11:38 INFO spark.ContextCleaner: Cleaned accumulator 396
24/01/05 02:11:38 INFO spark.ContextCleaner: Cleaned accumulator 372
24/01/05 02:11:38 INFO spark.ContextCleaner: Cleaned accumulator 408
24/01/05 02:11:38 INFO spark.ContextCleaner: Cleaned accumulator 369
24/01/05 02:11:38 INFO spark.ContextCleaner: Cleaned accumulator 395
24/01/05 02:11:38 INFO spark.ContextCleaner: Cleaned accumulator 410
24/01/05 02:11:38 INFO spark.ContextCleaner: Cleaned accumulator 397
24/01/05 02:11:38 INFO spark.ContextCleaner: Cleaned accumulator 384
24/01/05 02:11:38 INFO spark.ContextCleaner: Cleaned accumulator 336
24/01/05 02:11:38 INFO spark.ContextCleaner: Cleaned accumulator 304
24/01/05 02:11:38 INFO spark.ContextCleaner: Cleaned accumulator 368
24/01/05 02:11:38 INFO spark.ContextCleaner: Cleaned accumulator 333
24/01/05 02:11:38 INFO spark.ContextCleaner: Cleaned accumulator 305
24/01/05 02:11:38 INFO spark.ContextCleaner: Cleaned accumulator 377
24/01/05 02:11:38 INFO spark.ContextCleaner: Cleaned accumulator 361
24/01/05 02:11:38 INFO spark.ContextCleaner: Cleaned accumulator 366
24/01/05 02:11:38 INFO spark.ContextCleaner: Cleaned accumulator 394
24/01/05 02:11:38 INFO spark.ContextCleaner: Cleaned accumulator 371
24/01/05 02:11:38 INFO spark.ContextCleaner: Cleaned accumulator 308
24/01/05 02:11:38 INFO spark.ContextCleaner: Cleaned accumulator 355
24/01/05 02:11:38 INFO storage.BlockManagerInfo: Removed broadcast_12_piece0 on adhoc-2:38731 in memory (size: 127.0 KB, free: 366.2 MB)
24/01/05 02:11:38 INFO spark.ContextCleaner: Cleaned accumulator 424
24/01/05 02:11:38 INFO spark.ContextCleaner: Cleaned accumulator 353
24/01/05 02:11:38 INFO spark.ContextCleaner: Cleaned accumulator 301
24/01/05 02:11:38 INFO spark.ContextCleaner: Cleaned accumulator 420
24/01/05 02:11:38 INFO spark.ContextCleaner: Cleaned accumulator 401
24/01/05 02:11:38 INFO spark.ContextCleaner: Cleaned accumulator 417
24/01/05 02:11:38 INFO spark.ContextCleaner: Cleaned accumulator 381
24/01/05 02:11:38 INFO spark.ContextCleaner: Cleaned accumulator 383
24/01/05 02:11:38 INFO spark.ContextCleaner: Cleaned accumulator 326
24/01/05 02:11:38 INFO spark.ContextCleaner: Cleaned accumulator 321
24/01/05 02:11:38 INFO spark.ContextCleaner: Cleaned accumulator 359
24/01/05 02:11:38 INFO spark.ContextCleaner: Cleaned accumulator 390
24/01/05 02:11:38 INFO spark.ContextCleaner: Cleaned accumulator 306
24/01/05 02:11:38 INFO spark.ContextCleaner: Cleaned accumulator 318
24/01/05 02:11:38 INFO spark.ContextCleaner: Cleaned accumulator 315
24/01/05 02:11:39 INFO io.HoodieAppendHandle: AppendHandle for partitionPath files filePath files/.files-0000-0_00000000000000010.log.1_0-0-0, took 634 ms.
24/01/05 02:11:39 INFO memory.MemoryStore: Block rdd_60_0 stored as values in memory (estimated size 438.0 B, free 365.8 MB)
24/01/05 02:11:39 INFO storage.BlockManagerInfo: Added rdd_60_0 in memory on adhoc-2:38731 (size: 438.0 B, free: 366.2 MB)
24/01/05 02:11:39 INFO executor.Executor: Finished task 0.0 in stage 30.0 (TID 24). 1529 bytes result sent to driver
24/01/05 02:11:39 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 30.0 (TID 24) in 1093 ms on localhost (executor driver) (1/1)
24/01/05 02:11:39 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 30.0, whose tasks have all completed, from pool 
24/01/05 02:11:39 INFO scheduler.DAGScheduler: ResultStage 30 (collect at HoodieJavaRDD.java:177) finished in 1.131 s
24/01/05 02:11:39 INFO scheduler.DAGScheduler: Job 10 finished: collect at HoodieJavaRDD.java:177, took 1.179724 s
24/01/05 02:11:39 INFO util.CommitUtils: Creating  metadata for UPSERT_PREPPED numWriteStats:1 numReplaceFileIds:0
24/01/05 02:11:39 INFO commit.BaseSparkCommitActionExecutor: Committing 20240105021130096, action Type deltacommit, operation Type UPSERT_PREPPED
24/01/05 02:11:39 INFO spark.SparkContext: Starting job: collect at HoodieSparkEngineContext.java:150
24/01/05 02:11:39 INFO scheduler.DAGScheduler: Got job 11 (collect at HoodieSparkEngineContext.java:150) with 1 output partitions
24/01/05 02:11:39 INFO scheduler.DAGScheduler: Final stage: ResultStage 31 (collect at HoodieSparkEngineContext.java:150)
24/01/05 02:11:39 INFO scheduler.DAGScheduler: Parents of final stage: List()
24/01/05 02:11:39 INFO scheduler.DAGScheduler: Missing parents: List()
24/01/05 02:11:39 INFO scheduler.DAGScheduler: Submitting ResultStage 31 (MapPartitionsRDD[63] at flatMap at HoodieSparkEngineContext.java:150), which has no missing parents
24/01/05 02:11:39 INFO memory.MemoryStore: Block broadcast_19 stored as values in memory (estimated size 72.5 KB, free 365.8 MB)
24/01/05 02:11:39 INFO memory.MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 26.7 KB, free 365.7 MB)
24/01/05 02:11:39 INFO storage.BlockManagerInfo: Added broadcast_19_piece0 in memory on adhoc-2:38731 (size: 26.7 KB, free: 366.1 MB)
24/01/05 02:11:39 INFO spark.SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1161
24/01/05 02:11:39 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 31 (MapPartitionsRDD[63] at flatMap at HoodieSparkEngineContext.java:150) (first 15 tasks are for partitions Vector(0))
24/01/05 02:11:39 INFO scheduler.TaskSchedulerImpl: Adding task set 31.0 with 1 tasks
24/01/05 02:11:39 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 31.0 (TID 25, localhost, executor driver, partition 0, PROCESS_LOCAL, 7838 bytes)
24/01/05 02:11:39 INFO executor.Executor: Running task 0.0 in stage 31.0 (TID 25)
24/01/05 02:11:39 INFO executor.Executor: Finished task 0.0 in stage 31.0 (TID 25). 668 bytes result sent to driver
24/01/05 02:11:39 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 31.0 (TID 25) in 10 ms on localhost (executor driver) (1/1)
24/01/05 02:11:39 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 31.0, whose tasks have all completed, from pool 
24/01/05 02:11:39 INFO scheduler.DAGScheduler: ResultStage 31 (collect at HoodieSparkEngineContext.java:150) finished in 0.027 s
24/01/05 02:11:39 INFO scheduler.DAGScheduler: Job 11 finished: collect at HoodieSparkEngineContext.java:150, took 0.029686 s
24/01/05 02:11:39 INFO timeline.HoodieActiveTimeline: Marking instant complete [==>20240105021130096__deltacommit__INFLIGHT]
24/01/05 02:11:39 INFO timeline.HoodieActiveTimeline: Checking for file exists ?/user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/20240105021130096.deltacommit.inflight
24/01/05 02:11:39 INFO timeline.HoodieActiveTimeline: Create new file for toInstant ?/user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/20240105021130096.deltacommit
24/01/05 02:11:39 INFO timeline.HoodieActiveTimeline: Completed [==>20240105021130096__deltacommit__INFLIGHT]
24/01/05 02:11:39 INFO commit.BaseSparkCommitActionExecutor: Committed 20240105021130096
24/01/05 02:11:39 INFO spark.SparkContext: Starting job: collectAsMap at HoodieSparkEngineContext.java:164
24/01/05 02:11:39 INFO scheduler.DAGScheduler: Got job 12 (collectAsMap at HoodieSparkEngineContext.java:164) with 1 output partitions
24/01/05 02:11:39 INFO scheduler.DAGScheduler: Final stage: ResultStage 32 (collectAsMap at HoodieSparkEngineContext.java:164)
24/01/05 02:11:39 INFO scheduler.DAGScheduler: Parents of final stage: List()
24/01/05 02:11:39 INFO scheduler.DAGScheduler: Missing parents: List()
24/01/05 02:11:39 INFO scheduler.DAGScheduler: Submitting ResultStage 32 (MapPartitionsRDD[65] at mapToPair at HoodieSparkEngineContext.java:161), which has no missing parents
24/01/05 02:11:39 INFO memory.MemoryStore: Block broadcast_20 stored as values in memory (estimated size 72.8 KB, free 365.7 MB)
24/01/05 02:11:39 INFO memory.MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 26.8 KB, free 365.6 MB)
24/01/05 02:11:39 INFO storage.BlockManagerInfo: Added broadcast_20_piece0 in memory on adhoc-2:38731 (size: 26.8 KB, free: 366.1 MB)
24/01/05 02:11:39 INFO spark.SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:1161
24/01/05 02:11:39 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 32 (MapPartitionsRDD[65] at mapToPair at HoodieSparkEngineContext.java:161) (first 15 tasks are for partitions Vector(0))
24/01/05 02:11:39 INFO scheduler.TaskSchedulerImpl: Adding task set 32.0 with 1 tasks
24/01/05 02:11:39 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 32.0 (TID 26, localhost, executor driver, partition 0, PROCESS_LOCAL, 7838 bytes)
24/01/05 02:11:39 INFO executor.Executor: Running task 0.0 in stage 32.0 (TID 26)
24/01/05 02:11:39 INFO executor.Executor: Finished task 0.0 in stage 32.0 (TID 26). 787 bytes result sent to driver
24/01/05 02:11:39 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 32.0 (TID 26) in 18 ms on localhost (executor driver) (1/1)
24/01/05 02:11:39 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 32.0, whose tasks have all completed, from pool 
24/01/05 02:11:39 INFO scheduler.DAGScheduler: ResultStage 32 (collectAsMap at HoodieSparkEngineContext.java:164) finished in 0.029 s
24/01/05 02:11:39 INFO scheduler.DAGScheduler: Job 12 finished: collectAsMap at HoodieSparkEngineContext.java:164, took 0.031117 s
24/01/05 02:11:39 INFO fs.FSUtils: Removed directory at /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/.temp/20240105021130096
24/01/05 02:11:39 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:11:39 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:11:39 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:11:39 INFO table.HoodieTableMetaClient: Loading Active commit timeline for /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:11:39 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105021130096__deltacommit__COMPLETED__20240105021139601]}
24/01/05 02:11:39 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:11:39 INFO view.FileSystemViewManager: Creating View Manager with storage type :MEMORY
24/01/05 02:11:39 INFO view.FileSystemViewManager: Creating in-memory based Table View
24/01/05 02:11:39 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105021130096__deltacommit__COMPLETED__20240105021139601]}
24/01/05 02:11:39 INFO timeline.HoodieActiveTimeline: Marking instant complete [==>20240105021130096__deltacommit__INFLIGHT]
24/01/05 02:11:39 INFO timeline.HoodieActiveTimeline: Checking for file exists ?/user/hive/warehouse/stock_ticks_mor/.hoodie/20240105021130096.deltacommit.inflight
24/01/05 02:11:39 INFO timeline.HoodieActiveTimeline: Create new file for toInstant ?/user/hive/warehouse/stock_ticks_mor/.hoodie/20240105021130096.deltacommit
24/01/05 02:11:39 INFO timeline.HoodieActiveTimeline: Completed [==>20240105021130096__deltacommit__INFLIGHT]
24/01/05 02:11:39 WARN marker.WriteMarkersFactory: Timeline-server-based markers are not supported for HDFS: base path /user/hive/warehouse/stock_ticks_mor.  Falling back to direct markers.
24/01/05 02:11:39 INFO spark.SparkContext: Starting job: collectAsMap at HoodieSparkEngineContext.java:164
24/01/05 02:11:39 INFO scheduler.DAGScheduler: Got job 13 (collectAsMap at HoodieSparkEngineContext.java:164) with 1 output partitions
24/01/05 02:11:39 INFO scheduler.DAGScheduler: Final stage: ResultStage 33 (collectAsMap at HoodieSparkEngineContext.java:164)
24/01/05 02:11:39 INFO scheduler.DAGScheduler: Parents of final stage: List()
24/01/05 02:11:39 INFO scheduler.DAGScheduler: Missing parents: List()
24/01/05 02:11:39 INFO scheduler.DAGScheduler: Submitting ResultStage 33 (MapPartitionsRDD[67] at mapToPair at HoodieSparkEngineContext.java:161), which has no missing parents
24/01/05 02:11:39 INFO memory.MemoryStore: Block broadcast_21 stored as values in memory (estimated size 72.8 KB, free 365.6 MB)
24/01/05 02:11:39 INFO memory.MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 26.8 KB, free 365.5 MB)
24/01/05 02:11:39 INFO storage.BlockManagerInfo: Added broadcast_21_piece0 in memory on adhoc-2:38731 (size: 26.8 KB, free: 366.1 MB)
24/01/05 02:11:39 INFO spark.SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:1161
24/01/05 02:11:39 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 33 (MapPartitionsRDD[67] at mapToPair at HoodieSparkEngineContext.java:161) (first 15 tasks are for partitions Vector(0))
24/01/05 02:11:39 INFO scheduler.TaskSchedulerImpl: Adding task set 33.0 with 1 tasks
24/01/05 02:11:39 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 33.0 (TID 27, localhost, executor driver, partition 0, PROCESS_LOCAL, 7820 bytes)
24/01/05 02:11:39 INFO executor.Executor: Running task 0.0 in stage 33.0 (TID 27)
24/01/05 02:11:39 INFO executor.Executor: Finished task 0.0 in stage 33.0 (TID 27). 769 bytes result sent to driver
24/01/05 02:11:39 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 33.0 (TID 27) in 20 ms on localhost (executor driver) (1/1)
24/01/05 02:11:39 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 33.0, whose tasks have all completed, from pool 
24/01/05 02:11:39 INFO scheduler.DAGScheduler: ResultStage 33 (collectAsMap at HoodieSparkEngineContext.java:164) finished in 0.037 s
24/01/05 02:11:39 INFO scheduler.DAGScheduler: Job 13 finished: collectAsMap at HoodieSparkEngineContext.java:164, took 0.041593 s
24/01/05 02:11:39 INFO fs.FSUtils: Removed directory at /user/hive/warehouse/stock_ticks_mor/.hoodie/.temp/20240105021130096
24/01/05 02:11:39 INFO client.BaseHoodieWriteClient: Committed 20240105021130096
24/01/05 02:11:39 INFO rdd.MapPartitionsRDD: Removing RDD 26 from persistence list
24/01/05 02:11:39 INFO storage.BlockManager: Removing RDD 26
24/01/05 02:11:39 INFO rdd.MapPartitionsRDD: Removing RDD 36 from persistence list
24/01/05 02:11:39 INFO storage.BlockManager: Removing RDD 36
24/01/05 02:11:39 INFO rdd.UnionRDD: Removing RDD 50 from persistence list
24/01/05 02:11:39 INFO storage.BlockManager: Removing RDD 50
24/01/05 02:11:39 INFO rdd.MapPartitionsRDD: Removing RDD 60 from persistence list
24/01/05 02:11:39 INFO storage.BlockManager: Removing RDD 60
24/01/05 02:11:39 INFO transaction.TransactionManager: Transaction ending with transaction owner Option{val=[==>20240105021130096__deltacommit__INFLIGHT]}
24/01/05 02:11:39 INFO lock.InProcessLockProvider: Base Path /user/hive/warehouse/stock_ticks_mor, Lock Instance java.util.concurrent.locks.ReentrantReadWriteLock@2e204155[Write locks = 1, Read locks = 0], Thread main, In-process lock state RELEASING
24/01/05 02:11:39 INFO lock.InProcessLockProvider: Base Path /user/hive/warehouse/stock_ticks_mor, Lock Instance java.util.concurrent.locks.ReentrantReadWriteLock@2e204155[Write locks = 0, Read locks = 0], Thread main, In-process lock state RELEASED
24/01/05 02:11:39 INFO lock.InProcessLockProvider: Base Path /user/hive/warehouse/stock_ticks_mor, Lock Instance java.util.concurrent.locks.ReentrantReadWriteLock@2e204155[Write locks = 0, Read locks = 0], Thread main, In-process lock state ALREADY_RELEASED
24/01/05 02:11:39 INFO lock.LockManager: Released connection created for acquiring lock
24/01/05 02:11:39 INFO transaction.TransactionManager: Transaction ended with transaction owner Option{val=[==>20240105021130096__deltacommit__INFLIGHT]}
24/01/05 02:11:39 INFO client.BaseHoodieWriteClient: Start to clean synchronously.
24/01/05 02:11:39 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:11:39 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:11:39 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:11:39 INFO table.HoodieTableMetaClient: Loading Active commit timeline for /user/hive/warehouse/stock_ticks_mor
24/01/05 02:11:39 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105021130096__deltacommit__COMPLETED__20240105021139690]}
24/01/05 02:11:39 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:11:39 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:11:39 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:11:39 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:11:39 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:11:39 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:11:39 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105021130096__deltacommit__COMPLETED__20240105021139601]}
24/01/05 02:11:39 INFO view.AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
24/01/05 02:11:39 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:11:39 INFO view.FileSystemViewManager: Creating View Manager with storage type :REMOTE_FIRST
24/01/05 02:11:39 INFO view.FileSystemViewManager: Creating remote first table view
24/01/05 02:11:39 INFO client.BaseHoodieWriteClient: Cleaner started
24/01/05 02:11:39 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:11:39 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:11:39 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:11:39 INFO table.HoodieTableMetaClient: Loading Active commit timeline for /user/hive/warehouse/stock_ticks_mor
24/01/05 02:11:39 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105021130096__deltacommit__COMPLETED__20240105021139690]}
24/01/05 02:11:39 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:11:39 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:11:39 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:11:39 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:11:39 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:11:39 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:11:39 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105021130096__deltacommit__COMPLETED__20240105021139601]}
24/01/05 02:11:39 INFO view.AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
24/01/05 02:11:39 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:11:39 INFO view.FileSystemViewManager: Creating View Manager with storage type :REMOTE_FIRST
24/01/05 02:11:39 INFO view.FileSystemViewManager: Creating remote first table view
24/01/05 02:11:39 INFO client.BaseHoodieWriteClient: Scheduling cleaning at instant time :20240105021139776
24/01/05 02:11:39 INFO view.FileSystemViewManager: Creating remote view for basePath /user/hive/warehouse/stock_ticks_mor. Server=adhoc-2:35479, Timeout=300
24/01/05 02:11:39 INFO view.FileSystemViewManager: Creating InMemory based view for basePath /user/hive/warehouse/stock_ticks_mor
24/01/05 02:11:39 INFO view.AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
24/01/05 02:11:39 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:11:39 INFO view.RemoteHoodieTableFileSystemView: Sending request : (http://adhoc-2:35479/v1/hoodie/view/compactions/pending/?basepath=%2Fuser%2Fhive%2Fwarehouse%2Fstock_ticks_mor&lastinstantts=20240105021130096&timelinehash=103ad031554a22043ed4a74ba8b432d0278d2aeb876bc11f809fb9c1b2fb2c5b)
24/01/05 02:11:39 INFO service.RequestHandler: Syncing view as client passed last known instant 20240105021130096 as last known instant but server has the following last instant on timeline :Option{val=[20240105021110208__deltacommit__COMPLETED__20240105021118626]}
24/01/05 02:11:39 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105021130096__deltacommit__COMPLETED__20240105021139690]}
24/01/05 02:11:39 INFO view.RocksDbBasedFileSystemView: Deleting all rocksdb data associated with table filesystem view
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : [/db_impl/db_impl.cc:478] Shutdown: canceling all background work
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : [/db_impl/db_impl.cc:677] Shutdown complete
24/01/05 02:11:39 INFO collection.RocksDBDAO: DELETING RocksDB persisted at /tmp/hoodie_timeline_rocksdb/_user_hive_warehouse_stock_ticks_mor/8c84a041-4067-4789-994f-54f5766fb0f6
24/01/05 02:11:39 INFO collection.RocksDBDAO: No column family found. Loading default
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : RocksDB version: 7.5.3

24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Compile date 2022-09-02 09:50:20
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : DB SUMMARY

24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : DB Session ID:  W9H7ZZFBC6YV5VWRYXRM

24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : SST files in /tmp/hoodie_timeline_rocksdb/_user_hive_warehouse_stock_ticks_mor/8c84a041-4067-4789-994f-54f5766fb0f6 dir, Total Num: 0, files: 

24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Write Ahead Log file in /tmp/hoodie_timeline_rocksdb/_user_hive_warehouse_stock_ticks_mor/8c84a041-4067-4789-994f-54f5766fb0f6: 

24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                         Options.error_if_exists: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                       Options.create_if_missing: 1
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                         Options.paranoid_checks: 1
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :             Options.flush_verify_memtable_count: 1
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                               Options.track_and_verify_wals_in_manifest: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :        Options.verify_sst_unique_id_in_manifest: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                                     Options.env: 0x7f822f3f15e0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                                      Options.fs: PosixFileSystem
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                                Options.info_log: 0x7f81c00b3ee8
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                Options.max_file_opening_threads: 16
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                              Options.statistics: 0x7f81c0037e60
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                               Options.use_fsync: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                       Options.max_log_file_size: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                  Options.max_manifest_file_size: 1073741824
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                   Options.log_file_time_to_roll: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                       Options.keep_log_file_num: 1000
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                    Options.recycle_log_file_num: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                         Options.allow_fallocate: 1
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                        Options.allow_mmap_reads: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                       Options.allow_mmap_writes: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                        Options.use_direct_reads: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                        Options.use_direct_io_for_flush_and_compaction: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :          Options.create_missing_column_families: 1
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                              Options.db_log_dir: 
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                                 Options.wal_dir: 
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                Options.table_cache_numshardbits: 6
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                         Options.WAL_ttl_seconds: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                       Options.WAL_size_limit_MB: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                        Options.max_write_batch_group_size_bytes: 1048576
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :             Options.manifest_preallocation_size: 4194304
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                     Options.is_fd_close_on_exec: 1
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                   Options.advise_random_on_open: 1
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                    Options.db_write_buffer_size: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                    Options.write_buffer_manager: 0x7f81c003e8f0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :         Options.access_hint_on_compaction_start: 1
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :           Options.random_access_max_buffer_size: 1048576
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                      Options.use_adaptive_mutex: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                            Options.rate_limiter: (nil)
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :     Options.sst_file_manager.rate_bytes_per_sec: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                       Options.wal_recovery_mode: 2
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                  Options.enable_thread_tracking: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                  Options.enable_pipelined_write: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                  Options.unordered_write: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :         Options.allow_concurrent_memtable_write: 1
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :      Options.enable_write_thread_adaptive_yield: 1
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :             Options.write_thread_max_yield_usec: 100
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :            Options.write_thread_slow_yield_usec: 3
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                               Options.row_cache: None
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                              Options.wal_filter: None
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :             Options.avoid_flush_during_recovery: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :             Options.allow_ingest_behind: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :             Options.two_write_queues: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :             Options.manual_wal_flush: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :             Options.wal_compression: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :             Options.atomic_flush: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :             Options.avoid_unnecessary_blocking_io: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                 Options.persist_stats_to_disk: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                 Options.write_dbid_to_manifest: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                 Options.log_readahead_size: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                 Options.file_checksum_gen_factory: Unknown
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                 Options.best_efforts_recovery: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                Options.max_bgerror_resume_count: 2147483647
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :            Options.bgerror_resume_retry_interval: 1000000
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :             Options.allow_data_in_errors: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :             Options.db_host_id: __hostname__
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :             Options.enforce_single_del_contracts: true
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :             Options.max_background_jobs: 2
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :             Options.max_background_compactions: -1
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :             Options.max_subcompactions: 1
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :             Options.avoid_flush_during_shutdown: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :           Options.writable_file_max_buffer_size: 1048576
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :             Options.delayed_write_rate : 16777216
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :             Options.max_total_wal_size: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :             Options.delete_obsolete_files_period_micros: 21600000000
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                   Options.stats_dump_period_sec: 300
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                 Options.stats_persist_period_sec: 600
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                 Options.stats_history_buffer_size: 1048576
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                          Options.max_open_files: -1
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                          Options.bytes_per_sync: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                      Options.wal_bytes_per_sync: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                   Options.strict_bytes_per_sync: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :       Options.compaction_readahead_size: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                  Options.max_background_flushes: -1
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Compression algorithms supported:
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : 	kZSTDNotFinalCompression supported: 1
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : 	kZSTD supported: 1
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : 	kXpressCompression supported: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : 	kLZ4HCCompression supported: 1
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : 	kLZ4Compression supported: 1
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : 	kBZip2Compression supported: 1
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : 	kZlibCompression supported: 1
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : 	kSnappyCompression supported: 1
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Fast CRC32 supported: Not supported on x86
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : DMutex implementation: pthread_mutex_t
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : [/db_impl/db_impl_open.cc:313] Creating manifest 1 

24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : [/version_set.cc:4994] Recovering from manifest file: /tmp/hoodie_timeline_rocksdb/_user_hive_warehouse_stock_ticks_mor/8c84a041-4067-4789-994f-54f5766fb0f6/MANIFEST-000001

24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : [/column_family.cc:618] --------------- Options for column family [default]:

24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :               Options.comparator: leveldb.BytewiseComparator
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :           Options.merge_operator: None
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :        Options.compaction_filter: None
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :        Options.compaction_filter_factory: None
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :  Options.sst_partitioner_factory: None
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :         Options.memtable_factory: SkipListFactory
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :            Options.table_factory: BlockBasedTable
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :            table_factory options:   flush_block_policy_factory: FlushBlockBySizePolicyFactory (0x7f81c003b000)
  cache_index_and_filter_blocks: 0
  cache_index_and_filter_blocks_with_high_priority: 1
  pin_l0_filter_and_index_blocks_in_cache: 0
  pin_top_level_index_and_filter: 1
  index_type: 0
  data_block_index_type: 0
  index_shortening: 1
  data_block_hash_table_util_ratio: 0.750000
  checksum: 1
  no_block_cache: 0
  block_cache: 0x7f81c003b050
  block_cache_name: LRUCache
  block_cache_options:
    capacity : 8388608
    num_shard_bits : 4
    strict_capacity_limit : 0
    memory_allocator : None
    high_pri_pool_ratio: 0.000
  block_cache_compressed: (nil)
  persistent_cache: (nil)
  block_size: 4096
  block_size_deviation: 10
  block_restart_interval: 16
  index_block_restart_interval: 1
  metadata_block_size: 4096
  partition_filters: 0
  use_delta_encoding: 1
  filter_policy: nullptr
  whole_key_filtering: 1
  verify_compression: 0
  read_amp_bytes_per_bit: 0
  format_version: 5
  enable_index_compression: 1
  block_align: 0
  max_auto_readahead_size: 262144
  prepopulate_block_cache: 0
  initial_auto_readahead_size: 8192

24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :        Options.write_buffer_size: 67108864
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :  Options.max_write_buffer_number: 2
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :          Options.compression: Snappy
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression: Disabled
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :       Options.prefix_extractor: nullptr
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :   Options.memtable_insert_with_hint_prefix_extractor: nullptr
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :             Options.num_levels: 7
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :        Options.min_write_buffer_number_to_merge: 1
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :     Options.max_write_buffer_number_to_maintain: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :     Options.max_write_buffer_size_to_maintain: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :            Options.bottommost_compression_opts.window_bits: -14
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression_opts.level: 32767
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :               Options.bottommost_compression_opts.strategy: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.max_dict_bytes: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.zstd_max_train_bytes: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.parallel_threads: 1
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression_opts.enabled: false
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.max_dict_buffer_bytes: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.use_zstd_dict_trainer: true
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :            Options.compression_opts.window_bits: -14
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                  Options.compression_opts.level: 32767
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :               Options.compression_opts.strategy: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.max_dict_bytes: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.zstd_max_train_bytes: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.use_zstd_dict_trainer: true
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.parallel_threads: 1
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                  Options.compression_opts.enabled: false
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.max_dict_buffer_bytes: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :      Options.level0_file_num_compaction_trigger: 4
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :          Options.level0_slowdown_writes_trigger: 20
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :              Options.level0_stop_writes_trigger: 36
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                   Options.target_file_size_base: 67108864
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :             Options.target_file_size_multiplier: 1
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                Options.max_bytes_for_level_base: 268435456
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Options.level_compaction_dynamic_level_bytes: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :          Options.max_bytes_for_level_multiplier: 10.000000
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[0]: 1
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[1]: 1
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[2]: 1
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[3]: 1
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[4]: 1
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[5]: 1
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[6]: 1
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :       Options.max_sequential_skip_in_iterations: 8
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                    Options.max_compaction_bytes: 1677721600
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                        Options.arena_block_size: 1048576
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :   Options.soft_pending_compaction_bytes_limit: 68719476736
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :   Options.hard_pending_compaction_bytes_limit: 274877906944
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                Options.disable_auto_compactions: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                        Options.compaction_style: kCompactionStyleLevel
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                          Options.compaction_pri: kMinOverlappingRatio
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.size_ratio: 1
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.min_merge_width: 2
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.max_merge_width: 4294967295
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.max_size_amplification_percent: 200
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.compression_size_percent: -1
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.stop_style: kCompactionStopStyleTotalSize
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_fifo.max_table_files_size: 1073741824
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_fifo.allow_compaction: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                   Options.table_properties_collectors: 
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                   Options.inplace_update_support: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                 Options.inplace_update_num_locks: 10000
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :               Options.memtable_prefix_bloom_size_ratio: 0.000000
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :               Options.memtable_whole_key_filtering: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :   Options.memtable_huge_page_size: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                           Options.bloom_locality: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                    Options.max_successive_merges: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                Options.optimize_filters_for_hits: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                Options.paranoid_file_checks: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                Options.force_consistency_checks: 1
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                Options.report_bg_io_stats: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                               Options.ttl: 2592000
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :          Options.periodic_compaction_seconds: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :  Options.preclude_last_level_data_seconds: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                       Options.enable_blob_files: false
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                           Options.min_blob_size: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                          Options.blob_file_size: 268435456
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                   Options.blob_compression_type: NoCompression
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :          Options.enable_blob_garbage_collection: false
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :      Options.blob_garbage_collection_age_cutoff: 0.250000
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Options.blob_garbage_collection_force_threshold: 1.000000
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :          Options.blob_compaction_readahead_size: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                Options.blob_file_starting_level: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Options.experimental_mempurge_threshold: 0.000000
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : [/version_set.cc:5042] Recovered from manifest file:/tmp/hoodie_timeline_rocksdb/_user_hive_warehouse_stock_ticks_mor/8c84a041-4067-4789-994f-54f5766fb0f6/MANIFEST-000001 succeeded,manifest_file_number is 1, next_file_number is 3, last_sequence is 0, log_number is 0,prev_log_number is 0,max_column_family is 0,min_log_number_to_keep is 0

24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : [/version_set.cc:5051] Column family [default] (ID 0), log number is 0

24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : [/db_impl/db_impl_open.cc:532] DB ID: 584b6a67-bfb5-4544-a6f3-24624de58d85

24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : [/version_set.cc:4513] Creating manifest 5

24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : [/db_impl/db_impl_open.cc:2024] SstFileManager instance 0x7f81c003e9d0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : DB pointer 0x7f81c003d100
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : [/column_family.cc:618] --------------- Options for column family [hudi_view__user_hive_warehouse_stock_ticks_mor]:

24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :               Options.comparator: leveldb.BytewiseComparator
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :           Options.merge_operator: None
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :        Options.compaction_filter: None
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :        Options.compaction_filter_factory: None
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :  Options.sst_partitioner_factory: None
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :         Options.memtable_factory: SkipListFactory
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :            Options.table_factory: BlockBasedTable
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :            table_factory options:   flush_block_policy_factory: FlushBlockBySizePolicyFactory (0x7f81c00d4a90)
  cache_index_and_filter_blocks: 0
  cache_index_and_filter_blocks_with_high_priority: 1
  pin_l0_filter_and_index_blocks_in_cache: 0
  pin_top_level_index_and_filter: 1
  index_type: 0
  data_block_index_type: 0
  index_shortening: 1
  data_block_hash_table_util_ratio: 0.750000
  checksum: 1
  no_block_cache: 0
  block_cache: 0x7f81c00d45a0
  block_cache_name: LRUCache
  block_cache_options:
    capacity : 8388608
    num_shard_bits : 4
    strict_capacity_limit : 0
    memory_allocator : None
    high_pri_pool_ratio: 0.000
  block_cache_compressed: (nil)
  persistent_cache: (nil)
  block_size: 4096
  block_size_deviation: 10
  block_restart_interval: 16
  index_block_restart_interval: 1
  metadata_block_size: 4096
  partition_filters: 0
  use_delta_encoding: 1
  filter_policy: nullptr
  whole_key_filtering: 1
  verify_compression: 0
  read_amp_bytes_per_bit: 0
  format_version: 5
  enable_index_compression: 1
  block_align: 0
  max_auto_readahead_size: 262144
  prepopulate_block_cache: 0
  initial_auto_readahead_size: 8192

24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :        Options.write_buffer_size: 67108864
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :  Options.max_write_buffer_number: 2
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :          Options.compression: Snappy
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression: Disabled
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :       Options.prefix_extractor: nullptr
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :   Options.memtable_insert_with_hint_prefix_extractor: nullptr
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :             Options.num_levels: 7
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :        Options.min_write_buffer_number_to_merge: 1
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :     Options.max_write_buffer_number_to_maintain: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :     Options.max_write_buffer_size_to_maintain: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :            Options.bottommost_compression_opts.window_bits: -14
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression_opts.level: 32767
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :               Options.bottommost_compression_opts.strategy: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.max_dict_bytes: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.zstd_max_train_bytes: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.parallel_threads: 1
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression_opts.enabled: false
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.max_dict_buffer_bytes: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.use_zstd_dict_trainer: true
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :            Options.compression_opts.window_bits: -14
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                  Options.compression_opts.level: 32767
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :               Options.compression_opts.strategy: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.max_dict_bytes: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.zstd_max_train_bytes: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.use_zstd_dict_trainer: true
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.parallel_threads: 1
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                  Options.compression_opts.enabled: false
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.max_dict_buffer_bytes: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :      Options.level0_file_num_compaction_trigger: 4
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :          Options.level0_slowdown_writes_trigger: 20
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :              Options.level0_stop_writes_trigger: 36
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                   Options.target_file_size_base: 67108864
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :             Options.target_file_size_multiplier: 1
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                Options.max_bytes_for_level_base: 268435456
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Options.level_compaction_dynamic_level_bytes: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :          Options.max_bytes_for_level_multiplier: 10.000000
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[0]: 1
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[1]: 1
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[2]: 1
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[3]: 1
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[4]: 1
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[5]: 1
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[6]: 1
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :       Options.max_sequential_skip_in_iterations: 8
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                    Options.max_compaction_bytes: 1677721600
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                        Options.arena_block_size: 1048576
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :   Options.soft_pending_compaction_bytes_limit: 68719476736
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :   Options.hard_pending_compaction_bytes_limit: 274877906944
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                Options.disable_auto_compactions: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                        Options.compaction_style: kCompactionStyleLevel
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                          Options.compaction_pri: kMinOverlappingRatio
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.size_ratio: 1
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.min_merge_width: 2
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.max_merge_width: 4294967295
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.max_size_amplification_percent: 200
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.compression_size_percent: -1
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.stop_style: kCompactionStopStyleTotalSize
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_fifo.max_table_files_size: 1073741824
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_fifo.allow_compaction: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                   Options.table_properties_collectors: 
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                   Options.inplace_update_support: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                 Options.inplace_update_num_locks: 10000
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :               Options.memtable_prefix_bloom_size_ratio: 0.000000
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :               Options.memtable_whole_key_filtering: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :   Options.memtable_huge_page_size: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                           Options.bloom_locality: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                    Options.max_successive_merges: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                Options.optimize_filters_for_hits: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                Options.paranoid_file_checks: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                Options.force_consistency_checks: 1
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                Options.report_bg_io_stats: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                               Options.ttl: 2592000
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :          Options.periodic_compaction_seconds: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :  Options.preclude_last_level_data_seconds: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                       Options.enable_blob_files: false
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                           Options.min_blob_size: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                          Options.blob_file_size: 268435456
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                   Options.blob_compression_type: NoCompression
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :          Options.enable_blob_garbage_collection: false
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :      Options.blob_garbage_collection_age_cutoff: 0.250000
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Options.blob_garbage_collection_force_threshold: 1.000000
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :          Options.blob_compaction_readahead_size: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                Options.blob_file_starting_level: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Options.experimental_mempurge_threshold: 0.000000
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : [/db_impl/db_impl.cc:2861] Created column family [hudi_view__user_hive_warehouse_stock_ticks_mor] (ID 1)
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : [/column_family.cc:618] --------------- Options for column family [hudi_pending_compaction__user_hive_warehouse_stock_ticks_mor]:

24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :               Options.comparator: leveldb.BytewiseComparator
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :           Options.merge_operator: None
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :        Options.compaction_filter: None
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :        Options.compaction_filter_factory: None
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :  Options.sst_partitioner_factory: None
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :         Options.memtable_factory: SkipListFactory
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :            Options.table_factory: BlockBasedTable
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :            table_factory options:   flush_block_policy_factory: FlushBlockBySizePolicyFactory (0x7f81c00c4c00)
  cache_index_and_filter_blocks: 0
  cache_index_and_filter_blocks_with_high_priority: 1
  pin_l0_filter_and_index_blocks_in_cache: 0
  pin_top_level_index_and_filter: 1
  index_type: 0
  data_block_index_type: 0
  index_shortening: 1
  data_block_hash_table_util_ratio: 0.750000
  checksum: 1
  no_block_cache: 0
  block_cache: 0x7f81c00fba00
  block_cache_name: LRUCache
  block_cache_options:
    capacity : 8388608
    num_shard_bits : 4
    strict_capacity_limit : 0
    memory_allocator : None
    high_pri_pool_ratio: 0.000
  block_cache_compressed: (nil)
  persistent_cache: (nil)
  block_size: 4096
  block_size_deviation: 10
  block_restart_interval: 16
  index_block_restart_interval: 1
  metadata_block_size: 4096
  partition_filters: 0
  use_delta_encoding: 1
  filter_policy: nullptr
  whole_key_filtering: 1
  verify_compression: 0
  read_amp_bytes_per_bit: 0
  format_version: 5
  enable_index_compression: 1
  block_align: 0
  max_auto_readahead_size: 262144
  prepopulate_block_cache: 0
  initial_auto_readahead_size: 8192

24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :        Options.write_buffer_size: 67108864
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :  Options.max_write_buffer_number: 2
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :          Options.compression: Snappy
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression: Disabled
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :       Options.prefix_extractor: nullptr
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :   Options.memtable_insert_with_hint_prefix_extractor: nullptr
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :             Options.num_levels: 7
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :        Options.min_write_buffer_number_to_merge: 1
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :     Options.max_write_buffer_number_to_maintain: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :     Options.max_write_buffer_size_to_maintain: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :            Options.bottommost_compression_opts.window_bits: -14
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression_opts.level: 32767
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :               Options.bottommost_compression_opts.strategy: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.max_dict_bytes: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.zstd_max_train_bytes: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.parallel_threads: 1
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression_opts.enabled: false
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.max_dict_buffer_bytes: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.use_zstd_dict_trainer: true
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :            Options.compression_opts.window_bits: -14
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                  Options.compression_opts.level: 32767
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :               Options.compression_opts.strategy: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.max_dict_bytes: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.zstd_max_train_bytes: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.use_zstd_dict_trainer: true
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.parallel_threads: 1
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                  Options.compression_opts.enabled: false
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.max_dict_buffer_bytes: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :      Options.level0_file_num_compaction_trigger: 4
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :          Options.level0_slowdown_writes_trigger: 20
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :              Options.level0_stop_writes_trigger: 36
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                   Options.target_file_size_base: 67108864
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :             Options.target_file_size_multiplier: 1
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                Options.max_bytes_for_level_base: 268435456
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Options.level_compaction_dynamic_level_bytes: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :          Options.max_bytes_for_level_multiplier: 10.000000
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[0]: 1
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[1]: 1
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[2]: 1
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[3]: 1
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[4]: 1
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[5]: 1
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[6]: 1
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :       Options.max_sequential_skip_in_iterations: 8
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                    Options.max_compaction_bytes: 1677721600
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                        Options.arena_block_size: 1048576
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :   Options.soft_pending_compaction_bytes_limit: 68719476736
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :   Options.hard_pending_compaction_bytes_limit: 274877906944
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                Options.disable_auto_compactions: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                        Options.compaction_style: kCompactionStyleLevel
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                          Options.compaction_pri: kMinOverlappingRatio
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.size_ratio: 1
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.min_merge_width: 2
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.max_merge_width: 4294967295
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.max_size_amplification_percent: 200
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.compression_size_percent: -1
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.stop_style: kCompactionStopStyleTotalSize
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_fifo.max_table_files_size: 1073741824
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_fifo.allow_compaction: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                   Options.table_properties_collectors: 
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                   Options.inplace_update_support: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                 Options.inplace_update_num_locks: 10000
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :               Options.memtable_prefix_bloom_size_ratio: 0.000000
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :               Options.memtable_whole_key_filtering: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :   Options.memtable_huge_page_size: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                           Options.bloom_locality: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                    Options.max_successive_merges: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                Options.optimize_filters_for_hits: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                Options.paranoid_file_checks: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                Options.force_consistency_checks: 1
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                Options.report_bg_io_stats: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                               Options.ttl: 2592000
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :          Options.periodic_compaction_seconds: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :  Options.preclude_last_level_data_seconds: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                       Options.enable_blob_files: false
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                           Options.min_blob_size: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                          Options.blob_file_size: 268435456
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                   Options.blob_compression_type: NoCompression
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :          Options.enable_blob_garbage_collection: false
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :      Options.blob_garbage_collection_age_cutoff: 0.250000
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Options.blob_garbage_collection_force_threshold: 1.000000
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :          Options.blob_compaction_readahead_size: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                Options.blob_file_starting_level: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Options.experimental_mempurge_threshold: 0.000000
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : [/db_impl/db_impl.cc:2861] Created column family [hudi_pending_compaction__user_hive_warehouse_stock_ticks_mor] (ID 2)
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : [/column_family.cc:618] --------------- Options for column family [hudi_pending_log_compaction__user_hive_warehouse_stock_ticks_mor]:

24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :               Options.comparator: leveldb.BytewiseComparator
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :           Options.merge_operator: None
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :        Options.compaction_filter: None
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :        Options.compaction_filter_factory: None
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :  Options.sst_partitioner_factory: None
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :         Options.memtable_factory: SkipListFactory
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :            Options.table_factory: BlockBasedTable
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :            table_factory options:   flush_block_policy_factory: FlushBlockBySizePolicyFactory (0x7f81c00fda20)
  cache_index_and_filter_blocks: 0
  cache_index_and_filter_blocks_with_high_priority: 1
  pin_l0_filter_and_index_blocks_in_cache: 0
  pin_top_level_index_and_filter: 1
  index_type: 0
  data_block_index_type: 0
  index_shortening: 1
  data_block_hash_table_util_ratio: 0.750000
  checksum: 1
  no_block_cache: 0
  block_cache: 0x7f81c00fd870
  block_cache_name: LRUCache
  block_cache_options:
    capacity : 8388608
    num_shard_bits : 4
    strict_capacity_limit : 0
    memory_allocator : None
    high_pri_pool_ratio: 0.000
  block_cache_compressed: (nil)
  persistent_cache: (nil)
  block_size: 4096
  block_size_deviation: 10
  block_restart_interval: 16
  index_block_restart_interval: 1
  metadata_block_size: 4096
  partition_filters: 0
  use_delta_encoding: 1
  filter_policy: nullptr
  whole_key_filtering: 1
  verify_compression: 0
  read_amp_bytes_per_bit: 0
  format_version: 5
  enable_index_compression: 1
  block_align: 0
  max_auto_readahead_size: 262144
  prepopulate_block_cache: 0
  initial_auto_readahead_size: 8192

24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :        Options.write_buffer_size: 67108864
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :  Options.max_write_buffer_number: 2
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :          Options.compression: Snappy
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression: Disabled
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :       Options.prefix_extractor: nullptr
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :   Options.memtable_insert_with_hint_prefix_extractor: nullptr
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :             Options.num_levels: 7
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :        Options.min_write_buffer_number_to_merge: 1
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :     Options.max_write_buffer_number_to_maintain: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :     Options.max_write_buffer_size_to_maintain: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :            Options.bottommost_compression_opts.window_bits: -14
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression_opts.level: 32767
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :               Options.bottommost_compression_opts.strategy: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.max_dict_bytes: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.zstd_max_train_bytes: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.parallel_threads: 1
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression_opts.enabled: false
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.max_dict_buffer_bytes: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.use_zstd_dict_trainer: true
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :            Options.compression_opts.window_bits: -14
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                  Options.compression_opts.level: 32767
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :               Options.compression_opts.strategy: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.max_dict_bytes: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.zstd_max_train_bytes: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.use_zstd_dict_trainer: true
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.parallel_threads: 1
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                  Options.compression_opts.enabled: false
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.max_dict_buffer_bytes: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :      Options.level0_file_num_compaction_trigger: 4
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :          Options.level0_slowdown_writes_trigger: 20
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :              Options.level0_stop_writes_trigger: 36
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                   Options.target_file_size_base: 67108864
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :             Options.target_file_size_multiplier: 1
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                Options.max_bytes_for_level_base: 268435456
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Options.level_compaction_dynamic_level_bytes: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :          Options.max_bytes_for_level_multiplier: 10.000000
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[0]: 1
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[1]: 1
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[2]: 1
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[3]: 1
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[4]: 1
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[5]: 1
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[6]: 1
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :       Options.max_sequential_skip_in_iterations: 8
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                    Options.max_compaction_bytes: 1677721600
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                        Options.arena_block_size: 1048576
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :   Options.soft_pending_compaction_bytes_limit: 68719476736
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :   Options.hard_pending_compaction_bytes_limit: 274877906944
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                Options.disable_auto_compactions: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                        Options.compaction_style: kCompactionStyleLevel
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                          Options.compaction_pri: kMinOverlappingRatio
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.size_ratio: 1
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.min_merge_width: 2
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.max_merge_width: 4294967295
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.max_size_amplification_percent: 200
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.compression_size_percent: -1
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.stop_style: kCompactionStopStyleTotalSize
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_fifo.max_table_files_size: 1073741824
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_fifo.allow_compaction: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                   Options.table_properties_collectors: 
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                   Options.inplace_update_support: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                 Options.inplace_update_num_locks: 10000
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :               Options.memtable_prefix_bloom_size_ratio: 0.000000
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :               Options.memtable_whole_key_filtering: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :   Options.memtable_huge_page_size: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                           Options.bloom_locality: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                    Options.max_successive_merges: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                Options.optimize_filters_for_hits: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                Options.paranoid_file_checks: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                Options.force_consistency_checks: 1
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                Options.report_bg_io_stats: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                               Options.ttl: 2592000
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :          Options.periodic_compaction_seconds: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :  Options.preclude_last_level_data_seconds: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                       Options.enable_blob_files: false
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                           Options.min_blob_size: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                          Options.blob_file_size: 268435456
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                   Options.blob_compression_type: NoCompression
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :          Options.enable_blob_garbage_collection: false
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :      Options.blob_garbage_collection_age_cutoff: 0.250000
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Options.blob_garbage_collection_force_threshold: 1.000000
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :          Options.blob_compaction_readahead_size: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                Options.blob_file_starting_level: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Options.experimental_mempurge_threshold: 0.000000
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : [/db_impl/db_impl.cc:2861] Created column family [hudi_pending_log_compaction__user_hive_warehouse_stock_ticks_mor] (ID 3)
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : [/column_family.cc:618] --------------- Options for column family [hudi_bootstrap_basefile__user_hive_warehouse_stock_ticks_mor]:

24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :               Options.comparator: leveldb.BytewiseComparator
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :           Options.merge_operator: None
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :        Options.compaction_filter: None
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :        Options.compaction_filter_factory: None
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :  Options.sst_partitioner_factory: None
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :         Options.memtable_factory: SkipListFactory
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :            Options.table_factory: BlockBasedTable
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :            table_factory options:   flush_block_policy_factory: FlushBlockBySizePolicyFactory (0x7f81c0116670)
  cache_index_and_filter_blocks: 0
  cache_index_and_filter_blocks_with_high_priority: 1
  pin_l0_filter_and_index_blocks_in_cache: 0
  pin_top_level_index_and_filter: 1
  index_type: 0
  data_block_index_type: 0
  index_shortening: 1
  data_block_hash_table_util_ratio: 0.750000
  checksum: 1
  no_block_cache: 0
  block_cache: 0x7f81c010b210
  block_cache_name: LRUCache
  block_cache_options:
    capacity : 8388608
    num_shard_bits : 4
    strict_capacity_limit : 0
    memory_allocator : None
    high_pri_pool_ratio: 0.000
  block_cache_compressed: (nil)
  persistent_cache: (nil)
  block_size: 4096
  block_size_deviation: 10
  block_restart_interval: 16
  index_block_restart_interval: 1
  metadata_block_size: 4096
  partition_filters: 0
  use_delta_encoding: 1
  filter_policy: nullptr
  whole_key_filtering: 1
  verify_compression: 0
  read_amp_bytes_per_bit: 0
  format_version: 5
  enable_index_compression: 1
  block_align: 0
  max_auto_readahead_size: 262144
  prepopulate_block_cache: 0
  initial_auto_readahead_size: 8192

24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :        Options.write_buffer_size: 67108864
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :  Options.max_write_buffer_number: 2
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :          Options.compression: Snappy
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression: Disabled
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :       Options.prefix_extractor: nullptr
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :   Options.memtable_insert_with_hint_prefix_extractor: nullptr
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :             Options.num_levels: 7
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :        Options.min_write_buffer_number_to_merge: 1
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :     Options.max_write_buffer_number_to_maintain: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :     Options.max_write_buffer_size_to_maintain: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :            Options.bottommost_compression_opts.window_bits: -14
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression_opts.level: 32767
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :               Options.bottommost_compression_opts.strategy: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.max_dict_bytes: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.zstd_max_train_bytes: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.parallel_threads: 1
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression_opts.enabled: false
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.max_dict_buffer_bytes: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.use_zstd_dict_trainer: true
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :            Options.compression_opts.window_bits: -14
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                  Options.compression_opts.level: 32767
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :               Options.compression_opts.strategy: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.max_dict_bytes: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.zstd_max_train_bytes: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.use_zstd_dict_trainer: true
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.parallel_threads: 1
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                  Options.compression_opts.enabled: false
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.max_dict_buffer_bytes: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :      Options.level0_file_num_compaction_trigger: 4
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :          Options.level0_slowdown_writes_trigger: 20
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :              Options.level0_stop_writes_trigger: 36
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                   Options.target_file_size_base: 67108864
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :             Options.target_file_size_multiplier: 1
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                Options.max_bytes_for_level_base: 268435456
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Options.level_compaction_dynamic_level_bytes: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :          Options.max_bytes_for_level_multiplier: 10.000000
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[0]: 1
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[1]: 1
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[2]: 1
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[3]: 1
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[4]: 1
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[5]: 1
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[6]: 1
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :       Options.max_sequential_skip_in_iterations: 8
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                    Options.max_compaction_bytes: 1677721600
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                        Options.arena_block_size: 1048576
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :   Options.soft_pending_compaction_bytes_limit: 68719476736
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :   Options.hard_pending_compaction_bytes_limit: 274877906944
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                Options.disable_auto_compactions: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                        Options.compaction_style: kCompactionStyleLevel
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                          Options.compaction_pri: kMinOverlappingRatio
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.size_ratio: 1
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.min_merge_width: 2
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.max_merge_width: 4294967295
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.max_size_amplification_percent: 200
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.compression_size_percent: -1
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.stop_style: kCompactionStopStyleTotalSize
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_fifo.max_table_files_size: 1073741824
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_fifo.allow_compaction: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                   Options.table_properties_collectors: 
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                   Options.inplace_update_support: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                 Options.inplace_update_num_locks: 10000
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :               Options.memtable_prefix_bloom_size_ratio: 0.000000
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :               Options.memtable_whole_key_filtering: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :   Options.memtable_huge_page_size: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                           Options.bloom_locality: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                    Options.max_successive_merges: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                Options.optimize_filters_for_hits: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                Options.paranoid_file_checks: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                Options.force_consistency_checks: 1
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                Options.report_bg_io_stats: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                               Options.ttl: 2592000
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :          Options.periodic_compaction_seconds: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :  Options.preclude_last_level_data_seconds: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                       Options.enable_blob_files: false
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                           Options.min_blob_size: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                          Options.blob_file_size: 268435456
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                   Options.blob_compression_type: NoCompression
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :          Options.enable_blob_garbage_collection: false
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :      Options.blob_garbage_collection_age_cutoff: 0.250000
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Options.blob_garbage_collection_force_threshold: 1.000000
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :          Options.blob_compaction_readahead_size: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                Options.blob_file_starting_level: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Options.experimental_mempurge_threshold: 0.000000
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : [/db_impl/db_impl.cc:2861] Created column family [hudi_bootstrap_basefile__user_hive_warehouse_stock_ticks_mor] (ID 4)
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : [/column_family.cc:618] --------------- Options for column family [hudi_partitions__user_hive_warehouse_stock_ticks_mor]:

24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :               Options.comparator: leveldb.BytewiseComparator
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :           Options.merge_operator: None
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :        Options.compaction_filter: None
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :        Options.compaction_filter_factory: None
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :  Options.sst_partitioner_factory: None
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :         Options.memtable_factory: SkipListFactory
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :            Options.table_factory: BlockBasedTable
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :            table_factory options:   flush_block_policy_factory: FlushBlockBySizePolicyFactory (0x7f81c0118df0)
  cache_index_and_filter_blocks: 0
  cache_index_and_filter_blocks_with_high_priority: 1
  pin_l0_filter_and_index_blocks_in_cache: 0
  pin_top_level_index_and_filter: 1
  index_type: 0
  data_block_index_type: 0
  index_shortening: 1
  data_block_hash_table_util_ratio: 0.750000
  checksum: 1
  no_block_cache: 0
  block_cache: 0x7f81c0118c90
  block_cache_name: LRUCache
  block_cache_options:
    capacity : 8388608
    num_shard_bits : 4
    strict_capacity_limit : 0
    memory_allocator : None
    high_pri_pool_ratio: 0.000
  block_cache_compressed: (nil)
  persistent_cache: (nil)
  block_size: 4096
  block_size_deviation: 10
  block_restart_interval: 16
  index_block_restart_interval: 1
  metadata_block_size: 4096
  partition_filters: 0
  use_delta_encoding: 1
  filter_policy: nullptr
  whole_key_filtering: 1
  verify_compression: 0
  read_amp_bytes_per_bit: 0
  format_version: 5
  enable_index_compression: 1
  block_align: 0
  max_auto_readahead_size: 262144
  prepopulate_block_cache: 0
  initial_auto_readahead_size: 8192

24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :        Options.write_buffer_size: 67108864
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :  Options.max_write_buffer_number: 2
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :          Options.compression: Snappy
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression: Disabled
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :       Options.prefix_extractor: nullptr
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :   Options.memtable_insert_with_hint_prefix_extractor: nullptr
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :             Options.num_levels: 7
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :        Options.min_write_buffer_number_to_merge: 1
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :     Options.max_write_buffer_number_to_maintain: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :     Options.max_write_buffer_size_to_maintain: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :            Options.bottommost_compression_opts.window_bits: -14
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression_opts.level: 32767
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :               Options.bottommost_compression_opts.strategy: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.max_dict_bytes: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.zstd_max_train_bytes: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.parallel_threads: 1
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression_opts.enabled: false
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.max_dict_buffer_bytes: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.use_zstd_dict_trainer: true
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :            Options.compression_opts.window_bits: -14
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                  Options.compression_opts.level: 32767
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :               Options.compression_opts.strategy: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.max_dict_bytes: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.zstd_max_train_bytes: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.use_zstd_dict_trainer: true
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.parallel_threads: 1
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                  Options.compression_opts.enabled: false
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.max_dict_buffer_bytes: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :      Options.level0_file_num_compaction_trigger: 4
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :          Options.level0_slowdown_writes_trigger: 20
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :              Options.level0_stop_writes_trigger: 36
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                   Options.target_file_size_base: 67108864
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :             Options.target_file_size_multiplier: 1
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                Options.max_bytes_for_level_base: 268435456
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Options.level_compaction_dynamic_level_bytes: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :          Options.max_bytes_for_level_multiplier: 10.000000
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[0]: 1
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[1]: 1
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[2]: 1
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[3]: 1
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[4]: 1
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[5]: 1
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[6]: 1
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :       Options.max_sequential_skip_in_iterations: 8
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                    Options.max_compaction_bytes: 1677721600
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                        Options.arena_block_size: 1048576
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :   Options.soft_pending_compaction_bytes_limit: 68719476736
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :   Options.hard_pending_compaction_bytes_limit: 274877906944
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                Options.disable_auto_compactions: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                        Options.compaction_style: kCompactionStyleLevel
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                          Options.compaction_pri: kMinOverlappingRatio
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.size_ratio: 1
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.min_merge_width: 2
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.max_merge_width: 4294967295
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.max_size_amplification_percent: 200
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.compression_size_percent: -1
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.stop_style: kCompactionStopStyleTotalSize
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_fifo.max_table_files_size: 1073741824
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_fifo.allow_compaction: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                   Options.table_properties_collectors: 
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                   Options.inplace_update_support: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                 Options.inplace_update_num_locks: 10000
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :               Options.memtable_prefix_bloom_size_ratio: 0.000000
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :               Options.memtable_whole_key_filtering: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :   Options.memtable_huge_page_size: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                           Options.bloom_locality: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                    Options.max_successive_merges: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                Options.optimize_filters_for_hits: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                Options.paranoid_file_checks: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                Options.force_consistency_checks: 1
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                Options.report_bg_io_stats: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                               Options.ttl: 2592000
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :          Options.periodic_compaction_seconds: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :  Options.preclude_last_level_data_seconds: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                       Options.enable_blob_files: false
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                           Options.min_blob_size: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                          Options.blob_file_size: 268435456
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                   Options.blob_compression_type: NoCompression
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :          Options.enable_blob_garbage_collection: false
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :      Options.blob_garbage_collection_age_cutoff: 0.250000
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Options.blob_garbage_collection_force_threshold: 1.000000
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :          Options.blob_compaction_readahead_size: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                Options.blob_file_starting_level: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Options.experimental_mempurge_threshold: 0.000000
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : [/db_impl/db_impl.cc:2861] Created column family [hudi_partitions__user_hive_warehouse_stock_ticks_mor] (ID 5)
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : [/column_family.cc:618] --------------- Options for column family [hudi_replaced_fg_user_hive_warehouse_stock_ticks_mor]:

24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :               Options.comparator: leveldb.BytewiseComparator
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :           Options.merge_operator: None
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :        Options.compaction_filter: None
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :        Options.compaction_filter_factory: None
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :  Options.sst_partitioner_factory: None
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :         Options.memtable_factory: SkipListFactory
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :            Options.table_factory: BlockBasedTable
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :            table_factory options:   flush_block_policy_factory: FlushBlockBySizePolicyFactory (0x7f81c01319b0)
  cache_index_and_filter_blocks: 0
  cache_index_and_filter_blocks_with_high_priority: 1
  pin_l0_filter_and_index_blocks_in_cache: 0
  pin_top_level_index_and_filter: 1
  index_type: 0
  data_block_index_type: 0
  index_shortening: 1
  data_block_hash_table_util_ratio: 0.750000
  checksum: 1
  no_block_cache: 0
  block_cache: 0x7f81c0126610
  block_cache_name: LRUCache
  block_cache_options:
    capacity : 8388608
    num_shard_bits : 4
    strict_capacity_limit : 0
    memory_allocator : None
    high_pri_pool_ratio: 0.000
  block_cache_compressed: (nil)
  persistent_cache: (nil)
  block_size: 4096
  block_size_deviation: 10
  block_restart_interval: 16
  index_block_restart_interval: 1
  metadata_block_size: 4096
  partition_filters: 0
  use_delta_encoding: 1
  filter_policy: nullptr
  whole_key_filtering: 1
  verify_compression: 0
  read_amp_bytes_per_bit: 0
  format_version: 5
  enable_index_compression: 1
  block_align: 0
  max_auto_readahead_size: 262144
  prepopulate_block_cache: 0
  initial_auto_readahead_size: 8192

24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :        Options.write_buffer_size: 67108864
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :  Options.max_write_buffer_number: 2
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :          Options.compression: Snappy
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression: Disabled
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :       Options.prefix_extractor: nullptr
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :   Options.memtable_insert_with_hint_prefix_extractor: nullptr
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :             Options.num_levels: 7
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :        Options.min_write_buffer_number_to_merge: 1
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :     Options.max_write_buffer_number_to_maintain: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :     Options.max_write_buffer_size_to_maintain: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :            Options.bottommost_compression_opts.window_bits: -14
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression_opts.level: 32767
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :               Options.bottommost_compression_opts.strategy: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.max_dict_bytes: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.zstd_max_train_bytes: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.parallel_threads: 1
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression_opts.enabled: false
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.max_dict_buffer_bytes: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.use_zstd_dict_trainer: true
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :            Options.compression_opts.window_bits: -14
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                  Options.compression_opts.level: 32767
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :               Options.compression_opts.strategy: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.max_dict_bytes: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.zstd_max_train_bytes: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.use_zstd_dict_trainer: true
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.parallel_threads: 1
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                  Options.compression_opts.enabled: false
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.max_dict_buffer_bytes: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :      Options.level0_file_num_compaction_trigger: 4
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :          Options.level0_slowdown_writes_trigger: 20
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :              Options.level0_stop_writes_trigger: 36
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                   Options.target_file_size_base: 67108864
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :             Options.target_file_size_multiplier: 1
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                Options.max_bytes_for_level_base: 268435456
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Options.level_compaction_dynamic_level_bytes: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :          Options.max_bytes_for_level_multiplier: 10.000000
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[0]: 1
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[1]: 1
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[2]: 1
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[3]: 1
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[4]: 1
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[5]: 1
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[6]: 1
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :       Options.max_sequential_skip_in_iterations: 8
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                    Options.max_compaction_bytes: 1677721600
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                        Options.arena_block_size: 1048576
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :   Options.soft_pending_compaction_bytes_limit: 68719476736
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :   Options.hard_pending_compaction_bytes_limit: 274877906944
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                Options.disable_auto_compactions: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                        Options.compaction_style: kCompactionStyleLevel
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                          Options.compaction_pri: kMinOverlappingRatio
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.size_ratio: 1
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.min_merge_width: 2
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.max_merge_width: 4294967295
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.max_size_amplification_percent: 200
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.compression_size_percent: -1
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.stop_style: kCompactionStopStyleTotalSize
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_fifo.max_table_files_size: 1073741824
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_fifo.allow_compaction: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                   Options.table_properties_collectors: 
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                   Options.inplace_update_support: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                 Options.inplace_update_num_locks: 10000
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :               Options.memtable_prefix_bloom_size_ratio: 0.000000
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :               Options.memtable_whole_key_filtering: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :   Options.memtable_huge_page_size: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                           Options.bloom_locality: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                    Options.max_successive_merges: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                Options.optimize_filters_for_hits: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                Options.paranoid_file_checks: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                Options.force_consistency_checks: 1
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                Options.report_bg_io_stats: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                               Options.ttl: 2592000
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :          Options.periodic_compaction_seconds: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :  Options.preclude_last_level_data_seconds: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                       Options.enable_blob_files: false
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                           Options.min_blob_size: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                          Options.blob_file_size: 268435456
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                   Options.blob_compression_type: NoCompression
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :          Options.enable_blob_garbage_collection: false
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :      Options.blob_garbage_collection_age_cutoff: 0.250000
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Options.blob_garbage_collection_force_threshold: 1.000000
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :          Options.blob_compaction_readahead_size: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                Options.blob_file_starting_level: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Options.experimental_mempurge_threshold: 0.000000
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : [/db_impl/db_impl.cc:2861] Created column family [hudi_replaced_fg_user_hive_warehouse_stock_ticks_mor] (ID 6)
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : [/column_family.cc:618] --------------- Options for column family [hudi_pending_clustering_fg_user_hive_warehouse_stock_ticks_mor]:

24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :               Options.comparator: leveldb.BytewiseComparator
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :           Options.merge_operator: None
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :        Options.compaction_filter: None
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :        Options.compaction_filter_factory: None
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :  Options.sst_partitioner_factory: None
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :         Options.memtable_factory: SkipListFactory
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :            Options.table_factory: BlockBasedTable
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :            table_factory options:   flush_block_policy_factory: FlushBlockBySizePolicyFactory (0x7f81c013f3d0)
  cache_index_and_filter_blocks: 0
  cache_index_and_filter_blocks_with_high_priority: 1
  pin_l0_filter_and_index_blocks_in_cache: 0
  pin_top_level_index_and_filter: 1
  index_type: 0
  data_block_index_type: 0
  index_shortening: 1
  data_block_hash_table_util_ratio: 0.750000
  checksum: 1
  no_block_cache: 0
  block_cache: 0x7f81c0133fe0
  block_cache_name: LRUCache
  block_cache_options:
    capacity : 8388608
    num_shard_bits : 4
    strict_capacity_limit : 0
    memory_allocator : None
    high_pri_pool_ratio: 0.000
  block_cache_compressed: (nil)
  persistent_cache: (nil)
  block_size: 4096
  block_size_deviation: 10
  block_restart_interval: 16
  index_block_restart_interval: 1
  metadata_block_size: 4096
  partition_filters: 0
  use_delta_encoding: 1
  filter_policy: nullptr
  whole_key_filtering: 1
  verify_compression: 0
  read_amp_bytes_per_bit: 0
  format_version: 5
  enable_index_compression: 1
  block_align: 0
  max_auto_readahead_size: 262144
  prepopulate_block_cache: 0
  initial_auto_readahead_size: 8192

24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :        Options.write_buffer_size: 67108864
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :  Options.max_write_buffer_number: 2
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :          Options.compression: Snappy
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression: Disabled
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :       Options.prefix_extractor: nullptr
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :   Options.memtable_insert_with_hint_prefix_extractor: nullptr
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :             Options.num_levels: 7
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :        Options.min_write_buffer_number_to_merge: 1
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :     Options.max_write_buffer_number_to_maintain: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :     Options.max_write_buffer_size_to_maintain: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :            Options.bottommost_compression_opts.window_bits: -14
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression_opts.level: 32767
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :               Options.bottommost_compression_opts.strategy: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.max_dict_bytes: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.zstd_max_train_bytes: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.parallel_threads: 1
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression_opts.enabled: false
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.max_dict_buffer_bytes: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.use_zstd_dict_trainer: true
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :            Options.compression_opts.window_bits: -14
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                  Options.compression_opts.level: 32767
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :               Options.compression_opts.strategy: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.max_dict_bytes: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.zstd_max_train_bytes: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.use_zstd_dict_trainer: true
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.parallel_threads: 1
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                  Options.compression_opts.enabled: false
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.max_dict_buffer_bytes: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :      Options.level0_file_num_compaction_trigger: 4
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :          Options.level0_slowdown_writes_trigger: 20
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :              Options.level0_stop_writes_trigger: 36
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                   Options.target_file_size_base: 67108864
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :             Options.target_file_size_multiplier: 1
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                Options.max_bytes_for_level_base: 268435456
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Options.level_compaction_dynamic_level_bytes: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :          Options.max_bytes_for_level_multiplier: 10.000000
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[0]: 1
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[1]: 1
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[2]: 1
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[3]: 1
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[4]: 1
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[5]: 1
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[6]: 1
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :       Options.max_sequential_skip_in_iterations: 8
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                    Options.max_compaction_bytes: 1677721600
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                        Options.arena_block_size: 1048576
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :   Options.soft_pending_compaction_bytes_limit: 68719476736
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :   Options.hard_pending_compaction_bytes_limit: 274877906944
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                Options.disable_auto_compactions: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                        Options.compaction_style: kCompactionStyleLevel
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                          Options.compaction_pri: kMinOverlappingRatio
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.size_ratio: 1
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.min_merge_width: 2
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.max_merge_width: 4294967295
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.max_size_amplification_percent: 200
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.compression_size_percent: -1
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.stop_style: kCompactionStopStyleTotalSize
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_fifo.max_table_files_size: 1073741824
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_fifo.allow_compaction: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                   Options.table_properties_collectors: 
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                   Options.inplace_update_support: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                 Options.inplace_update_num_locks: 10000
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :               Options.memtable_prefix_bloom_size_ratio: 0.000000
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :               Options.memtable_whole_key_filtering: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :   Options.memtable_huge_page_size: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                           Options.bloom_locality: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                    Options.max_successive_merges: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                Options.optimize_filters_for_hits: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                Options.paranoid_file_checks: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                Options.force_consistency_checks: 1
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                Options.report_bg_io_stats: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                               Options.ttl: 2592000
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :          Options.periodic_compaction_seconds: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :  Options.preclude_last_level_data_seconds: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                       Options.enable_blob_files: false
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                           Options.min_blob_size: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                          Options.blob_file_size: 268435456
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                   Options.blob_compression_type: NoCompression
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :          Options.enable_blob_garbage_collection: false
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :      Options.blob_garbage_collection_age_cutoff: 0.250000
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Options.blob_garbage_collection_force_threshold: 1.000000
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :          Options.blob_compaction_readahead_size: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB :                Options.blob_file_starting_level: 0
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : Options.experimental_mempurge_threshold: 0.000000
24/01/05 02:11:39 INFO collection.RocksDBDAO: From Rocks DB : [/db_impl/db_impl.cc:2861] Created column family [hudi_pending_clustering_fg_user_hive_warehouse_stock_ticks_mor] (ID 7)
24/01/05 02:11:39 INFO view.RocksDbBasedFileSystemView: Resetting replacedFileGroups to ROCKSDB based file-system view at /tmp/hoodie_timeline_rocksdb, Total file-groups=0
24/01/05 02:11:39 INFO collection.RocksDBDAO: Prefix DELETE (query=part=) on hudi_replaced_fg_user_hive_warehouse_stock_ticks_mor
24/01/05 02:11:39 INFO view.RocksDbBasedFileSystemView: Resetting replacedFileGroups to ROCKSDB based file-system view complete
24/01/05 02:11:39 INFO view.AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
24/01/05 02:11:39 INFO view.RocksDbBasedFileSystemView: Initializing pending compaction operations. Count=0
24/01/05 02:11:39 INFO view.RocksDbBasedFileSystemView: Initializing pending Log compaction operations. Count=0
24/01/05 02:11:39 INFO view.RocksDbBasedFileSystemView: Initializing external data file mapping. Count=0
24/01/05 02:11:39 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:11:39 INFO view.RocksDbBasedFileSystemView: Resetting file groups in pending clustering to ROCKSDB based file-system view at /tmp/hoodie_timeline_rocksdb, Total file-groups=0
24/01/05 02:11:39 INFO collection.RocksDBDAO: Prefix DELETE (query=part=) on hudi_pending_clustering_fg_user_hive_warehouse_stock_ticks_mor
24/01/05 02:11:39 INFO view.RocksDbBasedFileSystemView: Resetting replacedFileGroups to ROCKSDB based file-system view complete
24/01/05 02:11:39 INFO view.RocksDbBasedFileSystemView: Created ROCKSDB based file-system view at /tmp/hoodie_timeline_rocksdb
24/01/05 02:11:39 INFO collection.RocksDBDAO: Prefix Search for (query=) on hudi_pending_compaction__user_hive_warehouse_stock_ticks_mor. Total Time Taken (msec)=0. Serialization Time taken(micro)=0, num entries=0
24/01/05 02:11:39 INFO view.RemoteHoodieTableFileSystemView: Sending request : (http://adhoc-2:35479/v1/hoodie/view/logcompactions/pending/?basepath=%2Fuser%2Fhive%2Fwarehouse%2Fstock_ticks_mor&lastinstantts=20240105021130096&timelinehash=103ad031554a22043ed4a74ba8b432d0278d2aeb876bc11f809fb9c1b2fb2c5b)
24/01/05 02:11:39 INFO collection.RocksDBDAO: Prefix Search for (query=) on hudi_pending_log_compaction__user_hive_warehouse_stock_ticks_mor. Total Time Taken (msec)=0. Serialization Time taken(micro)=0, num entries=0
24/01/05 02:11:39 INFO clean.CleanPlanner: No earliest commit to retain. No need to scan partitions !!
24/01/05 02:11:39 INFO clean.CleanPlanActionExecutor: Nothing to clean here. It is already clean
24/01/05 02:11:39 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105021130096__deltacommit__COMPLETED__20240105021139690]}
24/01/05 02:11:39 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:11:39 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:11:39 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:11:39 INFO table.HoodieTableMetaClient: Loading Active commit timeline for /user/hive/warehouse/stock_ticks_mor
24/01/05 02:11:39 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105021130096__deltacommit__COMPLETED__20240105021139690]}
24/01/05 02:11:39 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:11:39 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:11:39 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:11:39 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:11:40 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:11:40 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:11:40 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105021130096__deltacommit__COMPLETED__20240105021139601]}
24/01/05 02:11:40 INFO view.AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
24/01/05 02:11:40 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:11:40 INFO view.FileSystemViewManager: Creating View Manager with storage type :REMOTE_FIRST
24/01/05 02:11:40 INFO view.FileSystemViewManager: Creating remote first table view
24/01/05 02:11:40 INFO client.BaseHoodieWriteClient: Start to archive synchronously.
24/01/05 02:11:40 INFO transaction.TransactionManager: Transaction starting for Optional.empty with latest completed transaction instant Optional.empty
24/01/05 02:11:40 INFO lock.LockManager: LockProvider org.apache.hudi.client.transaction.lock.InProcessLockProvider
24/01/05 02:11:40 INFO lock.InProcessLockProvider: Base Path /user/hive/warehouse/stock_ticks_mor, Lock Instance java.util.concurrent.locks.ReentrantReadWriteLock@2e204155[Write locks = 0, Read locks = 0], Thread main, In-process lock state ACQUIRING
24/01/05 02:11:40 INFO lock.InProcessLockProvider: Base Path /user/hive/warehouse/stock_ticks_mor, Lock Instance java.util.concurrent.locks.ReentrantReadWriteLock@2e204155[Write locks = 1, Read locks = 0], Thread main, In-process lock state ACQUIRED
24/01/05 02:11:40 INFO transaction.TransactionManager: Transaction started for Optional.empty with latest completed transaction instant Optional.empty
24/01/05 02:11:40 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105021130096__deltacommit__COMPLETED__20240105021139690]}
24/01/05 02:11:40 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:11:40 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:11:40 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:11:40 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:11:40 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:11:40 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:11:40 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105021130096__deltacommit__COMPLETED__20240105021139601]}
24/01/05 02:11:40 INFO view.AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
24/01/05 02:11:40 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:11:40 INFO client.HoodieTimelineArchiver: Not archiving as there is no compaction yet on the metadata table
24/01/05 02:11:40 INFO client.HoodieTimelineArchiver: No Instants to archive
24/01/05 02:11:40 INFO transaction.TransactionManager: Transaction ending with transaction owner Optional.empty
24/01/05 02:11:40 INFO lock.InProcessLockProvider: Base Path /user/hive/warehouse/stock_ticks_mor, Lock Instance java.util.concurrent.locks.ReentrantReadWriteLock@2e204155[Write locks = 1, Read locks = 0], Thread main, In-process lock state RELEASING
24/01/05 02:11:40 INFO lock.InProcessLockProvider: Base Path /user/hive/warehouse/stock_ticks_mor, Lock Instance java.util.concurrent.locks.ReentrantReadWriteLock@2e204155[Write locks = 0, Read locks = 0], Thread main, In-process lock state RELEASED
24/01/05 02:11:40 INFO lock.InProcessLockProvider: Base Path /user/hive/warehouse/stock_ticks_mor, Lock Instance java.util.concurrent.locks.ReentrantReadWriteLock@2e204155[Write locks = 0, Read locks = 0], Thread main, In-process lock state ALREADY_RELEASED
24/01/05 02:11:40 INFO lock.LockManager: Released connection created for acquiring lock
24/01/05 02:11:40 INFO transaction.TransactionManager: Transaction ended with transaction owner Optional.empty
24/01/05 02:11:40 INFO view.FileSystemViewManager: Creating remote view for basePath /user/hive/warehouse/stock_ticks_mor. Server=adhoc-2:35479, Timeout=300
24/01/05 02:11:40 INFO view.FileSystemViewManager: Creating InMemory based view for basePath /user/hive/warehouse/stock_ticks_mor
24/01/05 02:11:40 INFO view.AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
24/01/05 02:11:40 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:11:40 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105021130096__deltacommit__COMPLETED__20240105021139690]}
24/01/05 02:11:40 INFO view.RemoteHoodieTableFileSystemView: Sending request : (http://adhoc-2:35479/v1/hoodie/view/refresh/?basepath=%2Fuser%2Fhive%2Fwarehouse%2Fstock_ticks_mor&lastinstantts=20240105021130096&timelinehash=103ad031554a22043ed4a74ba8b432d0278d2aeb876bc11f809fb9c1b2fb2c5b)
24/01/05 02:11:40 INFO view.RocksDbBasedFileSystemView: Closing Rocksdb !!
24/01/05 02:11:40 INFO collection.RocksDBDAO: From Rocks DB : [/db_impl/db_impl.cc:478] Shutdown: canceling all background work
24/01/05 02:11:40 INFO collection.RocksDBDAO: From Rocks DB : [/db_impl/db_impl.cc:677] Shutdown complete
24/01/05 02:11:40 INFO view.RocksDbBasedFileSystemView: Closed Rocksdb !!
24/01/05 02:11:40 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105021130096__deltacommit__COMPLETED__20240105021139690]}
24/01/05 02:11:40 INFO view.AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
24/01/05 02:11:40 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:11:40 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105021130096__deltacommit__COMPLETED__20240105021139690]}
24/01/05 02:11:40 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105021130096__deltacommit__COMPLETED__20240105021139601]}
24/01/05 02:11:40 INFO view.AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
24/01/05 02:11:40 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:11:40 INFO streamer.StreamSync: Commit 20240105021130096 successful!
24/01/05 02:11:40 INFO transaction.TransactionManager: Transaction manager closed
24/01/05 02:11:40 INFO transaction.TransactionManager: Transaction manager closed
24/01/05 02:11:40 INFO streamer.StreamSync: Shutting down embedded timeline server
24/01/05 02:11:40 INFO embedded.EmbeddedTimelineService: Closing Timeline server
24/01/05 02:11:40 INFO service.TimelineService: Closing Timeline Service
24/01/05 02:11:40 INFO javalin.Javalin: Stopping Javalin ...
24/01/05 02:11:40 INFO javalin.Javalin: Javalin has stopped
24/01/05 02:11:40 INFO service.TimelineService: Closed Timeline Service
24/01/05 02:11:40 INFO embedded.EmbeddedTimelineService: Closed Timeline server
24/01/05 02:11:40 INFO ingestion.HoodieIngestionService: Ingestion service (run-once mode) has been shut down.
24/01/05 02:11:40 INFO server.AbstractConnector: Stopped Spark@26f3d90c{HTTP/1.1,[http/1.1]}{0.0.0.0:8090}
24/01/05 02:11:40 INFO ui.SparkUI: Stopped Spark web UI at http://adhoc-2:8090
24/01/05 02:11:40 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
24/01/05 02:11:40 INFO memory.MemoryStore: MemoryStore cleared
24/01/05 02:11:40 INFO storage.BlockManager: BlockManager stopped
24/01/05 02:11:40 INFO storage.BlockManagerMaster: BlockManagerMaster stopped
24/01/05 02:11:40 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
24/01/05 02:11:40 INFO spark.SparkContext: Successfully stopped SparkContext
24/01/05 02:11:40 INFO util.ShutdownHookManager: Shutdown hook called
24/01/05 02:11:40 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-f96a29b7-0950-41a6-b903-4c5d5734b32c
24/01/05 02:11:40 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-9e3170af-6fa4-4409-ba40-255faf24ba10
+ docker exec -i adhoc-1 hadoop --config /etc/hadoop fs -ls -R '/user/hive/warehouse/stock_ticks*/20*'
drwxr-xr-x   - root supergroup          0 2024-01-05 02:11 /user/hive/warehouse/stock_ticks_mor/2018/08
drwxr-xr-x   - root supergroup          0 2024-01-05 02:11 /user/hive/warehouse/stock_ticks_mor/2018/08/31
-rw-r--r--   1 root supergroup       1786 2024-01-05 02:11 /user/hive/warehouse/stock_ticks_mor/2018/08/31/.63b57973-43fe-40da-bb65-66fab049ab22-0_20240105021110208.log.1_0-14-16
-rw-r--r--   1 root supergroup         96 2024-01-05 02:11 /user/hive/warehouse/stock_ticks_mor/2018/08/31/.hoodie_partition_metadata
-rw-r--r--   1 root supergroup     442449 2024-01-05 02:11 /user/hive/warehouse/stock_ticks_mor/2018/08/31/63b57973-43fe-40da-bb65-66fab049ab22-0_0-23-24_20240105021110208.parquet
