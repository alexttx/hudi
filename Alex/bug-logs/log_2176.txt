+ set -u
+ n=2176
+ shift
+ bash /home/alex/github/alexttx/hudi/docker/setup_demo.sh
 Container presto-worker-1  Stopping
 Container zookeeper  Stopping
 Container datanode1  Stopping
 Container spark-worker-1  Stopping
 Container adhoc-2  Stopping
 Container graphite  Stopping
 Container adhoc-1  Stopping
 Container kafkabroker  Stopping
 Container trino-worker-1  Stopping
 Container zookeeper  Stopped
 Container zookeeper  Removing
 Container zookeeper  Removed
 Container kafkabroker  Stopped
 Container kafkabroker  Removing
 Container kafkabroker  Removed
 Container presto-worker-1  Stopped
 Container presto-worker-1  Removing
 Container presto-worker-1  Removed
 Container graphite  Stopped
 Container graphite  Removing
 Container graphite  Removed
 Container adhoc-2  Stopped
 Container adhoc-2  Removing
 Container adhoc-2  Removed
 Container adhoc-1  Stopped
 Container adhoc-1  Removing
 Container adhoc-1  Removed
 Container presto-coordinator-1  Stopping
 Container spark-worker-1  Stopped
 Container spark-worker-1  Removing
 Container spark-worker-1  Removed
 Container sparkmaster  Stopping
 Container trino-worker-1  Stopped
 Container trino-worker-1  Removing
 Container datanode1  Stopped
 Container datanode1  Removing
 Container trino-worker-1  Removed
 Container trino-coordinator-1  Stopping
 Container datanode1  Removed
 Container historyserver  Stopping
 Container presto-coordinator-1  Stopped
 Container presto-coordinator-1  Removing
 Container presto-coordinator-1  Removed
 Container historyserver  Stopped
 Container historyserver  Removing
 Container historyserver  Removed
 Container sparkmaster  Stopped
 Container sparkmaster  Removing
 Container sparkmaster  Removed
 Container hiveserver  Stopping
 Container trino-coordinator-1  Stopped
 Container trino-coordinator-1  Removing
 Container trino-coordinator-1  Removed
 Container hiveserver  Stopped
 Container hiveserver  Removing
 Container hiveserver  Removed
 Container hivemetastore  Stopping
 Container hivemetastore  Stopped
 Container hivemetastore  Removing
 Container hivemetastore  Removed
 Container hive-metastore-postgresql  Stopping
 Container namenode  Stopping
 Container hive-metastore-postgresql  Stopped
 Container hive-metastore-postgresql  Removing
 Container hive-metastore-postgresql  Removed
 Container namenode  Stopped
 Container namenode  Removing
 Container namenode  Removed
 Network hudi  Removing
 Network hudi  Removed
Pulling docker demo images ...
 presto-worker-1 Skipped - Image is already being pulled by presto-coordinator-1 
 adhoc-1 Skipped - Image is already being pulled by adhoc-2 
 graphite Pulling 
 trino-coordinator-1 Pulling 
 sparkmaster Pulling 
 hivemetastore Skipped - Image is already being pulled by hiveserver 
 namenode Pulling 
 datanode1 Pulling 
 hive-metastore-postgresql Pulling 
 trino-worker-1 Pulling 
 zookeeper Pulling 
 hiveserver Pulling 
 presto-coordinator-1 Pulling 
 spark-worker-1 Pulling 
 kafka Pulling 
 adhoc-2 Pulling 
 historyserver Pulling 
 zookeeper Pulled 
 hive-metastore-postgresql Pulled 
 kafka Pulled 
 historyserver Pulled 
 adhoc-2 Pulled 
 hiveserver Pulled 
 sparkmaster Pulled 
 presto-coordinator-1 Pulled 
 namenode Pulled 
 spark-worker-1 Pulled 
 trino-worker-1 Pulled 
 trino-coordinator-1 Pulled 
 graphite Pulled 
 datanode1 Pulled 
 Network hudi  Creating
 Network hudi  Created
 Container kafkabroker  Creating
 Container graphite  Creating
 Container hive-metastore-postgresql  Creating
 Container zookeeper  Creating
 Container namenode  Creating
 Container zookeeper  Created
 Container kafkabroker  Created
 Container graphite  Created
 Container hive-metastore-postgresql  Created
 Container namenode  Created
 Container hivemetastore  Creating
 Container historyserver  Creating
 Container historyserver  Created
 Container datanode1  Creating
 Container hivemetastore  Created
 Container hiveserver  Creating
 Container presto-coordinator-1  Creating
 Container trino-coordinator-1  Creating
 Container presto-coordinator-1  Created
 Container datanode1  Created
 Container hiveserver  Created
 Container sparkmaster  Creating
 Container presto-worker-1  Creating
 Container trino-coordinator-1  Created
 Container trino-worker-1  Creating
 Container presto-worker-1  Created
 Container sparkmaster  Created
 Container spark-worker-1  Creating
 Container adhoc-1  Creating
 Container adhoc-2  Creating
 Container trino-worker-1  Created
 Container spark-worker-1  Created
 Container adhoc-1  Created
 Container adhoc-2  Created
 Container graphite  Starting
 Container namenode  Starting
 Container zookeeper  Starting
 Container kafkabroker  Starting
 Container hive-metastore-postgresql  Starting
 Container kafkabroker  Started
 Container graphite  Started
 Container zookeeper  Started
 Container hive-metastore-postgresql  Started
 Container namenode  Started
 Container historyserver  Starting
 Container hivemetastore  Starting
 Container historyserver  Started
 Container datanode1  Starting
 Container hivemetastore  Started
 Container presto-coordinator-1  Starting
 Container hiveserver  Starting
 Container trino-coordinator-1  Starting
 Container datanode1  Started
 Container hiveserver  Started
 Container sparkmaster  Starting
 Container trino-coordinator-1  Started
 Container trino-worker-1  Starting
 Container presto-coordinator-1  Started
 Container presto-worker-1  Starting
 Container trino-worker-1  Started
 Container presto-worker-1  Started
 Container sparkmaster  Started
 Container adhoc-1  Starting
 Container adhoc-2  Starting
 Container spark-worker-1  Starting
 Container spark-worker-1  Started
 Container adhoc-2  Started
 Container adhoc-1  Started
Copying spark default config and setting up configs
Copying spark default config and setting up configs
+ cat /home/alex/github/alexttx/hudi/docker/demo/data/batch_1.json
+ head -n2176
+ kcat -b kafkabroker -t stock_ticks -P
+ kcat -b kafkabroker -L -J
+ jq -C .
[1;39m{
  [0m[34;1m"originating_broker"[0m[1;39m: [0m[1;39m{
    [0m[34;1m"id"[0m[1;39m: [0m[0;39m1001[0m[1;39m,
    [0m[34;1m"name"[0m[1;39m: [0m[0;32m"kafkabroker:9092/1001"[0m[1;39m
  [1;39m}[0m[1;39m,
  [0m[34;1m"query"[0m[1;39m: [0m[1;39m{
    [0m[34;1m"topic"[0m[1;39m: [0m[0;32m"*"[0m[1;39m
  [1;39m}[0m[1;39m,
  [0m[34;1m"controllerid"[0m[1;39m: [0m[0;39m1001[0m[1;39m,
  [0m[34;1m"brokers"[0m[1;39m: [0m[1;39m[
    [1;39m{
      [0m[34;1m"id"[0m[1;39m: [0m[0;39m1001[0m[1;39m,
      [0m[34;1m"name"[0m[1;39m: [0m[0;32m"kafkabroker:9092"[0m[1;39m
    [1;39m}[0m[1;39m
  [1;39m][0m[1;39m,
  [0m[34;1m"topics"[0m[1;39m: [0m[1;39m[
    [1;39m{
      [0m[34;1m"topic"[0m[1;39m: [0m[0;32m"stock_ticks"[0m[1;39m,
      [0m[34;1m"partitions"[0m[1;39m: [0m[1;39m[
        [1;39m{
          [0m[34;1m"partition"[0m[1;39m: [0m[0;39m0[0m[1;39m,
          [0m[34;1m"leader"[0m[1;39m: [0m[0;39m1001[0m[1;39m,
          [0m[34;1m"replicas"[0m[1;39m: [0m[1;39m[
            [1;39m{
              [0m[34;1m"id"[0m[1;39m: [0m[0;39m1001[0m[1;39m
            [1;39m}[0m[1;39m
          [1;39m][0m[1;39m,
          [0m[34;1m"isrs"[0m[1;39m: [0m[1;39m[
            [1;39m{
              [0m[34;1m"id"[0m[1;39m: [0m[0;39m1001[0m[1;39m
            [1;39m}[0m[1;39m
          [1;39m][0m[1;39m
        [1;39m}[0m[1;39m
      [1;39m][0m[1;39m
    [1;39m}[0m[1;39m
  [1;39m][0m[1;39m
[1;39m}[0m
+ docker exec -i adhoc-1 /bin/bash -x
+ spark-submit --class org.apache.hudi.utilities.streamer.HoodieStreamer /var/hoodie/ws/docker/hoodie/hadoop/hive_base/target/hoodie-utilities.jar --table-type MERGE_ON_READ --source-class org.apache.hudi.utilities.sources.JsonKafkaSource --source-ordering-field ts --target-base-path /user/hive/warehouse/stock_ticks_mor --target-table stock_ticks_mor --props /var/demo/config/kafka-source.properties --schemaprovider-class org.apache.hudi.utilities.schema.FilebasedSchemaProvider --disable-compaction
24/01/05 02:13:44 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
24/01/05 02:13:44 WARN streamer.SchedulerConfGenerator: Job Scheduling Configs will not be in effect as spark.scheduler.mode is not set to FAIR at instantiation time. Continuing without scheduling configs
24/01/05 02:13:45 INFO spark.SparkContext: Running Spark version 2.4.4
24/01/05 02:13:45 INFO spark.SparkContext: Submitted application: streamer-stock_ticks_mor
24/01/05 02:13:45 INFO spark.SecurityManager: Changing view acls to: root
24/01/05 02:13:45 INFO spark.SecurityManager: Changing modify acls to: root
24/01/05 02:13:45 INFO spark.SecurityManager: Changing view acls groups to: 
24/01/05 02:13:45 INFO spark.SecurityManager: Changing modify acls groups to: 
24/01/05 02:13:45 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
24/01/05 02:13:45 INFO Configuration.deprecation: mapred.output.compression.codec is deprecated. Instead, use mapreduce.output.fileoutputformat.compress.codec
24/01/05 02:13:45 INFO Configuration.deprecation: mapred.output.compress is deprecated. Instead, use mapreduce.output.fileoutputformat.compress
24/01/05 02:13:45 INFO Configuration.deprecation: mapred.output.compression.type is deprecated. Instead, use mapreduce.output.fileoutputformat.compress.type
24/01/05 02:13:45 INFO util.Utils: Successfully started service 'sparkDriver' on port 43099.
24/01/05 02:13:45 INFO spark.SparkEnv: Registering MapOutputTracker
24/01/05 02:13:45 INFO spark.SparkEnv: Registering BlockManagerMaster
24/01/05 02:13:45 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
24/01/05 02:13:45 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
24/01/05 02:13:45 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-5d27769a-9d26-455e-a91b-c6f1f6ada10b
24/01/05 02:13:45 INFO memory.MemoryStore: MemoryStore started with capacity 366.3 MB
24/01/05 02:13:45 INFO spark.SparkEnv: Registering OutputCommitCoordinator
24/01/05 02:13:45 INFO util.log: Logging initialized @1561ms
24/01/05 02:13:45 INFO server.Server: jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
24/01/05 02:13:45 INFO server.Server: Started @1633ms
24/01/05 02:13:45 INFO server.AbstractConnector: Started ServerConnector@61019f59{HTTP/1.1,[http/1.1]}{0.0.0.0:8090}
24/01/05 02:13:45 INFO util.Utils: Successfully started service 'SparkUI' on port 8090.
24/01/05 02:13:45 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3e1162e7{/jobs,null,AVAILABLE,@Spark}
24/01/05 02:13:45 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@17f460bb{/jobs/json,null,AVAILABLE,@Spark}
24/01/05 02:13:45 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@64a1923a{/jobs/job,null,AVAILABLE,@Spark}
24/01/05 02:13:45 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@18ca3c62{/jobs/job/json,null,AVAILABLE,@Spark}
24/01/05 02:13:45 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2c0f7678{/stages,null,AVAILABLE,@Spark}
24/01/05 02:13:45 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@44d70181{/stages/json,null,AVAILABLE,@Spark}
24/01/05 02:13:45 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6aa648b9{/stages/stage,null,AVAILABLE,@Spark}
24/01/05 02:13:45 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@88a8218{/stages/stage/json,null,AVAILABLE,@Spark}
24/01/05 02:13:45 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@50b1f030{/stages/pool,null,AVAILABLE,@Spark}
24/01/05 02:13:45 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4163f1cd{/stages/pool/json,null,AVAILABLE,@Spark}
24/01/05 02:13:45 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5fa05212{/storage,null,AVAILABLE,@Spark}
24/01/05 02:13:45 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3e681bc{/storage/json,null,AVAILABLE,@Spark}
24/01/05 02:13:45 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5c09d180{/storage/rdd,null,AVAILABLE,@Spark}
24/01/05 02:13:45 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@23aae55{/storage/rdd/json,null,AVAILABLE,@Spark}
24/01/05 02:13:45 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5f574cc2{/environment,null,AVAILABLE,@Spark}
24/01/05 02:13:45 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@680bddf5{/environment/json,null,AVAILABLE,@Spark}
24/01/05 02:13:45 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7a9c84a5{/executors,null,AVAILABLE,@Spark}
24/01/05 02:13:45 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2d83c5a5{/executors/json,null,AVAILABLE,@Spark}
24/01/05 02:13:45 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@48d7ad8b{/executors/threadDump,null,AVAILABLE,@Spark}
24/01/05 02:13:45 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7e053511{/executors/threadDump/json,null,AVAILABLE,@Spark}
24/01/05 02:13:45 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@60222fd8{/static,null,AVAILABLE,@Spark}
24/01/05 02:13:45 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@37ff4054{/,null,AVAILABLE,@Spark}
24/01/05 02:13:45 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@894858{/api,null,AVAILABLE,@Spark}
24/01/05 02:13:45 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@74cf8b28{/jobs/job/kill,null,AVAILABLE,@Spark}
24/01/05 02:13:45 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@36c54a56{/stages/stage/kill,null,AVAILABLE,@Spark}
24/01/05 02:13:45 INFO ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://adhoc-1:8090
24/01/05 02:13:45 INFO spark.SparkContext: Added JAR file:/var/hoodie/ws/docker/hoodie/hadoop/hive_base/target/hoodie-utilities.jar at spark://adhoc-1:43099/jars/hoodie-utilities.jar with timestamp 1704420825593
24/01/05 02:13:45 INFO executor.Executor: Starting executor ID driver on host localhost
24/01/05 02:13:45 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33719.
24/01/05 02:13:45 INFO netty.NettyBlockTransferService: Server created on adhoc-1:33719
24/01/05 02:13:45 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
24/01/05 02:13:45 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, adhoc-1, 33719, None)
24/01/05 02:13:45 INFO storage.BlockManagerMasterEndpoint: Registering block manager adhoc-1:33719 with 366.3 MB RAM, BlockManagerId(driver, adhoc-1, 33719, None)
24/01/05 02:13:45 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, adhoc-1, 33719, None)
24/01/05 02:13:45 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, adhoc-1, 33719, None)
24/01/05 02:13:45 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@62e6a3ec{/metrics/json,null,AVAILABLE,@Spark}
24/01/05 02:13:46 WARN config.DFSPropertiesConfiguration: Cannot find HUDI_CONF_DIR, please set it as the dir of hudi-defaults.conf
24/01/05 02:13:46 WARN config.DFSPropertiesConfiguration: Properties file file:/etc/hudi/conf/hudi-defaults.conf not found. Ignoring to load props file
24/01/05 02:13:46 INFO internal.SharedState: loading hive config file: file:/opt/hadoop-2.8.4/etc/hadoop/hive-site.xml
24/01/05 02:13:46 INFO internal.SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/opt/spark-warehouse').
24/01/05 02:13:46 INFO internal.SharedState: Warehouse path is 'file:/opt/spark-warehouse'.
24/01/05 02:13:46 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@423c5404{/SQL,null,AVAILABLE,@Spark}
24/01/05 02:13:46 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5853ca50{/SQL/json,null,AVAILABLE,@Spark}
24/01/05 02:13:46 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3c79088e{/SQL/execution,null,AVAILABLE,@Spark}
24/01/05 02:13:46 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4a37191a{/SQL/execution/json,null,AVAILABLE,@Spark}
24/01/05 02:13:46 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@24eb65e3{/static/sql,null,AVAILABLE,@Spark}
24/01/05 02:13:47 INFO state.StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
24/01/05 02:13:47 WARN sql.SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
24/01/05 02:13:47 INFO streamer.HoodieStreamer: Creating Hudi Streamer with configs:
auto.offset.reset: earliest
bootstrap.servers: kafkabroker:9092
hoodie.auto.adjust.lock.configs: true
hoodie.bulkinsert.shuffle.parallelism: 2
hoodie.compact.inline: false
hoodie.datasource.write.partitionpath.field: date
hoodie.datasource.write.reconcile.schema: false
hoodie.datasource.write.recordkey.field: key
hoodie.delete.shuffle.parallelism: 2
hoodie.embed.timeline.server: true
hoodie.filesystem.view.type: EMBEDDED_KV_STORE
hoodie.insert.shuffle.parallelism: 2
hoodie.streamer.schemaprovider.source.schema.file: /var/demo/config/schema.avsc
hoodie.streamer.schemaprovider.target.schema.file: /var/demo/config/schema.avsc
hoodie.streamer.source.kafka.topic: stock_ticks
hoodie.table.type: MERGE_ON_READ
hoodie.upsert.shuffle.parallelism: 2

24/01/05 02:13:47 INFO fs.FSUtils: Resolving file /var/demo/config/schema.avscto be a remote file.
24/01/05 02:13:47 INFO factory.HoodieSparkKeyGeneratorFactory: The value of hoodie.datasource.write.keygenerator.type is empty; inferred to be SIMPLE
24/01/05 02:13:47 INFO factory.HoodieSparkKeyGeneratorFactory: The value of hoodie.datasource.write.keygenerator.type is empty; inferred to be SIMPLE
24/01/05 02:13:47 INFO table.HoodieTableMetaClient: Initializing /user/hive/warehouse/stock_ticks_mor as hoodie table /user/hive/warehouse/stock_ticks_mor
24/01/05 02:13:47 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:13:47 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:13:47 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:13:47 INFO table.HoodieTableMetaClient: Finished initializing Table of type MERGE_ON_READ from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:13:47 INFO config.HoodieWriteConfig: Automatically set hoodie.write.lock.provider=org.apache.hudi.client.transaction.lock.InProcessLockProvider since user has not set the lock provider for single writer with async table services
24/01/05 02:13:47 INFO config.HoodieWriteConfig: Automatically set hoodie.write.lock.provider=org.apache.hudi.client.transaction.lock.InProcessLockProvider since user has not set the lock provider for single writer with async table services
24/01/05 02:13:47 INFO ingestion.HoodieIngestionService: Ingestion service starts running in run-once mode
24/01/05 02:13:47 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:13:47 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:13:47 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:13:47 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Optional.empty
24/01/05 02:13:47 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:13:47 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:13:47 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:13:47 INFO streamer.StreamSync: Checkpoint to resume from : Optional.empty
24/01/05 02:13:47 INFO consumer.ConsumerConfig: ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [kafkabroker:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

24/01/05 02:13:47 INFO utils.AppInfoParser: Kafka version : 2.0.0
24/01/05 02:13:47 INFO utils.AppInfoParser: Kafka commitId : 3402a8361b734732
24/01/05 02:13:47 INFO clients.Metadata: Cluster ID: dMB3Nbq9TYSdXYXM4yK4DQ
24/01/05 02:13:47 INFO helpers.KafkaOffsetGen: SourceLimit not configured, set numEvents to default value : 5000000
24/01/05 02:13:47 INFO helpers.KafkaOffsetGen: getNextOffsetRanges set config hoodie.streamer.source.kafka.minPartitions to 0
24/01/05 02:13:47 INFO sources.KafkaSource: About to read 2176 from Kafka for topic :stock_ticks
24/01/05 02:13:47 WARN kafka010.KafkaUtils: overriding enable.auto.commit to false for executor
24/01/05 02:13:47 WARN kafka010.KafkaUtils: overriding auto.offset.reset to none for executor
24/01/05 02:13:47 ERROR kafka010.KafkaUtils: group.id is null, you should probably set it
24/01/05 02:13:47 WARN kafka010.KafkaUtils: overriding executor group.id to spark-executor-null
24/01/05 02:13:47 WARN kafka010.KafkaUtils: overriding receive.buffer.bytes to 65536 see KAFKA-3135
24/01/05 02:13:47 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Optional.empty
24/01/05 02:13:47 INFO factory.HoodieSparkKeyGeneratorFactory: The value of hoodie.datasource.write.keygenerator.type is empty; inferred to be SIMPLE
24/01/05 02:13:48 INFO streamer.StreamSync: Setting up new Hoodie Write Client
24/01/05 02:13:48 INFO config.HoodieWriteConfig: Automatically set hoodie.write.lock.provider=org.apache.hudi.client.transaction.lock.InProcessLockProvider since user has not set the lock provider for single writer with async table services
24/01/05 02:13:48 INFO embedded.EmbeddedTimelineService: Overriding hostIp to (adhoc-1) found in spark-conf. It was null
24/01/05 02:13:48 INFO view.FileSystemViewManager: Creating View Manager with storage type :EMBEDDED_KV_STORE
24/01/05 02:13:48 INFO view.FileSystemViewManager: Creating embedded rocks-db based Table View
24/01/05 02:13:48 INFO util.log: Logging initialized @4182ms to org.apache.hudi.org.eclipse.jetty.util.log.Slf4jLog
24/01/05 02:13:48 INFO javalin.Javalin: 
       __                      __ _            __ __
      / /____ _ _   __ ____ _ / /(_)____      / // /
 __  / // __ `/| | / // __ `// // // __ \    / // /_
/ /_/ // /_/ / | |/ // /_/ // // // / / /   /__  __/
\____/ \__,_/  |___/ \__,_//_//_//_/ /_/      /_/

          https://javalin.io/documentation

24/01/05 02:13:48 INFO javalin.Javalin: Starting Javalin ...
24/01/05 02:13:48 INFO javalin.Javalin: You are running Javalin 4.6.7 (released October 24, 2022. Your Javalin version is 437 days old. Consider checking for a newer version.).
24/01/05 02:13:48 INFO server.Server: jetty-9.4.48.v20220622; built: 2022-06-21T20:42:25.880Z; git: 6b67c5719d1f4371b33655ff2d047d24e171e49a; jvm 1.8.0_212-b04
24/01/05 02:13:48 INFO server.Server: Started @4607ms
24/01/05 02:13:48 INFO javalin.Javalin: Listening on http://localhost:34001/
24/01/05 02:13:48 INFO javalin.Javalin: Javalin started in 169ms \o/
24/01/05 02:13:48 INFO service.TimelineService: Starting Timeline server on port :34001
24/01/05 02:13:48 INFO embedded.EmbeddedTimelineService: Started embedded timeline server at adhoc-1:34001
24/01/05 02:13:48 INFO client.BaseHoodieClient: Timeline Server already running. Not restarting the service
24/01/05 02:13:48 INFO client.BaseHoodieClient: Timeline Server already running. Not restarting the service
24/01/05 02:13:48 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:13:48 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:13:48 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:13:48 INFO table.HoodieTableMetaClient: Loading Active commit timeline for /user/hive/warehouse/stock_ticks_mor
24/01/05 02:13:48 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Optional.empty
24/01/05 02:13:48 INFO util.CleanerUtils: Cleaned failed attempts if any
24/01/05 02:13:48 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:13:48 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:13:48 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:13:48 INFO table.HoodieTableMetaClient: Loading Active commit timeline for /user/hive/warehouse/stock_ticks_mor
24/01/05 02:13:48 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Optional.empty
24/01/05 02:13:48 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:13:48 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:13:48 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:13:48 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:13:48 INFO view.FileSystemViewManager: Creating View Manager with storage type :REMOTE_FIRST
24/01/05 02:13:48 INFO view.FileSystemViewManager: Creating remote first table view
24/01/05 02:13:48 INFO client.BaseHoodieWriteClient: Generate a new instant time: 20240105021347452 action: deltacommit
24/01/05 02:13:48 INFO timeline.HoodieActiveTimeline: Creating a new instant [==>20240105021347452__deltacommit__REQUESTED]
24/01/05 02:13:48 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:13:48 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:13:48 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:13:48 INFO table.HoodieTableMetaClient: Loading Active commit timeline for /user/hive/warehouse/stock_ticks_mor
24/01/05 02:13:48 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[==>20240105021347452__deltacommit__REQUESTED__20240105021348554]}
24/01/05 02:13:48 INFO transaction.TransactionManager: Transaction starting for Option{val=[==>20240105021347452__deltacommit__INFLIGHT]} with latest completed transaction instant Optional.empty
24/01/05 02:13:48 INFO lock.LockManager: LockProvider org.apache.hudi.client.transaction.lock.InProcessLockProvider
24/01/05 02:13:48 INFO lock.InProcessLockProvider: Base Path /user/hive/warehouse/stock_ticks_mor, Lock Instance java.util.concurrent.locks.ReentrantReadWriteLock@592a1882[Write locks = 0, Read locks = 0], Thread main, In-process lock state ACQUIRING
24/01/05 02:13:48 INFO lock.InProcessLockProvider: Base Path /user/hive/warehouse/stock_ticks_mor, Lock Instance java.util.concurrent.locks.ReentrantReadWriteLock@592a1882[Write locks = 1, Read locks = 0], Thread main, In-process lock state ACQUIRED
24/01/05 02:13:48 INFO transaction.TransactionManager: Transaction started for Option{val=[==>20240105021347452__deltacommit__INFLIGHT]} with latest completed transaction instant Optional.empty
24/01/05 02:13:48 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:13:48 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:13:48 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:13:48 INFO metadata.HoodieBackedTableMetadataWriter: Async metadata indexing disabled and following partitions already initialized: []
24/01/05 02:13:48 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[==>20240105021347452__deltacommit__REQUESTED__20240105021348554]}
24/01/05 02:13:48 INFO table.HoodieTableMetaClient: Initializing /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata as hoodie table /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:13:48 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:13:48 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:13:48 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:13:48 INFO table.HoodieTableMetaClient: Finished initializing Table of type MERGE_ON_READ from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:13:48 INFO spark.SparkContext: Starting job: collect at HoodieSparkEngineContext.java:116
24/01/05 02:13:48 INFO scheduler.DAGScheduler: Got job 0 (collect at HoodieSparkEngineContext.java:116) with 1 output partitions
24/01/05 02:13:48 INFO scheduler.DAGScheduler: Final stage: ResultStage 0 (collect at HoodieSparkEngineContext.java:116)
24/01/05 02:13:48 INFO scheduler.DAGScheduler: Parents of final stage: List()
24/01/05 02:13:48 INFO scheduler.DAGScheduler: Missing parents: List()
24/01/05 02:13:48 INFO scheduler.DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[8] at map at HoodieSparkEngineContext.java:116), which has no missing parents
24/01/05 02:13:48 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 72.2 KB, free 366.2 MB)
24/01/05 02:13:49 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 26.3 KB, free 366.2 MB)
24/01/05 02:13:49 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on adhoc-1:33719 (size: 26.3 KB, free: 366.3 MB)
24/01/05 02:13:49 INFO spark.SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1161
24/01/05 02:13:49 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[8] at map at HoodieSparkEngineContext.java:116) (first 15 tasks are for partitions Vector(0))
24/01/05 02:13:49 INFO scheduler.TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
24/01/05 02:13:49 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7842 bytes)
24/01/05 02:13:49 INFO executor.Executor: Running task 0.0 in stage 0.0 (TID 0)
24/01/05 02:13:49 INFO executor.Executor: Fetching spark://adhoc-1:43099/jars/hoodie-utilities.jar with timestamp 1704420825593
24/01/05 02:13:49 INFO client.TransportClientFactory: Successfully created connection to adhoc-1/172.21.0.18:43099 after 29 ms (0 ms spent in bootstraps)
24/01/05 02:13:49 INFO util.Utils: Fetching spark://adhoc-1:43099/jars/hoodie-utilities.jar to /tmp/spark-513cd1a1-8dff-4bc1-8164-daca0914589f/userFiles-b9af104d-2c17-4810-ab95-2c5d9a93984f/fetchFileTemp9031370558246863038.tmp
24/01/05 02:13:49 INFO executor.Executor: Adding file:/tmp/spark-513cd1a1-8dff-4bc1-8164-daca0914589f/userFiles-b9af104d-2c17-4810-ab95-2c5d9a93984f/hoodie-utilities.jar to class loader
24/01/05 02:13:49 INFO executor.Executor: Finished task 0.0 in stage 0.0 (TID 0). 833 bytes result sent to driver
24/01/05 02:13:49 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 514 ms on localhost (executor driver) (1/1)
24/01/05 02:13:49 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
24/01/05 02:13:49 INFO scheduler.DAGScheduler: ResultStage 0 (collect at HoodieSparkEngineContext.java:116) finished in 0.778 s
24/01/05 02:13:49 INFO scheduler.DAGScheduler: Job 0 finished: collect at HoodieSparkEngineContext.java:116, took 0.811592 s
24/01/05 02:13:49 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Optional.empty
24/01/05 02:13:49 INFO metadata.HoodieBackedTableMetadataWriter: Initializing MDT partition FILES at instant 00000000000000010
24/01/05 02:13:49 INFO metadata.HoodieBackedTableMetadataWriter: Committing total 0 partitions and 0 files to metadata
24/01/05 02:13:49 INFO spark.SparkContext: Starting job: count at HoodieJavaRDD.java:115
24/01/05 02:13:49 INFO scheduler.DAGScheduler: Got job 1 (count at HoodieJavaRDD.java:115) with 1 output partitions
24/01/05 02:13:49 INFO scheduler.DAGScheduler: Final stage: ResultStage 1 (count at HoodieJavaRDD.java:115)
24/01/05 02:13:49 INFO scheduler.DAGScheduler: Parents of final stage: List()
24/01/05 02:13:49 INFO scheduler.DAGScheduler: Missing parents: List()
24/01/05 02:13:49 INFO scheduler.DAGScheduler: Submitting ResultStage 1 (ParallelCollectionRDD[9] at parallelize at HoodieSparkEngineContext.java:111), which has no missing parents
24/01/05 02:13:49 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 1512.0 B, free 366.2 MB)
24/01/05 02:13:49 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 1046.0 B, free 366.2 MB)
24/01/05 02:13:49 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on adhoc-1:33719 (size: 1046.0 B, free: 366.3 MB)
24/01/05 02:13:49 INFO spark.SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1161
24/01/05 02:13:49 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (ParallelCollectionRDD[9] at parallelize at HoodieSparkEngineContext.java:111) (first 15 tasks are for partitions Vector(0))
24/01/05 02:13:49 INFO scheduler.TaskSchedulerImpl: Adding task set 1.0 with 1 tasks
24/01/05 02:13:49 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 7801 bytes)
24/01/05 02:13:49 INFO executor.Executor: Running task 0.0 in stage 1.0 (TID 1)
24/01/05 02:13:49 INFO executor.Executor: Finished task 0.0 in stage 1.0 (TID 1). 666 bytes result sent to driver
24/01/05 02:13:49 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 26 ms on localhost (executor driver) (1/1)
24/01/05 02:13:49 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
24/01/05 02:13:49 INFO scheduler.DAGScheduler: ResultStage 1 (count at HoodieJavaRDD.java:115) finished in 0.036 s
24/01/05 02:13:49 INFO scheduler.DAGScheduler: Job 1 finished: count at HoodieJavaRDD.java:115, took 0.039124 s
24/01/05 02:13:49 INFO metadata.HoodieBackedTableMetadataWriter: Initializing FILES index with 1 mappings and 1 file groups.
24/01/05 02:13:49 INFO metadata.HoodieBackedTableMetadataWriter: Creating 1 file groups for partition files with base fileId files- at instant time 00000000000000010
24/01/05 02:13:49 INFO spark.SparkContext: Starting job: foreach at HoodieSparkEngineContext.java:155
24/01/05 02:13:49 INFO scheduler.DAGScheduler: Got job 2 (foreach at HoodieSparkEngineContext.java:155) with 1 output partitions
24/01/05 02:13:49 INFO scheduler.DAGScheduler: Final stage: ResultStage 2 (foreach at HoodieSparkEngineContext.java:155)
24/01/05 02:13:49 INFO scheduler.DAGScheduler: Parents of final stage: List()
24/01/05 02:13:49 INFO scheduler.DAGScheduler: Missing parents: List()
24/01/05 02:13:49 INFO scheduler.DAGScheduler: Submitting ResultStage 2 (ParallelCollectionRDD[10] at parallelize at HoodieSparkEngineContext.java:155), which has no missing parents
24/01/05 02:13:49 INFO memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 296.9 KB, free 365.9 MB)
24/01/05 02:13:49 INFO memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 100.2 KB, free 365.8 MB)
24/01/05 02:13:49 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on adhoc-1:33719 (size: 100.2 KB, free: 366.2 MB)
24/01/05 02:13:49 INFO spark.SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1161
24/01/05 02:13:49 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (ParallelCollectionRDD[10] at parallelize at HoodieSparkEngineContext.java:155) (first 15 tasks are for partitions Vector(0))
24/01/05 02:13:49 INFO scheduler.TaskSchedulerImpl: Adding task set 2.0 with 1 tasks
24/01/05 02:13:49 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, PROCESS_LOCAL, 7737 bytes)
24/01/05 02:13:49 INFO executor.Executor: Running task 0.0 in stage 2.0 (TID 2)
24/01/05 02:13:49 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on adhoc-1:33719 in memory (size: 1046.0 B, free: 366.2 MB)
24/01/05 02:13:49 INFO storage.BlockManagerInfo: Removed broadcast_0_piece0 on adhoc-1:33719 in memory (size: 26.3 KB, free: 366.2 MB)
24/01/05 02:13:49 INFO log.HoodieLogFormat$WriterBuilder: Building HoodieLogFormat Writer
24/01/05 02:13:49 INFO log.HoodieLogFormat$WriterBuilder: HoodieLogFile on path /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/files/.files-0000-0_00000000000000010.log.1_0-0-0
24/01/05 02:13:49 INFO log.HoodieLogFormatWriter: HoodieLogFile{pathStr='/user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/files/.files-0000-0_00000000000000010.log.1_0-0-0', fileLen=0} does not exist. Create a new file
24/01/05 02:13:49 INFO executor.Executor: Finished task 0.0 in stage 2.0 (TID 2). 709 bytes result sent to driver
24/01/05 02:13:49 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 134 ms on localhost (executor driver) (1/1)
24/01/05 02:13:49 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
24/01/05 02:13:49 INFO scheduler.DAGScheduler: ResultStage 2 (foreach at HoodieSparkEngineContext.java:155) finished in 0.182 s
24/01/05 02:13:49 INFO scheduler.DAGScheduler: Job 2 finished: foreach at HoodieSparkEngineContext.java:155, took 0.184805 s
24/01/05 02:13:49 INFO view.AbstractTableFileSystemView: Took 2 ms to read  0 instants, 0 replaced file groups
24/01/05 02:13:49 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:13:49 INFO metadata.HoodieTableMetadataUtil: Loading latest file slices for metadata table partition files
24/01/05 02:13:49 INFO view.AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
24/01/05 02:13:49 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:13:49 INFO view.AbstractTableFileSystemView: Building file system view for partition (files)
24/01/05 02:13:49 INFO client.BaseHoodieClient: Embedded Timeline Server is disabled. Not starting timeline service
24/01/05 02:13:49 INFO client.BaseHoodieClient: Embedded Timeline Server is disabled. Not starting timeline service
24/01/05 02:13:49 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:13:49 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:13:49 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:13:49 INFO table.HoodieTableMetaClient: Loading Active commit timeline for /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:13:49 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Optional.empty
24/01/05 02:13:49 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:13:49 INFO view.FileSystemViewManager: Creating View Manager with storage type :MEMORY
24/01/05 02:13:49 INFO view.FileSystemViewManager: Creating in-memory based Table View
24/01/05 02:13:49 INFO metadata.HoodieBackedTableMetadataWriter: New commit at 00000000000000010 being applied to MDT.
24/01/05 02:13:49 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:13:49 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:13:49 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:13:49 INFO table.HoodieTableMetaClient: Loading Active commit timeline for /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:13:49 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Optional.empty
24/01/05 02:13:49 INFO util.CleanerUtils: Cleaned failed attempts if any
24/01/05 02:13:49 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:13:49 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:13:49 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:13:49 INFO table.HoodieTableMetaClient: Loading Active commit timeline for /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:13:50 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Optional.empty
24/01/05 02:13:50 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:13:50 INFO view.FileSystemViewManager: Creating View Manager with storage type :MEMORY
24/01/05 02:13:50 INFO view.FileSystemViewManager: Creating in-memory based Table View
24/01/05 02:13:50 INFO client.BaseHoodieWriteClient: Generate a new instant time: 00000000000000010 action: deltacommit
24/01/05 02:13:50 INFO timeline.HoodieActiveTimeline: Creating a new instant [==>00000000000000010__deltacommit__REQUESTED]
24/01/05 02:13:50 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:13:50 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:13:50 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:13:50 INFO table.HoodieTableMetaClient: Loading Active commit timeline for /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:13:50 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[==>00000000000000010__deltacommit__REQUESTED__20240105021350010]}
24/01/05 02:13:50 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:13:50 INFO view.FileSystemViewManager: Creating View Manager with storage type :MEMORY
24/01/05 02:13:50 INFO view.FileSystemViewManager: Creating in-memory based Table View
24/01/05 02:13:50 INFO async.AsyncCleanerService: The HoodieWriteClient is not configured to auto & async clean. Async clean service will not start.
24/01/05 02:13:50 INFO async.AsyncArchiveService: The HoodieWriteClient is not configured to auto & async archive. Async archive service will not start.
24/01/05 02:13:50 INFO timeline.HoodieActiveTimeline: Checking for file exists ?/user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/00000000000000010.deltacommit.requested
24/01/05 02:13:50 INFO timeline.HoodieActiveTimeline: Create new file for toInstant ?/user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/00000000000000010.deltacommit.inflight
24/01/05 02:13:50 INFO spark.SparkContext: Starting job: collect at SparkHoodieMetadataBulkInsertPartitioner.java:95
24/01/05 02:13:50 INFO scheduler.DAGScheduler: Registering RDD 14 (keyBy at SparkHoodieMetadataBulkInsertPartitioner.java:74)
24/01/05 02:13:50 INFO scheduler.DAGScheduler: Got job 3 (collect at SparkHoodieMetadataBulkInsertPartitioner.java:95) with 1 output partitions
24/01/05 02:13:50 INFO scheduler.DAGScheduler: Final stage: ResultStage 4 (collect at SparkHoodieMetadataBulkInsertPartitioner.java:95)
24/01/05 02:13:50 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)
24/01/05 02:13:50 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 3)
24/01/05 02:13:50 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 3 (MapPartitionsRDD[14] at keyBy at SparkHoodieMetadataBulkInsertPartitioner.java:74), which has no missing parents
24/01/05 02:13:50 INFO memory.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 7.0 KB, free 365.9 MB)
24/01/05 02:13:50 INFO memory.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 4.0 KB, free 365.9 MB)
24/01/05 02:13:50 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on adhoc-1:33719 (size: 4.0 KB, free: 366.2 MB)
24/01/05 02:13:50 INFO spark.SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1161
24/01/05 02:13:50 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[14] at keyBy at SparkHoodieMetadataBulkInsertPartitioner.java:74) (first 15 tasks are for partitions Vector(0))
24/01/05 02:13:50 INFO scheduler.TaskSchedulerImpl: Adding task set 3.0 with 1 tasks
24/01/05 02:13:50 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3, localhost, executor driver, partition 0, PROCESS_LOCAL, 7899 bytes)
24/01/05 02:13:50 INFO executor.Executor: Running task 0.0 in stage 3.0 (TID 3)
24/01/05 02:13:50 INFO executor.Executor: Finished task 0.0 in stage 3.0 (TID 3). 821 bytes result sent to driver
24/01/05 02:13:50 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 50 ms on localhost (executor driver) (1/1)
24/01/05 02:13:50 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
24/01/05 02:13:50 INFO scheduler.DAGScheduler: ShuffleMapStage 3 (keyBy at SparkHoodieMetadataBulkInsertPartitioner.java:74) finished in 0.072 s
24/01/05 02:13:50 INFO scheduler.DAGScheduler: looking for newly runnable stages
24/01/05 02:13:50 INFO scheduler.DAGScheduler: running: Set()
24/01/05 02:13:50 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 4)
24/01/05 02:13:50 INFO scheduler.DAGScheduler: failed: Set()
24/01/05 02:13:50 INFO scheduler.DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[17] at mapPartitions at SparkHoodieMetadataBulkInsertPartitioner.java:81), which has no missing parents
24/01/05 02:13:50 INFO memory.MemoryStore: Block broadcast_4 stored as values in memory (estimated size 5.4 KB, free 365.9 MB)
24/01/05 02:13:50 INFO memory.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 3.0 KB, free 365.9 MB)
24/01/05 02:13:50 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on adhoc-1:33719 (size: 3.0 KB, free: 366.2 MB)
24/01/05 02:13:50 INFO spark.SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1161
24/01/05 02:13:50 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[17] at mapPartitions at SparkHoodieMetadataBulkInsertPartitioner.java:81) (first 15 tasks are for partitions Vector(0))
24/01/05 02:13:50 INFO scheduler.TaskSchedulerImpl: Adding task set 4.0 with 1 tasks
24/01/05 02:13:50 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4, localhost, executor driver, partition 0, ANY, 7662 bytes)
24/01/05 02:13:50 INFO executor.Executor: Running task 0.0 in stage 4.0 (TID 4)
24/01/05 02:13:50 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
24/01/05 02:13:50 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 6 ms
24/01/05 02:13:50 INFO executor.Executor: Finished task 0.0 in stage 4.0 (TID 4). 1110 bytes result sent to driver
24/01/05 02:13:50 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 68 ms on localhost (executor driver) (1/1)
24/01/05 02:13:50 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 
24/01/05 02:13:50 INFO scheduler.DAGScheduler: ResultStage 4 (collect at SparkHoodieMetadataBulkInsertPartitioner.java:95) finished in 0.079 s
24/01/05 02:13:50 INFO scheduler.DAGScheduler: Job 3 finished: collect at SparkHoodieMetadataBulkInsertPartitioner.java:95, took 0.174094 s
24/01/05 02:13:50 INFO commit.BaseSparkCommitActionExecutor: no validators configured.
24/01/05 02:13:50 INFO commit.BaseCommitActionExecutor: Auto commit enabled: Committing 00000000000000010
24/01/05 02:13:50 INFO spark.SparkContext: Starting job: collect at HoodieJavaRDD.java:177
24/01/05 02:13:50 INFO scheduler.DAGScheduler: Got job 4 (collect at HoodieJavaRDD.java:177) with 1 output partitions
24/01/05 02:13:50 INFO scheduler.DAGScheduler: Final stage: ResultStage 6 (collect at HoodieJavaRDD.java:177)
24/01/05 02:13:50 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 5)
24/01/05 02:13:50 INFO scheduler.DAGScheduler: Missing parents: List()
24/01/05 02:13:50 INFO scheduler.DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[20] at map at HoodieJavaRDD.java:125), which has no missing parents
24/01/05 02:13:50 INFO memory.MemoryStore: Block broadcast_5 stored as values in memory (estimated size 273.1 KB, free 365.6 MB)
24/01/05 02:13:50 INFO memory.MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 96.5 KB, free 365.5 MB)
24/01/05 02:13:50 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on adhoc-1:33719 (size: 96.5 KB, free: 366.1 MB)
24/01/05 02:13:50 INFO spark.SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1161
24/01/05 02:13:50 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[20] at map at HoodieJavaRDD.java:125) (first 15 tasks are for partitions Vector(0))
24/01/05 02:13:50 INFO scheduler.TaskSchedulerImpl: Adding task set 6.0 with 1 tasks
24/01/05 02:13:50 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 6.0 (TID 5, localhost, executor driver, partition 0, ANY, 7662 bytes)
24/01/05 02:13:50 INFO executor.Executor: Running task 0.0 in stage 6.0 (TID 5)
24/01/05 02:13:50 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
24/01/05 02:13:50 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
24/01/05 02:13:50 INFO queue.SimpleExecutor: Starting consumer, consuming records from the records iterator directly
24/01/05 02:13:50 INFO marker.DirectWriteMarkers: Creating Marker Path=/user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/.temp/00000000000000010/files/files-0000-0_0-6-5_00000000000000010.hfile.marker.CREATE
24/01/05 02:13:50 INFO marker.DirectWriteMarkers: [direct] Created marker file /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/.temp/00000000000000010/files/files-0000-0_0-6-5_00000000000000010.hfile.marker.CREATE in 11 ms
24/01/05 02:13:51 INFO impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
24/01/05 02:13:51 INFO impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
24/01/05 02:13:51 INFO impl.MetricsSystemImpl: HBase metrics system started
24/01/05 02:13:51 INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.hudi.org.apache.hadoop.hbase.metrics.impl.MetricRegistriesImpl
24/01/05 02:13:51 INFO compress.CodecPool: Got brand-new compressor [.gz]
24/01/05 02:13:51 INFO compress.CodecPool: Got brand-new compressor [.gz]
24/01/05 02:13:51 INFO io.HoodieCreateHandle: New CreateHandle for partition :files with fileId files-0000-0
24/01/05 02:13:51 INFO io.HoodieCreateHandle: Closing the file files-0000-0 as we are done with all the records 1
24/01/05 02:13:51 INFO io.HoodieCreateHandle: CreateHandle for partitionPath files fileID files-0000-0, took 971 ms.
24/01/05 02:13:51 INFO memory.MemoryStore: Block rdd_19_0 stored as values in memory (estimated size 319.0 B, free 365.5 MB)
24/01/05 02:13:51 INFO storage.BlockManagerInfo: Added rdd_19_0 in memory on adhoc-1:33719 (size: 319.0 B, free: 366.1 MB)
24/01/05 02:13:51 INFO executor.Executor: Finished task 0.0 in stage 6.0 (TID 5). 1302 bytes result sent to driver
24/01/05 02:13:51 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 6.0 (TID 5) in 1060 ms on localhost (executor driver) (1/1)
24/01/05 02:13:51 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool 
24/01/05 02:13:51 INFO scheduler.DAGScheduler: ResultStage 6 (collect at HoodieJavaRDD.java:177) finished in 1.087 s
24/01/05 02:13:51 INFO scheduler.DAGScheduler: Job 4 finished: collect at HoodieJavaRDD.java:177, took 1.091965 s
24/01/05 02:13:51 INFO util.CommitUtils: Creating  metadata for BULK_INSERT numWriteStats:1 numReplaceFileIds:0
24/01/05 02:13:51 INFO commit.BaseSparkCommitActionExecutor: Committing 00000000000000010, action Type deltacommit, operation Type BULK_INSERT
24/01/05 02:13:51 INFO spark.SparkContext: Starting job: collect at HoodieSparkEngineContext.java:150
24/01/05 02:13:51 INFO scheduler.DAGScheduler: Got job 5 (collect at HoodieSparkEngineContext.java:150) with 1 output partitions
24/01/05 02:13:51 INFO scheduler.DAGScheduler: Final stage: ResultStage 7 (collect at HoodieSparkEngineContext.java:150)
24/01/05 02:13:51 INFO scheduler.DAGScheduler: Parents of final stage: List()
24/01/05 02:13:51 INFO scheduler.DAGScheduler: Missing parents: List()
24/01/05 02:13:51 INFO scheduler.DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[22] at flatMap at HoodieSparkEngineContext.java:150), which has no missing parents
24/01/05 02:13:51 INFO memory.MemoryStore: Block broadcast_6 stored as values in memory (estimated size 72.5 KB, free 365.5 MB)
24/01/05 02:13:51 INFO memory.MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 26.7 KB, free 365.4 MB)
24/01/05 02:13:51 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on adhoc-1:33719 (size: 26.7 KB, free: 366.1 MB)
24/01/05 02:13:51 INFO spark.SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1161
24/01/05 02:13:51 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[22] at flatMap at HoodieSparkEngineContext.java:150) (first 15 tasks are for partitions Vector(0))
24/01/05 02:13:51 INFO scheduler.TaskSchedulerImpl: Adding task set 7.0 with 1 tasks
24/01/05 02:13:51 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 7.0 (TID 6, localhost, executor driver, partition 0, PROCESS_LOCAL, 7838 bytes)
24/01/05 02:13:51 INFO executor.Executor: Running task 0.0 in stage 7.0 (TID 6)
24/01/05 02:13:51 INFO executor.Executor: Finished task 0.0 in stage 7.0 (TID 6). 718 bytes result sent to driver
24/01/05 02:13:51 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 7.0 (TID 6) in 25 ms on localhost (executor driver) (1/1)
24/01/05 02:13:51 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool 
24/01/05 02:13:51 INFO scheduler.DAGScheduler: ResultStage 7 (collect at HoodieSparkEngineContext.java:150) finished in 0.044 s
24/01/05 02:13:51 INFO scheduler.DAGScheduler: Job 5 finished: collect at HoodieSparkEngineContext.java:150, took 0.045938 s
24/01/05 02:13:51 INFO timeline.HoodieActiveTimeline: Marking instant complete [==>00000000000000010__deltacommit__INFLIGHT]
24/01/05 02:13:51 INFO timeline.HoodieActiveTimeline: Checking for file exists ?/user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/00000000000000010.deltacommit.inflight
24/01/05 02:13:51 INFO timeline.HoodieActiveTimeline: Create new file for toInstant ?/user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/00000000000000010.deltacommit
24/01/05 02:13:51 INFO timeline.HoodieActiveTimeline: Completed [==>00000000000000010__deltacommit__INFLIGHT]
24/01/05 02:13:51 INFO commit.BaseSparkCommitActionExecutor: Committed 00000000000000010
24/01/05 02:13:51 INFO spark.SparkContext: Starting job: collectAsMap at HoodieSparkEngineContext.java:164
24/01/05 02:13:51 INFO scheduler.DAGScheduler: Got job 6 (collectAsMap at HoodieSparkEngineContext.java:164) with 1 output partitions
24/01/05 02:13:51 INFO scheduler.DAGScheduler: Final stage: ResultStage 8 (collectAsMap at HoodieSparkEngineContext.java:164)
24/01/05 02:13:51 INFO scheduler.DAGScheduler: Parents of final stage: List()
24/01/05 02:13:51 INFO scheduler.DAGScheduler: Missing parents: List()
24/01/05 02:13:51 INFO scheduler.DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[24] at mapToPair at HoodieSparkEngineContext.java:161), which has no missing parents
24/01/05 02:13:51 INFO memory.MemoryStore: Block broadcast_7 stored as values in memory (estimated size 72.8 KB, free 365.4 MB)
24/01/05 02:13:51 INFO spark.ContextCleaner: Cleaned accumulator 107
24/01/05 02:13:51 INFO spark.ContextCleaner: Cleaned accumulator 106
24/01/05 02:13:51 INFO spark.ContextCleaner: Cleaned accumulator 162
24/01/05 02:13:51 INFO spark.ContextCleaner: Cleaned accumulator 126
24/01/05 02:13:51 INFO spark.ContextCleaner: Cleaned accumulator 114
24/01/05 02:13:51 INFO spark.ContextCleaner: Cleaned accumulator 160
24/01/05 02:13:51 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on adhoc-1:33719 in memory (size: 96.5 KB, free: 366.2 MB)
24/01/05 02:13:51 INFO spark.ContextCleaner: Cleaned accumulator 122
24/01/05 02:13:51 INFO spark.ContextCleaner: Cleaned accumulator 112
24/01/05 02:13:51 INFO spark.ContextCleaner: Cleaned accumulator 102
24/01/05 02:13:51 INFO spark.ContextCleaner: Cleaned accumulator 164
24/01/05 02:13:51 INFO spark.ContextCleaner: Cleaned accumulator 157
24/01/05 02:13:51 INFO spark.ContextCleaner: Cleaned accumulator 94
24/01/05 02:13:51 INFO spark.ContextCleaner: Cleaned accumulator 131
24/01/05 02:13:51 INFO spark.ContextCleaner: Cleaned accumulator 169
24/01/05 02:13:51 INFO spark.ContextCleaner: Cleaned accumulator 166
24/01/05 02:13:51 INFO spark.ContextCleaner: Cleaned accumulator 146
24/01/05 02:13:51 INFO spark.ContextCleaner: Cleaned accumulator 116
24/01/05 02:13:51 INFO spark.ContextCleaner: Cleaned accumulator 87
24/01/05 02:13:51 INFO spark.ContextCleaner: Cleaned accumulator 129
24/01/05 02:13:51 INFO spark.ContextCleaner: Cleaned accumulator 141
24/01/05 02:13:51 INFO spark.ContextCleaner: Cleaned accumulator 92
24/01/05 02:13:51 INFO spark.ContextCleaner: Cleaned accumulator 153
24/01/05 02:13:51 INFO spark.ContextCleaner: Cleaned accumulator 80
24/01/05 02:13:51 INFO spark.ContextCleaner: Cleaned accumulator 138
24/01/05 02:13:51 INFO spark.ContextCleaner: Cleaned accumulator 86
24/01/05 02:13:51 INFO spark.ContextCleaner: Cleaned accumulator 151
24/01/05 02:13:51 INFO spark.ContextCleaner: Cleaned accumulator 124
24/01/05 02:13:51 INFO spark.ContextCleaner: Cleaned accumulator 140
24/01/05 02:13:51 INFO spark.ContextCleaner: Cleaned accumulator 97
24/01/05 02:13:51 INFO spark.ContextCleaner: Cleaned accumulator 143
24/01/05 02:13:51 INFO spark.ContextCleaner: Cleaned accumulator 79
24/01/05 02:13:51 INFO memory.MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 26.8 KB, free 365.7 MB)
24/01/05 02:13:51 INFO spark.ContextCleaner: Cleaned accumulator 120
24/01/05 02:13:51 INFO spark.ContextCleaner: Cleaned accumulator 96
24/01/05 02:13:51 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on adhoc-1:33719 (size: 26.8 KB, free: 366.1 MB)
24/01/05 02:13:51 INFO spark.SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1161
24/01/05 02:13:51 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on adhoc-1:33719 in memory (size: 3.0 KB, free: 366.1 MB)
24/01/05 02:13:51 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[24] at mapToPair at HoodieSparkEngineContext.java:161) (first 15 tasks are for partitions Vector(0))
24/01/05 02:13:51 INFO scheduler.TaskSchedulerImpl: Adding task set 8.0 with 1 tasks
24/01/05 02:13:51 INFO spark.ContextCleaner: Cleaned accumulator 154
24/01/05 02:13:51 INFO spark.ContextCleaner: Cleaned accumulator 163
24/01/05 02:13:51 INFO spark.ContextCleaner: Cleaned accumulator 173
24/01/05 02:13:51 INFO spark.ContextCleaner: Cleaned accumulator 148
24/01/05 02:13:51 INFO spark.ContextCleaner: Cleaned accumulator 139
24/01/05 02:13:51 INFO storage.BlockManagerInfo: Removed broadcast_6_piece0 on adhoc-1:33719 in memory (size: 26.7 KB, free: 366.2 MB)
24/01/05 02:13:51 INFO spark.ContextCleaner: Cleaned accumulator 108
24/01/05 02:13:51 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 8.0 (TID 7, localhost, executor driver, partition 0, PROCESS_LOCAL, 7838 bytes)
24/01/05 02:13:51 INFO spark.ContextCleaner: Cleaned accumulator 135
24/01/05 02:13:51 INFO spark.ContextCleaner: Cleaned accumulator 98
24/01/05 02:13:51 INFO spark.ContextCleaner: Cleaned accumulator 115
24/01/05 02:13:51 INFO spark.ContextCleaner: Cleaned accumulator 89
24/01/05 02:13:51 INFO spark.ContextCleaner: Cleaned accumulator 142
24/01/05 02:13:51 INFO spark.ContextCleaner: Cleaned accumulator 171
24/01/05 02:13:51 INFO spark.ContextCleaner: Cleaned accumulator 78
24/01/05 02:13:51 INFO spark.ContextCleaner: Cleaned accumulator 134
24/01/05 02:13:51 INFO spark.ContextCleaner: Cleaned accumulator 158
24/01/05 02:13:51 INFO spark.ContextCleaner: Cleaned accumulator 93
24/01/05 02:13:51 INFO spark.ContextCleaner: Cleaned accumulator 145
24/01/05 02:13:51 INFO spark.ContextCleaner: Cleaned accumulator 103
24/01/05 02:13:51 INFO spark.ContextCleaner: Cleaned accumulator 76
24/01/05 02:13:51 INFO spark.ContextCleaner: Cleaned accumulator 75
24/01/05 02:13:51 INFO executor.Executor: Running task 0.0 in stage 8.0 (TID 7)
24/01/05 02:13:51 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on adhoc-1:33719 in memory (size: 4.0 KB, free: 366.2 MB)
24/01/05 02:13:51 INFO spark.ContextCleaner: Cleaned accumulator 155
24/01/05 02:13:51 INFO spark.ContextCleaner: Cleaned accumulator 167
24/01/05 02:13:51 INFO storage.BlockManagerInfo: Removed broadcast_2_piece0 on adhoc-1:33719 in memory (size: 100.2 KB, free: 366.3 MB)
24/01/05 02:13:51 INFO spark.ContextCleaner: Cleaned accumulator 105
24/01/05 02:13:51 INFO spark.ContextCleaner: Cleaned accumulator 111
24/01/05 02:13:51 INFO spark.ContextCleaner: Cleaned accumulator 168
24/01/05 02:13:51 INFO spark.ContextCleaner: Cleaned accumulator 99
24/01/05 02:13:51 INFO spark.ContextCleaner: Cleaned accumulator 132
24/01/05 02:13:51 INFO spark.ContextCleaner: Cleaned accumulator 137
24/01/05 02:13:51 INFO spark.ContextCleaner: Cleaned accumulator 159
24/01/05 02:13:51 INFO spark.ContextCleaner: Cleaned accumulator 174
24/01/05 02:13:51 INFO spark.ContextCleaner: Cleaned accumulator 147
24/01/05 02:13:51 INFO spark.ContextCleaner: Cleaned accumulator 125
24/01/05 02:13:51 INFO spark.ContextCleaner: Cleaned accumulator 110
24/01/05 02:13:51 INFO spark.ContextCleaner: Cleaned accumulator 88
24/01/05 02:13:51 INFO spark.ContextCleaner: Cleaned accumulator 123
24/01/05 02:13:51 INFO spark.ContextCleaner: Cleaned accumulator 104
24/01/05 02:13:51 INFO spark.ContextCleaner: Cleaned accumulator 119
24/01/05 02:13:51 INFO spark.ContextCleaner: Cleaned accumulator 101
24/01/05 02:13:51 INFO spark.ContextCleaner: Cleaned accumulator 156
24/01/05 02:13:51 INFO spark.ContextCleaner: Cleaned accumulator 161
24/01/05 02:13:51 INFO spark.ContextCleaner: Cleaned accumulator 133
24/01/05 02:13:51 INFO spark.ContextCleaner: Cleaned accumulator 121
24/01/05 02:13:51 INFO spark.ContextCleaner: Cleaned accumulator 149
24/01/05 02:13:51 INFO spark.ContextCleaner: Cleaned accumulator 90
24/01/05 02:13:51 INFO spark.ContextCleaner: Cleaned accumulator 128
24/01/05 02:13:51 INFO spark.ContextCleaner: Cleaned accumulator 150
24/01/05 02:13:51 INFO spark.ContextCleaner: Cleaned accumulator 95
24/01/05 02:13:51 INFO spark.ContextCleaner: Cleaned accumulator 81
24/01/05 02:13:51 INFO spark.ContextCleaner: Cleaned accumulator 170
24/01/05 02:13:51 INFO spark.ContextCleaner: Cleaned accumulator 136
24/01/05 02:13:51 INFO spark.ContextCleaner: Cleaned accumulator 165
24/01/05 02:13:51 INFO spark.ContextCleaner: Cleaned accumulator 127
24/01/05 02:13:51 INFO spark.ContextCleaner: Cleaned accumulator 83
24/01/05 02:13:51 INFO spark.ContextCleaner: Cleaned accumulator 100
24/01/05 02:13:51 INFO spark.ContextCleaner: Cleaned accumulator 144
24/01/05 02:13:51 INFO spark.ContextCleaner: Cleaned accumulator 172
24/01/05 02:13:51 INFO spark.ContextCleaner: Cleaned accumulator 84
24/01/05 02:13:51 INFO spark.ContextCleaner: Cleaned accumulator 152
24/01/05 02:13:51 INFO spark.ContextCleaner: Cleaned accumulator 85
24/01/05 02:13:51 INFO spark.ContextCleaner: Cleaned accumulator 77
24/01/05 02:13:51 INFO spark.ContextCleaner: Cleaned accumulator 118
24/01/05 02:13:51 INFO spark.ContextCleaner: Cleaned accumulator 91
24/01/05 02:13:51 INFO spark.ContextCleaner: Cleaned accumulator 117
24/01/05 02:13:51 INFO spark.ContextCleaner: Cleaned accumulator 109
24/01/05 02:13:51 INFO spark.ContextCleaner: Cleaned accumulator 82
24/01/05 02:13:51 INFO spark.ContextCleaner: Cleaned accumulator 113
24/01/05 02:13:51 INFO spark.ContextCleaner: Cleaned accumulator 130
24/01/05 02:13:51 INFO executor.Executor: Finished task 0.0 in stage 8.0 (TID 7). 787 bytes result sent to driver
24/01/05 02:13:51 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 8.0 (TID 7) in 24 ms on localhost (executor driver) (1/1)
24/01/05 02:13:51 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool 
24/01/05 02:13:51 INFO scheduler.DAGScheduler: ResultStage 8 (collectAsMap at HoodieSparkEngineContext.java:164) finished in 0.062 s
24/01/05 02:13:51 INFO scheduler.DAGScheduler: Job 6 finished: collectAsMap at HoodieSparkEngineContext.java:164, took 0.064650 s
24/01/05 02:13:51 INFO fs.FSUtils: Removed directory at /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/.temp/00000000000000010
24/01/05 02:13:51 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:13:51 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:13:51 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:13:51 INFO table.HoodieTableMetaClient: Loading Active commit timeline for /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:13:51 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20240105021351577]}
24/01/05 02:13:51 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:13:51 INFO view.FileSystemViewManager: Creating View Manager with storage type :MEMORY
24/01/05 02:13:51 INFO view.FileSystemViewManager: Creating in-memory based Table View
24/01/05 02:13:51 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20240105021351577]}
24/01/05 02:13:51 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20240105021351577]}
24/01/05 02:13:52 INFO table.HoodieTableConfig: MDT /user/hive/warehouse/stock_ticks_mor partition FILES has been enabled
24/01/05 02:13:52 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:13:52 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:13:52 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:13:52 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:13:52 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:13:52 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:13:52 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20240105021351577]}
24/01/05 02:13:52 INFO view.AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
24/01/05 02:13:52 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:13:52 INFO metadata.HoodieBackedTableMetadataWriter: Initializing FILES index in metadata table took 2528 in ms
24/01/05 02:13:52 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:13:52 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:13:52 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:13:52 INFO table.HoodieTableMetaClient: Loading Active commit timeline for /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:13:52 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20240105021351577]}
24/01/05 02:13:52 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:13:52 INFO view.FileSystemViewManager: Creating View Manager with storage type :MEMORY
24/01/05 02:13:52 INFO view.FileSystemViewManager: Creating in-memory based Table View
24/01/05 02:13:52 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:13:52 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:13:52 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:13:52 INFO table.HoodieTableMetaClient: Loading Active commit timeline for /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:13:52 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20240105021351577]}
24/01/05 02:13:52 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:13:52 INFO view.FileSystemViewManager: Creating View Manager with storage type :MEMORY
24/01/05 02:13:52 INFO view.FileSystemViewManager: Creating in-memory based Table View
24/01/05 02:13:52 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20240105021351577]}
24/01/05 02:13:52 INFO metadata.HoodieBackedTableMetadataWriter: Latest deltacommit time found is 00000000000000010, running clean operations.
24/01/05 02:13:52 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20240105021351577]}
24/01/05 02:13:52 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:13:52 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:13:52 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:13:52 INFO table.HoodieTableMetaClient: Loading Active commit timeline for /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:13:52 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20240105021351577]}
24/01/05 02:13:52 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:13:52 INFO view.FileSystemViewManager: Creating View Manager with storage type :MEMORY
24/01/05 02:13:52 INFO view.FileSystemViewManager: Creating in-memory based Table View
24/01/05 02:13:52 INFO client.BaseHoodieWriteClient: Cleaner started
24/01/05 02:13:52 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:13:52 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:13:52 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:13:52 INFO table.HoodieTableMetaClient: Loading Active commit timeline for /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:13:52 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20240105021351577]}
24/01/05 02:13:52 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:13:52 INFO view.FileSystemViewManager: Creating View Manager with storage type :MEMORY
24/01/05 02:13:52 INFO view.FileSystemViewManager: Creating in-memory based Table View
24/01/05 02:13:52 INFO client.BaseHoodieWriteClient: Scheduling cleaning at instant time :00000000000000010002
24/01/05 02:13:52 INFO view.FileSystemViewManager: Creating InMemory based view for basePath /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:13:52 INFO view.AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
24/01/05 02:13:52 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:13:52 INFO clean.CleanPlanner: No earliest commit to retain. No need to scan partitions !!
24/01/05 02:13:52 INFO clean.CleanPlanActionExecutor: Nothing to clean here. It is already clean
24/01/05 02:13:52 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20240105021351577]}
24/01/05 02:13:52 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:13:52 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:13:52 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:13:52 INFO table.HoodieTableMetaClient: Loading Active commit timeline for /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:13:52 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20240105021351577]}
24/01/05 02:13:52 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:13:52 INFO view.FileSystemViewManager: Creating View Manager with storage type :MEMORY
24/01/05 02:13:52 INFO view.FileSystemViewManager: Creating in-memory based Table View
24/01/05 02:13:52 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[==>20240105021347452__deltacommit__REQUESTED__20240105021348554]}
24/01/05 02:13:52 INFO client.BaseHoodieWriteClient: Scheduling table service COMPACT
24/01/05 02:13:52 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:13:52 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:13:52 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:13:52 INFO table.HoodieTableMetaClient: Loading Active commit timeline for /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:13:52 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20240105021351577]}
24/01/05 02:13:52 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:13:52 INFO view.FileSystemViewManager: Creating View Manager with storage type :MEMORY
24/01/05 02:13:52 INFO view.FileSystemViewManager: Creating in-memory based Table View
24/01/05 02:13:52 INFO client.BaseHoodieWriteClient: Scheduling compaction at instant time :00000000000000010001
24/01/05 02:13:52 INFO compact.ScheduleCompactionActionExecutor: Checking if compaction needs to be run on /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:13:52 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:13:52 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:13:52 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:13:52 INFO table.HoodieTableMetaClient: Loading Active commit timeline for /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:13:52 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20240105021351577]}
24/01/05 02:13:52 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:13:52 INFO view.FileSystemViewManager: Creating View Manager with storage type :MEMORY
24/01/05 02:13:52 INFO view.FileSystemViewManager: Creating in-memory based Table View
24/01/05 02:13:52 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20240105021351577]}
24/01/05 02:13:52 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:13:52 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:13:52 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:13:52 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[==>20240105021347452__deltacommit__REQUESTED__20240105021348554]}
24/01/05 02:13:52 INFO client.HoodieTimelineArchiver: No Instants to archive
24/01/05 02:13:52 INFO metadata.HoodieBackedTableMetadataWriter: All the table services operations on MDT completed successfully
24/01/05 02:13:52 INFO transaction.TransactionManager: Transaction ending with transaction owner Option{val=[==>20240105021347452__deltacommit__INFLIGHT]}
24/01/05 02:13:52 INFO lock.InProcessLockProvider: Base Path /user/hive/warehouse/stock_ticks_mor, Lock Instance java.util.concurrent.locks.ReentrantReadWriteLock@592a1882[Write locks = 1, Read locks = 0], Thread main, In-process lock state RELEASING
24/01/05 02:13:52 INFO lock.InProcessLockProvider: Base Path /user/hive/warehouse/stock_ticks_mor, Lock Instance java.util.concurrent.locks.ReentrantReadWriteLock@592a1882[Write locks = 0, Read locks = 0], Thread main, In-process lock state RELEASED
24/01/05 02:13:52 INFO lock.InProcessLockProvider: Base Path /user/hive/warehouse/stock_ticks_mor, Lock Instance java.util.concurrent.locks.ReentrantReadWriteLock@592a1882[Write locks = 0, Read locks = 0], Thread main, In-process lock state ALREADY_RELEASED
24/01/05 02:13:52 INFO lock.LockManager: Released connection created for acquiring lock
24/01/05 02:13:52 INFO transaction.TransactionManager: Transaction ended with transaction owner Option{val=[==>20240105021347452__deltacommit__INFLIGHT]}
24/01/05 02:13:52 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:13:52 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:13:52 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:13:52 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:13:52 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:13:52 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:13:52 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20240105021351577]}
24/01/05 02:13:52 INFO view.AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
24/01/05 02:13:52 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:13:52 INFO view.FileSystemViewManager: Creating View Manager with storage type :REMOTE_FIRST
24/01/05 02:13:52 INFO view.FileSystemViewManager: Creating remote first table view
24/01/05 02:13:52 INFO async.AsyncCleanerService: The HoodieWriteClient is not configured to auto & async clean. Async clean service will not start.
24/01/05 02:13:52 INFO async.AsyncArchiveService: The HoodieWriteClient is not configured to auto & async archive. Async archive service will not start.
24/01/05 02:13:52 INFO spark.SparkContext: Starting job: collect at HoodieJavaRDD.java:177
24/01/05 02:13:52 INFO scheduler.DAGScheduler: Registering RDD 25 (mapToPair at HoodieJavaRDD.java:149)
24/01/05 02:13:52 INFO scheduler.DAGScheduler: Registering RDD 31 (distinct at HoodieJavaRDD.java:157)
24/01/05 02:13:52 INFO scheduler.DAGScheduler: Got job 7 (collect at HoodieJavaRDD.java:177) with 2 output partitions
24/01/05 02:13:52 INFO scheduler.DAGScheduler: Final stage: ResultStage 11 (collect at HoodieJavaRDD.java:177)
24/01/05 02:13:52 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 10)
24/01/05 02:13:52 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 10)
24/01/05 02:13:52 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 9 (MapPartitionsRDD[25] at mapToPair at HoodieJavaRDD.java:149), which has no missing parents
24/01/05 02:13:52 INFO memory.MemoryStore: Block broadcast_8 stored as values in memory (estimated size 29.5 KB, free 366.2 MB)
24/01/05 02:13:52 INFO memory.MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 13.9 KB, free 366.2 MB)
24/01/05 02:13:52 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on adhoc-1:33719 (size: 13.9 KB, free: 366.3 MB)
24/01/05 02:13:52 INFO spark.SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1161
24/01/05 02:13:52 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 9 (MapPartitionsRDD[25] at mapToPair at HoodieJavaRDD.java:149) (first 15 tasks are for partitions Vector(0))
24/01/05 02:13:52 INFO scheduler.TaskSchedulerImpl: Adding task set 9.0 with 1 tasks
24/01/05 02:13:52 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 9.0 (TID 8, localhost, executor driver, partition 0, PROCESS_LOCAL, 7739 bytes)
24/01/05 02:13:52 INFO executor.Executor: Running task 0.0 in stage 9.0 (TID 8)
24/01/05 02:13:52 INFO kafka010.KafkaRDD: Computing topic stock_ticks, partition 0 offsets 0 -> 2176
24/01/05 02:13:52 INFO kafka010.KafkaDataConsumer: Initializing cache 16 64 0.75
24/01/05 02:13:52 INFO consumer.ConsumerConfig: ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [kafkabroker:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-executor-null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

24/01/05 02:13:52 INFO utils.AppInfoParser: Kafka version : 2.0.0
24/01/05 02:13:52 INFO utils.AppInfoParser: Kafka commitId : 3402a8361b734732
24/01/05 02:13:52 INFO factory.HoodieSparkKeyGeneratorFactory: The value of hoodie.datasource.write.keygenerator.type is empty; inferred to be SIMPLE
24/01/05 02:13:52 INFO kafka010.InternalKafkaConsumer: Initial fetch for spark-executor-null stock_ticks-0 0
24/01/05 02:13:52 INFO clients.Metadata: Cluster ID: dMB3Nbq9TYSdXYXM4yK4DQ
24/01/05 02:13:52 INFO executor.Executor: Finished task 0.0 in stage 9.0 (TID 8). 951 bytes result sent to driver
24/01/05 02:13:52 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 9.0 (TID 8) in 360 ms on localhost (executor driver) (1/1)
24/01/05 02:13:52 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool 
24/01/05 02:13:52 INFO scheduler.DAGScheduler: ShuffleMapStage 9 (mapToPair at HoodieJavaRDD.java:149) finished in 0.373 s
24/01/05 02:13:52 INFO scheduler.DAGScheduler: looking for newly runnable stages
24/01/05 02:13:52 INFO scheduler.DAGScheduler: running: Set()
24/01/05 02:13:52 INFO scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 10, ResultStage 11)
24/01/05 02:13:52 INFO scheduler.DAGScheduler: failed: Set()
24/01/05 02:13:52 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 10 (MapPartitionsRDD[31] at distinct at HoodieJavaRDD.java:157), which has no missing parents
24/01/05 02:13:52 INFO memory.MemoryStore: Block broadcast_9 stored as values in memory (estimated size 24.3 KB, free 366.1 MB)
24/01/05 02:13:52 INFO memory.MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 11.5 KB, free 366.1 MB)
24/01/05 02:13:52 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on adhoc-1:33719 (size: 11.5 KB, free: 366.2 MB)
24/01/05 02:13:52 INFO spark.SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1161
24/01/05 02:13:52 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 10 (MapPartitionsRDD[31] at distinct at HoodieJavaRDD.java:157) (first 15 tasks are for partitions Vector(0, 1))
24/01/05 02:13:52 INFO scheduler.TaskSchedulerImpl: Adding task set 10.0 with 2 tasks
24/01/05 02:13:52 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 10.0 (TID 9, localhost, executor driver, partition 0, ANY, 7651 bytes)
24/01/05 02:13:52 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 10.0 (TID 10, localhost, executor driver, partition 1, ANY, 7651 bytes)
24/01/05 02:13:52 INFO executor.Executor: Running task 0.0 in stage 10.0 (TID 9)
24/01/05 02:13:52 INFO executor.Executor: Running task 1.0 in stage 10.0 (TID 10)
24/01/05 02:13:52 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
24/01/05 02:13:52 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
24/01/05 02:13:52 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
24/01/05 02:13:52 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
24/01/05 02:13:52 INFO memory.MemoryStore: Block rdd_27_0 stored as values in memory (estimated size 6.7 KB, free 366.1 MB)
24/01/05 02:13:52 INFO storage.BlockManagerInfo: Added rdd_27_0 in memory on adhoc-1:33719 (size: 6.7 KB, free: 366.2 MB)
24/01/05 02:13:52 INFO memory.MemoryStore: Block rdd_27_1 stored as values in memory (estimated size 11.1 KB, free 366.1 MB)
24/01/05 02:13:52 INFO storage.BlockManagerInfo: Added rdd_27_1 in memory on adhoc-1:33719 (size: 11.1 KB, free: 366.2 MB)
24/01/05 02:13:52 INFO executor.Executor: Finished task 0.0 in stage 10.0 (TID 9). 1252 bytes result sent to driver
24/01/05 02:13:52 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 10.0 (TID 9) in 51 ms on localhost (executor driver) (1/2)
24/01/05 02:13:52 INFO executor.Executor: Finished task 1.0 in stage 10.0 (TID 10). 1252 bytes result sent to driver
24/01/05 02:13:52 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 10.0 (TID 10) in 55 ms on localhost (executor driver) (2/2)
24/01/05 02:13:52 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool 
24/01/05 02:13:52 INFO scheduler.DAGScheduler: ShuffleMapStage 10 (distinct at HoodieJavaRDD.java:157) finished in 0.065 s
24/01/05 02:13:52 INFO scheduler.DAGScheduler: looking for newly runnable stages
24/01/05 02:13:52 INFO scheduler.DAGScheduler: running: Set()
24/01/05 02:13:52 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 11)
24/01/05 02:13:52 INFO scheduler.DAGScheduler: failed: Set()
24/01/05 02:13:52 INFO scheduler.DAGScheduler: Submitting ResultStage 11 (MapPartitionsRDD[33] at distinct at HoodieJavaRDD.java:157), which has no missing parents
24/01/05 02:13:52 INFO memory.MemoryStore: Block broadcast_10 stored as values in memory (estimated size 4.1 KB, free 366.1 MB)
24/01/05 02:13:52 INFO memory.MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 2.4 KB, free 366.1 MB)
24/01/05 02:13:52 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on adhoc-1:33719 (size: 2.4 KB, free: 366.2 MB)
24/01/05 02:13:52 INFO spark.SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1161
24/01/05 02:13:52 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ResultStage 11 (MapPartitionsRDD[33] at distinct at HoodieJavaRDD.java:157) (first 15 tasks are for partitions Vector(0, 1))
24/01/05 02:13:52 INFO scheduler.TaskSchedulerImpl: Adding task set 11.0 with 2 tasks
24/01/05 02:13:52 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 11.0 (TID 11, localhost, executor driver, partition 0, PROCESS_LOCAL, 7662 bytes)
24/01/05 02:13:52 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 11.0 (TID 12, localhost, executor driver, partition 1, ANY, 7662 bytes)
24/01/05 02:13:52 INFO executor.Executor: Running task 0.0 in stage 11.0 (TID 11)
24/01/05 02:13:52 INFO executor.Executor: Running task 1.0 in stage 11.0 (TID 12)
24/01/05 02:13:52 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
24/01/05 02:13:52 INFO storage.ShuffleBlockFetcherIterator: Getting 2 non-empty blocks including 2 local blocks and 0 remote blocks
24/01/05 02:13:52 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
24/01/05 02:13:52 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
24/01/05 02:13:52 INFO executor.Executor: Finished task 0.0 in stage 11.0 (TID 11). 1098 bytes result sent to driver
24/01/05 02:13:52 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 11.0 (TID 11) in 15 ms on localhost (executor driver) (1/2)
24/01/05 02:13:52 INFO executor.Executor: Finished task 1.0 in stage 11.0 (TID 12). 1110 bytes result sent to driver
24/01/05 02:13:52 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 11.0 (TID 12) in 15 ms on localhost (executor driver) (2/2)
24/01/05 02:13:52 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool 
24/01/05 02:13:52 INFO scheduler.DAGScheduler: ResultStage 11 (collect at HoodieJavaRDD.java:177) finished in 0.026 s
24/01/05 02:13:52 INFO scheduler.DAGScheduler: Job 7 finished: collect at HoodieJavaRDD.java:177, took 0.485425 s
24/01/05 02:13:52 INFO spark.SparkContext: Starting job: collect at HoodieSparkEngineContext.java:150
24/01/05 02:13:52 INFO scheduler.DAGScheduler: Got job 8 (collect at HoodieSparkEngineContext.java:150) with 1 output partitions
24/01/05 02:13:52 INFO scheduler.DAGScheduler: Final stage: ResultStage 12 (collect at HoodieSparkEngineContext.java:150)
24/01/05 02:13:52 INFO scheduler.DAGScheduler: Parents of final stage: List()
24/01/05 02:13:52 INFO scheduler.DAGScheduler: Missing parents: List()
24/01/05 02:13:52 INFO scheduler.DAGScheduler: Submitting ResultStage 12 (MapPartitionsRDD[35] at flatMap at HoodieSparkEngineContext.java:150), which has no missing parents
24/01/05 02:13:53 INFO memory.MemoryStore: Block broadcast_11 stored as values in memory (estimated size 338.9 KB, free 365.8 MB)
24/01/05 02:13:53 INFO memory.MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 121.2 KB, free 365.7 MB)
24/01/05 02:13:53 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on adhoc-1:33719 (size: 121.2 KB, free: 366.1 MB)
24/01/05 02:13:53 INFO spark.SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1161
24/01/05 02:13:53 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 12 (MapPartitionsRDD[35] at flatMap at HoodieSparkEngineContext.java:150) (first 15 tasks are for partitions Vector(0))
24/01/05 02:13:53 INFO scheduler.TaskSchedulerImpl: Adding task set 12.0 with 1 tasks
24/01/05 02:13:53 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 12.0 (TID 13, localhost, executor driver, partition 0, PROCESS_LOCAL, 7735 bytes)
24/01/05 02:13:53 INFO executor.Executor: Running task 0.0 in stage 12.0 (TID 13)
24/01/05 02:13:53 INFO executor.Executor: Finished task 0.0 in stage 12.0 (TID 13). 668 bytes result sent to driver
24/01/05 02:13:53 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 12.0 (TID 13) in 33 ms on localhost (executor driver) (1/1)
24/01/05 02:13:53 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool 
24/01/05 02:13:53 INFO scheduler.DAGScheduler: ResultStage 12 (collect at HoodieSparkEngineContext.java:150) finished in 0.067 s
24/01/05 02:13:53 INFO scheduler.DAGScheduler: Job 8 finished: collect at HoodieSparkEngineContext.java:150, took 0.068902 s
24/01/05 02:13:53 INFO rdd.MapPartitionsRDD: Removing RDD 27 from persistence list
24/01/05 02:13:53 INFO storage.BlockManager: Removing RDD 27
24/01/05 02:13:53 INFO view.AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
24/01/05 02:13:53 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:13:53 INFO spark.SparkContext: Starting job: countByKey at HoodieJavaPairRDD.java:105
24/01/05 02:13:53 INFO scheduler.DAGScheduler: Registering RDD 28 (mapToPair at HoodieJavaRDD.java:149)
24/01/05 02:13:53 INFO scheduler.DAGScheduler: Registering RDD 38 (mapToPair at HoodieJavaRDD.java:149)
24/01/05 02:13:53 INFO scheduler.DAGScheduler: Registering RDD 46 (countByKey at HoodieJavaPairRDD.java:105)
24/01/05 02:13:53 INFO scheduler.DAGScheduler: Got job 9 (countByKey at HoodieJavaPairRDD.java:105) with 2 output partitions
24/01/05 02:13:53 INFO scheduler.DAGScheduler: Final stage: ResultStage 17 (countByKey at HoodieJavaPairRDD.java:105)
24/01/05 02:13:53 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 16)
24/01/05 02:13:53 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 16)
24/01/05 02:13:53 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 14 (MapPartitionsRDD[28] at mapToPair at HoodieJavaRDD.java:149), which has no missing parents
24/01/05 02:13:53 INFO memory.MemoryStore: Block broadcast_12 stored as values in memory (estimated size 23.7 KB, free 365.6 MB)
24/01/05 02:13:53 INFO memory.MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 11.4 KB, free 365.6 MB)
24/01/05 02:13:53 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on adhoc-1:33719 (size: 11.4 KB, free: 366.1 MB)
24/01/05 02:13:53 INFO spark.SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1161
24/01/05 02:13:53 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 14 (MapPartitionsRDD[28] at mapToPair at HoodieJavaRDD.java:149) (first 15 tasks are for partitions Vector(0, 1))
24/01/05 02:13:53 INFO scheduler.TaskSchedulerImpl: Adding task set 14.0 with 2 tasks
24/01/05 02:13:53 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 15 (MapPartitionsRDD[38] at mapToPair at HoodieJavaRDD.java:149), which has no missing parents
24/01/05 02:13:53 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 14.0 (TID 14, localhost, executor driver, partition 0, ANY, 7651 bytes)
24/01/05 02:13:53 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 14.0 (TID 15, localhost, executor driver, partition 1, ANY, 7651 bytes)
24/01/05 02:13:53 INFO executor.Executor: Running task 0.0 in stage 14.0 (TID 14)
24/01/05 02:13:53 INFO executor.Executor: Running task 1.0 in stage 14.0 (TID 15)
24/01/05 02:13:53 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
24/01/05 02:13:53 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
24/01/05 02:13:53 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
24/01/05 02:13:53 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
24/01/05 02:13:53 INFO executor.Executor: Finished task 0.0 in stage 14.0 (TID 14). 1252 bytes result sent to driver
24/01/05 02:13:53 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 14.0 (TID 14) in 24 ms on localhost (executor driver) (1/2)
24/01/05 02:13:53 INFO executor.Executor: Finished task 1.0 in stage 14.0 (TID 15). 1252 bytes result sent to driver
24/01/05 02:13:53 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 14.0 (TID 15) in 39 ms on localhost (executor driver) (2/2)
24/01/05 02:13:53 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 14.0, whose tasks have all completed, from pool 
24/01/05 02:13:53 INFO memory.MemoryStore: Block broadcast_13 stored as values in memory (estimated size 340.7 KB, free 365.3 MB)
24/01/05 02:13:53 INFO memory.MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 122.2 KB, free 365.2 MB)
24/01/05 02:13:53 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on adhoc-1:33719 (size: 122.2 KB, free: 366.0 MB)
24/01/05 02:13:53 INFO spark.SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1161
24/01/05 02:13:53 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 15 (MapPartitionsRDD[38] at mapToPair at HoodieJavaRDD.java:149) (first 15 tasks are for partitions Vector(0))
24/01/05 02:13:53 INFO scheduler.TaskSchedulerImpl: Adding task set 15.0 with 1 tasks
24/01/05 02:13:53 INFO scheduler.DAGScheduler: ShuffleMapStage 14 (mapToPair at HoodieJavaRDD.java:149) finished in 0.056 s
24/01/05 02:13:53 INFO scheduler.DAGScheduler: looking for newly runnable stages
24/01/05 02:13:53 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 15)
24/01/05 02:13:53 INFO scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 16, ResultStage 17)
24/01/05 02:13:53 INFO scheduler.DAGScheduler: failed: Set()
24/01/05 02:13:53 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 15.0 (TID 16, localhost, executor driver, partition 0, PROCESS_LOCAL, 7712 bytes)
24/01/05 02:13:53 INFO executor.Executor: Running task 0.0 in stage 15.0 (TID 16)
24/01/05 02:13:53 INFO executor.Executor: Finished task 0.0 in stage 15.0 (TID 16). 693 bytes result sent to driver
24/01/05 02:13:53 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 15.0 (TID 16) in 35 ms on localhost (executor driver) (1/1)
24/01/05 02:13:53 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 15.0, whose tasks have all completed, from pool 
24/01/05 02:13:53 INFO scheduler.DAGScheduler: ShuffleMapStage 15 (mapToPair at HoodieJavaRDD.java:149) finished in 0.081 s
24/01/05 02:13:53 INFO scheduler.DAGScheduler: looking for newly runnable stages
24/01/05 02:13:53 INFO scheduler.DAGScheduler: running: Set()
24/01/05 02:13:53 INFO scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 16, ResultStage 17)
24/01/05 02:13:53 INFO scheduler.DAGScheduler: failed: Set()
24/01/05 02:13:53 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 16 (MapPartitionsRDD[46] at countByKey at HoodieJavaPairRDD.java:105), which has no missing parents
24/01/05 02:13:53 INFO memory.MemoryStore: Block broadcast_14 stored as values in memory (estimated size 7.7 KB, free 365.2 MB)
24/01/05 02:13:53 INFO memory.MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 4.0 KB, free 365.2 MB)
24/01/05 02:13:53 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on adhoc-1:33719 (size: 4.0 KB, free: 366.0 MB)
24/01/05 02:13:53 INFO spark.SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1161
24/01/05 02:13:53 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 16 (MapPartitionsRDD[46] at countByKey at HoodieJavaPairRDD.java:105) (first 15 tasks are for partitions Vector(0, 1))
24/01/05 02:13:53 INFO scheduler.TaskSchedulerImpl: Adding task set 16.0 with 2 tasks
24/01/05 02:13:53 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 16.0 (TID 17, localhost, executor driver, partition 0, PROCESS_LOCAL, 7714 bytes)
24/01/05 02:13:53 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 16.0 (TID 18, localhost, executor driver, partition 1, PROCESS_LOCAL, 7714 bytes)
24/01/05 02:13:53 INFO executor.Executor: Running task 0.0 in stage 16.0 (TID 17)
24/01/05 02:13:53 INFO executor.Executor: Running task 1.0 in stage 16.0 (TID 18)
24/01/05 02:13:53 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
24/01/05 02:13:53 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
24/01/05 02:13:53 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
24/01/05 02:13:53 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
24/01/05 02:13:53 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
24/01/05 02:13:53 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
24/01/05 02:13:53 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
24/01/05 02:13:53 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
24/01/05 02:13:53 INFO memory.MemoryStore: Block rdd_44_0 stored as values in memory (estimated size 6.7 KB, free 365.2 MB)
24/01/05 02:13:53 INFO storage.BlockManagerInfo: Added rdd_44_0 in memory on adhoc-1:33719 (size: 6.7 KB, free: 366.0 MB)
24/01/05 02:13:53 INFO memory.MemoryStore: Block rdd_44_1 stored as values in memory (estimated size 11.1 KB, free 365.2 MB)
24/01/05 02:13:53 INFO storage.BlockManagerInfo: Added rdd_44_1 in memory on adhoc-1:33719 (size: 11.1 KB, free: 366.0 MB)
24/01/05 02:13:53 INFO executor.Executor: Finished task 0.0 in stage 16.0 (TID 17). 1252 bytes result sent to driver
24/01/05 02:13:53 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 16.0 (TID 17) in 49 ms on localhost (executor driver) (1/2)
24/01/05 02:13:53 INFO executor.Executor: Finished task 1.0 in stage 16.0 (TID 18). 1252 bytes result sent to driver
24/01/05 02:13:53 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 16.0 (TID 18) in 55 ms on localhost (executor driver) (2/2)
24/01/05 02:13:53 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 16.0, whose tasks have all completed, from pool 
24/01/05 02:13:53 INFO scheduler.DAGScheduler: ShuffleMapStage 16 (countByKey at HoodieJavaPairRDD.java:105) finished in 0.067 s
24/01/05 02:13:53 INFO scheduler.DAGScheduler: looking for newly runnable stages
24/01/05 02:13:53 INFO scheduler.DAGScheduler: running: Set()
24/01/05 02:13:53 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 17)
24/01/05 02:13:53 INFO scheduler.DAGScheduler: failed: Set()
24/01/05 02:13:53 INFO scheduler.DAGScheduler: Submitting ResultStage 17 (ShuffledRDD[47] at countByKey at HoodieJavaPairRDD.java:105), which has no missing parents
24/01/05 02:13:53 INFO memory.MemoryStore: Block broadcast_15 stored as values in memory (estimated size 3.6 KB, free 365.2 MB)
24/01/05 02:13:53 INFO memory.MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 2.1 KB, free 365.1 MB)
24/01/05 02:13:53 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on adhoc-1:33719 (size: 2.1 KB, free: 366.0 MB)
24/01/05 02:13:53 INFO spark.SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1161
24/01/05 02:13:53 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ResultStage 17 (ShuffledRDD[47] at countByKey at HoodieJavaPairRDD.java:105) (first 15 tasks are for partitions Vector(0, 1))
24/01/05 02:13:53 INFO scheduler.TaskSchedulerImpl: Adding task set 17.0 with 2 tasks
24/01/05 02:13:53 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 17.0 (TID 19, localhost, executor driver, partition 0, PROCESS_LOCAL, 7662 bytes)
24/01/05 02:13:53 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 17.0 (TID 20, localhost, executor driver, partition 1, ANY, 7662 bytes)
24/01/05 02:13:53 INFO executor.Executor: Running task 0.0 in stage 17.0 (TID 19)
24/01/05 02:13:53 INFO executor.Executor: Running task 1.0 in stage 17.0 (TID 20)
24/01/05 02:13:53 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
24/01/05 02:13:53 INFO storage.ShuffleBlockFetcherIterator: Getting 2 non-empty blocks including 2 local blocks and 0 remote blocks
24/01/05 02:13:53 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
24/01/05 02:13:53 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
24/01/05 02:13:53 INFO executor.Executor: Finished task 0.0 in stage 17.0 (TID 19). 1098 bytes result sent to driver
24/01/05 02:13:53 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 17.0 (TID 19) in 11 ms on localhost (executor driver) (1/2)
24/01/05 02:13:53 INFO executor.Executor: Finished task 1.0 in stage 17.0 (TID 20). 1155 bytes result sent to driver
24/01/05 02:13:53 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 17.0 (TID 20) in 14 ms on localhost (executor driver) (2/2)
24/01/05 02:13:53 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 17.0, whose tasks have all completed, from pool 
24/01/05 02:13:53 INFO scheduler.DAGScheduler: ResultStage 17 (countByKey at HoodieJavaPairRDD.java:105) finished in 0.020 s
24/01/05 02:13:53 INFO scheduler.DAGScheduler: Job 9 finished: countByKey at HoodieJavaPairRDD.java:105, took 0.185490 s
24/01/05 02:13:53 INFO commit.UpsertPartitioner: AvgRecordSize => 1024
24/01/05 02:13:53 INFO spark.SparkContext: Starting job: collectAsMap at UpsertPartitioner.java:282
24/01/05 02:13:53 INFO scheduler.DAGScheduler: Got job 10 (collectAsMap at UpsertPartitioner.java:282) with 1 output partitions
24/01/05 02:13:53 INFO scheduler.DAGScheduler: Final stage: ResultStage 18 (collectAsMap at UpsertPartitioner.java:282)
24/01/05 02:13:53 INFO scheduler.DAGScheduler: Parents of final stage: List()
24/01/05 02:13:53 INFO scheduler.DAGScheduler: Missing parents: List()
24/01/05 02:13:53 INFO scheduler.DAGScheduler: Submitting ResultStage 18 (MapPartitionsRDD[49] at mapToPair at UpsertPartitioner.java:281), which has no missing parents
24/01/05 02:13:53 INFO memory.MemoryStore: Block broadcast_16 stored as values in memory (estimated size 339.7 KB, free 364.8 MB)
24/01/05 02:13:53 INFO memory.MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 121.4 KB, free 364.7 MB)
24/01/05 02:13:53 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on adhoc-1:33719 (size: 121.4 KB, free: 365.9 MB)
24/01/05 02:13:53 INFO spark.SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1161
24/01/05 02:13:53 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 18 (MapPartitionsRDD[49] at mapToPair at UpsertPartitioner.java:281) (first 15 tasks are for partitions Vector(0))
24/01/05 02:13:53 INFO scheduler.TaskSchedulerImpl: Adding task set 18.0 with 1 tasks
24/01/05 02:13:53 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 18.0 (TID 21, localhost, executor driver, partition 0, PROCESS_LOCAL, 7735 bytes)
24/01/05 02:13:53 INFO executor.Executor: Running task 0.0 in stage 18.0 (TID 21)
24/01/05 02:13:53 INFO executor.Executor: Finished task 0.0 in stage 18.0 (TID 21). 716 bytes result sent to driver
24/01/05 02:13:53 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 18.0 (TID 21) in 28 ms on localhost (executor driver) (1/1)
24/01/05 02:13:53 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 18.0, whose tasks have all completed, from pool 
24/01/05 02:13:53 INFO scheduler.DAGScheduler: ResultStage 18 (collectAsMap at UpsertPartitioner.java:282) finished in 0.057 s
24/01/05 02:13:53 INFO scheduler.DAGScheduler: Job 10 finished: collectAsMap at UpsertPartitioner.java:282, took 0.058732 s
24/01/05 02:13:53 INFO view.AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
24/01/05 02:13:53 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:13:53 INFO commit.UpsertPartitioner: Total Buckets: 1
24/01/05 02:13:53 INFO timeline.HoodieActiveTimeline: Checking for file exists ?/user/hive/warehouse/stock_ticks_mor/.hoodie/20240105021347452.deltacommit.requested
24/01/05 02:13:53 INFO timeline.HoodieActiveTimeline: Create new file for toInstant ?/user/hive/warehouse/stock_ticks_mor/.hoodie/20240105021347452.deltacommit.inflight
24/01/05 02:13:53 INFO commit.BaseSparkCommitActionExecutor: no validators configured.
24/01/05 02:13:53 INFO commit.BaseCommitActionExecutor: Auto commit disabled for 20240105021347452
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 369
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 315
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 337
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 181
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 334
24/01/05 02:13:53 INFO storage.BlockManagerInfo: Removed broadcast_10_piece0 on adhoc-1:33719 in memory (size: 2.4 KB, free: 365.9 MB)
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 321
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 228
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 345
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 360
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 303
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 210
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 410
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 176
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 387
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 412
24/01/05 02:13:53 INFO storage.BlockManagerInfo: Removed broadcast_11_piece0 on adhoc-1:33719 in memory (size: 121.2 KB, free: 366.0 MB)
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 422
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 368
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 400
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 344
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 411
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 222
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 402
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 188
24/01/05 02:13:53 INFO storage.BlockManagerInfo: Removed broadcast_9_piece0 on adhoc-1:33719 in memory (size: 11.5 KB, free: 366.0 MB)
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 336
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 362
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 371
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 289
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 234
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 239
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 190
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 288
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 330
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 409
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 322
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 324
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 279
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 280
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 183
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 379
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 314
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 406
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 354
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 367
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 177
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 355
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 286
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 318
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 325
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 180
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 203
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 272
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 423
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 373
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 349
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 374
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 235
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 232
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 277
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 346
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 241
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned shuffle 5
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned shuffle 1
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 384
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 407
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 401
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 341
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 364
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 424
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 237
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 421
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 191
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 310
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 312
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 261
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 297
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 274
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 252
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 416
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 316
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 248
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 221
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 226
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 233
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 319
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 244
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 419
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 195
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 265
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 388
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 292
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 311
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 238
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 269
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 418
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 182
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 307
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 175
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 212
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 393
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned shuffle 0
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 389
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 372
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 201
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 218
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 242
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 262
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 216
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 395
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 333
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 208
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 258
24/01/05 02:13:53 INFO storage.BlockManagerInfo: Removed broadcast_14_piece0 on adhoc-1:33719 in memory (size: 4.0 KB, free: 366.0 MB)
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 376
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 391
24/01/05 02:13:53 INFO storage.BlockManagerInfo: Removed broadcast_7_piece0 on adhoc-1:33719 in memory (size: 26.8 KB, free: 366.0 MB)
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 370
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 178
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 396
24/01/05 02:13:53 INFO storage.BlockManagerInfo: Removed broadcast_16_piece0 on adhoc-1:33719 in memory (size: 121.4 KB, free: 366.1 MB)
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 404
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 317
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 215
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 294
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 383
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 189
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 283
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 193
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 380
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 382
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 342
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 179
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 284
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 405
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 329
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 264
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 240
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 250
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 357
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 282
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 350
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 251
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 299
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 397
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 209
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 267
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 268
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 184
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 313
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 253
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 399
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 186
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 198
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 323
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 328
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 257
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 266
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 270
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 217
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 206
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 229
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 390
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 263
24/01/05 02:13:53 INFO storage.BlockManagerInfo: Removed broadcast_12_piece0 on adhoc-1:33719 in memory (size: 11.4 KB, free: 366.1 MB)
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 331
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 420
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 225
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 219
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 291
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 202
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 296
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 339
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 194
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 408
24/01/05 02:13:53 INFO storage.BlockManager: Removing RDD 19
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned RDD 19
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 200
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 305
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 351
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 254
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 309
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 385
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 343
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 295
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 340
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 326
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 361
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 211
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 213
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 273
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 259
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 365
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 245
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 247
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 205
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 403
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 413
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 230
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 327
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 236
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 275
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 417
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 185
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 207
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 276
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 192
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 227
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 398
24/01/05 02:13:53 INFO storage.BlockManagerInfo: Removed broadcast_13_piece0 on adhoc-1:33719 in memory (size: 122.2 KB, free: 366.3 MB)
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 281
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 363
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 358
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 214
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 381
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 366
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 301
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 224
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 308
24/01/05 02:13:53 INFO storage.BlockManagerInfo: Removed broadcast_15_piece0 on adhoc-1:33719 in memory (size: 2.1 KB, free: 366.3 MB)
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 196
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 285
24/01/05 02:13:53 INFO spark.SparkContext: Starting job: sum at StreamSync.java:783
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 335
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 231
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 293
24/01/05 02:13:53 INFO scheduler.DAGScheduler: Registering RDD 50 (mapToPair at HoodieJavaRDD.java:149)
24/01/05 02:13:53 INFO storage.BlockManagerInfo: Removed broadcast_8_piece0 on adhoc-1:33719 in memory (size: 13.9 KB, free: 366.3 MB)
24/01/05 02:13:53 INFO scheduler.DAGScheduler: Got job 11 (sum at StreamSync.java:783) with 1 output partitions
24/01/05 02:13:53 INFO scheduler.DAGScheduler: Final stage: ResultStage 23 (sum at StreamSync.java:783)
24/01/05 02:13:53 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 22)
24/01/05 02:13:53 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 22)
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 414
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 243
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 320
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 377
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 394
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 246
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 356
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 306
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 415
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 375
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 260
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 290
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 348
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 352
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 347
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 220
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 255
24/01/05 02:13:53 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 22 (MapPartitionsRDD[50] at mapToPair at HoodieJavaRDD.java:149), which has no missing parents
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 300
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 223
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 302
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 332
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 256
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 338
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 199
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 298
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 392
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 204
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 197
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 187
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 359
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 378
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 353
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 386
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 287
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 249
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 271
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 304
24/01/05 02:13:53 INFO spark.ContextCleaner: Cleaned accumulator 278
24/01/05 02:13:53 INFO memory.MemoryStore: Block broadcast_17 stored as values in memory (estimated size 344.3 KB, free 365.9 MB)
24/01/05 02:13:53 INFO memory.MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 123.0 KB, free 365.8 MB)
24/01/05 02:13:53 INFO storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on adhoc-1:33719 (size: 123.0 KB, free: 366.2 MB)
24/01/05 02:13:53 INFO spark.SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1161
24/01/05 02:13:53 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 22 (MapPartitionsRDD[50] at mapToPair at HoodieJavaRDD.java:149) (first 15 tasks are for partitions Vector(0, 1))
24/01/05 02:13:53 INFO scheduler.TaskSchedulerImpl: Adding task set 22.0 with 2 tasks
24/01/05 02:13:53 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 22.0 (TID 22, localhost, executor driver, partition 0, PROCESS_LOCAL, 7714 bytes)
24/01/05 02:13:53 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 22.0 (TID 23, localhost, executor driver, partition 1, PROCESS_LOCAL, 7714 bytes)
24/01/05 02:13:53 INFO executor.Executor: Running task 0.0 in stage 22.0 (TID 22)
24/01/05 02:13:53 INFO executor.Executor: Running task 1.0 in stage 22.0 (TID 23)
24/01/05 02:13:53 INFO storage.BlockManager: Found block rdd_44_0 locally
24/01/05 02:13:53 INFO storage.BlockManager: Found block rdd_44_1 locally
24/01/05 02:13:53 INFO executor.Executor: Finished task 1.0 in stage 22.0 (TID 23). 907 bytes result sent to driver
24/01/05 02:13:53 INFO executor.Executor: Finished task 0.0 in stage 22.0 (TID 22). 907 bytes result sent to driver
24/01/05 02:13:53 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 22.0 (TID 23) in 35 ms on localhost (executor driver) (1/2)
24/01/05 02:13:53 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 22.0 (TID 22) in 36 ms on localhost (executor driver) (2/2)
24/01/05 02:13:53 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 22.0, whose tasks have all completed, from pool 
24/01/05 02:13:53 INFO scheduler.DAGScheduler: ShuffleMapStage 22 (mapToPair at HoodieJavaRDD.java:149) finished in 0.069 s
24/01/05 02:13:53 INFO scheduler.DAGScheduler: looking for newly runnable stages
24/01/05 02:13:53 INFO scheduler.DAGScheduler: running: Set()
24/01/05 02:13:53 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 23)
24/01/05 02:13:53 INFO scheduler.DAGScheduler: failed: Set()
24/01/05 02:13:53 INFO scheduler.DAGScheduler: Submitting ResultStage 23 (MapPartitionsRDD[55] at mapToDouble at StreamSync.java:783), which has no missing parents
24/01/05 02:13:53 INFO memory.MemoryStore: Block broadcast_18 stored as values in memory (estimated size 351.4 KB, free 365.5 MB)
24/01/05 02:13:53 INFO memory.MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 127.2 KB, free 365.4 MB)
24/01/05 02:13:53 INFO storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on adhoc-1:33719 (size: 127.2 KB, free: 366.0 MB)
24/01/05 02:13:53 INFO spark.SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1161
24/01/05 02:13:53 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 23 (MapPartitionsRDD[55] at mapToDouble at StreamSync.java:783) (first 15 tasks are for partitions Vector(0))
24/01/05 02:13:53 INFO scheduler.TaskSchedulerImpl: Adding task set 23.0 with 1 tasks
24/01/05 02:13:53 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 23.0 (TID 24, localhost, executor driver, partition 0, ANY, 7662 bytes)
24/01/05 02:13:53 INFO executor.Executor: Running task 0.0 in stage 23.0 (TID 24)
24/01/05 02:13:53 INFO storage.ShuffleBlockFetcherIterator: Getting 2 non-empty blocks including 2 local blocks and 0 remote blocks
24/01/05 02:13:53 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
24/01/05 02:13:53 INFO queue.SimpleExecutor: Starting consumer, consuming records from the records iterator directly
24/01/05 02:13:53 WARN marker.WriteMarkersFactory: Timeline-server-based markers are not supported for HDFS: base path /user/hive/warehouse/stock_ticks_mor.  Falling back to direct markers.
24/01/05 02:13:53 INFO marker.DirectWriteMarkers: Creating Marker Path=/user/hive/warehouse/stock_ticks_mor/.hoodie/.temp/20240105021347452/2018/08/31/0245deea-bfb7-4901-bd55-849b7d6fa366-0_0-23-24_20240105021347452.parquet.marker.CREATE
24/01/05 02:13:53 INFO marker.DirectWriteMarkers: [direct] Created marker file /user/hive/warehouse/stock_ticks_mor/.hoodie/.temp/20240105021347452/2018/08/31/0245deea-bfb7-4901-bd55-849b7d6fa366-0_0-23-24_20240105021347452.parquet.marker.CREATE in 6 ms
24/01/05 02:13:53 INFO compress.CodecPool: Got brand-new compressor [.gz]
24/01/05 02:13:54 INFO io.HoodieCreateHandle: New CreateHandle for partition :2018/08/31 with fileId 0245deea-bfb7-4901-bd55-849b7d6fa366-0
24/01/05 02:13:54 INFO io.HoodieCreateHandle: Closing the file 0245deea-bfb7-4901-bd55-849b7d6fa366-0 as we are done with all the records 115
24/01/05 02:13:54 INFO hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 21557
24/01/05 02:13:54 INFO io.HoodieCreateHandle: CreateHandle for partitionPath 2018/08/31 fileID 0245deea-bfb7-4901-bd55-849b7d6fa366-0, took 532 ms.
24/01/05 02:13:54 INFO memory.MemoryStore: Block rdd_54_0 stored as values in memory (estimated size 376.0 B, free 365.4 MB)
24/01/05 02:13:54 INFO storage.BlockManagerInfo: Added rdd_54_0 in memory on adhoc-1:33719 (size: 376.0 B, free: 366.0 MB)
24/01/05 02:13:54 INFO executor.Executor: Finished task 0.0 in stage 23.0 (TID 24). 974 bytes result sent to driver
24/01/05 02:13:54 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 23.0 (TID 24) in 563 ms on localhost (executor driver) (1/1)
24/01/05 02:13:54 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 23.0, whose tasks have all completed, from pool 
24/01/05 02:13:54 INFO scheduler.DAGScheduler: ResultStage 23 (sum at StreamSync.java:783) finished in 0.598 s
24/01/05 02:13:54 INFO scheduler.DAGScheduler: Job 11 finished: sum at StreamSync.java:783, took 0.675977 s
24/01/05 02:13:54 INFO spark.SparkContext: Starting job: sum at StreamSync.java:784
24/01/05 02:13:54 INFO scheduler.DAGScheduler: Got job 12 (sum at StreamSync.java:784) with 1 output partitions
24/01/05 02:13:54 INFO scheduler.DAGScheduler: Final stage: ResultStage 28 (sum at StreamSync.java:784)
24/01/05 02:13:54 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 27)
24/01/05 02:13:54 INFO scheduler.DAGScheduler: Missing parents: List()
24/01/05 02:13:54 INFO scheduler.DAGScheduler: Submitting ResultStage 28 (MapPartitionsRDD[57] at mapToDouble at StreamSync.java:784), which has no missing parents
24/01/05 02:13:54 INFO memory.MemoryStore: Block broadcast_19 stored as values in memory (estimated size 351.3 KB, free 365.0 MB)
24/01/05 02:13:54 INFO memory.MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 127.2 KB, free 364.9 MB)
24/01/05 02:13:54 INFO storage.BlockManagerInfo: Added broadcast_19_piece0 in memory on adhoc-1:33719 (size: 127.2 KB, free: 365.9 MB)
24/01/05 02:13:54 INFO spark.SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1161
24/01/05 02:13:54 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 28 (MapPartitionsRDD[57] at mapToDouble at StreamSync.java:784) (first 15 tasks are for partitions Vector(0))
24/01/05 02:13:54 INFO scheduler.TaskSchedulerImpl: Adding task set 28.0 with 1 tasks
24/01/05 02:13:54 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 28.0 (TID 25, localhost, executor driver, partition 0, PROCESS_LOCAL, 7662 bytes)
24/01/05 02:13:54 INFO executor.Executor: Running task 0.0 in stage 28.0 (TID 25)
24/01/05 02:13:54 INFO storage.BlockManager: Found block rdd_54_0 locally
24/01/05 02:13:54 INFO executor.Executor: Finished task 0.0 in stage 28.0 (TID 25). 759 bytes result sent to driver
24/01/05 02:13:54 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 28.0 (TID 25) in 28 ms on localhost (executor driver) (1/1)
24/01/05 02:13:54 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 28.0, whose tasks have all completed, from pool 
24/01/05 02:13:54 INFO scheduler.DAGScheduler: ResultStage 28 (sum at StreamSync.java:784) finished in 0.069 s
24/01/05 02:13:54 INFO scheduler.DAGScheduler: Job 12 finished: sum at StreamSync.java:784, took 0.072940 s
24/01/05 02:13:54 INFO streamer.StreamSync: instantTime=20240105021347452, totalRecords=115, totalErrorRecords=0, totalSuccessfulRecords=115
24/01/05 02:13:54 INFO spark.SparkContext: Starting job: collect at SparkRDDWriteClient.java:103
24/01/05 02:13:54 INFO scheduler.DAGScheduler: Got job 13 (collect at SparkRDDWriteClient.java:103) with 1 output partitions
24/01/05 02:13:54 INFO scheduler.DAGScheduler: Final stage: ResultStage 33 (collect at SparkRDDWriteClient.java:103)
24/01/05 02:13:54 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 32)
24/01/05 02:13:54 INFO scheduler.DAGScheduler: Missing parents: List()
24/01/05 02:13:54 INFO scheduler.DAGScheduler: Submitting ResultStage 33 (MapPartitionsRDD[59] at map at SparkRDDWriteClient.java:103), which has no missing parents
24/01/05 02:13:54 INFO memory.MemoryStore: Block broadcast_20 stored as values in memory (estimated size 351.6 KB, free 364.5 MB)
24/01/05 02:13:54 INFO memory.MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 127.3 KB, free 364.4 MB)
24/01/05 02:13:54 INFO storage.BlockManagerInfo: Added broadcast_20_piece0 in memory on adhoc-1:33719 (size: 127.3 KB, free: 365.8 MB)
24/01/05 02:13:54 INFO spark.SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:1161
24/01/05 02:13:54 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 33 (MapPartitionsRDD[59] at map at SparkRDDWriteClient.java:103) (first 15 tasks are for partitions Vector(0))
24/01/05 02:13:54 INFO scheduler.TaskSchedulerImpl: Adding task set 33.0 with 1 tasks
24/01/05 02:13:54 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 33.0 (TID 26, localhost, executor driver, partition 0, PROCESS_LOCAL, 7662 bytes)
24/01/05 02:13:54 INFO executor.Executor: Running task 0.0 in stage 33.0 (TID 26)
24/01/05 02:13:54 INFO storage.BlockManager: Found block rdd_54_0 locally
24/01/05 02:13:54 INFO executor.Executor: Finished task 0.0 in stage 33.0 (TID 26). 1033 bytes result sent to driver
24/01/05 02:13:54 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 33.0 (TID 26) in 17 ms on localhost (executor driver) (1/1)
24/01/05 02:13:54 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 33.0, whose tasks have all completed, from pool 
24/01/05 02:13:54 INFO scheduler.DAGScheduler: ResultStage 33 (collect at SparkRDDWriteClient.java:103) finished in 0.044 s
24/01/05 02:13:54 INFO scheduler.DAGScheduler: Job 13 finished: collect at SparkRDDWriteClient.java:103, took 0.045905 s
24/01/05 02:13:54 INFO client.BaseHoodieWriteClient: Committing 20240105021347452 action deltacommit
24/01/05 02:13:54 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:13:54 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:13:54 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:13:54 INFO table.HoodieTableMetaClient: Loading Active commit timeline for /user/hive/warehouse/stock_ticks_mor
24/01/05 02:13:54 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[==>20240105021347452__deltacommit__INFLIGHT__20240105021353468]}
24/01/05 02:13:54 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:13:54 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:13:54 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:13:54 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:13:54 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:13:54 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:13:54 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20240105021351577]}
24/01/05 02:13:54 INFO view.AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
24/01/05 02:13:54 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:13:54 INFO view.FileSystemViewManager: Creating View Manager with storage type :REMOTE_FIRST
24/01/05 02:13:54 INFO view.FileSystemViewManager: Creating remote first table view
24/01/05 02:13:54 INFO util.CommitUtils: Creating  metadata for UPSERT numWriteStats:1 numReplaceFileIds:0
24/01/05 02:13:54 INFO transaction.TransactionManager: Transaction starting for Option{val=[==>20240105021347452__deltacommit__INFLIGHT]} with latest completed transaction instant Optional.empty
24/01/05 02:13:54 INFO lock.LockManager: LockProvider org.apache.hudi.client.transaction.lock.InProcessLockProvider
24/01/05 02:13:54 INFO lock.InProcessLockProvider: Base Path /user/hive/warehouse/stock_ticks_mor, Lock Instance java.util.concurrent.locks.ReentrantReadWriteLock@592a1882[Write locks = 0, Read locks = 0], Thread main, In-process lock state ACQUIRING
24/01/05 02:13:54 INFO lock.InProcessLockProvider: Base Path /user/hive/warehouse/stock_ticks_mor, Lock Instance java.util.concurrent.locks.ReentrantReadWriteLock@592a1882[Write locks = 1, Read locks = 0], Thread main, In-process lock state ACQUIRED
24/01/05 02:13:54 INFO transaction.TransactionManager: Transaction started for Option{val=[==>20240105021347452__deltacommit__INFLIGHT]} with latest completed transaction instant Optional.empty
24/01/05 02:13:54 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:13:54 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:13:54 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:13:54 INFO table.HoodieTableMetaClient: Loading Active commit timeline for /user/hive/warehouse/stock_ticks_mor
24/01/05 02:13:54 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[==>20240105021347452__deltacommit__INFLIGHT__20240105021353468]}
24/01/05 02:13:54 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:13:54 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:13:54 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:13:54 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:13:54 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:13:54 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:13:54 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20240105021351577]}
24/01/05 02:13:54 INFO view.AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
24/01/05 02:13:54 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:13:54 INFO view.FileSystemViewManager: Creating View Manager with storage type :REMOTE_FIRST
24/01/05 02:13:54 INFO view.FileSystemViewManager: Creating remote first table view
24/01/05 02:13:54 INFO client.BaseHoodieWriteClient: Committing 20240105021347452 action deltacommit
24/01/05 02:13:54 WARN marker.WriteMarkersFactory: Timeline-server-based markers are not supported for HDFS: base path /user/hive/warehouse/stock_ticks_mor.  Falling back to direct markers.
24/01/05 02:13:54 INFO spark.SparkContext: Starting job: collect at HoodieSparkEngineContext.java:150
24/01/05 02:13:54 INFO scheduler.DAGScheduler: Got job 14 (collect at HoodieSparkEngineContext.java:150) with 1 output partitions
24/01/05 02:13:54 INFO scheduler.DAGScheduler: Final stage: ResultStage 34 (collect at HoodieSparkEngineContext.java:150)
24/01/05 02:13:54 INFO scheduler.DAGScheduler: Parents of final stage: List()
24/01/05 02:13:54 INFO scheduler.DAGScheduler: Missing parents: List()
24/01/05 02:13:54 INFO scheduler.DAGScheduler: Submitting ResultStage 34 (MapPartitionsRDD[61] at flatMap at HoodieSparkEngineContext.java:150), which has no missing parents
24/01/05 02:13:54 INFO memory.MemoryStore: Block broadcast_21 stored as values in memory (estimated size 72.5 KB, free 364.4 MB)
24/01/05 02:13:54 INFO memory.MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 26.7 KB, free 364.3 MB)
24/01/05 02:13:54 INFO storage.BlockManagerInfo: Added broadcast_21_piece0 in memory on adhoc-1:33719 (size: 26.7 KB, free: 365.8 MB)
24/01/05 02:13:54 INFO spark.SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:1161
24/01/05 02:13:54 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 34 (MapPartitionsRDD[61] at flatMap at HoodieSparkEngineContext.java:150) (first 15 tasks are for partitions Vector(0))
24/01/05 02:13:54 INFO scheduler.TaskSchedulerImpl: Adding task set 34.0 with 1 tasks
24/01/05 02:13:54 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 34.0 (TID 27, localhost, executor driver, partition 0, PROCESS_LOCAL, 7820 bytes)
24/01/05 02:13:54 INFO executor.Executor: Running task 0.0 in stage 34.0 (TID 27)
24/01/05 02:13:54 INFO executor.Executor: Finished task 0.0 in stage 34.0 (TID 27). 755 bytes result sent to driver
24/01/05 02:13:54 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 34.0 (TID 27) in 12 ms on localhost (executor driver) (1/1)
24/01/05 02:13:54 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 34.0, whose tasks have all completed, from pool 
24/01/05 02:13:54 INFO scheduler.DAGScheduler: ResultStage 34 (collect at HoodieSparkEngineContext.java:150) finished in 0.023 s
24/01/05 02:13:54 INFO scheduler.DAGScheduler: Job 14 finished: collect at HoodieSparkEngineContext.java:150, took 0.024583 s
24/01/05 02:13:54 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:13:54 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:13:54 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:13:54 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:13:54 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:13:54 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:13:54 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20240105021351577]}
24/01/05 02:13:54 INFO metadata.HoodieBackedTableMetadataWriter: Async metadata indexing disabled and following partitions already initialized: [files]
24/01/05 02:13:54 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:13:54 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:13:54 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:13:54 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:13:54 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:13:54 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:13:54 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20240105021351577]}
24/01/05 02:13:54 INFO view.AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
24/01/05 02:13:54 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:13:54 INFO metadata.HoodieTableMetadataUtil: Updating at 20240105021347452 from Commit/UPSERT. #partitions_updated=2, #files_added=1
24/01/05 02:13:54 INFO view.AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
24/01/05 02:13:54 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:13:54 INFO metadata.HoodieTableMetadataUtil: Loading latest file slices for metadata table partition files
24/01/05 02:13:54 INFO view.AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
24/01/05 02:13:54 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:13:54 INFO view.AbstractTableFileSystemView: Building file system view for partition (files)
24/01/05 02:13:54 INFO client.BaseHoodieClient: Embedded Timeline Server is disabled. Not starting timeline service
24/01/05 02:13:54 INFO client.BaseHoodieClient: Embedded Timeline Server is disabled. Not starting timeline service
24/01/05 02:13:54 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:13:54 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:13:54 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:13:54 INFO table.HoodieTableMetaClient: Loading Active commit timeline for /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:13:54 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20240105021351577]}
24/01/05 02:13:54 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:13:54 INFO view.FileSystemViewManager: Creating View Manager with storage type :MEMORY
24/01/05 02:13:54 INFO view.FileSystemViewManager: Creating in-memory based Table View
24/01/05 02:13:54 INFO metadata.HoodieBackedTableMetadataWriter: New commit at 20240105021347452 being applied to MDT.
24/01/05 02:13:54 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:13:54 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:13:54 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:13:54 INFO table.HoodieTableMetaClient: Loading Active commit timeline for /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:13:54 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20240105021351577]}
24/01/05 02:13:54 INFO util.CleanerUtils: Cleaned failed attempts if any
24/01/05 02:13:54 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:13:54 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:13:54 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:13:54 INFO table.HoodieTableMetaClient: Loading Active commit timeline for /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:13:54 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20240105021351577]}
24/01/05 02:13:54 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:13:54 INFO view.FileSystemViewManager: Creating View Manager with storage type :MEMORY
24/01/05 02:13:54 INFO view.FileSystemViewManager: Creating in-memory based Table View
24/01/05 02:13:54 INFO client.BaseHoodieWriteClient: Generate a new instant time: 20240105021347452 action: deltacommit
24/01/05 02:13:54 INFO timeline.HoodieActiveTimeline: Creating a new instant [==>20240105021347452__deltacommit__REQUESTED]
24/01/05 02:13:54 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:13:54 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:13:54 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:13:54 INFO table.HoodieTableMetaClient: Loading Active commit timeline for /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:13:54 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[==>20240105021347452__deltacommit__REQUESTED__20240105021354694]}
24/01/05 02:13:54 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:13:54 INFO view.FileSystemViewManager: Creating View Manager with storage type :MEMORY
24/01/05 02:13:54 INFO view.FileSystemViewManager: Creating in-memory based Table View
24/01/05 02:13:54 INFO async.AsyncCleanerService: The HoodieWriteClient is not configured to auto & async clean. Async clean service will not start.
24/01/05 02:13:54 INFO async.AsyncArchiveService: The HoodieWriteClient is not configured to auto & async archive. Async archive service will not start.
24/01/05 02:13:54 INFO view.AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
24/01/05 02:13:54 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:13:54 INFO spark.SparkContext: Starting job: countByKey at HoodieJavaPairRDD.java:105
24/01/05 02:13:54 INFO scheduler.DAGScheduler: Registering RDD 70 (countByKey at HoodieJavaPairRDD.java:105)
24/01/05 02:13:54 INFO scheduler.DAGScheduler: Got job 15 (countByKey at HoodieJavaPairRDD.java:105) with 1 output partitions
24/01/05 02:13:54 INFO scheduler.DAGScheduler: Final stage: ResultStage 36 (countByKey at HoodieJavaPairRDD.java:105)
24/01/05 02:13:54 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 35)
24/01/05 02:13:54 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 35)
24/01/05 02:13:54 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 35 (MapPartitionsRDD[70] at countByKey at HoodieJavaPairRDD.java:105), which has no missing parents
24/01/05 02:13:54 INFO memory.MemoryStore: Block broadcast_22 stored as values in memory (estimated size 8.6 KB, free 364.3 MB)
24/01/05 02:13:54 INFO memory.MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 4.7 KB, free 364.3 MB)
24/01/05 02:13:54 INFO storage.BlockManagerInfo: Added broadcast_22_piece0 in memory on adhoc-1:33719 (size: 4.7 KB, free: 365.8 MB)
24/01/05 02:13:54 INFO spark.SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:1161
24/01/05 02:13:54 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 35 (MapPartitionsRDD[70] at countByKey at HoodieJavaPairRDD.java:105) (first 15 tasks are for partitions Vector(0))
24/01/05 02:13:54 INFO scheduler.TaskSchedulerImpl: Adding task set 35.0 with 1 tasks
24/01/05 02:13:54 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 35.0 (TID 28, localhost, executor driver, partition 0, PROCESS_LOCAL, 8086 bytes)
24/01/05 02:13:54 INFO executor.Executor: Running task 0.0 in stage 35.0 (TID 28)
24/01/05 02:13:54 INFO spark.ContextCleaner: Cleaned accumulator 513
24/01/05 02:13:54 INFO spark.ContextCleaner: Cleaned accumulator 495
24/01/05 02:13:54 INFO spark.ContextCleaner: Cleaned accumulator 546
24/01/05 02:13:54 INFO spark.ContextCleaner: Cleaned accumulator 438
24/01/05 02:13:54 INFO spark.ContextCleaner: Cleaned accumulator 444
24/01/05 02:13:54 INFO spark.ContextCleaner: Cleaned accumulator 436
24/01/05 02:13:54 INFO spark.ContextCleaner: Cleaned accumulator 540
24/01/05 02:13:54 INFO spark.ContextCleaner: Cleaned accumulator 548
24/01/05 02:13:54 INFO spark.ContextCleaner: Cleaned accumulator 485
24/01/05 02:13:54 INFO spark.ContextCleaner: Cleaned accumulator 519
24/01/05 02:13:54 INFO spark.ContextCleaner: Cleaned accumulator 467
24/01/05 02:13:54 INFO spark.ContextCleaner: Cleaned accumulator 514
24/01/05 02:13:54 INFO spark.ContextCleaner: Cleaned accumulator 503
24/01/05 02:13:54 INFO spark.ContextCleaner: Cleaned accumulator 520
24/01/05 02:13:54 INFO spark.ContextCleaner: Cleaned accumulator 543
24/01/05 02:13:54 INFO spark.ContextCleaner: Cleaned accumulator 464
24/01/05 02:13:54 INFO spark.ContextCleaner: Cleaned accumulator 480
24/01/05 02:13:54 INFO spark.ContextCleaner: Cleaned accumulator 475
24/01/05 02:13:54 INFO spark.ContextCleaner: Cleaned accumulator 443
24/01/05 02:13:54 INFO spark.ContextCleaner: Cleaned accumulator 456
24/01/05 02:13:54 INFO spark.ContextCleaner: Cleaned accumulator 498
24/01/05 02:13:54 INFO spark.ContextCleaner: Cleaned accumulator 530
24/01/05 02:13:54 INFO spark.ContextCleaner: Cleaned accumulator 434
24/01/05 02:13:54 INFO spark.ContextCleaner: Cleaned accumulator 459
24/01/05 02:13:54 INFO spark.ContextCleaner: Cleaned accumulator 450
24/01/05 02:13:54 INFO spark.ContextCleaner: Cleaned accumulator 544
24/01/05 02:13:54 INFO spark.ContextCleaner: Cleaned accumulator 460
24/01/05 02:13:54 INFO spark.ContextCleaner: Cleaned accumulator 538
24/01/05 02:13:54 INFO spark.ContextCleaner: Cleaned accumulator 472
24/01/05 02:13:54 INFO spark.ContextCleaner: Cleaned accumulator 435
24/01/05 02:13:54 INFO spark.ContextCleaner: Cleaned accumulator 433
24/01/05 02:13:54 INFO spark.ContextCleaner: Cleaned accumulator 515
24/01/05 02:13:54 INFO memory.MemoryStore: Block rdd_68_0 stored as values in memory (estimated size 398.0 B, free 364.3 MB)
24/01/05 02:13:54 INFO storage.BlockManagerInfo: Removed broadcast_21_piece0 on adhoc-1:33719 in memory (size: 26.7 KB, free: 365.8 MB)
24/01/05 02:13:54 INFO storage.BlockManagerInfo: Added rdd_68_0 in memory on adhoc-1:33719 (size: 398.0 B, free: 365.8 MB)
24/01/05 02:13:54 INFO spark.ContextCleaner: Cleaned accumulator 461
24/01/05 02:13:54 INFO spark.ContextCleaner: Cleaned accumulator 535
24/01/05 02:13:54 INFO spark.ContextCleaner: Cleaned accumulator 449
24/01/05 02:13:54 INFO spark.ContextCleaner: Cleaned accumulator 451
24/01/05 02:13:54 INFO spark.ContextCleaner: Cleaned accumulator 441
24/01/05 02:13:54 INFO spark.ContextCleaner: Cleaned accumulator 541
24/01/05 02:13:54 INFO spark.ContextCleaner: Cleaned accumulator 512
24/01/05 02:13:54 INFO spark.ContextCleaner: Cleaned accumulator 468
24/01/05 02:13:54 INFO spark.ContextCleaner: Cleaned accumulator 474
24/01/05 02:13:54 INFO spark.ContextCleaner: Cleaned accumulator 494
24/01/05 02:13:54 INFO spark.ContextCleaner: Cleaned accumulator 428
24/01/05 02:13:54 INFO spark.ContextCleaner: Cleaned accumulator 502
24/01/05 02:13:54 INFO spark.ContextCleaner: Cleaned accumulator 478
24/01/05 02:13:54 INFO spark.ContextCleaner: Cleaned accumulator 500
24/01/05 02:13:54 INFO spark.ContextCleaner: Cleaned accumulator 484
24/01/05 02:13:54 INFO spark.ContextCleaner: Cleaned accumulator 501
24/01/05 02:13:54 INFO spark.ContextCleaner: Cleaned accumulator 511
24/01/05 02:13:54 INFO spark.ContextCleaner: Cleaned accumulator 531
24/01/05 02:13:54 INFO spark.ContextCleaner: Cleaned accumulator 427
24/01/05 02:13:54 INFO spark.ContextCleaner: Cleaned accumulator 492
24/01/05 02:13:54 INFO storage.BlockManagerInfo: Removed broadcast_18_piece0 on adhoc-1:33719 in memory (size: 127.2 KB, free: 365.9 MB)
24/01/05 02:13:54 INFO spark.ContextCleaner: Cleaned accumulator 488
24/01/05 02:13:54 INFO spark.ContextCleaner: Cleaned accumulator 507
24/01/05 02:13:54 INFO spark.ContextCleaner: Cleaned accumulator 528
24/01/05 02:13:54 INFO spark.ContextCleaner: Cleaned accumulator 448
24/01/05 02:13:54 INFO spark.ContextCleaner: Cleaned accumulator 524
24/01/05 02:13:54 INFO spark.ContextCleaner: Cleaned accumulator 447
24/01/05 02:13:54 INFO spark.ContextCleaner: Cleaned accumulator 529
24/01/05 02:13:54 INFO spark.ContextCleaner: Cleaned accumulator 527
24/01/05 02:13:54 INFO spark.ContextCleaner: Cleaned accumulator 431
24/01/05 02:13:54 INFO spark.ContextCleaner: Cleaned accumulator 465
24/01/05 02:13:54 INFO spark.ContextCleaner: Cleaned accumulator 453
24/01/05 02:13:54 INFO spark.ContextCleaner: Cleaned accumulator 473
24/01/05 02:13:54 INFO spark.ContextCleaner: Cleaned accumulator 462
24/01/05 02:13:54 INFO spark.ContextCleaner: Cleaned accumulator 445
24/01/05 02:13:54 INFO spark.ContextCleaner: Cleaned accumulator 470
24/01/05 02:13:54 INFO spark.ContextCleaner: Cleaned accumulator 425
24/01/05 02:13:54 INFO spark.ContextCleaner: Cleaned accumulator 486
24/01/05 02:13:54 INFO spark.ContextCleaner: Cleaned accumulator 476
24/01/05 02:13:54 INFO spark.ContextCleaner: Cleaned accumulator 533
24/01/05 02:13:54 INFO spark.ContextCleaner: Cleaned accumulator 526
24/01/05 02:13:54 INFO spark.ContextCleaner: Cleaned accumulator 523
24/01/05 02:13:54 INFO spark.ContextCleaner: Cleaned accumulator 545
24/01/05 02:13:54 INFO spark.ContextCleaner: Cleaned accumulator 463
24/01/05 02:13:54 INFO storage.BlockManagerInfo: Removed broadcast_17_piece0 on adhoc-1:33719 in memory (size: 123.0 KB, free: 366.0 MB)
24/01/05 02:13:54 INFO spark.ContextCleaner: Cleaned accumulator 455
24/01/05 02:13:54 INFO spark.ContextCleaner: Cleaned accumulator 499
24/01/05 02:13:54 INFO spark.ContextCleaner: Cleaned accumulator 481
24/01/05 02:13:54 INFO spark.ContextCleaner: Cleaned accumulator 534
24/01/05 02:13:54 INFO spark.ContextCleaner: Cleaned accumulator 510
24/01/05 02:13:54 INFO spark.ContextCleaner: Cleaned accumulator 496
24/01/05 02:13:54 INFO storage.BlockManagerInfo: Removed broadcast_20_piece0 on adhoc-1:33719 in memory (size: 127.3 KB, free: 366.2 MB)
24/01/05 02:13:54 INFO spark.ContextCleaner: Cleaned accumulator 469
24/01/05 02:13:54 INFO executor.Executor: Finished task 0.0 in stage 35.0 (TID 28). 950 bytes result sent to driver
24/01/05 02:13:54 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 35.0 (TID 28) in 30 ms on localhost (executor driver) (1/1)
24/01/05 02:13:54 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 35.0, whose tasks have all completed, from pool 
24/01/05 02:13:54 INFO storage.BlockManagerInfo: Removed broadcast_19_piece0 on adhoc-1:33719 in memory (size: 127.2 KB, free: 366.3 MB)
24/01/05 02:13:54 INFO scheduler.DAGScheduler: ShuffleMapStage 35 (countByKey at HoodieJavaPairRDD.java:105) finished in 0.065 s
24/01/05 02:13:54 INFO scheduler.DAGScheduler: looking for newly runnable stages
24/01/05 02:13:54 INFO scheduler.DAGScheduler: running: Set()
24/01/05 02:13:54 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 36)
24/01/05 02:13:54 INFO scheduler.DAGScheduler: failed: Set()
24/01/05 02:13:54 INFO scheduler.DAGScheduler: Submitting ResultStage 36 (ShuffledRDD[71] at countByKey at HoodieJavaPairRDD.java:105), which has no missing parents
24/01/05 02:13:54 INFO spark.ContextCleaner: Cleaned accumulator 457
24/01/05 02:13:54 INFO spark.ContextCleaner: Cleaned accumulator 479
24/01/05 02:13:54 INFO spark.ContextCleaner: Cleaned accumulator 471
24/01/05 02:13:54 INFO spark.ContextCleaner: Cleaned accumulator 483
24/01/05 02:13:54 INFO spark.ContextCleaner: Cleaned accumulator 432
24/01/05 02:13:54 INFO spark.ContextCleaner: Cleaned accumulator 454
24/01/05 02:13:54 INFO spark.ContextCleaner: Cleaned accumulator 536
24/01/05 02:13:54 INFO spark.ContextCleaner: Cleaned accumulator 497
24/01/05 02:13:54 INFO spark.ContextCleaner: Cleaned accumulator 439
24/01/05 02:13:54 INFO spark.ContextCleaner: Cleaned accumulator 458
24/01/05 02:13:54 INFO spark.ContextCleaner: Cleaned accumulator 517
24/01/05 02:13:54 INFO spark.ContextCleaner: Cleaned accumulator 505
24/01/05 02:13:54 INFO spark.ContextCleaner: Cleaned accumulator 518
24/01/05 02:13:54 INFO spark.ContextCleaner: Cleaned accumulator 429
24/01/05 02:13:54 INFO spark.ContextCleaner: Cleaned accumulator 442
24/01/05 02:13:54 INFO spark.ContextCleaner: Cleaned accumulator 489
24/01/05 02:13:54 INFO spark.ContextCleaner: Cleaned accumulator 525
24/01/05 02:13:54 INFO spark.ContextCleaner: Cleaned accumulator 522
24/01/05 02:13:54 INFO spark.ContextCleaner: Cleaned accumulator 493
24/01/05 02:13:54 INFO spark.ContextCleaner: Cleaned accumulator 430
24/01/05 02:13:54 INFO spark.ContextCleaner: Cleaned accumulator 426
24/01/05 02:13:54 INFO spark.ContextCleaner: Cleaned accumulator 516
24/01/05 02:13:54 INFO spark.ContextCleaner: Cleaned accumulator 532
24/01/05 02:13:54 INFO spark.ContextCleaner: Cleaned accumulator 539
24/01/05 02:13:54 INFO spark.ContextCleaner: Cleaned accumulator 547
24/01/05 02:13:54 INFO spark.ContextCleaner: Cleaned accumulator 482
24/01/05 02:13:54 INFO spark.ContextCleaner: Cleaned accumulator 506
24/01/05 02:13:54 INFO spark.ContextCleaner: Cleaned accumulator 537
24/01/05 02:13:54 INFO spark.ContextCleaner: Cleaned accumulator 490
24/01/05 02:13:54 INFO spark.ContextCleaner: Cleaned accumulator 504
24/01/05 02:13:54 INFO spark.ContextCleaner: Cleaned accumulator 549
24/01/05 02:13:54 INFO spark.ContextCleaner: Cleaned accumulator 440
24/01/05 02:13:54 INFO spark.ContextCleaner: Cleaned accumulator 446
24/01/05 02:13:54 INFO spark.ContextCleaner: Cleaned accumulator 491
24/01/05 02:13:54 INFO spark.ContextCleaner: Cleaned accumulator 452
24/01/05 02:13:54 INFO spark.ContextCleaner: Cleaned accumulator 508
24/01/05 02:13:54 INFO spark.ContextCleaner: Cleaned accumulator 521
24/01/05 02:13:54 INFO spark.ContextCleaner: Cleaned accumulator 477
24/01/05 02:13:54 INFO spark.ContextCleaner: Cleaned accumulator 487
24/01/05 02:13:54 INFO spark.ContextCleaner: Cleaned accumulator 466
24/01/05 02:13:54 INFO spark.ContextCleaner: Cleaned accumulator 542
24/01/05 02:13:54 INFO spark.ContextCleaner: Cleaned accumulator 437
24/01/05 02:13:54 INFO spark.ContextCleaner: Cleaned accumulator 509
24/01/05 02:13:54 INFO memory.MemoryStore: Block broadcast_23 stored as values in memory (estimated size 3.6 KB, free 366.3 MB)
24/01/05 02:13:54 INFO memory.MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 2.1 KB, free 366.3 MB)
24/01/05 02:13:54 INFO storage.BlockManagerInfo: Added broadcast_23_piece0 in memory on adhoc-1:33719 (size: 2.1 KB, free: 366.3 MB)
24/01/05 02:13:54 INFO spark.SparkContext: Created broadcast 23 from broadcast at DAGScheduler.scala:1161
24/01/05 02:13:54 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 36 (ShuffledRDD[71] at countByKey at HoodieJavaPairRDD.java:105) (first 15 tasks are for partitions Vector(0))
24/01/05 02:13:54 INFO scheduler.TaskSchedulerImpl: Adding task set 36.0 with 1 tasks
24/01/05 02:13:54 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 36.0 (TID 29, localhost, executor driver, partition 0, ANY, 7662 bytes)
24/01/05 02:13:54 INFO executor.Executor: Running task 0.0 in stage 36.0 (TID 29)
24/01/05 02:13:54 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
24/01/05 02:13:54 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
24/01/05 02:13:54 INFO executor.Executor: Finished task 0.0 in stage 36.0 (TID 29). 1179 bytes result sent to driver
24/01/05 02:13:54 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 36.0 (TID 29) in 12 ms on localhost (executor driver) (1/1)
24/01/05 02:13:54 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 36.0, whose tasks have all completed, from pool 
24/01/05 02:13:54 INFO scheduler.DAGScheduler: ResultStage 36 (countByKey at HoodieJavaPairRDD.java:105) finished in 0.022 s
24/01/05 02:13:54 INFO scheduler.DAGScheduler: Job 15 finished: countByKey at HoodieJavaPairRDD.java:105, took 0.090320 s
24/01/05 02:13:54 INFO commit.UpsertPartitioner: AvgRecordSize => 1024
24/01/05 02:13:54 INFO spark.SparkContext: Starting job: collectAsMap at UpsertPartitioner.java:282
24/01/05 02:13:54 INFO scheduler.DAGScheduler: Got job 16 (collectAsMap at UpsertPartitioner.java:282) with 1 output partitions
24/01/05 02:13:54 INFO scheduler.DAGScheduler: Final stage: ResultStage 37 (collectAsMap at UpsertPartitioner.java:282)
24/01/05 02:13:54 INFO scheduler.DAGScheduler: Parents of final stage: List()
24/01/05 02:13:54 INFO scheduler.DAGScheduler: Missing parents: List()
24/01/05 02:13:54 INFO scheduler.DAGScheduler: Submitting ResultStage 37 (MapPartitionsRDD[73] at mapToPair at UpsertPartitioner.java:281), which has no missing parents
24/01/05 02:13:54 INFO memory.MemoryStore: Block broadcast_24 stored as values in memory (estimated size 268.9 KB, free 366.0 MB)
24/01/05 02:13:54 INFO memory.MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 95.8 KB, free 365.9 MB)
24/01/05 02:13:54 INFO storage.BlockManagerInfo: Added broadcast_24_piece0 in memory on adhoc-1:33719 (size: 95.8 KB, free: 366.2 MB)
24/01/05 02:13:54 INFO spark.SparkContext: Created broadcast 24 from broadcast at DAGScheduler.scala:1161
24/01/05 02:13:54 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 37 (MapPartitionsRDD[73] at mapToPair at UpsertPartitioner.java:281) (first 15 tasks are for partitions Vector(0))
24/01/05 02:13:54 INFO scheduler.TaskSchedulerImpl: Adding task set 37.0 with 1 tasks
24/01/05 02:13:54 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 37.0 (TID 30, localhost, executor driver, partition 0, PROCESS_LOCAL, 7730 bytes)
24/01/05 02:13:54 INFO executor.Executor: Running task 0.0 in stage 37.0 (TID 30)
24/01/05 02:13:54 INFO view.FileSystemViewManager: Creating View Manager with storage type :MEMORY
24/01/05 02:13:54 INFO view.FileSystemViewManager: Creating in-memory based Table View
24/01/05 02:13:54 INFO view.FileSystemViewManager: Creating InMemory based view for basePath /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:13:54 INFO view.AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
24/01/05 02:13:54 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:13:54 INFO view.AbstractTableFileSystemView: Building file system view for partition (files)
24/01/05 02:13:54 INFO executor.Executor: Finished task 0.0 in stage 37.0 (TID 30). 700 bytes result sent to driver
24/01/05 02:13:54 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 37.0 (TID 30) in 26 ms on localhost (executor driver) (1/1)
24/01/05 02:13:54 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 37.0, whose tasks have all completed, from pool 
24/01/05 02:13:54 INFO scheduler.DAGScheduler: ResultStage 37 (collectAsMap at UpsertPartitioner.java:282) finished in 0.058 s
24/01/05 02:13:54 INFO scheduler.DAGScheduler: Job 16 finished: collectAsMap at UpsertPartitioner.java:282, took 0.059174 s
24/01/05 02:13:54 INFO view.AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
24/01/05 02:13:54 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:13:54 INFO commit.UpsertPartitioner: Total Buckets: 1
24/01/05 02:13:54 INFO timeline.HoodieActiveTimeline: Checking for file exists ?/user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/20240105021347452.deltacommit.requested
24/01/05 02:13:54 INFO timeline.HoodieActiveTimeline: Create new file for toInstant ?/user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/20240105021347452.deltacommit.inflight
24/01/05 02:13:54 INFO commit.BaseSparkCommitActionExecutor: no validators configured.
24/01/05 02:13:54 INFO commit.BaseCommitActionExecutor: Auto commit enabled: Committing 20240105021347452
24/01/05 02:13:55 INFO spark.SparkContext: Starting job: collect at HoodieJavaRDD.java:177
24/01/05 02:13:55 INFO scheduler.DAGScheduler: Registering RDD 74 (mapToPair at HoodieJavaRDD.java:149)
24/01/05 02:13:55 INFO scheduler.DAGScheduler: Got job 17 (collect at HoodieJavaRDD.java:177) with 1 output partitions
24/01/05 02:13:55 INFO scheduler.DAGScheduler: Final stage: ResultStage 39 (collect at HoodieJavaRDD.java:177)
24/01/05 02:13:55 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 38)
24/01/05 02:13:55 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 38)
24/01/05 02:13:55 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 38 (MapPartitionsRDD[74] at mapToPair at HoodieJavaRDD.java:149), which has no missing parents
24/01/05 02:13:55 INFO memory.MemoryStore: Block broadcast_25 stored as values in memory (estimated size 273.6 KB, free 365.6 MB)
24/01/05 02:13:55 INFO memory.MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 95.5 KB, free 365.5 MB)
24/01/05 02:13:55 INFO storage.BlockManagerInfo: Added broadcast_25_piece0 in memory on adhoc-1:33719 (size: 95.5 KB, free: 366.1 MB)
24/01/05 02:13:55 INFO spark.SparkContext: Created broadcast 25 from broadcast at DAGScheduler.scala:1161
24/01/05 02:13:55 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 38 (MapPartitionsRDD[74] at mapToPair at HoodieJavaRDD.java:149) (first 15 tasks are for partitions Vector(0))
24/01/05 02:13:55 INFO scheduler.TaskSchedulerImpl: Adding task set 38.0 with 1 tasks
24/01/05 02:13:55 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 38.0 (TID 31, localhost, executor driver, partition 0, PROCESS_LOCAL, 8086 bytes)
24/01/05 02:13:55 INFO executor.Executor: Running task 0.0 in stage 38.0 (TID 31)
24/01/05 02:13:55 INFO storage.BlockManager: Found block rdd_68_0 locally
24/01/05 02:13:55 INFO executor.Executor: Finished task 0.0 in stage 38.0 (TID 31). 907 bytes result sent to driver
24/01/05 02:13:55 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 38.0 (TID 31) in 28 ms on localhost (executor driver) (1/1)
24/01/05 02:13:55 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 38.0, whose tasks have all completed, from pool 
24/01/05 02:13:55 INFO scheduler.DAGScheduler: ShuffleMapStage 38 (mapToPair at HoodieJavaRDD.java:149) finished in 0.060 s
24/01/05 02:13:55 INFO scheduler.DAGScheduler: looking for newly runnable stages
24/01/05 02:13:55 INFO scheduler.DAGScheduler: running: Set()
24/01/05 02:13:55 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 39)
24/01/05 02:13:55 INFO scheduler.DAGScheduler: failed: Set()
24/01/05 02:13:55 INFO scheduler.DAGScheduler: Submitting ResultStage 39 (MapPartitionsRDD[79] at map at HoodieJavaRDD.java:125), which has no missing parents
24/01/05 02:13:55 INFO memory.MemoryStore: Block broadcast_26 stored as values in memory (estimated size 348.5 KB, free 365.2 MB)
24/01/05 02:13:55 INFO memory.MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 126.8 KB, free 365.1 MB)
24/01/05 02:13:55 INFO storage.BlockManagerInfo: Added broadcast_26_piece0 in memory on adhoc-1:33719 (size: 126.8 KB, free: 366.0 MB)
24/01/05 02:13:55 INFO spark.SparkContext: Created broadcast 26 from broadcast at DAGScheduler.scala:1161
24/01/05 02:13:55 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 39 (MapPartitionsRDD[79] at map at HoodieJavaRDD.java:125) (first 15 tasks are for partitions Vector(0))
24/01/05 02:13:55 INFO scheduler.TaskSchedulerImpl: Adding task set 39.0 with 1 tasks
24/01/05 02:13:55 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 39.0 (TID 32, localhost, executor driver, partition 0, ANY, 7662 bytes)
24/01/05 02:13:55 INFO executor.Executor: Running task 0.0 in stage 39.0 (TID 32)
24/01/05 02:13:55 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
24/01/05 02:13:55 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
24/01/05 02:13:55 INFO deltacommit.BaseSparkDeltaCommitActionExecutor: Merging updates for commit 20240105021347452 for file files-0000-0
24/01/05 02:13:55 INFO view.FileSystemViewManager: Creating View Manager with storage type :MEMORY
24/01/05 02:13:55 INFO view.FileSystemViewManager: Creating in-memory based Table View
24/01/05 02:13:55 INFO view.FileSystemViewManager: Creating InMemory based view for basePath /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:13:55 INFO view.AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
24/01/05 02:13:55 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:13:55 INFO view.AbstractTableFileSystemView: Building file system view for partition (files)
# WARNING: Unable to attach Serviceability Agent. Unable to attach even with module exceptions: [org.apache.hudi.org.openjdk.jol.vm.sa.SASupportException: Sense failed., org.apache.hudi.org.openjdk.jol.vm.sa.SASupportException: Sense failed., org.apache.hudi.org.openjdk.jol.vm.sa.SASupportException: Sense failed.]
24/01/05 02:13:55 INFO marker.DirectWriteMarkers: Creating Marker Path=/user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/.temp/20240105021347452/files/files-0000-0_0-39-32_00000000000000010.hfile.marker.APPEND
24/01/05 02:13:55 INFO marker.DirectWriteMarkers: [direct] Created marker file /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/.temp/20240105021347452/files/files-0000-0_0-39-32_00000000000000010.hfile.marker.APPEND in 6 ms
24/01/05 02:13:55 INFO log.HoodieLogFormat$WriterBuilder: Building HoodieLogFormat Writer
24/01/05 02:13:55 INFO log.HoodieLogFormat$WriterBuilder: HoodieLogFile on path /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/files/.files-0000-0_00000000000000010.log.1_0-0-0
24/01/05 02:13:55 INFO log.HoodieLogFormatWriter: HoodieLogFile{pathStr='/user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/files/.files-0000-0_00000000000000010.log.1_0-0-0', fileLen=80} exists. Appending to existing file
24/01/05 02:13:55 INFO compress.CodecPool: Got brand-new compressor [.gz]
24/01/05 02:13:55 INFO compress.CodecPool: Got brand-new compressor [.gz]
24/01/05 02:13:55 INFO io.HoodieAppendHandle: AppendHandle for partitionPath files filePath files/.files-0000-0_00000000000000010.log.1_0-0-0, took 769 ms.
24/01/05 02:13:56 INFO memory.MemoryStore: Block rdd_78_0 stored as values in memory (estimated size 437.0 B, free 365.1 MB)
24/01/05 02:13:56 INFO storage.BlockManagerInfo: Added rdd_78_0 in memory on adhoc-1:33719 (size: 437.0 B, free: 366.0 MB)
24/01/05 02:13:56 INFO executor.Executor: Finished task 0.0 in stage 39.0 (TID 32). 1485 bytes result sent to driver
24/01/05 02:13:56 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 39.0 (TID 32) in 1249 ms on localhost (executor driver) (1/1)
24/01/05 02:13:56 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 39.0, whose tasks have all completed, from pool 
24/01/05 02:13:56 INFO scheduler.DAGScheduler: ResultStage 39 (collect at HoodieJavaRDD.java:177) finished in 1.281 s
24/01/05 02:13:56 INFO scheduler.DAGScheduler: Job 17 finished: collect at HoodieJavaRDD.java:177, took 1.344355 s
24/01/05 02:13:56 INFO util.CommitUtils: Creating  metadata for UPSERT_PREPPED numWriteStats:1 numReplaceFileIds:0
24/01/05 02:13:56 INFO commit.BaseSparkCommitActionExecutor: Committing 20240105021347452, action Type deltacommit, operation Type UPSERT_PREPPED
24/01/05 02:13:56 INFO spark.SparkContext: Starting job: collect at HoodieSparkEngineContext.java:150
24/01/05 02:13:56 INFO scheduler.DAGScheduler: Got job 18 (collect at HoodieSparkEngineContext.java:150) with 1 output partitions
24/01/05 02:13:56 INFO scheduler.DAGScheduler: Final stage: ResultStage 40 (collect at HoodieSparkEngineContext.java:150)
24/01/05 02:13:56 INFO scheduler.DAGScheduler: Parents of final stage: List()
24/01/05 02:13:56 INFO scheduler.DAGScheduler: Missing parents: List()
24/01/05 02:13:56 INFO scheduler.DAGScheduler: Submitting ResultStage 40 (MapPartitionsRDD[81] at flatMap at HoodieSparkEngineContext.java:150), which has no missing parents
24/01/05 02:13:56 INFO memory.MemoryStore: Block broadcast_27 stored as values in memory (estimated size 72.5 KB, free 365.0 MB)
24/01/05 02:13:56 INFO memory.MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 26.7 KB, free 365.0 MB)
24/01/05 02:13:56 INFO storage.BlockManagerInfo: Added broadcast_27_piece0 in memory on adhoc-1:33719 (size: 26.7 KB, free: 365.9 MB)
24/01/05 02:13:56 INFO spark.SparkContext: Created broadcast 27 from broadcast at DAGScheduler.scala:1161
24/01/05 02:13:56 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 40 (MapPartitionsRDD[81] at flatMap at HoodieSparkEngineContext.java:150) (first 15 tasks are for partitions Vector(0))
24/01/05 02:13:56 INFO scheduler.TaskSchedulerImpl: Adding task set 40.0 with 1 tasks
24/01/05 02:13:56 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 40.0 (TID 33, localhost, executor driver, partition 0, PROCESS_LOCAL, 7838 bytes)
24/01/05 02:13:56 INFO executor.Executor: Running task 0.0 in stage 40.0 (TID 33)
24/01/05 02:13:56 INFO executor.Executor: Finished task 0.0 in stage 40.0 (TID 33). 668 bytes result sent to driver
24/01/05 02:13:56 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 40.0 (TID 33) in 13 ms on localhost (executor driver) (1/1)
24/01/05 02:13:56 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 40.0, whose tasks have all completed, from pool 
24/01/05 02:13:56 INFO scheduler.DAGScheduler: ResultStage 40 (collect at HoodieSparkEngineContext.java:150) finished in 0.024 s
24/01/05 02:13:56 INFO scheduler.DAGScheduler: Job 18 finished: collect at HoodieSparkEngineContext.java:150, took 0.026665 s
24/01/05 02:13:56 INFO timeline.HoodieActiveTimeline: Marking instant complete [==>20240105021347452__deltacommit__INFLIGHT]
24/01/05 02:13:56 INFO timeline.HoodieActiveTimeline: Checking for file exists ?/user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/20240105021347452.deltacommit.inflight
24/01/05 02:13:56 INFO timeline.HoodieActiveTimeline: Create new file for toInstant ?/user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/20240105021347452.deltacommit
24/01/05 02:13:56 INFO timeline.HoodieActiveTimeline: Completed [==>20240105021347452__deltacommit__INFLIGHT]
24/01/05 02:13:56 INFO commit.BaseSparkCommitActionExecutor: Committed 20240105021347452
24/01/05 02:13:56 INFO spark.SparkContext: Starting job: collectAsMap at HoodieSparkEngineContext.java:164
24/01/05 02:13:56 INFO scheduler.DAGScheduler: Got job 19 (collectAsMap at HoodieSparkEngineContext.java:164) with 1 output partitions
24/01/05 02:13:56 INFO scheduler.DAGScheduler: Final stage: ResultStage 41 (collectAsMap at HoodieSparkEngineContext.java:164)
24/01/05 02:13:56 INFO scheduler.DAGScheduler: Parents of final stage: List()
24/01/05 02:13:56 INFO scheduler.DAGScheduler: Missing parents: List()
24/01/05 02:13:56 INFO scheduler.DAGScheduler: Submitting ResultStage 41 (MapPartitionsRDD[83] at mapToPair at HoodieSparkEngineContext.java:161), which has no missing parents
24/01/05 02:13:56 INFO memory.MemoryStore: Block broadcast_28 stored as values in memory (estimated size 72.8 KB, free 364.9 MB)
24/01/05 02:13:56 INFO memory.MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 26.8 KB, free 364.9 MB)
24/01/05 02:13:56 INFO storage.BlockManagerInfo: Added broadcast_28_piece0 in memory on adhoc-1:33719 (size: 26.8 KB, free: 365.9 MB)
24/01/05 02:13:56 INFO spark.SparkContext: Created broadcast 28 from broadcast at DAGScheduler.scala:1161
24/01/05 02:13:56 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 41 (MapPartitionsRDD[83] at mapToPair at HoodieSparkEngineContext.java:161) (first 15 tasks are for partitions Vector(0))
24/01/05 02:13:56 INFO scheduler.TaskSchedulerImpl: Adding task set 41.0 with 1 tasks
24/01/05 02:13:56 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 41.0 (TID 34, localhost, executor driver, partition 0, PROCESS_LOCAL, 7838 bytes)
24/01/05 02:13:56 INFO executor.Executor: Running task 0.0 in stage 41.0 (TID 34)
24/01/05 02:13:56 INFO executor.Executor: Finished task 0.0 in stage 41.0 (TID 34). 787 bytes result sent to driver
24/01/05 02:13:56 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 41.0 (TID 34) in 11 ms on localhost (executor driver) (1/1)
24/01/05 02:13:56 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 41.0, whose tasks have all completed, from pool 
24/01/05 02:13:56 INFO scheduler.DAGScheduler: ResultStage 41 (collectAsMap at HoodieSparkEngineContext.java:164) finished in 0.022 s
24/01/05 02:13:56 INFO scheduler.DAGScheduler: Job 19 finished: collectAsMap at HoodieSparkEngineContext.java:164, took 0.024300 s
24/01/05 02:13:56 INFO fs.FSUtils: Removed directory at /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/.temp/20240105021347452
24/01/05 02:13:56 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:13:56 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:13:56 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:13:56 INFO table.HoodieTableMetaClient: Loading Active commit timeline for /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:13:56 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105021347452__deltacommit__COMPLETED__20240105021356832]}
24/01/05 02:13:56 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:13:56 INFO view.FileSystemViewManager: Creating View Manager with storage type :MEMORY
24/01/05 02:13:56 INFO view.FileSystemViewManager: Creating in-memory based Table View
24/01/05 02:13:56 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105021347452__deltacommit__COMPLETED__20240105021356832]}
24/01/05 02:13:56 INFO timeline.HoodieActiveTimeline: Marking instant complete [==>20240105021347452__deltacommit__INFLIGHT]
24/01/05 02:13:56 INFO timeline.HoodieActiveTimeline: Checking for file exists ?/user/hive/warehouse/stock_ticks_mor/.hoodie/20240105021347452.deltacommit.inflight
24/01/05 02:13:56 INFO timeline.HoodieActiveTimeline: Create new file for toInstant ?/user/hive/warehouse/stock_ticks_mor/.hoodie/20240105021347452.deltacommit
24/01/05 02:13:56 INFO timeline.HoodieActiveTimeline: Completed [==>20240105021347452__deltacommit__INFLIGHT]
24/01/05 02:13:56 WARN marker.WriteMarkersFactory: Timeline-server-based markers are not supported for HDFS: base path /user/hive/warehouse/stock_ticks_mor.  Falling back to direct markers.
24/01/05 02:13:56 INFO spark.SparkContext: Starting job: collectAsMap at HoodieSparkEngineContext.java:164
24/01/05 02:13:56 INFO scheduler.DAGScheduler: Got job 20 (collectAsMap at HoodieSparkEngineContext.java:164) with 1 output partitions
24/01/05 02:13:56 INFO scheduler.DAGScheduler: Final stage: ResultStage 42 (collectAsMap at HoodieSparkEngineContext.java:164)
24/01/05 02:13:56 INFO scheduler.DAGScheduler: Parents of final stage: List()
24/01/05 02:13:56 INFO scheduler.DAGScheduler: Missing parents: List()
24/01/05 02:13:56 INFO scheduler.DAGScheduler: Submitting ResultStage 42 (MapPartitionsRDD[85] at mapToPair at HoodieSparkEngineContext.java:161), which has no missing parents
24/01/05 02:13:56 INFO memory.MemoryStore: Block broadcast_29 stored as values in memory (estimated size 72.8 KB, free 364.8 MB)
24/01/05 02:13:56 INFO memory.MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 26.8 KB, free 364.8 MB)
24/01/05 02:13:56 INFO storage.BlockManagerInfo: Added broadcast_29_piece0 in memory on adhoc-1:33719 (size: 26.8 KB, free: 365.9 MB)
24/01/05 02:13:56 INFO spark.SparkContext: Created broadcast 29 from broadcast at DAGScheduler.scala:1161
24/01/05 02:13:56 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 42 (MapPartitionsRDD[85] at mapToPair at HoodieSparkEngineContext.java:161) (first 15 tasks are for partitions Vector(0))
24/01/05 02:13:56 INFO scheduler.TaskSchedulerImpl: Adding task set 42.0 with 1 tasks
24/01/05 02:13:56 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 42.0 (TID 35, localhost, executor driver, partition 0, PROCESS_LOCAL, 7820 bytes)
24/01/05 02:13:56 INFO executor.Executor: Running task 0.0 in stage 42.0 (TID 35)
24/01/05 02:13:56 INFO executor.Executor: Finished task 0.0 in stage 42.0 (TID 35). 769 bytes result sent to driver
24/01/05 02:13:56 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 42.0 (TID 35) in 12 ms on localhost (executor driver) (1/1)
24/01/05 02:13:56 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 42.0, whose tasks have all completed, from pool 
24/01/05 02:13:56 INFO scheduler.DAGScheduler: ResultStage 42 (collectAsMap at HoodieSparkEngineContext.java:164) finished in 0.022 s
24/01/05 02:13:56 INFO scheduler.DAGScheduler: Job 20 finished: collectAsMap at HoodieSparkEngineContext.java:164, took 0.023578 s
24/01/05 02:13:56 INFO fs.FSUtils: Removed directory at /user/hive/warehouse/stock_ticks_mor/.hoodie/.temp/20240105021347452
24/01/05 02:13:56 INFO client.BaseHoodieWriteClient: Committed 20240105021347452
24/01/05 02:13:56 INFO rdd.MapPartitionsRDD: Removing RDD 44 from persistence list
24/01/05 02:13:56 INFO storage.BlockManager: Removing RDD 44
24/01/05 02:13:56 INFO rdd.MapPartitionsRDD: Removing RDD 54 from persistence list
24/01/05 02:13:56 INFO storage.BlockManager: Removing RDD 54
24/01/05 02:13:56 INFO rdd.UnionRDD: Removing RDD 68 from persistence list
24/01/05 02:13:56 INFO storage.BlockManager: Removing RDD 68
24/01/05 02:13:56 INFO rdd.MapPartitionsRDD: Removing RDD 78 from persistence list
24/01/05 02:13:56 INFO storage.BlockManager: Removing RDD 78
24/01/05 02:13:57 INFO transaction.TransactionManager: Transaction ending with transaction owner Option{val=[==>20240105021347452__deltacommit__INFLIGHT]}
24/01/05 02:13:57 INFO lock.InProcessLockProvider: Base Path /user/hive/warehouse/stock_ticks_mor, Lock Instance java.util.concurrent.locks.ReentrantReadWriteLock@592a1882[Write locks = 1, Read locks = 0], Thread main, In-process lock state RELEASING
24/01/05 02:13:57 INFO lock.InProcessLockProvider: Base Path /user/hive/warehouse/stock_ticks_mor, Lock Instance java.util.concurrent.locks.ReentrantReadWriteLock@592a1882[Write locks = 0, Read locks = 0], Thread main, In-process lock state RELEASED
24/01/05 02:13:57 INFO lock.InProcessLockProvider: Base Path /user/hive/warehouse/stock_ticks_mor, Lock Instance java.util.concurrent.locks.ReentrantReadWriteLock@592a1882[Write locks = 0, Read locks = 0], Thread main, In-process lock state ALREADY_RELEASED
24/01/05 02:13:57 INFO lock.LockManager: Released connection created for acquiring lock
24/01/05 02:13:57 INFO transaction.TransactionManager: Transaction ended with transaction owner Option{val=[==>20240105021347452__deltacommit__INFLIGHT]}
24/01/05 02:13:57 INFO client.BaseHoodieWriteClient: Start to clean synchronously.
24/01/05 02:13:57 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:13:57 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:13:57 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:13:57 INFO table.HoodieTableMetaClient: Loading Active commit timeline for /user/hive/warehouse/stock_ticks_mor
24/01/05 02:13:57 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105021347452__deltacommit__COMPLETED__20240105021356931]}
24/01/05 02:13:57 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:13:57 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:13:57 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:13:57 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:13:57 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:13:57 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:13:57 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105021347452__deltacommit__COMPLETED__20240105021356832]}
24/01/05 02:13:57 INFO view.AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
24/01/05 02:13:57 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:13:57 INFO view.FileSystemViewManager: Creating View Manager with storage type :REMOTE_FIRST
24/01/05 02:13:57 INFO view.FileSystemViewManager: Creating remote first table view
24/01/05 02:13:57 INFO client.BaseHoodieWriteClient: Cleaner started
24/01/05 02:13:57 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:13:57 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:13:57 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:13:57 INFO table.HoodieTableMetaClient: Loading Active commit timeline for /user/hive/warehouse/stock_ticks_mor
24/01/05 02:13:57 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105021347452__deltacommit__COMPLETED__20240105021356931]}
24/01/05 02:13:57 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:13:57 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:13:57 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:13:57 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:13:57 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:13:57 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:13:57 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105021347452__deltacommit__COMPLETED__20240105021356832]}
24/01/05 02:13:57 INFO view.AbstractTableFileSystemView: Took 1 ms to read  0 instants, 0 replaced file groups
24/01/05 02:13:57 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:13:57 INFO view.FileSystemViewManager: Creating View Manager with storage type :REMOTE_FIRST
24/01/05 02:13:57 INFO view.FileSystemViewManager: Creating remote first table view
24/01/05 02:13:57 INFO client.BaseHoodieWriteClient: Scheduling cleaning at instant time :20240105021357000
24/01/05 02:13:57 INFO view.FileSystemViewManager: Creating remote view for basePath /user/hive/warehouse/stock_ticks_mor. Server=adhoc-1:34001, Timeout=300
24/01/05 02:13:57 INFO view.FileSystemViewManager: Creating InMemory based view for basePath /user/hive/warehouse/stock_ticks_mor
24/01/05 02:13:57 INFO view.AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
24/01/05 02:13:57 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:13:57 INFO view.RemoteHoodieTableFileSystemView: Sending request : (http://adhoc-1:34001/v1/hoodie/view/compactions/pending/?basepath=%2Fuser%2Fhive%2Fwarehouse%2Fstock_ticks_mor&lastinstantts=20240105021347452&timelinehash=4a3ab074d69f26b49cb5f0dfceaf1fd8f4760ea789bb800ca6cea65c8323c7a6)
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 653
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 726
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 647
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 723
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 582
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 605
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 587
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 670
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 743
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 688
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 735
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 571
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned shuffle 8
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 568
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 649
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 567
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 671
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 672
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 720
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 636
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 555
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 558
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 616
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 631
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 716
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 560
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 572
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 711
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 687
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 701
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 706
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 592
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 559
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 669
24/01/05 02:13:57 INFO storage.BlockManagerInfo: Removed broadcast_23_piece0 on adhoc-1:33719 in memory (size: 2.1 KB, free: 365.9 MB)
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 715
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 643
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 746
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 603
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 655
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 692
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 610
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 744
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 668
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 596
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 627
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 690
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 612
24/01/05 02:13:57 INFO storage.BlockManager: Removing RDD 78
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned RDD 78
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 641
24/01/05 02:13:57 INFO storage.BlockManager: Removing RDD 68
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned RDD 68
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 699
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 747
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 604
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 661
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 729
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 593
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 749
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 664
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 680
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 708
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 717
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 557
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 704
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 662
24/01/05 02:13:57 INFO storage.BlockManagerInfo: Removed broadcast_27_piece0 on adhoc-1:33719 in memory (size: 26.7 KB, free: 365.9 MB)
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 728
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 562
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 656
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 606
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 707
24/01/05 02:13:57 INFO storage.BlockManagerInfo: Removed broadcast_29_piece0 on adhoc-1:33719 in memory (size: 26.8 KB, free: 366.0 MB)
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 644
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 674
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 697
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 586
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 615
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 683
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 589
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 591
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 651
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 673
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 702
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 724
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 719
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 698
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 625
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 712
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 714
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 611
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 660
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 736
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 648
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 657
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 590
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 681
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 741
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 594
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 686
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 642
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 563
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 554
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 659
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 580
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 654
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 584
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 550
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 695
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 709
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 613
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 620
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 693
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 645
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 633
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 573
24/01/05 02:13:57 INFO storage.BlockManagerInfo: Removed broadcast_25_piece0 on adhoc-1:33719 in memory (size: 95.5 KB, free: 366.1 MB)
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 679
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 632
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 630
24/01/05 02:13:57 INFO storage.BlockManagerInfo: Removed broadcast_28_piece0 on adhoc-1:33719 in memory (size: 26.8 KB, free: 366.1 MB)
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 585
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 634
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 689
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 737
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 607
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 637
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 666
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 663
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 614
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 623
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 738
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 609
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 718
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 566
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 713
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 748
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 635
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 579
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 581
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 696
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 722
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 583
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 552
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 640
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 675
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 595
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 691
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 575
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 570
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 734
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 561
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 569
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 731
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 601
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 646
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 676
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 721
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 700
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 600
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 682
24/01/05 02:13:57 INFO storage.BlockManagerInfo: Removed broadcast_24_piece0 on adhoc-1:33719 in memory (size: 95.8 KB, free: 366.2 MB)
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 598
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 551
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 652
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 577
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 597
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 624
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 576
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 684
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 621
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 564
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 574
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 733
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 739
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 710
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 629
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 742
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 617
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned shuffle 7
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 730
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 732
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 678
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 556
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 703
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 745
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 705
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 639
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 638
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 618
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 740
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 599
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 685
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 667
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 694
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 626
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 650
24/01/05 02:13:57 INFO storage.BlockManagerInfo: Removed broadcast_26_piece0 on adhoc-1:33719 in memory (size: 126.8 KB, free: 366.3 MB)
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 658
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 588
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 553
24/01/05 02:13:57 INFO storage.BlockManagerInfo: Removed broadcast_22_piece0 on adhoc-1:33719 in memory (size: 4.7 KB, free: 366.3 MB)
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 565
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 578
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 619
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 677
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 622
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 725
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 727
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 608
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 602
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 628
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 665
24/01/05 02:13:57 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:13:57 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:13:57 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:13:57 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105021347452__deltacommit__COMPLETED__20240105021356931]}
24/01/05 02:13:57 INFO collection.RocksDBDAO: DELETING RocksDB persisted at /tmp/hoodie_timeline_rocksdb/_user_hive_warehouse_stock_ticks_mor/55a052a7-3432-4009-afa1-a3df5149cc01
24/01/05 02:13:57 INFO collection.RocksDBDAO: No column family found. Loading default
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 33
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 50
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 36
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 27
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 15
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 18
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 10
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 3
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 38
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 7
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 4
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 6
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 48
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 66
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 42
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 1
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 45
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 41
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 22
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 32
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 47
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 70
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 55
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 68
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 52
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 31
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 39
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 13
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 61
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 40
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 43
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 12
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 26
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 11
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 59
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 8
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 60
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 65
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 0
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 19
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 63
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 30
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 23
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 71
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 54
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 64
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 67
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 34
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 49
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 58
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 9
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 46
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 21
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 73
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 69
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 25
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 51
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 17
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 62
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 72
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 56
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 24
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 29
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 5
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 57
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 14
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 2
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 28
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 35
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 44
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 53
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 20
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 16
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 74
24/01/05 02:13:57 INFO spark.ContextCleaner: Cleaned accumulator 37
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : RocksDB version: 7.5.3

24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Compile date 2022-09-02 09:50:20
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : DB SUMMARY

24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : DB Session ID:  DG17H468PME47L1SWXGK

24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : SST files in /tmp/hoodie_timeline_rocksdb/_user_hive_warehouse_stock_ticks_mor/55a052a7-3432-4009-afa1-a3df5149cc01 dir, Total Num: 0, files: 

24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Write Ahead Log file in /tmp/hoodie_timeline_rocksdb/_user_hive_warehouse_stock_ticks_mor/55a052a7-3432-4009-afa1-a3df5149cc01: 

24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                         Options.error_if_exists: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                       Options.create_if_missing: 1
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                         Options.paranoid_checks: 1
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :             Options.flush_verify_memtable_count: 1
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                               Options.track_and_verify_wals_in_manifest: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :        Options.verify_sst_unique_id_in_manifest: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                                     Options.env: 0x7f8e856345e0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                                      Options.fs: PosixFileSystem
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                                Options.info_log: 0x7f8ea0100ba8
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                Options.max_file_opening_threads: 16
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                              Options.statistics: 0x7f8ea008f000
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                               Options.use_fsync: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                       Options.max_log_file_size: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                  Options.max_manifest_file_size: 1073741824
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                   Options.log_file_time_to_roll: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                       Options.keep_log_file_num: 1000
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                    Options.recycle_log_file_num: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                         Options.allow_fallocate: 1
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                        Options.allow_mmap_reads: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                       Options.allow_mmap_writes: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                        Options.use_direct_reads: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                        Options.use_direct_io_for_flush_and_compaction: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :          Options.create_missing_column_families: 1
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                              Options.db_log_dir: 
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                                 Options.wal_dir: 
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                Options.table_cache_numshardbits: 6
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                         Options.WAL_ttl_seconds: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                       Options.WAL_size_limit_MB: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                        Options.max_write_batch_group_size_bytes: 1048576
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :             Options.manifest_preallocation_size: 4194304
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                     Options.is_fd_close_on_exec: 1
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                   Options.advise_random_on_open: 1
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                    Options.db_write_buffer_size: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                    Options.write_buffer_manager: 0x7f8ea0105360
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :         Options.access_hint_on_compaction_start: 1
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :           Options.random_access_max_buffer_size: 1048576
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                      Options.use_adaptive_mutex: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                            Options.rate_limiter: (nil)
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :     Options.sst_file_manager.rate_bytes_per_sec: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                       Options.wal_recovery_mode: 2
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                  Options.enable_thread_tracking: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                  Options.enable_pipelined_write: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                  Options.unordered_write: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :         Options.allow_concurrent_memtable_write: 1
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :      Options.enable_write_thread_adaptive_yield: 1
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :             Options.write_thread_max_yield_usec: 100
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :            Options.write_thread_slow_yield_usec: 3
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                               Options.row_cache: None
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                              Options.wal_filter: None
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :             Options.avoid_flush_during_recovery: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :             Options.allow_ingest_behind: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :             Options.two_write_queues: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :             Options.manual_wal_flush: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :             Options.wal_compression: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :             Options.atomic_flush: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :             Options.avoid_unnecessary_blocking_io: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                 Options.persist_stats_to_disk: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                 Options.write_dbid_to_manifest: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                 Options.log_readahead_size: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                 Options.file_checksum_gen_factory: Unknown
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                 Options.best_efforts_recovery: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                Options.max_bgerror_resume_count: 2147483647
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :            Options.bgerror_resume_retry_interval: 1000000
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :             Options.allow_data_in_errors: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :             Options.db_host_id: __hostname__
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :             Options.enforce_single_del_contracts: true
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :             Options.max_background_jobs: 2
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :             Options.max_background_compactions: -1
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :             Options.max_subcompactions: 1
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :             Options.avoid_flush_during_shutdown: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :           Options.writable_file_max_buffer_size: 1048576
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :             Options.delayed_write_rate : 16777216
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :             Options.max_total_wal_size: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :             Options.delete_obsolete_files_period_micros: 21600000000
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                   Options.stats_dump_period_sec: 300
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                 Options.stats_persist_period_sec: 600
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                 Options.stats_history_buffer_size: 1048576
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                          Options.max_open_files: -1
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                          Options.bytes_per_sync: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                      Options.wal_bytes_per_sync: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                   Options.strict_bytes_per_sync: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :       Options.compaction_readahead_size: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                  Options.max_background_flushes: -1
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Compression algorithms supported:
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : 	kZSTDNotFinalCompression supported: 1
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : 	kZSTD supported: 1
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : 	kXpressCompression supported: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : 	kLZ4HCCompression supported: 1
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : 	kLZ4Compression supported: 1
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : 	kBZip2Compression supported: 1
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : 	kZlibCompression supported: 1
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : 	kSnappyCompression supported: 1
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Fast CRC32 supported: Not supported on x86
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : DMutex implementation: pthread_mutex_t
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : [/db_impl/db_impl_open.cc:313] Creating manifest 1 

24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : [/version_set.cc:4994] Recovering from manifest file: /tmp/hoodie_timeline_rocksdb/_user_hive_warehouse_stock_ticks_mor/55a052a7-3432-4009-afa1-a3df5149cc01/MANIFEST-000001

24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : [/column_family.cc:618] --------------- Options for column family [default]:

24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :               Options.comparator: leveldb.BytewiseComparator
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :           Options.merge_operator: None
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :        Options.compaction_filter: None
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :        Options.compaction_filter_factory: None
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :  Options.sst_partitioner_factory: None
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :         Options.memtable_factory: SkipListFactory
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :            Options.table_factory: BlockBasedTable
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :            table_factory options:   flush_block_policy_factory: FlushBlockBySizePolicyFactory (0x7f8ea01032a0)
  cache_index_and_filter_blocks: 0
  cache_index_and_filter_blocks_with_high_priority: 1
  pin_l0_filter_and_index_blocks_in_cache: 0
  pin_top_level_index_and_filter: 1
  index_type: 0
  data_block_index_type: 0
  index_shortening: 1
  data_block_hash_table_util_ratio: 0.750000
  checksum: 1
  no_block_cache: 0
  block_cache: 0x7f8ea01032d0
  block_cache_name: LRUCache
  block_cache_options:
    capacity : 8388608
    num_shard_bits : 4
    strict_capacity_limit : 0
    memory_allocator : None
    high_pri_pool_ratio: 0.000
  block_cache_compressed: (nil)
  persistent_cache: (nil)
  block_size: 4096
  block_size_deviation: 10
  block_restart_interval: 16
  index_block_restart_interval: 1
  metadata_block_size: 4096
  partition_filters: 0
  use_delta_encoding: 1
  filter_policy: nullptr
  whole_key_filtering: 1
  verify_compression: 0
  read_amp_bytes_per_bit: 0
  format_version: 5
  enable_index_compression: 1
  block_align: 0
  max_auto_readahead_size: 262144
  prepopulate_block_cache: 0
  initial_auto_readahead_size: 8192

24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :        Options.write_buffer_size: 67108864
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :  Options.max_write_buffer_number: 2
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :          Options.compression: Snappy
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression: Disabled
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :       Options.prefix_extractor: nullptr
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :   Options.memtable_insert_with_hint_prefix_extractor: nullptr
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :             Options.num_levels: 7
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :        Options.min_write_buffer_number_to_merge: 1
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :     Options.max_write_buffer_number_to_maintain: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :     Options.max_write_buffer_size_to_maintain: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :            Options.bottommost_compression_opts.window_bits: -14
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression_opts.level: 32767
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :               Options.bottommost_compression_opts.strategy: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.max_dict_bytes: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.zstd_max_train_bytes: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.parallel_threads: 1
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression_opts.enabled: false
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.max_dict_buffer_bytes: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.use_zstd_dict_trainer: true
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :            Options.compression_opts.window_bits: -14
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                  Options.compression_opts.level: 32767
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :               Options.compression_opts.strategy: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.max_dict_bytes: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.zstd_max_train_bytes: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.use_zstd_dict_trainer: true
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.parallel_threads: 1
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                  Options.compression_opts.enabled: false
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.max_dict_buffer_bytes: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :      Options.level0_file_num_compaction_trigger: 4
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :          Options.level0_slowdown_writes_trigger: 20
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :              Options.level0_stop_writes_trigger: 36
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                   Options.target_file_size_base: 67108864
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :             Options.target_file_size_multiplier: 1
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                Options.max_bytes_for_level_base: 268435456
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Options.level_compaction_dynamic_level_bytes: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :          Options.max_bytes_for_level_multiplier: 10.000000
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[0]: 1
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[1]: 1
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[2]: 1
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[3]: 1
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[4]: 1
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[5]: 1
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[6]: 1
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :       Options.max_sequential_skip_in_iterations: 8
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                    Options.max_compaction_bytes: 1677721600
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                        Options.arena_block_size: 1048576
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :   Options.soft_pending_compaction_bytes_limit: 68719476736
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :   Options.hard_pending_compaction_bytes_limit: 274877906944
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                Options.disable_auto_compactions: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                        Options.compaction_style: kCompactionStyleLevel
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                          Options.compaction_pri: kMinOverlappingRatio
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.size_ratio: 1
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.min_merge_width: 2
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.max_merge_width: 4294967295
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.max_size_amplification_percent: 200
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.compression_size_percent: -1
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.stop_style: kCompactionStopStyleTotalSize
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_fifo.max_table_files_size: 1073741824
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_fifo.allow_compaction: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                   Options.table_properties_collectors: 
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                   Options.inplace_update_support: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                 Options.inplace_update_num_locks: 10000
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :               Options.memtable_prefix_bloom_size_ratio: 0.000000
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :               Options.memtable_whole_key_filtering: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :   Options.memtable_huge_page_size: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                           Options.bloom_locality: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                    Options.max_successive_merges: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                Options.optimize_filters_for_hits: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                Options.paranoid_file_checks: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                Options.force_consistency_checks: 1
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                Options.report_bg_io_stats: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                               Options.ttl: 2592000
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :          Options.periodic_compaction_seconds: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :  Options.preclude_last_level_data_seconds: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                       Options.enable_blob_files: false
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                           Options.min_blob_size: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                          Options.blob_file_size: 268435456
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                   Options.blob_compression_type: NoCompression
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :          Options.enable_blob_garbage_collection: false
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :      Options.blob_garbage_collection_age_cutoff: 0.250000
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Options.blob_garbage_collection_force_threshold: 1.000000
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :          Options.blob_compaction_readahead_size: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                Options.blob_file_starting_level: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Options.experimental_mempurge_threshold: 0.000000
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : [/version_set.cc:5042] Recovered from manifest file:/tmp/hoodie_timeline_rocksdb/_user_hive_warehouse_stock_ticks_mor/55a052a7-3432-4009-afa1-a3df5149cc01/MANIFEST-000001 succeeded,manifest_file_number is 1, next_file_number is 3, last_sequence is 0, log_number is 0,prev_log_number is 0,max_column_family is 0,min_log_number_to_keep is 0

24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : [/version_set.cc:5051] Column family [default] (ID 0), log number is 0

24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : [/db_impl/db_impl_open.cc:532] DB ID: 302bc884-c4ab-4d0d-afff-e1647cbfd454

24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : [/version_set.cc:4513] Creating manifest 5

24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : [/db_impl/db_impl_open.cc:2024] SstFileManager instance 0x7f8ea0105a70
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : DB pointer 0x7f8ea0106040
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : [/db_impl/db_impl.cc:1070] ------- DUMPING STATS -------
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : [/db_impl/db_impl.cc:1071] 
** DB Stats **
Uptime(secs): 0.0 total, 0.0 interval
Cumulative writes: 0 writes, 0 keys, 0 commit groups, 0.0 writes per commit group, ingest: 0.00 GB, 0.00 MB/s
Cumulative WAL: 0 writes, 0 syncs, 0.00 writes per sync, written: 0.00 GB, 0.00 MB/s
Cumulative stall: 00:00:0.000 H:M:S, 0.0 percent
Interval writes: 0 writes, 0 keys, 0 commit groups, 0.0 writes per commit group, ingest: 0.00 MB, 0.00 MB/s
Interval WAL: 0 writes, 0 syncs, 0.00 writes per sync, written: 0.00 GB, 0.00 MB/s
Interval stall: 00:00:0.000 H:M:S, 0.0 percent

** Compaction Stats [default] **
Level    Files   Size     Score Read(GB)  Rn(GB) Rnp1(GB) Write(GB) Wnew(GB) Moved(GB) W-Amp Rd(MB/s) Wr(MB/s) Comp(sec) CompMergeCPU(sec) Comp(cnt) Avg(sec) KeyIn KeyDrop Rblob(GB) Wblob(GB)
------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
 Sum      0/0    0.00 KB   0.0      0.0     0.0      0.0       0.0      0.0       0.0   0.0      0.0      0.0      0.00              0.00         0    0.000       0      0       0.0       0.0
 Int      0/0    0.00 KB   0.0      0.0     0.0      0.0       0.0      0.0       0.0   0.0      0.0      0.0      0.00              0.00         0    0.000       0      0       0.0       0.0

** Compaction Stats [default] **
Priority    Files   Size     Score Read(GB)  Rn(GB) Rnp1(GB) Write(GB) Wnew(GB) Moved(GB) W-Amp Rd(MB/s) Wr(MB/s) Comp(sec) CompMergeCPU(sec) Comp(cnt) Avg(sec) KeyIn KeyDrop Rblob(GB) Wblob(GB)
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Blob file count: 0, total size: 0.0 GB, garbage size: 0.0 GB, space amp: 0.0

Uptime(secs): 0.0 total, 0.0 interval
Flush(GB): cumulative 0.000, interval 0.000
AddFile(GB): cumulative 0.000, interval 0.000
AddFile(Total Files): cumulative 0, interval 0
AddFile(L0 Files): cumulative 0, interval 0
AddFile(Keys): cumulative 0, interval 0
Cumulative compaction: 0.00 GB write, 0.00 MB/s write, 0.00 GB read, 0.00 MB/s read, 0.0 seconds
Interval compaction: 0.00 GB write, 0.00 MB/s write, 0.00 GB read, 0.00 MB/s read, 0.0 seconds
Stalls(count): 0 level0_slowdown, 0 level0_slowdown_with_compaction, 0 level0_numfiles, 0 level0_numfiles_with_compaction, 0 stop for pending_compaction_bytes, 0 slowdown for pending_compaction_bytes, 0 memtable_compaction, 0 memtable_slowdown, interval 0 total count
Block cache LRUCache@0x7f8ea01032d0#600 capacity: 8.00 MB collections: 1 last_copies: 0 last_secs: 4.6e-05 secs_since: 0
Block cache entry stats(count,size,portion): Misc(1,0.00 KB,0%)

** File Read Latency Histogram By Level [default] **

24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : [/db_impl/db_impl.cc:762] STATISTICS:
 rocksdb.block.cache.miss COUNT : 0
rocksdb.block.cache.hit COUNT : 0
rocksdb.block.cache.add COUNT : 0
rocksdb.block.cache.add.failures COUNT : 0
rocksdb.block.cache.index.miss COUNT : 0
rocksdb.block.cache.index.hit COUNT : 0
rocksdb.block.cache.index.add COUNT : 0
rocksdb.block.cache.index.bytes.insert COUNT : 0
rocksdb.block.cache.index.bytes.evict COUNT : 0
rocksdb.block.cache.filter.miss COUNT : 0
rocksdb.block.cache.filter.hit COUNT : 0
rocksdb.block.cache.filter.add COUNT : 0
rocksdb.block.cache.filter.bytes.insert COUNT : 0
rocksdb.block.cache.filter.bytes.evict COUNT : 0
rocksdb.block.cache.data.miss COUNT : 0
rocksdb.block.cache.data.hit COUNT : 0
rocksdb.block.cache.data.add COUNT : 0
rocksdb.block.cache.data.bytes.insert COUNT : 0
rocksdb.block.cache.bytes.read COUNT : 0
rocksdb.block.cache.bytes.write COUNT : 0
rocksdb.bloom.filter.useful COUNT : 0
rocksdb.bloom.filter.full.positive COUNT : 0
rocksdb.bloom.filter.full.true.positive COUNT : 0
rocksdb.bloom.filter.micros COUNT : 0
rocksdb.persistent.cache.hit COUNT : 0
rocksdb.persistent.cache.miss COUNT : 0
rocksdb.sim.block.cache.hit COUNT : 0
rocksdb.sim.block.cache.miss COUNT : 0
rocksdb.memtable.hit COUNT : 0
rocksdb.memtable.miss COUNT : 0
rocksdb.l0.hit COUNT : 0
rocksdb.l1.hit COUNT : 0
rocksdb.l2andup.hit COUNT : 0
rocksdb.compaction.key.drop.new COUNT : 0
rocksdb.compaction.key.drop.obsolete COUNT : 0
rocksdb.compaction.key.drop.range_del COUNT : 0
rocksdb.compaction.key.drop.user COUNT : 0
rocksdb.compaction.range_del.drop.obsolete COUNT : 0
rocksdb.compaction.optimized.del.drop.obsolete COUNT : 0
rocksdb.compaction.cancelled COUNT : 0
rocksdb.number.keys.written COUNT : 0
rocksdb.number.keys.read COUNT : 0
rocksdb.number.keys.updated COUNT : 0
rocksdb.bytes.written COUNT : 0
rocksdb.bytes.read COUNT : 0
rocksdb.number.db.seek COUNT : 0
rocksdb.number.db.next COUNT : 0
rocksdb.number.db.prev COUNT : 0
rocksdb.number.db.seek.found COUNT : 0
rocksdb.number.db.next.found COUNT : 0
rocksdb.number.db.prev.found COUNT : 0
rocksdb.db.iter.bytes.read COUNT : 0
rocksdb.no.file.closes COUNT : 0
rocksdb.no.file.opens COUNT : 0
rocksdb.no.file.errors COUNT : 0
rocksdb.l0.slowdown.micros COUNT : 0
rocksdb.memtable.compaction.micros COUNT : 0
rocksdb.l0.num.files.stall.micros COUNT : 0
rocksdb.stall.micros COUNT : 0
rocksdb.db.mutex.wait.micros COUNT : 0
rocksdb.rate.limit.delay.millis COUNT : 0
rocksdb.num.iterators COUNT : 0
rocksdb.number.multiget.get COUNT : 0
rocksdb.number.multiget.keys.read COUNT : 0
rocksdb.number.multiget.bytes.read COUNT : 0
rocksdb.number.deletes.filtered COUNT : 0
rocksdb.number.merge.failures COUNT : 0
rocksdb.bloom.filter.prefix.checked COUNT : 0
rocksdb.bloom.filter.prefix.useful COUNT : 0
rocksdb.number.reseeks.iteration COUNT : 0
rocksdb.getupdatessince.calls COUNT : 0
rocksdb.block.cachecompressed.miss COUNT : 0
rocksdb.block.cachecompressed.hit COUNT : 0
rocksdb.block.cachecompressed.add COUNT : 0
rocksdb.block.cachecompressed.add.failures COUNT : 0
rocksdb.wal.synced COUNT : 0
rocksdb.wal.bytes COUNT : 0
rocksdb.write.self COUNT : 0
rocksdb.write.other COUNT : 0
rocksdb.write.timeout COUNT : 0
rocksdb.write.wal COUNT : 0
rocksdb.compact.read.bytes COUNT : 0
rocksdb.compact.write.bytes COUNT : 0
rocksdb.flush.write.bytes COUNT : 0
rocksdb.compact.read.marked.bytes COUNT : 0
rocksdb.compact.read.periodic.bytes COUNT : 0
rocksdb.compact.read.ttl.bytes COUNT : 0
rocksdb.compact.write.marked.bytes COUNT : 0
rocksdb.compact.write.periodic.bytes COUNT : 0
rocksdb.compact.write.ttl.bytes COUNT : 0
rocksdb.number.direct.load.table.properties COUNT : 0
rocksdb.number.superversion_acquires COUNT : 0
rocksdb.number.superversion_releases COUNT : 0
rocksdb.number.superversion_cleanups COUNT : 0
rocksdb.number.block.compressed COUNT : 0
rocksdb.number.block.decompressed COUNT : 0
rocksdb.number.block.not_compressed COUNT : 0
rocksdb.merge.operation.time.nanos COUNT : 0
rocksdb.filter.operation.time.nanos COUNT : 0
rocksdb.row.cache.hit COUNT : 0
rocksdb.row.cache.miss COUNT : 0
rocksdb.read.amp.estimate.useful.bytes COUNT : 0
rocksdb.read.amp.total.read.bytes COUNT : 0
rocksdb.number.rate_limiter.drains COUNT : 0
rocksdb.number.iter.skip COUNT : 0
rocksdb.blobdb.num.put COUNT : 0
rocksdb.blobdb.num.write COUNT : 0
rocksdb.blobdb.num.get COUNT : 0
rocksdb.blobdb.num.multiget COUNT : 0
rocksdb.blobdb.num.seek COUNT : 0
rocksdb.blobdb.num.next COUNT : 0
rocksdb.blobdb.num.prev COUNT : 0
rocksdb.blobdb.num.keys.written COUNT : 0
rocksdb.blobdb.num.keys.read COUNT : 0
rocksdb.blobdb.bytes.written COUNT : 0
rocksdb.blobdb.bytes.read COUNT : 0
rocksdb.blobdb.write.inlined COUNT : 0
rocksdb.blobdb.write.inlined.ttl COUNT : 0
rocksdb.blobdb.write.blob COUNT : 0
rocksdb.blobdb.write.blob.ttl COUNT : 0
rocksdb.blobdb.blob.file.bytes.written COUNT : 0
rocksdb.blobdb.blob.file.bytes.read COUNT : 0
rocksdb.blobdb.blob.file.synced COUNT : 0
rocksdb.blobdb.blob.index.expired.count COUNT : 0
rocksdb.blobdb.blob.index.expired.size COUNT : 0
rocksdb.blobdb.blob.index.evicted.count COUNT : 0
rocksdb.blobdb.blob.index.evicted.size COUNT : 0
rocksdb.blobdb.gc.num.files COUNT : 0
rocksdb.blobdb.gc.num.new.files COUNT : 0
rocksdb.blobdb.gc.failures COUNT : 0
rocksdb.blobdb.gc.num.keys.overwritten COUNT : 0
rocksdb.blobdb.gc.num.keys.expired COUNT : 0
rocksdb.blobdb.gc.num.keys.relocated COUNT : 0
rocksdb.blobdb.gc.bytes.overwritten COUNT : 0
rocksdb.blobdb.gc.bytes.expired COUNT : 0
rocksdb.blobdb.gc.bytes.relocated COUNT : 0
rocksdb.blobdb.fifo.num.files.evicted COUNT : 0
rocksdb.blobdb.fifo.num.keys.evicted COUNT : 0
rocksdb.blobdb.fifo.bytes.evicted COUNT : 0
rocksdb.txn.overhead.mutex.prepare COUNT : 0
rocksdb.txn.overhead.mutex.old.commit.map COUNT : 0
rocksdb.txn.overhead.duplicate.key COUNT : 0
rocksdb.txn.overhead.mutex.snapshot COUNT : 0
rocksdb.txn.get.tryagain COUNT : 0
rocksdb.number.multiget.keys.found COUNT : 0
rocksdb.num.iterator.created COUNT : 0
rocksdb.num.iterator.deleted COUNT : 0
rocksdb.block.cache.compression.dict.miss COUNT : 0
rocksdb.block.cache.compression.dict.hit COUNT : 0
rocksdb.block.cache.compression.dict.add COUNT : 0
rocksdb.block.cache.compression.dict.bytes.insert COUNT : 0
rocksdb.block.cache.compression.dict.bytes.evict COUNT : 0
rocksdb.block.cache.add.redundant COUNT : 0
rocksdb.block.cache.index.add.redundant COUNT : 0
rocksdb.block.cache.filter.add.redundant COUNT : 0
rocksdb.block.cache.data.add.redundant COUNT : 0
rocksdb.block.cache.compression.dict.add.redundant COUNT : 0
rocksdb.files.marked.trash COUNT : 0
rocksdb.files.deleted.immediately COUNT : 0
rocksdb.error.handler.bg.errro.count COUNT : 0
rocksdb.error.handler.bg.io.errro.count COUNT : 0
rocksdb.error.handler.bg.retryable.io.errro.count COUNT : 0
rocksdb.error.handler.autoresume.count COUNT : 0
rocksdb.error.handler.autoresume.retry.total.count COUNT : 0
rocksdb.error.handler.autoresume.success.count COUNT : 0
rocksdb.memtable.payload.bytes.at.flush COUNT : 0
rocksdb.memtable.garbage.bytes.at.flush COUNT : 0
rocksdb.secondary.cache.hits COUNT : 0
rocksdb.verify_checksum.read.bytes COUNT : 0
rocksdb.backup.read.bytes COUNT : 0
rocksdb.backup.write.bytes COUNT : 0
rocksdb.remote.compact.read.bytes COUNT : 0
rocksdb.remote.compact.write.bytes COUNT : 0
rocksdb.hot.file.read.bytes COUNT : 0
rocksdb.warm.file.read.bytes COUNT : 0
rocksdb.cold.file.read.bytes COUNT : 0
rocksdb.hot.file.read.count COUNT : 0
rocksdb.warm.file.read.count COUNT : 0
rocksdb.cold.file.read.count COUNT : 0
rocksdb.last.level.read.bytes COUNT : 0
rocksdb.last.level.read.count COUNT : 0
rocksdb.non.last.level.read.bytes COUNT : 0
rocksdb.non.last.level.read.count COUNT : 0
rocksdb.block.checksum.compute.count COUNT : 0
rocksdb.multiget.coroutine.count COUNT : 0
rocksdb.blobdb.cache.miss COUNT : 0
rocksdb.blobdb.cache.hit COUNT : 0
rocksdb.blobdb.cache.add COUNT : 0
rocksdb.blobdb.cache.add.failures COUNT : 0
rocksdb.blobdb.cache.bytes.read COUNT : 0
rocksdb.blobdb.cache.bytes.write COUNT : 0
rocksdb.db.get.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.db.write.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.compaction.times.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.compaction.times.cpu_micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.subcompaction.setup.times.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.table.sync.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.compaction.outfile.sync.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.wal.file.sync.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.manifest.file.sync.micros P50 : 725.000000 P95 : 745.000000 P99 : 745.000000 P100 : 745.000000 COUNT : 2 SUM : 1444
rocksdb.table.open.io.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.db.multiget.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.read.block.compaction.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.read.block.get.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.write.raw.block.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.l0.slowdown.count P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.memtable.compaction.count P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.num.files.stall.count P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.hard.rate.limit.delay.count P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.soft.rate.limit.delay.count P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.numfiles.in.singlecompaction P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.db.seek.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.db.write.stall P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.sst.read.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.num.subcompactions.scheduled P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.bytes.per.read P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.bytes.per.write P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.bytes.per.multiget P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.bytes.compressed P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.bytes.decompressed P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.compression.times.nanos P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.decompression.times.nanos P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.read.num.merge_operands P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.blobdb.key.size P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.blobdb.value.size P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.blobdb.write.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.blobdb.get.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.blobdb.multiget.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.blobdb.seek.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.blobdb.next.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.blobdb.prev.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.blobdb.blob.file.write.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.blobdb.blob.file.read.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.blobdb.blob.file.sync.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.blobdb.gc.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.blobdb.compression.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.blobdb.decompression.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.db.flush.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.sst.batch.size P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.num.index.and.filter.blocks.read.per.level P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.num.data.blocks.read.per.level P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.num.sst.read.per.level P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.error.handler.autoresume.retry.count P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.async.read.bytes P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.poll.wait.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.prefetched.bytes.discarded P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.multiget.io.batch.size P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.num.level.read.per.multiget P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0

24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : [/column_family.cc:618] --------------- Options for column family [hudi_view__user_hive_warehouse_stock_ticks_mor]:

24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :               Options.comparator: leveldb.BytewiseComparator
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :           Options.merge_operator: None
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :        Options.compaction_filter: None
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :        Options.compaction_filter_factory: None
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :  Options.sst_partitioner_factory: None
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :         Options.memtable_factory: SkipListFactory
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :            Options.table_factory: BlockBasedTable
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :            table_factory options:   flush_block_policy_factory: FlushBlockBySizePolicyFactory (0x7f8ea01ece50)
  cache_index_and_filter_blocks: 0
  cache_index_and_filter_blocks_with_high_priority: 1
  pin_l0_filter_and_index_blocks_in_cache: 0
  pin_top_level_index_and_filter: 1
  index_type: 0
  data_block_index_type: 0
  index_shortening: 1
  data_block_hash_table_util_ratio: 0.750000
  checksum: 1
  no_block_cache: 0
  block_cache: 0x7f8ea01ed520
  block_cache_name: LRUCache
  block_cache_options:
    capacity : 8388608
    num_shard_bits : 4
    strict_capacity_limit : 0
    memory_allocator : None
    high_pri_pool_ratio: 0.000
  block_cache_compressed: (nil)
  persistent_cache: (nil)
  block_size: 4096
  block_size_deviation: 10
  block_restart_interval: 16
  index_block_restart_interval: 1
  metadata_block_size: 4096
  partition_filters: 0
  use_delta_encoding: 1
  filter_policy: nullptr
  whole_key_filtering: 1
  verify_compression: 0
  read_amp_bytes_per_bit: 0
  format_version: 5
  enable_index_compression: 1
  block_align: 0
  max_auto_readahead_size: 262144
  prepopulate_block_cache: 0
  initial_auto_readahead_size: 8192

24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :        Options.write_buffer_size: 67108864
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :  Options.max_write_buffer_number: 2
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :          Options.compression: Snappy
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression: Disabled
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :       Options.prefix_extractor: nullptr
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :   Options.memtable_insert_with_hint_prefix_extractor: nullptr
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :             Options.num_levels: 7
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :        Options.min_write_buffer_number_to_merge: 1
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :     Options.max_write_buffer_number_to_maintain: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :     Options.max_write_buffer_size_to_maintain: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :            Options.bottommost_compression_opts.window_bits: -14
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression_opts.level: 32767
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :               Options.bottommost_compression_opts.strategy: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.max_dict_bytes: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.zstd_max_train_bytes: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.parallel_threads: 1
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression_opts.enabled: false
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.max_dict_buffer_bytes: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.use_zstd_dict_trainer: true
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :            Options.compression_opts.window_bits: -14
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                  Options.compression_opts.level: 32767
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :               Options.compression_opts.strategy: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.max_dict_bytes: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.zstd_max_train_bytes: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.use_zstd_dict_trainer: true
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.parallel_threads: 1
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                  Options.compression_opts.enabled: false
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.max_dict_buffer_bytes: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :      Options.level0_file_num_compaction_trigger: 4
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :          Options.level0_slowdown_writes_trigger: 20
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :              Options.level0_stop_writes_trigger: 36
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                   Options.target_file_size_base: 67108864
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :             Options.target_file_size_multiplier: 1
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                Options.max_bytes_for_level_base: 268435456
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Options.level_compaction_dynamic_level_bytes: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :          Options.max_bytes_for_level_multiplier: 10.000000
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[0]: 1
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[1]: 1
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[2]: 1
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[3]: 1
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[4]: 1
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[5]: 1
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[6]: 1
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :       Options.max_sequential_skip_in_iterations: 8
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                    Options.max_compaction_bytes: 1677721600
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                        Options.arena_block_size: 1048576
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :   Options.soft_pending_compaction_bytes_limit: 68719476736
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :   Options.hard_pending_compaction_bytes_limit: 274877906944
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                Options.disable_auto_compactions: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                        Options.compaction_style: kCompactionStyleLevel
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                          Options.compaction_pri: kMinOverlappingRatio
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.size_ratio: 1
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.min_merge_width: 2
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.max_merge_width: 4294967295
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.max_size_amplification_percent: 200
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.compression_size_percent: -1
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.stop_style: kCompactionStopStyleTotalSize
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_fifo.max_table_files_size: 1073741824
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_fifo.allow_compaction: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                   Options.table_properties_collectors: 
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                   Options.inplace_update_support: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                 Options.inplace_update_num_locks: 10000
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :               Options.memtable_prefix_bloom_size_ratio: 0.000000
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :               Options.memtable_whole_key_filtering: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :   Options.memtable_huge_page_size: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                           Options.bloom_locality: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                    Options.max_successive_merges: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                Options.optimize_filters_for_hits: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                Options.paranoid_file_checks: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                Options.force_consistency_checks: 1
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                Options.report_bg_io_stats: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                               Options.ttl: 2592000
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :          Options.periodic_compaction_seconds: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :  Options.preclude_last_level_data_seconds: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                       Options.enable_blob_files: false
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                           Options.min_blob_size: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                          Options.blob_file_size: 268435456
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                   Options.blob_compression_type: NoCompression
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :          Options.enable_blob_garbage_collection: false
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :      Options.blob_garbage_collection_age_cutoff: 0.250000
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Options.blob_garbage_collection_force_threshold: 1.000000
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :          Options.blob_compaction_readahead_size: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                Options.blob_file_starting_level: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Options.experimental_mempurge_threshold: 0.000000
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : [/db_impl/db_impl.cc:2861] Created column family [hudi_view__user_hive_warehouse_stock_ticks_mor] (ID 1)
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : [/column_family.cc:618] --------------- Options for column family [hudi_pending_compaction__user_hive_warehouse_stock_ticks_mor]:

24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :               Options.comparator: leveldb.BytewiseComparator
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :           Options.merge_operator: None
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :        Options.compaction_filter: None
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :        Options.compaction_filter_factory: None
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :  Options.sst_partitioner_factory: None
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :         Options.memtable_factory: SkipListFactory
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :            Options.table_factory: BlockBasedTable
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :            table_factory options:   flush_block_policy_factory: FlushBlockBySizePolicyFactory (0x7f8ea0123030)
  cache_index_and_filter_blocks: 0
  cache_index_and_filter_blocks_with_high_priority: 1
  pin_l0_filter_and_index_blocks_in_cache: 0
  pin_top_level_index_and_filter: 1
  index_type: 0
  data_block_index_type: 0
  index_shortening: 1
  data_block_hash_table_util_ratio: 0.750000
  checksum: 1
  no_block_cache: 0
  block_cache: 0x7f8ea0130e70
  block_cache_name: LRUCache
  block_cache_options:
    capacity : 8388608
    num_shard_bits : 4
    strict_capacity_limit : 0
    memory_allocator : None
    high_pri_pool_ratio: 0.000
  block_cache_compressed: (nil)
  persistent_cache: (nil)
  block_size: 4096
  block_size_deviation: 10
  block_restart_interval: 16
  index_block_restart_interval: 1
  metadata_block_size: 4096
  partition_filters: 0
  use_delta_encoding: 1
  filter_policy: nullptr
  whole_key_filtering: 1
  verify_compression: 0
  read_amp_bytes_per_bit: 0
  format_version: 5
  enable_index_compression: 1
  block_align: 0
  max_auto_readahead_size: 262144
  prepopulate_block_cache: 0
  initial_auto_readahead_size: 8192

24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :        Options.write_buffer_size: 67108864
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :  Options.max_write_buffer_number: 2
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :          Options.compression: Snappy
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression: Disabled
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :       Options.prefix_extractor: nullptr
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :   Options.memtable_insert_with_hint_prefix_extractor: nullptr
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :             Options.num_levels: 7
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :        Options.min_write_buffer_number_to_merge: 1
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :     Options.max_write_buffer_number_to_maintain: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :     Options.max_write_buffer_size_to_maintain: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :            Options.bottommost_compression_opts.window_bits: -14
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression_opts.level: 32767
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :               Options.bottommost_compression_opts.strategy: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.max_dict_bytes: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.zstd_max_train_bytes: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.parallel_threads: 1
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression_opts.enabled: false
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.max_dict_buffer_bytes: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.use_zstd_dict_trainer: true
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :            Options.compression_opts.window_bits: -14
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                  Options.compression_opts.level: 32767
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :               Options.compression_opts.strategy: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.max_dict_bytes: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.zstd_max_train_bytes: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.use_zstd_dict_trainer: true
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.parallel_threads: 1
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                  Options.compression_opts.enabled: false
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.max_dict_buffer_bytes: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :      Options.level0_file_num_compaction_trigger: 4
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :          Options.level0_slowdown_writes_trigger: 20
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :              Options.level0_stop_writes_trigger: 36
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                   Options.target_file_size_base: 67108864
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :             Options.target_file_size_multiplier: 1
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                Options.max_bytes_for_level_base: 268435456
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Options.level_compaction_dynamic_level_bytes: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :          Options.max_bytes_for_level_multiplier: 10.000000
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[0]: 1
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[1]: 1
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[2]: 1
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[3]: 1
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[4]: 1
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[5]: 1
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[6]: 1
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :       Options.max_sequential_skip_in_iterations: 8
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                    Options.max_compaction_bytes: 1677721600
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                        Options.arena_block_size: 1048576
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :   Options.soft_pending_compaction_bytes_limit: 68719476736
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :   Options.hard_pending_compaction_bytes_limit: 274877906944
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                Options.disable_auto_compactions: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                        Options.compaction_style: kCompactionStyleLevel
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                          Options.compaction_pri: kMinOverlappingRatio
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.size_ratio: 1
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.min_merge_width: 2
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.max_merge_width: 4294967295
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.max_size_amplification_percent: 200
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.compression_size_percent: -1
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.stop_style: kCompactionStopStyleTotalSize
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_fifo.max_table_files_size: 1073741824
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_fifo.allow_compaction: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                   Options.table_properties_collectors: 
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                   Options.inplace_update_support: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                 Options.inplace_update_num_locks: 10000
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :               Options.memtable_prefix_bloom_size_ratio: 0.000000
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :               Options.memtable_whole_key_filtering: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :   Options.memtable_huge_page_size: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                           Options.bloom_locality: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                    Options.max_successive_merges: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                Options.optimize_filters_for_hits: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                Options.paranoid_file_checks: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                Options.force_consistency_checks: 1
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                Options.report_bg_io_stats: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                               Options.ttl: 2592000
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :          Options.periodic_compaction_seconds: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :  Options.preclude_last_level_data_seconds: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                       Options.enable_blob_files: false
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                           Options.min_blob_size: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                          Options.blob_file_size: 268435456
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                   Options.blob_compression_type: NoCompression
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :          Options.enable_blob_garbage_collection: false
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :      Options.blob_garbage_collection_age_cutoff: 0.250000
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Options.blob_garbage_collection_force_threshold: 1.000000
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :          Options.blob_compaction_readahead_size: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                Options.blob_file_starting_level: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Options.experimental_mempurge_threshold: 0.000000
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : [/db_impl/db_impl.cc:2861] Created column family [hudi_pending_compaction__user_hive_warehouse_stock_ticks_mor] (ID 2)
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : [/column_family.cc:618] --------------- Options for column family [hudi_pending_log_compaction__user_hive_warehouse_stock_ticks_mor]:

24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :               Options.comparator: leveldb.BytewiseComparator
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :           Options.merge_operator: None
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :        Options.compaction_filter: None
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :        Options.compaction_filter_factory: None
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :  Options.sst_partitioner_factory: None
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :         Options.memtable_factory: SkipListFactory
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :            Options.table_factory: BlockBasedTable
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :            table_factory options:   flush_block_policy_factory: FlushBlockBySizePolicyFactory (0x7f8ea01321a0)
  cache_index_and_filter_blocks: 0
  cache_index_and_filter_blocks_with_high_priority: 1
  pin_l0_filter_and_index_blocks_in_cache: 0
  pin_top_level_index_and_filter: 1
  index_type: 0
  data_block_index_type: 0
  index_shortening: 1
  data_block_hash_table_util_ratio: 0.750000
  checksum: 1
  no_block_cache: 0
  block_cache: 0x7f8ea0040bf0
  block_cache_name: LRUCache
  block_cache_options:
    capacity : 8388608
    num_shard_bits : 4
    strict_capacity_limit : 0
    memory_allocator : None
    high_pri_pool_ratio: 0.000
  block_cache_compressed: (nil)
  persistent_cache: (nil)
  block_size: 4096
  block_size_deviation: 10
  block_restart_interval: 16
  index_block_restart_interval: 1
  metadata_block_size: 4096
  partition_filters: 0
  use_delta_encoding: 1
  filter_policy: nullptr
  whole_key_filtering: 1
  verify_compression: 0
  read_amp_bytes_per_bit: 0
  format_version: 5
  enable_index_compression: 1
  block_align: 0
  max_auto_readahead_size: 262144
  prepopulate_block_cache: 0
  initial_auto_readahead_size: 8192

24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :        Options.write_buffer_size: 67108864
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :  Options.max_write_buffer_number: 2
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :          Options.compression: Snappy
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression: Disabled
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :       Options.prefix_extractor: nullptr
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :   Options.memtable_insert_with_hint_prefix_extractor: nullptr
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :             Options.num_levels: 7
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :        Options.min_write_buffer_number_to_merge: 1
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :     Options.max_write_buffer_number_to_maintain: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :     Options.max_write_buffer_size_to_maintain: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :            Options.bottommost_compression_opts.window_bits: -14
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression_opts.level: 32767
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :               Options.bottommost_compression_opts.strategy: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.max_dict_bytes: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.zstd_max_train_bytes: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.parallel_threads: 1
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression_opts.enabled: false
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.max_dict_buffer_bytes: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.use_zstd_dict_trainer: true
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :            Options.compression_opts.window_bits: -14
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                  Options.compression_opts.level: 32767
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :               Options.compression_opts.strategy: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.max_dict_bytes: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.zstd_max_train_bytes: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.use_zstd_dict_trainer: true
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.parallel_threads: 1
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                  Options.compression_opts.enabled: false
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.max_dict_buffer_bytes: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :      Options.level0_file_num_compaction_trigger: 4
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :          Options.level0_slowdown_writes_trigger: 20
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :              Options.level0_stop_writes_trigger: 36
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                   Options.target_file_size_base: 67108864
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :             Options.target_file_size_multiplier: 1
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                Options.max_bytes_for_level_base: 268435456
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Options.level_compaction_dynamic_level_bytes: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :          Options.max_bytes_for_level_multiplier: 10.000000
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[0]: 1
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[1]: 1
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[2]: 1
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[3]: 1
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[4]: 1
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[5]: 1
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[6]: 1
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :       Options.max_sequential_skip_in_iterations: 8
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                    Options.max_compaction_bytes: 1677721600
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                        Options.arena_block_size: 1048576
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :   Options.soft_pending_compaction_bytes_limit: 68719476736
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :   Options.hard_pending_compaction_bytes_limit: 274877906944
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                Options.disable_auto_compactions: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                        Options.compaction_style: kCompactionStyleLevel
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                          Options.compaction_pri: kMinOverlappingRatio
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.size_ratio: 1
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.min_merge_width: 2
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.max_merge_width: 4294967295
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.max_size_amplification_percent: 200
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.compression_size_percent: -1
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.stop_style: kCompactionStopStyleTotalSize
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_fifo.max_table_files_size: 1073741824
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_fifo.allow_compaction: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                   Options.table_properties_collectors: 
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                   Options.inplace_update_support: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                 Options.inplace_update_num_locks: 10000
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :               Options.memtable_prefix_bloom_size_ratio: 0.000000
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :               Options.memtable_whole_key_filtering: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :   Options.memtable_huge_page_size: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                           Options.bloom_locality: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                    Options.max_successive_merges: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                Options.optimize_filters_for_hits: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                Options.paranoid_file_checks: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                Options.force_consistency_checks: 1
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                Options.report_bg_io_stats: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                               Options.ttl: 2592000
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :          Options.periodic_compaction_seconds: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :  Options.preclude_last_level_data_seconds: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                       Options.enable_blob_files: false
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                           Options.min_blob_size: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                          Options.blob_file_size: 268435456
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                   Options.blob_compression_type: NoCompression
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :          Options.enable_blob_garbage_collection: false
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :      Options.blob_garbage_collection_age_cutoff: 0.250000
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Options.blob_garbage_collection_force_threshold: 1.000000
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :          Options.blob_compaction_readahead_size: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                Options.blob_file_starting_level: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Options.experimental_mempurge_threshold: 0.000000
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : [/db_impl/db_impl.cc:2861] Created column family [hudi_pending_log_compaction__user_hive_warehouse_stock_ticks_mor] (ID 3)
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : [/column_family.cc:618] --------------- Options for column family [hudi_bootstrap_basefile__user_hive_warehouse_stock_ticks_mor]:

24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :               Options.comparator: leveldb.BytewiseComparator
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :           Options.merge_operator: None
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :        Options.compaction_filter: None
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :        Options.compaction_filter_factory: None
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :  Options.sst_partitioner_factory: None
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :         Options.memtable_factory: SkipListFactory
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :            Options.table_factory: BlockBasedTable
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :            table_factory options:   flush_block_policy_factory: FlushBlockBySizePolicyFactory (0x7f8ea015e4c0)
  cache_index_and_filter_blocks: 0
  cache_index_and_filter_blocks_with_high_priority: 1
  pin_l0_filter_and_index_blocks_in_cache: 0
  pin_top_level_index_and_filter: 1
  index_type: 0
  data_block_index_type: 0
  index_shortening: 1
  data_block_hash_table_util_ratio: 0.750000
  checksum: 1
  no_block_cache: 0
  block_cache: 0x7f8ea015e510
  block_cache_name: LRUCache
  block_cache_options:
    capacity : 8388608
    num_shard_bits : 4
    strict_capacity_limit : 0
    memory_allocator : None
    high_pri_pool_ratio: 0.000
  block_cache_compressed: (nil)
  persistent_cache: (nil)
  block_size: 4096
  block_size_deviation: 10
  block_restart_interval: 16
  index_block_restart_interval: 1
  metadata_block_size: 4096
  partition_filters: 0
  use_delta_encoding: 1
  filter_policy: nullptr
  whole_key_filtering: 1
  verify_compression: 0
  read_amp_bytes_per_bit: 0
  format_version: 5
  enable_index_compression: 1
  block_align: 0
  max_auto_readahead_size: 262144
  prepopulate_block_cache: 0
  initial_auto_readahead_size: 8192

24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :        Options.write_buffer_size: 67108864
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :  Options.max_write_buffer_number: 2
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :          Options.compression: Snappy
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression: Disabled
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :       Options.prefix_extractor: nullptr
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :   Options.memtable_insert_with_hint_prefix_extractor: nullptr
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :             Options.num_levels: 7
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :        Options.min_write_buffer_number_to_merge: 1
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :     Options.max_write_buffer_number_to_maintain: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :     Options.max_write_buffer_size_to_maintain: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :            Options.bottommost_compression_opts.window_bits: -14
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression_opts.level: 32767
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :               Options.bottommost_compression_opts.strategy: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.max_dict_bytes: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.zstd_max_train_bytes: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.parallel_threads: 1
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression_opts.enabled: false
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.max_dict_buffer_bytes: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.use_zstd_dict_trainer: true
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :            Options.compression_opts.window_bits: -14
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                  Options.compression_opts.level: 32767
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :               Options.compression_opts.strategy: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.max_dict_bytes: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.zstd_max_train_bytes: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.use_zstd_dict_trainer: true
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.parallel_threads: 1
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                  Options.compression_opts.enabled: false
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.max_dict_buffer_bytes: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :      Options.level0_file_num_compaction_trigger: 4
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :          Options.level0_slowdown_writes_trigger: 20
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :              Options.level0_stop_writes_trigger: 36
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                   Options.target_file_size_base: 67108864
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :             Options.target_file_size_multiplier: 1
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                Options.max_bytes_for_level_base: 268435456
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Options.level_compaction_dynamic_level_bytes: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :          Options.max_bytes_for_level_multiplier: 10.000000
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[0]: 1
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[1]: 1
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[2]: 1
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[3]: 1
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[4]: 1
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[5]: 1
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[6]: 1
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :       Options.max_sequential_skip_in_iterations: 8
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                    Options.max_compaction_bytes: 1677721600
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                        Options.arena_block_size: 1048576
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :   Options.soft_pending_compaction_bytes_limit: 68719476736
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :   Options.hard_pending_compaction_bytes_limit: 274877906944
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                Options.disable_auto_compactions: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                        Options.compaction_style: kCompactionStyleLevel
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                          Options.compaction_pri: kMinOverlappingRatio
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.size_ratio: 1
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.min_merge_width: 2
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.max_merge_width: 4294967295
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.max_size_amplification_percent: 200
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.compression_size_percent: -1
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.stop_style: kCompactionStopStyleTotalSize
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_fifo.max_table_files_size: 1073741824
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_fifo.allow_compaction: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                   Options.table_properties_collectors: 
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                   Options.inplace_update_support: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                 Options.inplace_update_num_locks: 10000
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :               Options.memtable_prefix_bloom_size_ratio: 0.000000
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :               Options.memtable_whole_key_filtering: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :   Options.memtable_huge_page_size: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                           Options.bloom_locality: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                    Options.max_successive_merges: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                Options.optimize_filters_for_hits: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                Options.paranoid_file_checks: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                Options.force_consistency_checks: 1
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                Options.report_bg_io_stats: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                               Options.ttl: 2592000
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :          Options.periodic_compaction_seconds: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :  Options.preclude_last_level_data_seconds: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                       Options.enable_blob_files: false
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                           Options.min_blob_size: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                          Options.blob_file_size: 268435456
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                   Options.blob_compression_type: NoCompression
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :          Options.enable_blob_garbage_collection: false
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :      Options.blob_garbage_collection_age_cutoff: 0.250000
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Options.blob_garbage_collection_force_threshold: 1.000000
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :          Options.blob_compaction_readahead_size: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                Options.blob_file_starting_level: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Options.experimental_mempurge_threshold: 0.000000
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : [/db_impl/db_impl.cc:2861] Created column family [hudi_bootstrap_basefile__user_hive_warehouse_stock_ticks_mor] (ID 4)
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : [/column_family.cc:618] --------------- Options for column family [hudi_partitions__user_hive_warehouse_stock_ticks_mor]:

24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :               Options.comparator: leveldb.BytewiseComparator
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :           Options.merge_operator: None
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :        Options.compaction_filter: None
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :        Options.compaction_filter_factory: None
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :  Options.sst_partitioner_factory: None
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :         Options.memtable_factory: SkipListFactory
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :            Options.table_factory: BlockBasedTable
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :            table_factory options:   flush_block_policy_factory: FlushBlockBySizePolicyFactory (0x7f8ea016f240)
  cache_index_and_filter_blocks: 0
  cache_index_and_filter_blocks_with_high_priority: 1
  pin_l0_filter_and_index_blocks_in_cache: 0
  pin_top_level_index_and_filter: 1
  index_type: 0
  data_block_index_type: 0
  index_shortening: 1
  data_block_hash_table_util_ratio: 0.750000
  checksum: 1
  no_block_cache: 0
  block_cache: 0x7f8ea016f290
  block_cache_name: LRUCache
  block_cache_options:
    capacity : 8388608
    num_shard_bits : 4
    strict_capacity_limit : 0
    memory_allocator : None
    high_pri_pool_ratio: 0.000
  block_cache_compressed: (nil)
  persistent_cache: (nil)
  block_size: 4096
  block_size_deviation: 10
  block_restart_interval: 16
  index_block_restart_interval: 1
  metadata_block_size: 4096
  partition_filters: 0
  use_delta_encoding: 1
  filter_policy: nullptr
  whole_key_filtering: 1
  verify_compression: 0
  read_amp_bytes_per_bit: 0
  format_version: 5
  enable_index_compression: 1
  block_align: 0
  max_auto_readahead_size: 262144
  prepopulate_block_cache: 0
  initial_auto_readahead_size: 8192

24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :        Options.write_buffer_size: 67108864
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :  Options.max_write_buffer_number: 2
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :          Options.compression: Snappy
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression: Disabled
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :       Options.prefix_extractor: nullptr
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :   Options.memtable_insert_with_hint_prefix_extractor: nullptr
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :             Options.num_levels: 7
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :        Options.min_write_buffer_number_to_merge: 1
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :     Options.max_write_buffer_number_to_maintain: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :     Options.max_write_buffer_size_to_maintain: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :            Options.bottommost_compression_opts.window_bits: -14
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression_opts.level: 32767
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :               Options.bottommost_compression_opts.strategy: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.max_dict_bytes: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.zstd_max_train_bytes: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.parallel_threads: 1
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression_opts.enabled: false
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.max_dict_buffer_bytes: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.use_zstd_dict_trainer: true
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :            Options.compression_opts.window_bits: -14
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                  Options.compression_opts.level: 32767
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :               Options.compression_opts.strategy: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.max_dict_bytes: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.zstd_max_train_bytes: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.use_zstd_dict_trainer: true
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.parallel_threads: 1
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                  Options.compression_opts.enabled: false
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.max_dict_buffer_bytes: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :      Options.level0_file_num_compaction_trigger: 4
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :          Options.level0_slowdown_writes_trigger: 20
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :              Options.level0_stop_writes_trigger: 36
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                   Options.target_file_size_base: 67108864
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :             Options.target_file_size_multiplier: 1
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                Options.max_bytes_for_level_base: 268435456
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Options.level_compaction_dynamic_level_bytes: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :          Options.max_bytes_for_level_multiplier: 10.000000
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[0]: 1
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[1]: 1
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[2]: 1
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[3]: 1
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[4]: 1
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[5]: 1
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[6]: 1
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :       Options.max_sequential_skip_in_iterations: 8
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                    Options.max_compaction_bytes: 1677721600
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                        Options.arena_block_size: 1048576
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :   Options.soft_pending_compaction_bytes_limit: 68719476736
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :   Options.hard_pending_compaction_bytes_limit: 274877906944
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                Options.disable_auto_compactions: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                        Options.compaction_style: kCompactionStyleLevel
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                          Options.compaction_pri: kMinOverlappingRatio
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.size_ratio: 1
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.min_merge_width: 2
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.max_merge_width: 4294967295
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.max_size_amplification_percent: 200
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.compression_size_percent: -1
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.stop_style: kCompactionStopStyleTotalSize
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_fifo.max_table_files_size: 1073741824
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_fifo.allow_compaction: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                   Options.table_properties_collectors: 
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                   Options.inplace_update_support: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                 Options.inplace_update_num_locks: 10000
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :               Options.memtable_prefix_bloom_size_ratio: 0.000000
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :               Options.memtable_whole_key_filtering: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :   Options.memtable_huge_page_size: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                           Options.bloom_locality: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                    Options.max_successive_merges: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                Options.optimize_filters_for_hits: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                Options.paranoid_file_checks: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                Options.force_consistency_checks: 1
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                Options.report_bg_io_stats: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                               Options.ttl: 2592000
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :          Options.periodic_compaction_seconds: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :  Options.preclude_last_level_data_seconds: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                       Options.enable_blob_files: false
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                           Options.min_blob_size: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                          Options.blob_file_size: 268435456
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                   Options.blob_compression_type: NoCompression
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :          Options.enable_blob_garbage_collection: false
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :      Options.blob_garbage_collection_age_cutoff: 0.250000
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Options.blob_garbage_collection_force_threshold: 1.000000
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :          Options.blob_compaction_readahead_size: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                Options.blob_file_starting_level: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Options.experimental_mempurge_threshold: 0.000000
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : [/db_impl/db_impl.cc:2861] Created column family [hudi_partitions__user_hive_warehouse_stock_ticks_mor] (ID 5)
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : [/column_family.cc:618] --------------- Options for column family [hudi_replaced_fg_user_hive_warehouse_stock_ticks_mor]:

24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :               Options.comparator: leveldb.BytewiseComparator
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :           Options.merge_operator: None
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :        Options.compaction_filter: None
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :        Options.compaction_filter_factory: None
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :  Options.sst_partitioner_factory: None
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :         Options.memtable_factory: SkipListFactory
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :            Options.table_factory: BlockBasedTable
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :            table_factory options:   flush_block_policy_factory: FlushBlockBySizePolicyFactory (0x7f8ea01798f0)
  cache_index_and_filter_blocks: 0
  cache_index_and_filter_blocks_with_high_priority: 1
  pin_l0_filter_and_index_blocks_in_cache: 0
  pin_top_level_index_and_filter: 1
  index_type: 0
  data_block_index_type: 0
  index_shortening: 1
  data_block_hash_table_util_ratio: 0.750000
  checksum: 1
  no_block_cache: 0
  block_cache: 0x7f8ea0179940
  block_cache_name: LRUCache
  block_cache_options:
    capacity : 8388608
    num_shard_bits : 4
    strict_capacity_limit : 0
    memory_allocator : None
    high_pri_pool_ratio: 0.000
  block_cache_compressed: (nil)
  persistent_cache: (nil)
  block_size: 4096
  block_size_deviation: 10
  block_restart_interval: 16
  index_block_restart_interval: 1
  metadata_block_size: 4096
  partition_filters: 0
  use_delta_encoding: 1
  filter_policy: nullptr
  whole_key_filtering: 1
  verify_compression: 0
  read_amp_bytes_per_bit: 0
  format_version: 5
  enable_index_compression: 1
  block_align: 0
  max_auto_readahead_size: 262144
  prepopulate_block_cache: 0
  initial_auto_readahead_size: 8192

24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :        Options.write_buffer_size: 67108864
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :  Options.max_write_buffer_number: 2
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :          Options.compression: Snappy
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression: Disabled
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :       Options.prefix_extractor: nullptr
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :   Options.memtable_insert_with_hint_prefix_extractor: nullptr
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :             Options.num_levels: 7
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :        Options.min_write_buffer_number_to_merge: 1
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :     Options.max_write_buffer_number_to_maintain: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :     Options.max_write_buffer_size_to_maintain: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :            Options.bottommost_compression_opts.window_bits: -14
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression_opts.level: 32767
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :               Options.bottommost_compression_opts.strategy: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.max_dict_bytes: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.zstd_max_train_bytes: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.parallel_threads: 1
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression_opts.enabled: false
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.max_dict_buffer_bytes: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.use_zstd_dict_trainer: true
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :            Options.compression_opts.window_bits: -14
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                  Options.compression_opts.level: 32767
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :               Options.compression_opts.strategy: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.max_dict_bytes: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.zstd_max_train_bytes: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.use_zstd_dict_trainer: true
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.parallel_threads: 1
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                  Options.compression_opts.enabled: false
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.max_dict_buffer_bytes: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :      Options.level0_file_num_compaction_trigger: 4
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :          Options.level0_slowdown_writes_trigger: 20
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :              Options.level0_stop_writes_trigger: 36
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                   Options.target_file_size_base: 67108864
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :             Options.target_file_size_multiplier: 1
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                Options.max_bytes_for_level_base: 268435456
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Options.level_compaction_dynamic_level_bytes: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :          Options.max_bytes_for_level_multiplier: 10.000000
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[0]: 1
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[1]: 1
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[2]: 1
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[3]: 1
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[4]: 1
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[5]: 1
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[6]: 1
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :       Options.max_sequential_skip_in_iterations: 8
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                    Options.max_compaction_bytes: 1677721600
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                        Options.arena_block_size: 1048576
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :   Options.soft_pending_compaction_bytes_limit: 68719476736
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :   Options.hard_pending_compaction_bytes_limit: 274877906944
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                Options.disable_auto_compactions: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                        Options.compaction_style: kCompactionStyleLevel
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                          Options.compaction_pri: kMinOverlappingRatio
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.size_ratio: 1
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.min_merge_width: 2
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.max_merge_width: 4294967295
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.max_size_amplification_percent: 200
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.compression_size_percent: -1
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.stop_style: kCompactionStopStyleTotalSize
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_fifo.max_table_files_size: 1073741824
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_fifo.allow_compaction: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                   Options.table_properties_collectors: 
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                   Options.inplace_update_support: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                 Options.inplace_update_num_locks: 10000
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :               Options.memtable_prefix_bloom_size_ratio: 0.000000
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :               Options.memtable_whole_key_filtering: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :   Options.memtable_huge_page_size: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                           Options.bloom_locality: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                    Options.max_successive_merges: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                Options.optimize_filters_for_hits: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                Options.paranoid_file_checks: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                Options.force_consistency_checks: 1
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                Options.report_bg_io_stats: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                               Options.ttl: 2592000
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :          Options.periodic_compaction_seconds: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :  Options.preclude_last_level_data_seconds: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                       Options.enable_blob_files: false
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                           Options.min_blob_size: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                          Options.blob_file_size: 268435456
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                   Options.blob_compression_type: NoCompression
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :          Options.enable_blob_garbage_collection: false
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :      Options.blob_garbage_collection_age_cutoff: 0.250000
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Options.blob_garbage_collection_force_threshold: 1.000000
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :          Options.blob_compaction_readahead_size: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                Options.blob_file_starting_level: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Options.experimental_mempurge_threshold: 0.000000
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : [/db_impl/db_impl.cc:2861] Created column family [hudi_replaced_fg_user_hive_warehouse_stock_ticks_mor] (ID 6)
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : [/column_family.cc:618] --------------- Options for column family [hudi_pending_clustering_fg_user_hive_warehouse_stock_ticks_mor]:

24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :               Options.comparator: leveldb.BytewiseComparator
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :           Options.merge_operator: None
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :        Options.compaction_filter: None
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :        Options.compaction_filter_factory: None
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :  Options.sst_partitioner_factory: None
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :         Options.memtable_factory: SkipListFactory
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :            Options.table_factory: BlockBasedTable
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :            table_factory options:   flush_block_policy_factory: FlushBlockBySizePolicyFactory (0x7f8ea01960a0)
  cache_index_and_filter_blocks: 0
  cache_index_and_filter_blocks_with_high_priority: 1
  pin_l0_filter_and_index_blocks_in_cache: 0
  pin_top_level_index_and_filter: 1
  index_type: 0
  data_block_index_type: 0
  index_shortening: 1
  data_block_hash_table_util_ratio: 0.750000
  checksum: 1
  no_block_cache: 0
  block_cache: 0x7f8ea01960f0
  block_cache_name: LRUCache
  block_cache_options:
    capacity : 8388608
    num_shard_bits : 4
    strict_capacity_limit : 0
    memory_allocator : None
    high_pri_pool_ratio: 0.000
  block_cache_compressed: (nil)
  persistent_cache: (nil)
  block_size: 4096
  block_size_deviation: 10
  block_restart_interval: 16
  index_block_restart_interval: 1
  metadata_block_size: 4096
  partition_filters: 0
  use_delta_encoding: 1
  filter_policy: nullptr
  whole_key_filtering: 1
  verify_compression: 0
  read_amp_bytes_per_bit: 0
  format_version: 5
  enable_index_compression: 1
  block_align: 0
  max_auto_readahead_size: 262144
  prepopulate_block_cache: 0
  initial_auto_readahead_size: 8192

24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :        Options.write_buffer_size: 67108864
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :  Options.max_write_buffer_number: 2
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :          Options.compression: Snappy
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression: Disabled
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :       Options.prefix_extractor: nullptr
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :   Options.memtable_insert_with_hint_prefix_extractor: nullptr
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :             Options.num_levels: 7
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :        Options.min_write_buffer_number_to_merge: 1
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :     Options.max_write_buffer_number_to_maintain: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :     Options.max_write_buffer_size_to_maintain: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :            Options.bottommost_compression_opts.window_bits: -14
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression_opts.level: 32767
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :               Options.bottommost_compression_opts.strategy: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.max_dict_bytes: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.zstd_max_train_bytes: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.parallel_threads: 1
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression_opts.enabled: false
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.max_dict_buffer_bytes: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.use_zstd_dict_trainer: true
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :            Options.compression_opts.window_bits: -14
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                  Options.compression_opts.level: 32767
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :               Options.compression_opts.strategy: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.max_dict_bytes: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.zstd_max_train_bytes: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.use_zstd_dict_trainer: true
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.parallel_threads: 1
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                  Options.compression_opts.enabled: false
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.max_dict_buffer_bytes: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :      Options.level0_file_num_compaction_trigger: 4
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :          Options.level0_slowdown_writes_trigger: 20
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :              Options.level0_stop_writes_trigger: 36
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                   Options.target_file_size_base: 67108864
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :             Options.target_file_size_multiplier: 1
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                Options.max_bytes_for_level_base: 268435456
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Options.level_compaction_dynamic_level_bytes: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :          Options.max_bytes_for_level_multiplier: 10.000000
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[0]: 1
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[1]: 1
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[2]: 1
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[3]: 1
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[4]: 1
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[5]: 1
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[6]: 1
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :       Options.max_sequential_skip_in_iterations: 8
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                    Options.max_compaction_bytes: 1677721600
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                        Options.arena_block_size: 1048576
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :   Options.soft_pending_compaction_bytes_limit: 68719476736
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :   Options.hard_pending_compaction_bytes_limit: 274877906944
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                Options.disable_auto_compactions: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                        Options.compaction_style: kCompactionStyleLevel
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                          Options.compaction_pri: kMinOverlappingRatio
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.size_ratio: 1
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.min_merge_width: 2
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.max_merge_width: 4294967295
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.max_size_amplification_percent: 200
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.compression_size_percent: -1
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.stop_style: kCompactionStopStyleTotalSize
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_fifo.max_table_files_size: 1073741824
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_fifo.allow_compaction: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                   Options.table_properties_collectors: 
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                   Options.inplace_update_support: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                 Options.inplace_update_num_locks: 10000
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :               Options.memtable_prefix_bloom_size_ratio: 0.000000
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :               Options.memtable_whole_key_filtering: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :   Options.memtable_huge_page_size: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                           Options.bloom_locality: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                    Options.max_successive_merges: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                Options.optimize_filters_for_hits: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                Options.paranoid_file_checks: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                Options.force_consistency_checks: 1
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                Options.report_bg_io_stats: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                               Options.ttl: 2592000
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :          Options.periodic_compaction_seconds: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :  Options.preclude_last_level_data_seconds: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                       Options.enable_blob_files: false
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                           Options.min_blob_size: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                          Options.blob_file_size: 268435456
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                   Options.blob_compression_type: NoCompression
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :          Options.enable_blob_garbage_collection: false
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :      Options.blob_garbage_collection_age_cutoff: 0.250000
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Options.blob_garbage_collection_force_threshold: 1.000000
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :          Options.blob_compaction_readahead_size: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB :                Options.blob_file_starting_level: 0
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : Options.experimental_mempurge_threshold: 0.000000
24/01/05 02:13:57 INFO collection.RocksDBDAO: From Rocks DB : [/db_impl/db_impl.cc:2861] Created column family [hudi_pending_clustering_fg_user_hive_warehouse_stock_ticks_mor] (ID 7)
24/01/05 02:13:57 INFO view.RocksDbBasedFileSystemView: Resetting replacedFileGroups to ROCKSDB based file-system view at /tmp/hoodie_timeline_rocksdb, Total file-groups=0
24/01/05 02:13:57 INFO collection.RocksDBDAO: Prefix DELETE (query=part=) on hudi_replaced_fg_user_hive_warehouse_stock_ticks_mor
24/01/05 02:13:57 INFO view.RocksDbBasedFileSystemView: Resetting replacedFileGroups to ROCKSDB based file-system view complete
24/01/05 02:13:57 INFO view.AbstractTableFileSystemView: Took 3 ms to read  0 instants, 0 replaced file groups
24/01/05 02:13:57 INFO view.RocksDbBasedFileSystemView: Initializing pending compaction operations. Count=0
24/01/05 02:13:57 INFO view.RocksDbBasedFileSystemView: Initializing pending Log compaction operations. Count=0
24/01/05 02:13:57 INFO view.RocksDbBasedFileSystemView: Initializing external data file mapping. Count=0
24/01/05 02:13:57 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:13:57 INFO view.RocksDbBasedFileSystemView: Resetting file groups in pending clustering to ROCKSDB based file-system view at /tmp/hoodie_timeline_rocksdb, Total file-groups=0
24/01/05 02:13:57 INFO collection.RocksDBDAO: Prefix DELETE (query=part=) on hudi_pending_clustering_fg_user_hive_warehouse_stock_ticks_mor
24/01/05 02:13:57 INFO view.RocksDbBasedFileSystemView: Resetting replacedFileGroups to ROCKSDB based file-system view complete
24/01/05 02:13:57 INFO view.RocksDbBasedFileSystemView: Created ROCKSDB based file-system view at /tmp/hoodie_timeline_rocksdb
24/01/05 02:13:57 INFO collection.RocksDBDAO: Prefix Search for (query=) on hudi_pending_compaction__user_hive_warehouse_stock_ticks_mor. Total Time Taken (msec)=0. Serialization Time taken(micro)=0, num entries=0
24/01/05 02:13:57 INFO view.RemoteHoodieTableFileSystemView: Sending request : (http://adhoc-1:34001/v1/hoodie/view/logcompactions/pending/?basepath=%2Fuser%2Fhive%2Fwarehouse%2Fstock_ticks_mor&lastinstantts=20240105021347452&timelinehash=4a3ab074d69f26b49cb5f0dfceaf1fd8f4760ea789bb800ca6cea65c8323c7a6)
24/01/05 02:13:57 INFO collection.RocksDBDAO: Prefix Search for (query=) on hudi_pending_log_compaction__user_hive_warehouse_stock_ticks_mor. Total Time Taken (msec)=0. Serialization Time taken(micro)=0, num entries=0
24/01/05 02:13:57 INFO clean.CleanPlanner: No earliest commit to retain. No need to scan partitions !!
24/01/05 02:13:57 INFO clean.CleanPlanActionExecutor: Nothing to clean here. It is already clean
24/01/05 02:13:57 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105021347452__deltacommit__COMPLETED__20240105021356931]}
24/01/05 02:13:57 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:13:57 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:13:57 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:13:57 INFO table.HoodieTableMetaClient: Loading Active commit timeline for /user/hive/warehouse/stock_ticks_mor
24/01/05 02:13:57 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105021347452__deltacommit__COMPLETED__20240105021356931]}
24/01/05 02:13:57 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:13:57 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:13:57 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:13:57 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:13:57 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:13:57 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:13:57 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105021347452__deltacommit__COMPLETED__20240105021356832]}
24/01/05 02:13:57 INFO view.AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
24/01/05 02:13:57 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:13:57 INFO view.FileSystemViewManager: Creating View Manager with storage type :REMOTE_FIRST
24/01/05 02:13:57 INFO view.FileSystemViewManager: Creating remote first table view
24/01/05 02:13:57 INFO client.BaseHoodieWriteClient: Start to archive synchronously.
24/01/05 02:13:57 INFO transaction.TransactionManager: Transaction starting for Optional.empty with latest completed transaction instant Optional.empty
24/01/05 02:13:57 INFO lock.LockManager: LockProvider org.apache.hudi.client.transaction.lock.InProcessLockProvider
24/01/05 02:13:57 INFO lock.InProcessLockProvider: Base Path /user/hive/warehouse/stock_ticks_mor, Lock Instance java.util.concurrent.locks.ReentrantReadWriteLock@592a1882[Write locks = 0, Read locks = 0], Thread main, In-process lock state ACQUIRING
24/01/05 02:13:57 INFO lock.InProcessLockProvider: Base Path /user/hive/warehouse/stock_ticks_mor, Lock Instance java.util.concurrent.locks.ReentrantReadWriteLock@592a1882[Write locks = 1, Read locks = 0], Thread main, In-process lock state ACQUIRED
24/01/05 02:13:57 INFO transaction.TransactionManager: Transaction started for Optional.empty with latest completed transaction instant Optional.empty
24/01/05 02:13:57 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105021347452__deltacommit__COMPLETED__20240105021356931]}
24/01/05 02:13:57 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:13:58 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:13:58 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:13:58 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:13:58 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:13:58 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:13:58 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105021347452__deltacommit__COMPLETED__20240105021356832]}
24/01/05 02:13:58 INFO view.AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
24/01/05 02:13:58 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:13:58 INFO client.HoodieTimelineArchiver: Not archiving as there is no compaction yet on the metadata table
24/01/05 02:13:58 INFO client.HoodieTimelineArchiver: No Instants to archive
24/01/05 02:13:58 INFO transaction.TransactionManager: Transaction ending with transaction owner Optional.empty
24/01/05 02:13:58 INFO lock.InProcessLockProvider: Base Path /user/hive/warehouse/stock_ticks_mor, Lock Instance java.util.concurrent.locks.ReentrantReadWriteLock@592a1882[Write locks = 1, Read locks = 0], Thread main, In-process lock state RELEASING
24/01/05 02:13:58 INFO lock.InProcessLockProvider: Base Path /user/hive/warehouse/stock_ticks_mor, Lock Instance java.util.concurrent.locks.ReentrantReadWriteLock@592a1882[Write locks = 0, Read locks = 0], Thread main, In-process lock state RELEASED
24/01/05 02:13:58 INFO lock.InProcessLockProvider: Base Path /user/hive/warehouse/stock_ticks_mor, Lock Instance java.util.concurrent.locks.ReentrantReadWriteLock@592a1882[Write locks = 0, Read locks = 0], Thread main, In-process lock state ALREADY_RELEASED
24/01/05 02:13:58 INFO lock.LockManager: Released connection created for acquiring lock
24/01/05 02:13:58 INFO transaction.TransactionManager: Transaction ended with transaction owner Optional.empty
24/01/05 02:13:58 INFO view.FileSystemViewManager: Creating remote view for basePath /user/hive/warehouse/stock_ticks_mor. Server=adhoc-1:34001, Timeout=300
24/01/05 02:13:58 INFO view.FileSystemViewManager: Creating InMemory based view for basePath /user/hive/warehouse/stock_ticks_mor
24/01/05 02:13:58 INFO view.AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
24/01/05 02:13:58 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:13:58 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105021347452__deltacommit__COMPLETED__20240105021356931]}
24/01/05 02:13:58 INFO view.RemoteHoodieTableFileSystemView: Sending request : (http://adhoc-1:34001/v1/hoodie/view/refresh/?basepath=%2Fuser%2Fhive%2Fwarehouse%2Fstock_ticks_mor&lastinstantts=20240105021347452&timelinehash=4a3ab074d69f26b49cb5f0dfceaf1fd8f4760ea789bb800ca6cea65c8323c7a6)
24/01/05 02:13:58 INFO view.RocksDbBasedFileSystemView: Closing Rocksdb !!
24/01/05 02:13:58 INFO collection.RocksDBDAO: From Rocks DB : [/db_impl/db_impl.cc:478] Shutdown: canceling all background work
24/01/05 02:13:58 INFO collection.RocksDBDAO: From Rocks DB : [/db_impl/db_impl.cc:677] Shutdown complete
24/01/05 02:13:58 INFO view.RocksDbBasedFileSystemView: Closed Rocksdb !!
24/01/05 02:13:58 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105021347452__deltacommit__COMPLETED__20240105021356931]}
24/01/05 02:13:58 INFO view.AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
24/01/05 02:13:58 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:13:58 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105021347452__deltacommit__COMPLETED__20240105021356931]}
24/01/05 02:13:58 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105021347452__deltacommit__COMPLETED__20240105021356832]}
24/01/05 02:13:58 INFO view.AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
24/01/05 02:13:58 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:13:58 INFO streamer.StreamSync: Commit 20240105021347452 successful!
24/01/05 02:13:58 INFO transaction.TransactionManager: Transaction manager closed
24/01/05 02:13:58 INFO transaction.TransactionManager: Transaction manager closed
24/01/05 02:13:58 INFO streamer.StreamSync: Shutting down embedded timeline server
24/01/05 02:13:58 INFO embedded.EmbeddedTimelineService: Closing Timeline server
24/01/05 02:13:58 INFO service.TimelineService: Closing Timeline Service
24/01/05 02:13:58 INFO javalin.Javalin: Stopping Javalin ...
24/01/05 02:13:58 INFO javalin.Javalin: Javalin has stopped
24/01/05 02:13:58 INFO service.TimelineService: Closed Timeline Service
24/01/05 02:13:58 INFO embedded.EmbeddedTimelineService: Closed Timeline server
24/01/05 02:13:58 INFO ingestion.HoodieIngestionService: Ingestion service (run-once mode) has been shut down.
24/01/05 02:13:58 INFO server.AbstractConnector: Stopped Spark@61019f59{HTTP/1.1,[http/1.1]}{0.0.0.0:8090}
24/01/05 02:13:58 INFO ui.SparkUI: Stopped Spark web UI at http://adhoc-1:8090
24/01/05 02:13:58 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
24/01/05 02:13:58 INFO memory.MemoryStore: MemoryStore cleared
24/01/05 02:13:58 INFO storage.BlockManager: BlockManager stopped
24/01/05 02:13:58 INFO storage.BlockManagerMaster: BlockManagerMaster stopped
24/01/05 02:13:58 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
24/01/05 02:13:58 INFO spark.SparkContext: Successfully stopped SparkContext
24/01/05 02:13:58 INFO util.ShutdownHookManager: Shutdown hook called
24/01/05 02:13:58 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-513cd1a1-8dff-4bc1-8164-daca0914589f
24/01/05 02:13:58 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-19f343b7-8046-4562-9ff3-ae587bad2931
+ /var/hoodie/ws/hudi-sync/hudi-hive-sync/run_sync_tool.sh --jdbc-url jdbc:hive2://hiveserver:10000 --user hive --pass hive --partitioned-by dt --base-path /user/hive/warehouse/stock_ticks_mor --database default --table stock_ticks_mor --partition-value-extractor org.apache.hudi.hive.SlashEncodedDayPartitionValueExtractor
Running Command : java -cp /opt/hive/lib/hive-metastore-2.3.3.jar::/opt/hive/lib/hive-service-2.3.3.jar::/opt/hive/lib/hive-exec-2.3.3.jar::/opt/hive/lib/hive-jdbc-2.3.3.jar:/opt/hive/lib/hive-jdbc-handler-2.3.3.jar::/opt/hive/lib/jackson-annotations-2.6.0.jar:/opt/hive/lib/jackson-core-2.6.5.jar:/opt/hive/lib/jackson-databind-2.6.5.jar:/opt/hive/lib/jackson-dataformat-smile-2.4.6.jar:/opt/hive/lib/jackson-datatype-guava-2.4.6.jar:/opt/hive/lib/jackson-datatype-joda-2.4.6.jar:/opt/hive/lib/jackson-jaxrs-1.9.13.jar:/opt/hive/lib/jackson-jaxrs-base-2.4.6.jar:/opt/hive/lib/jackson-jaxrs-json-provider-2.4.6.jar:/opt/hive/lib/jackson-jaxrs-smile-provider-2.4.6.jar:/opt/hive/lib/jackson-module-jaxb-annotations-2.4.6.jar:/opt/hive/lib/jackson-xc-1.9.13.jar::/opt/hadoop-2.8.4/share/hadoop/common/*:/opt/hadoop-2.8.4/share/hadoop/mapreduce/*:/opt/hadoop-2.8.4/share/hadoop/hdfs/*:/opt/hadoop-2.8.4/share/hadoop/common/lib/*:/opt/hadoop-2.8.4/share/hadoop/hdfs/lib/*:/etc/hadoop:/var/hoodie/ws/hudi-sync/hudi-hive-sync/../../packaging/hudi-hive-sync-bundle/target/hudi-hive-sync-bundle-0.14.1-rc2.jar org.apache.hudi.hive.HiveSyncTool --jdbc-url jdbc:hive2://hiveserver:10000 --user hive --pass hive --partitioned-by dt --base-path /user/hive/warehouse/stock_ticks_mor --database default --table stock_ticks_mor --partition-value-extractor org.apache.hudi.hive.SlashEncodedDayPartitionValueExtractor
2024-01-05 02:13:58,966 INFO  [main] conf.HiveConf (HiveConf.java:findConfigFile(181)) - Found configuration file file:/opt/hadoop-2.8.4/etc/hadoop/hive-site.xml
2024-01-05 02:13:59,475 WARN  [main] util.NativeCodeLoader (NativeCodeLoader.java:<clinit>(62)) - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2024-01-05 02:13:59,557 INFO  [main] table.HoodieTableMetaClient (HoodieTableMetaClient.java:<init>(133)) - Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
2024-01-05 02:14:00,044 INFO  [main] table.HoodieTableConfig (HoodieTableConfig.java:<init>(276)) - Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
2024-01-05 02:14:00,106 INFO  [main] table.HoodieTableMetaClient (HoodieTableMetaClient.java:<init>(152)) - Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
2024-01-05 02:14:00,106 INFO  [main] table.HoodieTableMetaClient (HoodieTableMetaClient.java:<init>(155)) - Loading Active commit timeline for /user/hive/warehouse/stock_ticks_mor
2024-01-05 02:14:00,129 INFO  [main] timeline.HoodieActiveTimeline (HoodieActiveTimeline.java:<init>(172)) - Loaded instants upto : Option{val=[20240105021347452__deltacommit__COMPLETED__20240105021356931]}
2024-01-05 02:14:00,406 INFO  [main] hive.metastore (HiveMetaStoreClient.java:open(407)) - Trying to connect to metastore with URI thrift://hivemetastore:9083
2024-01-05 02:14:00,421 INFO  [main] hive.metastore (HiveMetaStoreClient.java:open(481)) - Opened a connection to metastore, current connections: 1
2024-01-05 02:14:00,436 INFO  [main] hive.metastore (HiveMetaStoreClient.java:open(534)) - Connected to metastore.
2024-01-05 02:14:00,574 INFO  [main] jdbc.Utils (Utils.java:parseURL(325)) - Supplied authorities: hiveserver:10000
2024-01-05 02:14:00,575 INFO  [main] jdbc.Utils (Utils.java:parseURL(444)) - Resolved authority: hiveserver:10000
2024-01-05 02:14:00,999 INFO  [main] ddl.QueryBasedDDLExecutor (JDBCExecutor.java:createHiveConnection(105)) - Successfully established Hive connection to  jdbc:hive2://hiveserver:10000
2024-01-05 02:14:01,000 INFO  [main] hive.HiveSyncTool (HiveSyncTool.java:syncHoodieTable(162)) - Syncing target hoodie table with hive table(default.stock_ticks_mor). Hive metastore URL from HiveConf:thrift://hivemetastore:9083). Hive metastore URL from HiveSyncConfig:null, basePath :/user/hive/warehouse/stock_ticks_mor
2024-01-05 02:14:01,000 INFO  [main] hive.HiveSyncTool (HiveSyncTool.java:syncHoodieTable(224)) - Trying to sync hoodie table stock_ticks_mor_ro with base path /user/hive/warehouse/stock_ticks_mor of type MERGE_ON_READ
2024-01-05 02:14:01,305 INFO  [main] table.TableSchemaResolver (TableSchemaResolver.java:readSchemaFromParquetBaseFile(329)) - Reading schema from /user/hive/warehouse/stock_ticks_mor/2018/08/31/0245deea-bfb7-4901-bd55-849b7d6fa366-0_0-23-24_20240105021347452.parquet
SLF4J: Failed to load class "org.slf4j.impl.StaticLoggerBinder".
SLF4J: Defaulting to no-operation (NOP) logger implementation
SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.
2024-01-05 02:14:01,614 INFO  [main] hive.HiveSyncTool (HiveSyncTool.java:syncFirstTime(321)) - Sync table stock_ticks_mor_ro for the first time.
2024-01-05 02:14:01,623 INFO  [main] ddl.QueryBasedDDLExecutor (QueryBasedDDLExecutor.java:createTable(91)) - Creating table with CREATE EXTERNAL TABLE IF NOT EXISTS `default`.`stock_ticks_mor_ro`( `_hoodie_commit_time` string, `_hoodie_commit_seqno` string, `_hoodie_record_key` string, `_hoodie_partition_path` string, `_hoodie_file_name` string, `volume` bigint, `ts` string, `symbol` string, `year` int, `month` string, `high` double, `low` double, `key` string, `date` string, `close` double, `open` double, `day` string) PARTITIONED BY (`dt` string) ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe' WITH SERDEPROPERTIES ('hoodie.query.as.ro.table'='true','path'='/user/hive/warehouse/stock_ticks_mor') STORED AS INPUTFORMAT 'org.apache.hudi.hadoop.HoodieParquetInputFormat' OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat' LOCATION '/user/hive/warehouse/stock_ticks_mor' TBLPROPERTIES('spark.sql.sources.schema.partCol.0'='dt','spark.sql.sources.schema.numParts'='1','spark.sql.sources.schema.numPartCols'='1','spark.sql.sources.provider'='hudi','spark.sql.sources.schema.part.0'='{"type":"struct","fields":[{"name":"_hoodie_commit_time","type":"string","nullable":true,"metadata":{}},{"name":"_hoodie_commit_seqno","type":"string","nullable":true,"metadata":{}},{"name":"_hoodie_record_key","type":"string","nullable":true,"metadata":{}},{"name":"_hoodie_partition_path","type":"string","nullable":true,"metadata":{}},{"name":"_hoodie_file_name","type":"string","nullable":true,"metadata":{}},{"name":"volume","type":"long","nullable":false,"metadata":{}},{"name":"ts","type":"string","nullable":false,"metadata":{}},{"name":"symbol","type":"string","nullable":false,"metadata":{}},{"name":"year","type":"integer","nullable":false,"metadata":{}},{"name":"month","type":"string","nullable":false,"metadata":{}},{"name":"high","type":"double","nullable":false,"metadata":{}},{"name":"low","type":"double","nullable":false,"metadata":{}},{"name":"key","type":"string","nullable":false,"metadata":{}},{"name":"date","type":"string","nullable":false,"metadata":{}},{"name":"close","type":"double","nullable":false,"metadata":{}},{"name":"open","type":"double","nullable":false,"metadata":{}},{"name":"day","type":"string","nullable":false,"metadata":{}},{"name":"dt","type":"string","nullable":false,"metadata":{}}]}')
2024-01-05 02:14:01,625 INFO  [main] ddl.QueryBasedDDLExecutor (JDBCExecutor.java:runSQL(67)) - Executing SQL CREATE EXTERNAL TABLE IF NOT EXISTS `default`.`stock_ticks_mor_ro`( `_hoodie_commit_time` string, `_hoodie_commit_seqno` string, `_hoodie_record_key` string, `_hoodie_partition_path` string, `_hoodie_file_name` string, `volume` bigint, `ts` string, `symbol` string, `year` int, `month` string, `high` double, `low` double, `key` string, `date` string, `close` double, `open` double, `day` string) PARTITIONED BY (`dt` string) ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe' WITH SERDEPROPERTIES ('hoodie.query.as.ro.table'='true','path'='/user/hive/warehouse/stock_ticks_mor') STORED AS INPUTFORMAT 'org.apache.hudi.hadoop.HoodieParquetInputFormat' OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat' LOCATION '/user/hive/warehouse/stock_ticks_mor' TBLPROPERTIES('spark.sql.sources.schema.partCol.0'='dt','spark.sql.sources.schema.numParts'='1','spark.sql.sources.schema.numPartCols'='1','spark.sql.sources.provider'='hudi','spark.sql.sources.schema.part.0'='{"type":"struct","fields":[{"name":"_hoodie_commit_time","type":"string","nullable":true,"metadata":{}},{"name":"_hoodie_commit_seqno","type":"string","nullable":true,"metadata":{}},{"name":"_hoodie_record_key","type":"string","nullable":true,"metadata":{}},{"name":"_hoodie_partition_path","type":"string","nullable":true,"metadata":{}},{"name":"_hoodie_file_name","type":"string","nullable":true,"metadata":{}},{"name":"volume","type":"long","nullable":false,"metadata":{}},{"name":"ts","type":"string","nullable":false,"metadata":{}},{"name":"symbol","type":"string","nullable":false,"metadata":{}},{"name":"year","type":"integer","nullable":false,"metadata":{}},{"name":"month","type":"string","nullable":false,"metadata":{}},{"name":"high","type":"double","nullable":false,"metadata":{}},{"name":"low","type":"double","nullable":false,"metadata":{}},{"name":"key","type":"string","nullable":false,"metadata":{}},{"name":"date","type":"string","nullable":false,"metadata":{}},{"name":"close","type":"double","nullable":false,"metadata":{}},{"name":"open","type":"double","nullable":false,"metadata":{}},{"name":"day","type":"string","nullable":false,"metadata":{}},{"name":"dt","type":"string","nullable":false,"metadata":{}}]}')
2024-01-05 02:14:02,719 INFO  [main] hive.HiveSyncTool (HiveSyncTool.java:syncHoodieTable(265)) - Last commit time synced was found to be null, last commit completion time is found to be null
2024-01-05 02:14:02,719 INFO  [main] hive.HiveSyncTool (HiveSyncTool.java:syncHoodieTable(279)) - Sync all partitions given the last commit time synced is empty or before the start of the active timeline. Listing all partitions in /user/hive/warehouse/stock_ticks_mor, file system: DFS[DFSClient[clientName=DFSClient_NONMAPREDUCE_-140898155_1, ugi=root (auth:SIMPLE)]]
2024-01-05 02:14:02,775 INFO  [main] table.HoodieTableConfig (HoodieTableConfig.java:<init>(276)) - Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
2024-01-05 02:14:02,806 INFO  [main] hive.HiveSyncTool (HiveSyncTool.java:syncPartitions(459)) - New Partitions [2018/08/31]
2024-01-05 02:14:02,806 INFO  [main] ddl.QueryBasedDDLExecutor (QueryBasedDDLExecutor.java:addPartitionsToTable(122)) - Adding partitions 1 to table stock_ticks_mor_ro
2024-01-05 02:14:02,807 INFO  [main] ddl.QueryBasedDDLExecutor (JDBCExecutor.java:runSQL(67)) - Executing SQL ALTER TABLE `default`.`stock_ticks_mor_ro` ADD IF NOT EXISTS   PARTITION (`dt`='2018-08-31') LOCATION '/user/hive/warehouse/stock_ticks_mor/2018/08/31' 
2024-01-05 02:14:03,066 INFO  [main] hive.HiveSyncTool (HiveSyncTool.java:syncHoodieTable(298)) - Sync complete for stock_ticks_mor_ro
2024-01-05 02:14:03,066 INFO  [main] hive.HiveSyncTool (HiveSyncTool.java:syncHoodieTable(224)) - Trying to sync hoodie table stock_ticks_mor_rt with base path /user/hive/warehouse/stock_ticks_mor of type MERGE_ON_READ
2024-01-05 02:14:03,079 INFO  [main] table.TableSchemaResolver (TableSchemaResolver.java:readSchemaFromParquetBaseFile(329)) - Reading schema from /user/hive/warehouse/stock_ticks_mor/2018/08/31/0245deea-bfb7-4901-bd55-849b7d6fa366-0_0-23-24_20240105021347452.parquet
2024-01-05 02:14:03,092 INFO  [main] hive.HiveSyncTool (HiveSyncTool.java:syncFirstTime(321)) - Sync table stock_ticks_mor_rt for the first time.
2024-01-05 02:14:03,093 INFO  [main] ddl.QueryBasedDDLExecutor (QueryBasedDDLExecutor.java:createTable(91)) - Creating table with CREATE EXTERNAL TABLE IF NOT EXISTS `default`.`stock_ticks_mor_rt`( `_hoodie_commit_time` string, `_hoodie_commit_seqno` string, `_hoodie_record_key` string, `_hoodie_partition_path` string, `_hoodie_file_name` string, `volume` bigint, `ts` string, `symbol` string, `year` int, `month` string, `high` double, `low` double, `key` string, `date` string, `close` double, `open` double, `day` string) PARTITIONED BY (`dt` string) ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe' WITH SERDEPROPERTIES ('hoodie.query.as.ro.table'='false','path'='/user/hive/warehouse/stock_ticks_mor') STORED AS INPUTFORMAT 'org.apache.hudi.hadoop.realtime.HoodieParquetRealtimeInputFormat' OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat' LOCATION '/user/hive/warehouse/stock_ticks_mor' TBLPROPERTIES('spark.sql.sources.schema.partCol.0'='dt','spark.sql.sources.schema.numParts'='1','spark.sql.sources.schema.numPartCols'='1','spark.sql.sources.provider'='hudi','spark.sql.sources.schema.part.0'='{"type":"struct","fields":[{"name":"_hoodie_commit_time","type":"string","nullable":true,"metadata":{}},{"name":"_hoodie_commit_seqno","type":"string","nullable":true,"metadata":{}},{"name":"_hoodie_record_key","type":"string","nullable":true,"metadata":{}},{"name":"_hoodie_partition_path","type":"string","nullable":true,"metadata":{}},{"name":"_hoodie_file_name","type":"string","nullable":true,"metadata":{}},{"name":"volume","type":"long","nullable":false,"metadata":{}},{"name":"ts","type":"string","nullable":false,"metadata":{}},{"name":"symbol","type":"string","nullable":false,"metadata":{}},{"name":"year","type":"integer","nullable":false,"metadata":{}},{"name":"month","type":"string","nullable":false,"metadata":{}},{"name":"high","type":"double","nullable":false,"metadata":{}},{"name":"low","type":"double","nullable":false,"metadata":{}},{"name":"key","type":"string","nullable":false,"metadata":{}},{"name":"date","type":"string","nullable":false,"metadata":{}},{"name":"close","type":"double","nullable":false,"metadata":{}},{"name":"open","type":"double","nullable":false,"metadata":{}},{"name":"day","type":"string","nullable":false,"metadata":{}},{"name":"dt","type":"string","nullable":false,"metadata":{}}]}')
2024-01-05 02:14:03,093 INFO  [main] ddl.QueryBasedDDLExecutor (JDBCExecutor.java:runSQL(67)) - Executing SQL CREATE EXTERNAL TABLE IF NOT EXISTS `default`.`stock_ticks_mor_rt`( `_hoodie_commit_time` string, `_hoodie_commit_seqno` string, `_hoodie_record_key` string, `_hoodie_partition_path` string, `_hoodie_file_name` string, `volume` bigint, `ts` string, `symbol` string, `year` int, `month` string, `high` double, `low` double, `key` string, `date` string, `close` double, `open` double, `day` string) PARTITIONED BY (`dt` string) ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe' WITH SERDEPROPERTIES ('hoodie.query.as.ro.table'='false','path'='/user/hive/warehouse/stock_ticks_mor') STORED AS INPUTFORMAT 'org.apache.hudi.hadoop.realtime.HoodieParquetRealtimeInputFormat' OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat' LOCATION '/user/hive/warehouse/stock_ticks_mor' TBLPROPERTIES('spark.sql.sources.schema.partCol.0'='dt','spark.sql.sources.schema.numParts'='1','spark.sql.sources.schema.numPartCols'='1','spark.sql.sources.provider'='hudi','spark.sql.sources.schema.part.0'='{"type":"struct","fields":[{"name":"_hoodie_commit_time","type":"string","nullable":true,"metadata":{}},{"name":"_hoodie_commit_seqno","type":"string","nullable":true,"metadata":{}},{"name":"_hoodie_record_key","type":"string","nullable":true,"metadata":{}},{"name":"_hoodie_partition_path","type":"string","nullable":true,"metadata":{}},{"name":"_hoodie_file_name","type":"string","nullable":true,"metadata":{}},{"name":"volume","type":"long","nullable":false,"metadata":{}},{"name":"ts","type":"string","nullable":false,"metadata":{}},{"name":"symbol","type":"string","nullable":false,"metadata":{}},{"name":"year","type":"integer","nullable":false,"metadata":{}},{"name":"month","type":"string","nullable":false,"metadata":{}},{"name":"high","type":"double","nullable":false,"metadata":{}},{"name":"low","type":"double","nullable":false,"metadata":{}},{"name":"key","type":"string","nullable":false,"metadata":{}},{"name":"date","type":"string","nullable":false,"metadata":{}},{"name":"close","type":"double","nullable":false,"metadata":{}},{"name":"open","type":"double","nullable":false,"metadata":{}},{"name":"day","type":"string","nullable":false,"metadata":{}},{"name":"dt","type":"string","nullable":false,"metadata":{}}]}')
2024-01-05 02:14:03,153 INFO  [main] hive.HiveSyncTool (HiveSyncTool.java:syncHoodieTable(265)) - Last commit time synced was found to be null, last commit completion time is found to be null
2024-01-05 02:14:03,153 INFO  [main] hive.HiveSyncTool (HiveSyncTool.java:syncHoodieTable(279)) - Sync all partitions given the last commit time synced is empty or before the start of the active timeline. Listing all partitions in /user/hive/warehouse/stock_ticks_mor, file system: DFS[DFSClient[clientName=DFSClient_NONMAPREDUCE_-140898155_1, ugi=root (auth:SIMPLE)]]
2024-01-05 02:14:03,171 INFO  [main] table.HoodieTableConfig (HoodieTableConfig.java:<init>(276)) - Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
2024-01-05 02:14:03,185 INFO  [main] hive.HiveSyncTool (HiveSyncTool.java:syncPartitions(459)) - New Partitions [2018/08/31]
2024-01-05 02:14:03,185 INFO  [main] ddl.QueryBasedDDLExecutor (QueryBasedDDLExecutor.java:addPartitionsToTable(122)) - Adding partitions 1 to table stock_ticks_mor_rt
2024-01-05 02:14:03,185 INFO  [main] ddl.QueryBasedDDLExecutor (JDBCExecutor.java:runSQL(67)) - Executing SQL ALTER TABLE `default`.`stock_ticks_mor_rt` ADD IF NOT EXISTS   PARTITION (`dt`='2018-08-31') LOCATION '/user/hive/warehouse/stock_ticks_mor/2018/08/31' 
2024-01-05 02:14:03,302 INFO  [main] hive.HiveSyncTool (HiveSyncTool.java:syncHoodieTable(298)) - Sync complete for stock_ticks_mor_rt
2024-01-05 02:14:03,336 INFO  [main] hive.metastore (HiveMetaStoreClient.java:close(564)) - Closed a connection to metastore, current connections: 0
+ cat /home/alex/github/alexttx/hudi/docker/demo/data/batch_2.json
+ head -n100
+ kcat -b kafkabroker -t stock_ticks -P
+ docker exec -i adhoc-2 /bin/bash -x
+ spark-submit --class org.apache.hudi.utilities.streamer.HoodieStreamer /var/hoodie/ws/docker/hoodie/hadoop/hive_base/target/hoodie-utilities.jar --table-type MERGE_ON_READ --source-class org.apache.hudi.utilities.sources.JsonKafkaSource --source-ordering-field ts --target-base-path /user/hive/warehouse/stock_ticks_mor --target-table stock_ticks_mor --props /var/demo/config/kafka-source.properties --schemaprovider-class org.apache.hudi.utilities.schema.FilebasedSchemaProvider --disable-compaction
24/01/05 02:14:04 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
24/01/05 02:14:04 WARN streamer.SchedulerConfGenerator: Job Scheduling Configs will not be in effect as spark.scheduler.mode is not set to FAIR at instantiation time. Continuing without scheduling configs
24/01/05 02:14:04 INFO spark.SparkContext: Running Spark version 2.4.4
24/01/05 02:14:04 INFO spark.SparkContext: Submitted application: streamer-stock_ticks_mor
24/01/05 02:14:04 INFO spark.SecurityManager: Changing view acls to: root
24/01/05 02:14:04 INFO spark.SecurityManager: Changing modify acls to: root
24/01/05 02:14:04 INFO spark.SecurityManager: Changing view acls groups to: 
24/01/05 02:14:04 INFO spark.SecurityManager: Changing modify acls groups to: 
24/01/05 02:14:04 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
24/01/05 02:14:05 INFO Configuration.deprecation: mapred.output.compression.codec is deprecated. Instead, use mapreduce.output.fileoutputformat.compress.codec
24/01/05 02:14:05 INFO Configuration.deprecation: mapred.output.compress is deprecated. Instead, use mapreduce.output.fileoutputformat.compress
24/01/05 02:14:05 INFO Configuration.deprecation: mapred.output.compression.type is deprecated. Instead, use mapreduce.output.fileoutputformat.compress.type
24/01/05 02:14:05 INFO util.Utils: Successfully started service 'sparkDriver' on port 38467.
24/01/05 02:14:05 INFO spark.SparkEnv: Registering MapOutputTracker
24/01/05 02:14:05 INFO spark.SparkEnv: Registering BlockManagerMaster
24/01/05 02:14:05 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
24/01/05 02:14:05 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
24/01/05 02:14:05 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-03dc7105-2774-4a69-83e3-14b8281787d0
24/01/05 02:14:05 INFO memory.MemoryStore: MemoryStore started with capacity 366.3 MB
24/01/05 02:14:05 INFO spark.SparkEnv: Registering OutputCommitCoordinator
24/01/05 02:14:05 INFO util.log: Logging initialized @1608ms
24/01/05 02:14:05 INFO server.Server: jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
24/01/05 02:14:05 INFO server.Server: Started @1670ms
24/01/05 02:14:05 INFO server.AbstractConnector: Started ServerConnector@61019f59{HTTP/1.1,[http/1.1]}{0.0.0.0:8090}
24/01/05 02:14:05 INFO util.Utils: Successfully started service 'SparkUI' on port 8090.
24/01/05 02:14:05 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3e1162e7{/jobs,null,AVAILABLE,@Spark}
24/01/05 02:14:05 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@17f460bb{/jobs/json,null,AVAILABLE,@Spark}
24/01/05 02:14:05 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@64a1923a{/jobs/job,null,AVAILABLE,@Spark}
24/01/05 02:14:05 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@18ca3c62{/jobs/job/json,null,AVAILABLE,@Spark}
24/01/05 02:14:05 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2c0f7678{/stages,null,AVAILABLE,@Spark}
24/01/05 02:14:05 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@44d70181{/stages/json,null,AVAILABLE,@Spark}
24/01/05 02:14:05 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6aa648b9{/stages/stage,null,AVAILABLE,@Spark}
24/01/05 02:14:05 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@88a8218{/stages/stage/json,null,AVAILABLE,@Spark}
24/01/05 02:14:05 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@50b1f030{/stages/pool,null,AVAILABLE,@Spark}
24/01/05 02:14:05 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4163f1cd{/stages/pool/json,null,AVAILABLE,@Spark}
24/01/05 02:14:05 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5fa05212{/storage,null,AVAILABLE,@Spark}
24/01/05 02:14:05 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3e681bc{/storage/json,null,AVAILABLE,@Spark}
24/01/05 02:14:05 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5c09d180{/storage/rdd,null,AVAILABLE,@Spark}
24/01/05 02:14:05 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@23aae55{/storage/rdd/json,null,AVAILABLE,@Spark}
24/01/05 02:14:05 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5f574cc2{/environment,null,AVAILABLE,@Spark}
24/01/05 02:14:05 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@680bddf5{/environment/json,null,AVAILABLE,@Spark}
24/01/05 02:14:05 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7a9c84a5{/executors,null,AVAILABLE,@Spark}
24/01/05 02:14:05 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2d83c5a5{/executors/json,null,AVAILABLE,@Spark}
24/01/05 02:14:05 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@48d7ad8b{/executors/threadDump,null,AVAILABLE,@Spark}
24/01/05 02:14:05 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7e053511{/executors/threadDump/json,null,AVAILABLE,@Spark}
24/01/05 02:14:05 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@60222fd8{/static,null,AVAILABLE,@Spark}
24/01/05 02:14:05 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@37ff4054{/,null,AVAILABLE,@Spark}
24/01/05 02:14:05 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@894858{/api,null,AVAILABLE,@Spark}
24/01/05 02:14:05 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@74cf8b28{/jobs/job/kill,null,AVAILABLE,@Spark}
24/01/05 02:14:05 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@36c54a56{/stages/stage/kill,null,AVAILABLE,@Spark}
24/01/05 02:14:05 INFO ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://adhoc-2:8090
24/01/05 02:14:05 INFO spark.SparkContext: Added JAR file:/var/hoodie/ws/docker/hoodie/hadoop/hive_base/target/hoodie-utilities.jar at spark://adhoc-2:38467/jars/hoodie-utilities.jar with timestamp 1704420845501
24/01/05 02:14:05 INFO executor.Executor: Starting executor ID driver on host localhost
24/01/05 02:14:05 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38763.
24/01/05 02:14:05 INFO netty.NettyBlockTransferService: Server created on adhoc-2:38763
24/01/05 02:14:05 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
24/01/05 02:14:05 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, adhoc-2, 38763, None)
24/01/05 02:14:05 INFO storage.BlockManagerMasterEndpoint: Registering block manager adhoc-2:38763 with 366.3 MB RAM, BlockManagerId(driver, adhoc-2, 38763, None)
24/01/05 02:14:05 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, adhoc-2, 38763, None)
24/01/05 02:14:05 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, adhoc-2, 38763, None)
24/01/05 02:14:05 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@62e6a3ec{/metrics/json,null,AVAILABLE,@Spark}
24/01/05 02:14:06 WARN config.DFSPropertiesConfiguration: Cannot find HUDI_CONF_DIR, please set it as the dir of hudi-defaults.conf
24/01/05 02:14:06 WARN config.DFSPropertiesConfiguration: Properties file file:/etc/hudi/conf/hudi-defaults.conf not found. Ignoring to load props file
24/01/05 02:14:06 INFO internal.SharedState: loading hive config file: file:/opt/hadoop-2.8.4/etc/hadoop/hive-site.xml
24/01/05 02:14:06 INFO internal.SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/opt/spark-warehouse').
24/01/05 02:14:06 INFO internal.SharedState: Warehouse path is 'file:/opt/spark-warehouse'.
24/01/05 02:14:06 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@423c5404{/SQL,null,AVAILABLE,@Spark}
24/01/05 02:14:06 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5853ca50{/SQL/json,null,AVAILABLE,@Spark}
24/01/05 02:14:06 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3c79088e{/SQL/execution,null,AVAILABLE,@Spark}
24/01/05 02:14:06 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4a37191a{/SQL/execution/json,null,AVAILABLE,@Spark}
24/01/05 02:14:06 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@24eb65e3{/static/sql,null,AVAILABLE,@Spark}
24/01/05 02:14:07 INFO state.StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
24/01/05 02:14:07 WARN sql.SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
24/01/05 02:14:07 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:14:07 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:14:07 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:14:07 INFO factory.HoodieSparkKeyGeneratorFactory: The value of hoodie.datasource.write.keygenerator.type is empty; inferred to be SIMPLE
24/01/05 02:14:07 INFO streamer.HoodieStreamer: Creating Hudi Streamer with configs:
auto.offset.reset: earliest
bootstrap.servers: kafkabroker:9092
hoodie.auto.adjust.lock.configs: true
hoodie.bulkinsert.shuffle.parallelism: 2
hoodie.compact.inline: false
hoodie.datasource.write.partitionpath.field: date
hoodie.datasource.write.reconcile.schema: false
hoodie.datasource.write.recordkey.field: key
hoodie.delete.shuffle.parallelism: 2
hoodie.embed.timeline.server: true
hoodie.filesystem.view.type: EMBEDDED_KV_STORE
hoodie.insert.shuffle.parallelism: 2
hoodie.streamer.schemaprovider.source.schema.file: /var/demo/config/schema.avsc
hoodie.streamer.schemaprovider.target.schema.file: /var/demo/config/schema.avsc
hoodie.streamer.source.kafka.topic: stock_ticks
hoodie.table.type: MERGE_ON_READ
hoodie.upsert.shuffle.parallelism: 2

24/01/05 02:14:07 INFO fs.FSUtils: Resolving file /var/demo/config/schema.avscto be a remote file.
24/01/05 02:14:07 INFO factory.HoodieSparkKeyGeneratorFactory: The value of hoodie.datasource.write.keygenerator.type is empty; inferred to be SIMPLE
24/01/05 02:14:07 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:14:07 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:14:07 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:14:07 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105021347452__deltacommit__COMPLETED__20240105021356931]}
24/01/05 02:14:07 INFO config.HoodieWriteConfig: Automatically set hoodie.write.lock.provider=org.apache.hudi.client.transaction.lock.InProcessLockProvider since user has not set the lock provider for single writer with async table services
24/01/05 02:14:07 INFO config.HoodieWriteConfig: Automatically set hoodie.write.lock.provider=org.apache.hudi.client.transaction.lock.InProcessLockProvider since user has not set the lock provider for single writer with async table services
24/01/05 02:14:07 INFO ingestion.HoodieIngestionService: Ingestion service starts running in run-once mode
24/01/05 02:14:07 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:14:07 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:14:07 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:14:07 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105021347452__deltacommit__COMPLETED__20240105021356931]}
24/01/05 02:14:07 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:14:07 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:14:07 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:14:07 INFO streamer.StreamSync: Checkpoint to resume from : Option{val=stock_ticks,0:2176}
24/01/05 02:14:07 INFO consumer.ConsumerConfig: ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [kafkabroker:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

24/01/05 02:14:07 INFO utils.AppInfoParser: Kafka version : 2.0.0
24/01/05 02:14:07 INFO utils.AppInfoParser: Kafka commitId : 3402a8361b734732
24/01/05 02:14:07 INFO clients.Metadata: Cluster ID: dMB3Nbq9TYSdXYXM4yK4DQ
24/01/05 02:14:07 INFO helpers.KafkaOffsetGen: SourceLimit not configured, set numEvents to default value : 5000000
24/01/05 02:14:07 INFO helpers.KafkaOffsetGen: getNextOffsetRanges set config hoodie.streamer.source.kafka.minPartitions to 0
24/01/05 02:14:07 INFO sources.KafkaSource: About to read 100 from Kafka for topic :stock_ticks
24/01/05 02:14:07 WARN kafka010.KafkaUtils: overriding enable.auto.commit to false for executor
24/01/05 02:14:07 WARN kafka010.KafkaUtils: overriding auto.offset.reset to none for executor
24/01/05 02:14:07 ERROR kafka010.KafkaUtils: group.id is null, you should probably set it
24/01/05 02:14:07 WARN kafka010.KafkaUtils: overriding executor group.id to spark-executor-null
24/01/05 02:14:07 WARN kafka010.KafkaUtils: overriding receive.buffer.bytes to 65536 see KAFKA-3135
24/01/05 02:14:07 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105021347452__deltacommit__COMPLETED__20240105021356931]}
24/01/05 02:14:07 INFO factory.HoodieSparkKeyGeneratorFactory: The value of hoodie.datasource.write.keygenerator.type is empty; inferred to be SIMPLE
24/01/05 02:14:07 INFO streamer.StreamSync: Setting up new Hoodie Write Client
24/01/05 02:14:07 INFO config.HoodieWriteConfig: Automatically set hoodie.write.lock.provider=org.apache.hudi.client.transaction.lock.InProcessLockProvider since user has not set the lock provider for single writer with async table services
24/01/05 02:14:08 INFO embedded.EmbeddedTimelineService: Overriding hostIp to (adhoc-2) found in spark-conf. It was null
24/01/05 02:14:08 INFO view.FileSystemViewManager: Creating View Manager with storage type :EMBEDDED_KV_STORE
24/01/05 02:14:08 INFO view.FileSystemViewManager: Creating embedded rocks-db based Table View
24/01/05 02:14:08 INFO util.log: Logging initialized @4283ms to org.apache.hudi.org.eclipse.jetty.util.log.Slf4jLog
24/01/05 02:14:08 INFO javalin.Javalin: 
       __                      __ _            __ __
      / /____ _ _   __ ____ _ / /(_)____      / // /
 __  / // __ `/| | / // __ `// // // __ \    / // /_
/ /_/ // /_/ / | |/ // /_/ // // // / / /   /__  __/
\____/ \__,_/  |___/ \__,_//_//_//_/ /_/      /_/

          https://javalin.io/documentation

24/01/05 02:14:08 INFO javalin.Javalin: Starting Javalin ...
24/01/05 02:14:08 INFO javalin.Javalin: You are running Javalin 4.6.7 (released October 24, 2022. Your Javalin version is 437 days old. Consider checking for a newer version.).
24/01/05 02:14:08 INFO server.Server: jetty-9.4.48.v20220622; built: 2022-06-21T20:42:25.880Z; git: 6b67c5719d1f4371b33655ff2d047d24e171e49a; jvm 1.8.0_212-b04
24/01/05 02:14:08 INFO server.Server: Started @4733ms
24/01/05 02:14:08 INFO javalin.Javalin: Listening on http://localhost:43195/
24/01/05 02:14:08 INFO javalin.Javalin: Javalin started in 161ms \o/
24/01/05 02:14:08 INFO service.TimelineService: Starting Timeline server on port :43195
24/01/05 02:14:08 INFO embedded.EmbeddedTimelineService: Started embedded timeline server at adhoc-2:43195
24/01/05 02:14:08 INFO client.BaseHoodieClient: Timeline Server already running. Not restarting the service
24/01/05 02:14:08 INFO client.BaseHoodieClient: Timeline Server already running. Not restarting the service
24/01/05 02:14:08 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:14:08 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:14:08 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:14:08 INFO table.HoodieTableMetaClient: Loading Active commit timeline for /user/hive/warehouse/stock_ticks_mor
24/01/05 02:14:08 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105021347452__deltacommit__COMPLETED__20240105021356931]}
24/01/05 02:14:08 INFO util.CleanerUtils: Cleaned failed attempts if any
24/01/05 02:14:08 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:14:08 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:14:08 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:14:08 INFO table.HoodieTableMetaClient: Loading Active commit timeline for /user/hive/warehouse/stock_ticks_mor
24/01/05 02:14:08 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105021347452__deltacommit__COMPLETED__20240105021356931]}
24/01/05 02:14:08 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:14:08 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:14:08 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:14:08 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:14:08 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:14:08 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:14:08 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105021347452__deltacommit__COMPLETED__20240105021356832]}
24/01/05 02:14:08 INFO view.AbstractTableFileSystemView: Took 1 ms to read  0 instants, 0 replaced file groups
24/01/05 02:14:08 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:14:08 INFO view.FileSystemViewManager: Creating View Manager with storage type :REMOTE_FIRST
24/01/05 02:14:08 INFO view.FileSystemViewManager: Creating remote first table view
24/01/05 02:14:08 INFO client.BaseHoodieWriteClient: Generate a new instant time: 20240105021407331 action: deltacommit
24/01/05 02:14:08 INFO timeline.HoodieActiveTimeline: Creating a new instant [==>20240105021407331__deltacommit__REQUESTED]
24/01/05 02:14:08 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:14:08 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:14:08 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:14:08 INFO table.HoodieTableMetaClient: Loading Active commit timeline for /user/hive/warehouse/stock_ticks_mor
24/01/05 02:14:08 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[==>20240105021407331__deltacommit__REQUESTED__20240105021408609]}
24/01/05 02:14:08 INFO transaction.TransactionManager: Transaction starting for Option{val=[==>20240105021407331__deltacommit__INFLIGHT]} with latest completed transaction instant Optional.empty
24/01/05 02:14:08 INFO lock.LockManager: LockProvider org.apache.hudi.client.transaction.lock.InProcessLockProvider
24/01/05 02:14:08 INFO lock.InProcessLockProvider: Base Path /user/hive/warehouse/stock_ticks_mor, Lock Instance java.util.concurrent.locks.ReentrantReadWriteLock@7254838[Write locks = 0, Read locks = 0], Thread main, In-process lock state ACQUIRING
24/01/05 02:14:08 INFO lock.InProcessLockProvider: Base Path /user/hive/warehouse/stock_ticks_mor, Lock Instance java.util.concurrent.locks.ReentrantReadWriteLock@7254838[Write locks = 1, Read locks = 0], Thread main, In-process lock state ACQUIRED
24/01/05 02:14:08 INFO transaction.TransactionManager: Transaction started for Option{val=[==>20240105021407331__deltacommit__INFLIGHT]} with latest completed transaction instant Optional.empty
24/01/05 02:14:08 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:14:08 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:14:08 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:14:08 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:14:08 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:14:08 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:14:08 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105021347452__deltacommit__COMPLETED__20240105021356832]}
24/01/05 02:14:08 INFO metadata.HoodieBackedTableMetadataWriter: Async metadata indexing disabled and following partitions already initialized: [files]
24/01/05 02:14:08 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:14:08 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:14:08 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:14:08 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:14:08 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:14:08 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:14:08 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105021347452__deltacommit__COMPLETED__20240105021356832]}
24/01/05 02:14:08 INFO view.AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
24/01/05 02:14:08 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:14:08 INFO client.BaseHoodieClient: Embedded Timeline Server is disabled. Not starting timeline service
24/01/05 02:14:08 INFO client.BaseHoodieClient: Embedded Timeline Server is disabled. Not starting timeline service
24/01/05 02:14:08 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:14:08 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:14:08 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:14:08 INFO table.HoodieTableMetaClient: Loading Active commit timeline for /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:14:08 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105021347452__deltacommit__COMPLETED__20240105021356832]}
24/01/05 02:14:08 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:14:08 INFO view.FileSystemViewManager: Creating View Manager with storage type :MEMORY
24/01/05 02:14:08 INFO view.FileSystemViewManager: Creating in-memory based Table View
24/01/05 02:14:08 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:14:08 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:14:08 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:14:08 INFO table.HoodieTableMetaClient: Loading Active commit timeline for /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:14:08 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105021347452__deltacommit__COMPLETED__20240105021356832]}
24/01/05 02:14:08 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:14:08 INFO view.FileSystemViewManager: Creating View Manager with storage type :MEMORY
24/01/05 02:14:08 INFO view.FileSystemViewManager: Creating in-memory based Table View
24/01/05 02:14:08 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105021347452__deltacommit__COMPLETED__20240105021356832]}
24/01/05 02:14:08 INFO metadata.HoodieBackedTableMetadataWriter: Latest deltacommit time found is 20240105021347452, running clean operations.
24/01/05 02:14:08 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105021347452__deltacommit__COMPLETED__20240105021356832]}
24/01/05 02:14:08 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:14:08 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:14:08 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:14:08 INFO table.HoodieTableMetaClient: Loading Active commit timeline for /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:14:08 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105021347452__deltacommit__COMPLETED__20240105021356832]}
24/01/05 02:14:08 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:14:08 INFO view.FileSystemViewManager: Creating View Manager with storage type :MEMORY
24/01/05 02:14:08 INFO view.FileSystemViewManager: Creating in-memory based Table View
24/01/05 02:14:08 INFO client.BaseHoodieWriteClient: Cleaner started
24/01/05 02:14:08 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:14:08 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:14:08 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:14:08 INFO table.HoodieTableMetaClient: Loading Active commit timeline for /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:14:08 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105021347452__deltacommit__COMPLETED__20240105021356832]}
24/01/05 02:14:08 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:14:08 INFO view.FileSystemViewManager: Creating View Manager with storage type :MEMORY
24/01/05 02:14:08 INFO view.FileSystemViewManager: Creating in-memory based Table View
24/01/05 02:14:08 INFO client.BaseHoodieWriteClient: Scheduling cleaning at instant time :20240105021347452002
24/01/05 02:14:08 INFO view.FileSystemViewManager: Creating InMemory based view for basePath /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:14:08 INFO view.AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
24/01/05 02:14:08 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:14:08 INFO clean.CleanPlanner: No earliest commit to retain. No need to scan partitions !!
24/01/05 02:14:08 INFO clean.CleanPlanActionExecutor: Nothing to clean here. It is already clean
24/01/05 02:14:08 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105021347452__deltacommit__COMPLETED__20240105021356832]}
24/01/05 02:14:08 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:14:08 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:14:08 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:14:08 INFO table.HoodieTableMetaClient: Loading Active commit timeline for /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:14:08 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105021347452__deltacommit__COMPLETED__20240105021356832]}
24/01/05 02:14:08 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:14:08 INFO view.FileSystemViewManager: Creating View Manager with storage type :MEMORY
24/01/05 02:14:08 INFO view.FileSystemViewManager: Creating in-memory based Table View
24/01/05 02:14:08 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[==>20240105021407331__deltacommit__REQUESTED__20240105021408609]}
24/01/05 02:14:08 INFO client.BaseHoodieWriteClient: Scheduling table service COMPACT
24/01/05 02:14:08 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:14:08 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:14:08 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:14:08 INFO table.HoodieTableMetaClient: Loading Active commit timeline for /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:14:08 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105021347452__deltacommit__COMPLETED__20240105021356832]}
24/01/05 02:14:08 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:14:08 INFO view.FileSystemViewManager: Creating View Manager with storage type :MEMORY
24/01/05 02:14:08 INFO view.FileSystemViewManager: Creating in-memory based Table View
24/01/05 02:14:08 INFO client.BaseHoodieWriteClient: Scheduling compaction at instant time :20240105021347452001
24/01/05 02:14:08 INFO compact.ScheduleCompactionActionExecutor: Checking if compaction needs to be run on /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:14:08 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:14:08 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:14:08 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:14:08 INFO table.HoodieTableMetaClient: Loading Active commit timeline for /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:14:08 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105021347452__deltacommit__COMPLETED__20240105021356832]}
24/01/05 02:14:08 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:14:08 INFO view.FileSystemViewManager: Creating View Manager with storage type :MEMORY
24/01/05 02:14:08 INFO view.FileSystemViewManager: Creating in-memory based Table View
24/01/05 02:14:08 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105021347452__deltacommit__COMPLETED__20240105021356832]}
24/01/05 02:14:08 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:14:08 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:14:08 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:14:08 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[==>20240105021407331__deltacommit__REQUESTED__20240105021408609]}
24/01/05 02:14:08 INFO client.HoodieTimelineArchiver: No Instants to archive
24/01/05 02:14:08 INFO metadata.HoodieBackedTableMetadataWriter: All the table services operations on MDT completed successfully
24/01/05 02:14:08 INFO transaction.TransactionManager: Transaction ending with transaction owner Option{val=[==>20240105021407331__deltacommit__INFLIGHT]}
24/01/05 02:14:08 INFO lock.InProcessLockProvider: Base Path /user/hive/warehouse/stock_ticks_mor, Lock Instance java.util.concurrent.locks.ReentrantReadWriteLock@7254838[Write locks = 1, Read locks = 0], Thread main, In-process lock state RELEASING
24/01/05 02:14:08 INFO lock.InProcessLockProvider: Base Path /user/hive/warehouse/stock_ticks_mor, Lock Instance java.util.concurrent.locks.ReentrantReadWriteLock@7254838[Write locks = 0, Read locks = 0], Thread main, In-process lock state RELEASED
24/01/05 02:14:08 INFO lock.InProcessLockProvider: Base Path /user/hive/warehouse/stock_ticks_mor, Lock Instance java.util.concurrent.locks.ReentrantReadWriteLock@7254838[Write locks = 0, Read locks = 0], Thread main, In-process lock state ALREADY_RELEASED
24/01/05 02:14:08 INFO lock.LockManager: Released connection created for acquiring lock
24/01/05 02:14:08 INFO transaction.TransactionManager: Transaction ended with transaction owner Option{val=[==>20240105021407331__deltacommit__INFLIGHT]}
24/01/05 02:14:08 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:14:08 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:14:08 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:14:08 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:14:08 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:14:08 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:14:08 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105021347452__deltacommit__COMPLETED__20240105021356832]}
24/01/05 02:14:08 INFO view.AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
24/01/05 02:14:08 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:14:08 INFO view.FileSystemViewManager: Creating View Manager with storage type :REMOTE_FIRST
24/01/05 02:14:08 INFO view.FileSystemViewManager: Creating remote first table view
24/01/05 02:14:08 INFO async.AsyncCleanerService: The HoodieWriteClient is not configured to auto & async clean. Async clean service will not start.
24/01/05 02:14:08 INFO async.AsyncArchiveService: The HoodieWriteClient is not configured to auto & async archive. Async archive service will not start.
24/01/05 02:14:09 INFO spark.SparkContext: Starting job: collect at HoodieJavaRDD.java:177
24/01/05 02:14:09 INFO scheduler.DAGScheduler: Registering RDD 7 (mapToPair at HoodieJavaRDD.java:149)
24/01/05 02:14:09 INFO scheduler.DAGScheduler: Registering RDD 13 (distinct at HoodieJavaRDD.java:157)
24/01/05 02:14:09 INFO scheduler.DAGScheduler: Got job 0 (collect at HoodieJavaRDD.java:177) with 2 output partitions
24/01/05 02:14:09 INFO scheduler.DAGScheduler: Final stage: ResultStage 2 (collect at HoodieJavaRDD.java:177)
24/01/05 02:14:09 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 1)
24/01/05 02:14:09 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 1)
24/01/05 02:14:09 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[7] at mapToPair at HoodieJavaRDD.java:149), which has no missing parents
24/01/05 02:14:09 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 29.5 KB, free 366.3 MB)
24/01/05 02:14:09 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 14.0 KB, free 366.3 MB)
24/01/05 02:14:09 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on adhoc-2:38763 (size: 14.0 KB, free: 366.3 MB)
24/01/05 02:14:09 INFO spark.SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1161
24/01/05 02:14:09 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[7] at mapToPair at HoodieJavaRDD.java:149) (first 15 tasks are for partitions Vector(0))
24/01/05 02:14:09 INFO scheduler.TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
24/01/05 02:14:09 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7739 bytes)
24/01/05 02:14:09 INFO executor.Executor: Running task 0.0 in stage 0.0 (TID 0)
24/01/05 02:14:09 INFO executor.Executor: Fetching spark://adhoc-2:38467/jars/hoodie-utilities.jar with timestamp 1704420845501
24/01/05 02:14:09 INFO client.TransportClientFactory: Successfully created connection to adhoc-2/172.21.0.17:38467 after 27 ms (0 ms spent in bootstraps)
24/01/05 02:14:09 INFO util.Utils: Fetching spark://adhoc-2:38467/jars/hoodie-utilities.jar to /tmp/spark-17eacc4f-209c-4e6d-8b63-007c3ed26ca8/userFiles-ceb4c4a6-e089-4113-b3df-286a6f156736/fetchFileTemp7550926953083894016.tmp
24/01/05 02:14:09 INFO executor.Executor: Adding file:/tmp/spark-17eacc4f-209c-4e6d-8b63-007c3ed26ca8/userFiles-ceb4c4a6-e089-4113-b3df-286a6f156736/hoodie-utilities.jar to class loader
24/01/05 02:14:09 INFO kafka010.KafkaRDD: Computing topic stock_ticks, partition 0 offsets 2176 -> 2276
24/01/05 02:14:09 INFO kafka010.KafkaDataConsumer: Initializing cache 16 64 0.75
24/01/05 02:14:09 INFO consumer.ConsumerConfig: ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [kafkabroker:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-executor-null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

24/01/05 02:14:09 INFO utils.AppInfoParser: Kafka version : 2.0.0
24/01/05 02:14:09 INFO utils.AppInfoParser: Kafka commitId : 3402a8361b734732
24/01/05 02:14:09 INFO factory.HoodieSparkKeyGeneratorFactory: The value of hoodie.datasource.write.keygenerator.type is empty; inferred to be SIMPLE
24/01/05 02:14:09 INFO kafka010.InternalKafkaConsumer: Initial fetch for spark-executor-null stock_ticks-0 2176
24/01/05 02:14:09 INFO clients.Metadata: Cluster ID: dMB3Nbq9TYSdXYXM4yK4DQ
24/01/05 02:14:09 INFO executor.Executor: Finished task 0.0 in stage 0.0 (TID 0). 951 bytes result sent to driver
24/01/05 02:14:10 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 635 ms on localhost (executor driver) (1/1)
24/01/05 02:14:10 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
24/01/05 02:14:10 INFO scheduler.DAGScheduler: ShuffleMapStage 0 (mapToPair at HoodieJavaRDD.java:149) finished in 0.762 s
24/01/05 02:14:10 INFO scheduler.DAGScheduler: looking for newly runnable stages
24/01/05 02:14:10 INFO scheduler.DAGScheduler: running: Set()
24/01/05 02:14:10 INFO scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 1, ResultStage 2)
24/01/05 02:14:10 INFO scheduler.DAGScheduler: failed: Set()
24/01/05 02:14:10 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 1 (MapPartitionsRDD[13] at distinct at HoodieJavaRDD.java:157), which has no missing parents
24/01/05 02:14:10 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 24.3 KB, free 366.2 MB)
24/01/05 02:14:10 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 11.6 KB, free 366.2 MB)
24/01/05 02:14:10 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on adhoc-2:38763 (size: 11.6 KB, free: 366.3 MB)
24/01/05 02:14:10 INFO spark.SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1161
24/01/05 02:14:10 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[13] at distinct at HoodieJavaRDD.java:157) (first 15 tasks are for partitions Vector(0, 1))
24/01/05 02:14:10 INFO scheduler.TaskSchedulerImpl: Adding task set 1.0 with 2 tasks
24/01/05 02:14:10 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 7651 bytes)
24/01/05 02:14:10 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 1.0 (TID 2, localhost, executor driver, partition 1, ANY, 7651 bytes)
24/01/05 02:14:10 INFO executor.Executor: Running task 0.0 in stage 1.0 (TID 1)
24/01/05 02:14:10 INFO executor.Executor: Running task 1.0 in stage 1.0 (TID 2)
24/01/05 02:14:10 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
24/01/05 02:14:10 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 7 ms
24/01/05 02:14:10 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
24/01/05 02:14:10 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 18 ms
24/01/05 02:14:10 INFO memory.MemoryStore: Block rdd_9_0 stored as values in memory (estimated size 0.0 B, free 366.2 MB)
24/01/05 02:14:10 INFO storage.BlockManagerInfo: Added rdd_9_0 in memory on adhoc-2:38763 (size: 0.0 B, free: 366.3 MB)
24/01/05 02:14:10 INFO executor.Executor: Finished task 0.0 in stage 1.0 (TID 1). 1166 bytes result sent to driver
24/01/05 02:14:10 INFO memory.MemoryStore: Block rdd_9_1 stored as values in memory (estimated size 634.0 B, free 366.2 MB)
24/01/05 02:14:10 INFO storage.BlockManagerInfo: Added rdd_9_1 in memory on adhoc-2:38763 (size: 634.0 B, free: 366.3 MB)
24/01/05 02:14:10 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 113 ms on localhost (executor driver) (1/2)
24/01/05 02:14:10 INFO executor.Executor: Finished task 1.0 in stage 1.0 (TID 2). 1295 bytes result sent to driver
24/01/05 02:14:10 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 1.0 (TID 2) in 157 ms on localhost (executor driver) (2/2)
24/01/05 02:14:10 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
24/01/05 02:14:10 INFO scheduler.DAGScheduler: ShuffleMapStage 1 (distinct at HoodieJavaRDD.java:157) finished in 0.177 s
24/01/05 02:14:10 INFO scheduler.DAGScheduler: looking for newly runnable stages
24/01/05 02:14:10 INFO scheduler.DAGScheduler: running: Set()
24/01/05 02:14:10 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 2)
24/01/05 02:14:10 INFO scheduler.DAGScheduler: failed: Set()
24/01/05 02:14:10 INFO scheduler.DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[15] at distinct at HoodieJavaRDD.java:157), which has no missing parents
24/01/05 02:14:10 INFO storage.BlockManagerInfo: Removed broadcast_0_piece0 on adhoc-2:38763 in memory (size: 14.0 KB, free: 366.3 MB)
24/01/05 02:14:10 INFO memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 4.1 KB, free 366.3 MB)
24/01/05 02:14:10 INFO memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.4 KB, free 366.3 MB)
24/01/05 02:14:10 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on adhoc-2:38763 (size: 2.4 KB, free: 366.3 MB)
24/01/05 02:14:10 INFO spark.SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1161
24/01/05 02:14:10 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ResultStage 2 (MapPartitionsRDD[15] at distinct at HoodieJavaRDD.java:157) (first 15 tasks are for partitions Vector(0, 1))
24/01/05 02:14:10 INFO scheduler.TaskSchedulerImpl: Adding task set 2.0 with 2 tasks
24/01/05 02:14:10 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 2.0 (TID 3, localhost, executor driver, partition 0, PROCESS_LOCAL, 7662 bytes)
24/01/05 02:14:10 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 2.0 (TID 4, localhost, executor driver, partition 1, ANY, 7662 bytes)
24/01/05 02:14:10 INFO executor.Executor: Running task 0.0 in stage 2.0 (TID 3)
24/01/05 02:14:10 INFO executor.Executor: Running task 1.0 in stage 2.0 (TID 4)
24/01/05 02:14:10 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
24/01/05 02:14:10 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
24/01/05 02:14:10 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
24/01/05 02:14:10 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
24/01/05 02:14:10 INFO executor.Executor: Finished task 0.0 in stage 2.0 (TID 3). 1098 bytes result sent to driver
24/01/05 02:14:10 INFO executor.Executor: Finished task 1.0 in stage 2.0 (TID 4). 1110 bytes result sent to driver
24/01/05 02:14:10 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 2.0 (TID 4) in 16 ms on localhost (executor driver) (1/2)
24/01/05 02:14:10 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 2.0 (TID 3) in 19 ms on localhost (executor driver) (2/2)
24/01/05 02:14:10 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
24/01/05 02:14:10 INFO scheduler.DAGScheduler: ResultStage 2 (collect at HoodieJavaRDD.java:177) finished in 0.031 s
24/01/05 02:14:10 INFO scheduler.DAGScheduler: Job 0 finished: collect at HoodieJavaRDD.java:177, took 1.216288 s
24/01/05 02:14:10 INFO spark.SparkContext: Starting job: collect at HoodieSparkEngineContext.java:150
24/01/05 02:14:10 INFO scheduler.DAGScheduler: Got job 1 (collect at HoodieSparkEngineContext.java:150) with 1 output partitions
24/01/05 02:14:10 INFO scheduler.DAGScheduler: Final stage: ResultStage 3 (collect at HoodieSparkEngineContext.java:150)
24/01/05 02:14:10 INFO scheduler.DAGScheduler: Parents of final stage: List()
24/01/05 02:14:10 INFO scheduler.DAGScheduler: Missing parents: List()
24/01/05 02:14:10 INFO scheduler.DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[17] at flatMap at HoodieSparkEngineContext.java:150), which has no missing parents
24/01/05 02:14:10 INFO memory.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 339.1 KB, free 365.9 MB)
24/01/05 02:14:10 INFO memory.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 121.3 KB, free 365.8 MB)
24/01/05 02:14:10 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on adhoc-2:38763 (size: 121.3 KB, free: 366.2 MB)
24/01/05 02:14:10 INFO spark.SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1161
24/01/05 02:14:10 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[17] at flatMap at HoodieSparkEngineContext.java:150) (first 15 tasks are for partitions Vector(0))
24/01/05 02:14:10 INFO scheduler.TaskSchedulerImpl: Adding task set 3.0 with 1 tasks
24/01/05 02:14:10 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 3.0 (TID 5, localhost, executor driver, partition 0, PROCESS_LOCAL, 7735 bytes)
24/01/05 02:14:10 INFO executor.Executor: Running task 0.0 in stage 3.0 (TID 5)
24/01/05 02:14:10 INFO view.FileSystemViewManager: Creating View Manager with storage type :REMOTE_FIRST
24/01/05 02:14:10 INFO view.FileSystemViewManager: Creating remote first table view
24/01/05 02:14:10 INFO view.FileSystemViewManager: Creating remote view for basePath /user/hive/warehouse/stock_ticks_mor. Server=adhoc-2:43195, Timeout=300
24/01/05 02:14:10 INFO view.FileSystemViewManager: Creating InMemory based view for basePath /user/hive/warehouse/stock_ticks_mor
24/01/05 02:14:10 INFO view.AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
24/01/05 02:14:10 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:14:10 INFO view.RemoteHoodieTableFileSystemView: Sending request : (http://adhoc-2:43195/v1/hoodie/view/datafiles/beforeoron/latest/?partition=2018%2F08%2F31&maxinstant=20240105021347452&basepath=%2Fuser%2Fhive%2Fwarehouse%2Fstock_ticks_mor&lastinstantts=20240105021347452&timelinehash=4a3ab074d69f26b49cb5f0dfceaf1fd8f4760ea789bb800ca6cea65c8323c7a6)
24/01/05 02:14:10 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:14:10 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:14:10 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:14:10 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[==>20240105021407331__deltacommit__REQUESTED__20240105021408609]}
24/01/05 02:14:11 INFO collection.RocksDBDAO: DELETING RocksDB persisted at /tmp/hoodie_timeline_rocksdb/_user_hive_warehouse_stock_ticks_mor/0c7d3c4d-d593-4616-9a3a-110a03646c74
24/01/05 02:14:11 INFO collection.RocksDBDAO: No column family found. Loading default
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : RocksDB version: 7.5.3

24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Compile date 2022-09-02 09:50:20
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : DB SUMMARY

24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : DB Session ID:  JQIOZABS498AA5408TNR

24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : SST files in /tmp/hoodie_timeline_rocksdb/_user_hive_warehouse_stock_ticks_mor/0c7d3c4d-d593-4616-9a3a-110a03646c74 dir, Total Num: 0, files: 

24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Write Ahead Log file in /tmp/hoodie_timeline_rocksdb/_user_hive_warehouse_stock_ticks_mor/0c7d3c4d-d593-4616-9a3a-110a03646c74: 

24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                         Options.error_if_exists: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                       Options.create_if_missing: 1
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                         Options.paranoid_checks: 1
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :             Options.flush_verify_memtable_count: 1
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                               Options.track_and_verify_wals_in_manifest: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :        Options.verify_sst_unique_id_in_manifest: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                                     Options.env: 0x7f9da2f945e0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                                      Options.fs: PosixFileSystem
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                                Options.info_log: 0x7f9cc80f5b68
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                Options.max_file_opening_threads: 16
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                              Options.statistics: 0x7f9cc8081f20
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                               Options.use_fsync: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                       Options.max_log_file_size: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                  Options.max_manifest_file_size: 1073741824
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                   Options.log_file_time_to_roll: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                       Options.keep_log_file_num: 1000
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                    Options.recycle_log_file_num: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                         Options.allow_fallocate: 1
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                        Options.allow_mmap_reads: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                       Options.allow_mmap_writes: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                        Options.use_direct_reads: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                        Options.use_direct_io_for_flush_and_compaction: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :          Options.create_missing_column_families: 1
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                              Options.db_log_dir: 
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                                 Options.wal_dir: 
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                Options.table_cache_numshardbits: 6
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                         Options.WAL_ttl_seconds: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                       Options.WAL_size_limit_MB: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                        Options.max_write_batch_group_size_bytes: 1048576
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :             Options.manifest_preallocation_size: 4194304
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                     Options.is_fd_close_on_exec: 1
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                   Options.advise_random_on_open: 1
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                    Options.db_write_buffer_size: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                    Options.write_buffer_manager: 0x7f9cc80fa670
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :         Options.access_hint_on_compaction_start: 1
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :           Options.random_access_max_buffer_size: 1048576
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                      Options.use_adaptive_mutex: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                            Options.rate_limiter: (nil)
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :     Options.sst_file_manager.rate_bytes_per_sec: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                       Options.wal_recovery_mode: 2
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                  Options.enable_thread_tracking: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                  Options.enable_pipelined_write: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                  Options.unordered_write: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :         Options.allow_concurrent_memtable_write: 1
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :      Options.enable_write_thread_adaptive_yield: 1
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :             Options.write_thread_max_yield_usec: 100
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :            Options.write_thread_slow_yield_usec: 3
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                               Options.row_cache: None
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                              Options.wal_filter: None
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :             Options.avoid_flush_during_recovery: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :             Options.allow_ingest_behind: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :             Options.two_write_queues: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :             Options.manual_wal_flush: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :             Options.wal_compression: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :             Options.atomic_flush: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :             Options.avoid_unnecessary_blocking_io: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                 Options.persist_stats_to_disk: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                 Options.write_dbid_to_manifest: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                 Options.log_readahead_size: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                 Options.file_checksum_gen_factory: Unknown
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                 Options.best_efforts_recovery: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                Options.max_bgerror_resume_count: 2147483647
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :            Options.bgerror_resume_retry_interval: 1000000
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :             Options.allow_data_in_errors: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :             Options.db_host_id: __hostname__
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :             Options.enforce_single_del_contracts: true
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :             Options.max_background_jobs: 2
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :             Options.max_background_compactions: -1
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :             Options.max_subcompactions: 1
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :             Options.avoid_flush_during_shutdown: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :           Options.writable_file_max_buffer_size: 1048576
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :             Options.delayed_write_rate : 16777216
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :             Options.max_total_wal_size: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :             Options.delete_obsolete_files_period_micros: 21600000000
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                   Options.stats_dump_period_sec: 300
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                 Options.stats_persist_period_sec: 600
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                 Options.stats_history_buffer_size: 1048576
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                          Options.max_open_files: -1
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                          Options.bytes_per_sync: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                      Options.wal_bytes_per_sync: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                   Options.strict_bytes_per_sync: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :       Options.compaction_readahead_size: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                  Options.max_background_flushes: -1
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Compression algorithms supported:
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : 	kZSTDNotFinalCompression supported: 1
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : 	kZSTD supported: 1
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : 	kXpressCompression supported: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : 	kLZ4HCCompression supported: 1
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : 	kLZ4Compression supported: 1
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : 	kBZip2Compression supported: 1
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : 	kZlibCompression supported: 1
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : 	kSnappyCompression supported: 1
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Fast CRC32 supported: Not supported on x86
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : DMutex implementation: pthread_mutex_t
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : [/db_impl/db_impl_open.cc:313] Creating manifest 1 

24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : [/version_set.cc:4994] Recovering from manifest file: /tmp/hoodie_timeline_rocksdb/_user_hive_warehouse_stock_ticks_mor/0c7d3c4d-d593-4616-9a3a-110a03646c74/MANIFEST-000001

24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : [/column_family.cc:618] --------------- Options for column family [default]:

24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :               Options.comparator: leveldb.BytewiseComparator
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :           Options.merge_operator: None
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :        Options.compaction_filter: None
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :        Options.compaction_filter_factory: None
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :  Options.sst_partitioner_factory: None
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :         Options.memtable_factory: SkipListFactory
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :            Options.table_factory: BlockBasedTable
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :            table_factory options:   flush_block_policy_factory: FlushBlockBySizePolicyFactory (0x7f9cc80f85c0)
  cache_index_and_filter_blocks: 0
  cache_index_and_filter_blocks_with_high_priority: 1
  pin_l0_filter_and_index_blocks_in_cache: 0
  pin_top_level_index_and_filter: 1
  index_type: 0
  data_block_index_type: 0
  index_shortening: 1
  data_block_hash_table_util_ratio: 0.750000
  checksum: 1
  no_block_cache: 0
  block_cache: 0x7f9cc80f8610
  block_cache_name: LRUCache
  block_cache_options:
    capacity : 8388608
    num_shard_bits : 4
    strict_capacity_limit : 0
    memory_allocator : None
    high_pri_pool_ratio: 0.000
  block_cache_compressed: (nil)
  persistent_cache: (nil)
  block_size: 4096
  block_size_deviation: 10
  block_restart_interval: 16
  index_block_restart_interval: 1
  metadata_block_size: 4096
  partition_filters: 0
  use_delta_encoding: 1
  filter_policy: nullptr
  whole_key_filtering: 1
  verify_compression: 0
  read_amp_bytes_per_bit: 0
  format_version: 5
  enable_index_compression: 1
  block_align: 0
  max_auto_readahead_size: 262144
  prepopulate_block_cache: 0
  initial_auto_readahead_size: 8192

24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :        Options.write_buffer_size: 67108864
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :  Options.max_write_buffer_number: 2
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :          Options.compression: Snappy
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression: Disabled
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :       Options.prefix_extractor: nullptr
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :   Options.memtable_insert_with_hint_prefix_extractor: nullptr
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :             Options.num_levels: 7
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :        Options.min_write_buffer_number_to_merge: 1
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :     Options.max_write_buffer_number_to_maintain: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :     Options.max_write_buffer_size_to_maintain: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :            Options.bottommost_compression_opts.window_bits: -14
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression_opts.level: 32767
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :               Options.bottommost_compression_opts.strategy: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.max_dict_bytes: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.zstd_max_train_bytes: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.parallel_threads: 1
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression_opts.enabled: false
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.max_dict_buffer_bytes: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.use_zstd_dict_trainer: true
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :            Options.compression_opts.window_bits: -14
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                  Options.compression_opts.level: 32767
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :               Options.compression_opts.strategy: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.max_dict_bytes: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.zstd_max_train_bytes: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.use_zstd_dict_trainer: true
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.parallel_threads: 1
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                  Options.compression_opts.enabled: false
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.max_dict_buffer_bytes: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :      Options.level0_file_num_compaction_trigger: 4
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :          Options.level0_slowdown_writes_trigger: 20
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :              Options.level0_stop_writes_trigger: 36
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                   Options.target_file_size_base: 67108864
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :             Options.target_file_size_multiplier: 1
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                Options.max_bytes_for_level_base: 268435456
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Options.level_compaction_dynamic_level_bytes: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :          Options.max_bytes_for_level_multiplier: 10.000000
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[0]: 1
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[1]: 1
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[2]: 1
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[3]: 1
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[4]: 1
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[5]: 1
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[6]: 1
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :       Options.max_sequential_skip_in_iterations: 8
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                    Options.max_compaction_bytes: 1677721600
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                        Options.arena_block_size: 1048576
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :   Options.soft_pending_compaction_bytes_limit: 68719476736
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :   Options.hard_pending_compaction_bytes_limit: 274877906944
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                Options.disable_auto_compactions: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                        Options.compaction_style: kCompactionStyleLevel
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                          Options.compaction_pri: kMinOverlappingRatio
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.size_ratio: 1
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.min_merge_width: 2
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.max_merge_width: 4294967295
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.max_size_amplification_percent: 200
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.compression_size_percent: -1
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.stop_style: kCompactionStopStyleTotalSize
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_fifo.max_table_files_size: 1073741824
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_fifo.allow_compaction: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                   Options.table_properties_collectors: 
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                   Options.inplace_update_support: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                 Options.inplace_update_num_locks: 10000
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :               Options.memtable_prefix_bloom_size_ratio: 0.000000
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :               Options.memtable_whole_key_filtering: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :   Options.memtable_huge_page_size: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                           Options.bloom_locality: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                    Options.max_successive_merges: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                Options.optimize_filters_for_hits: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                Options.paranoid_file_checks: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                Options.force_consistency_checks: 1
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                Options.report_bg_io_stats: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                               Options.ttl: 2592000
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :          Options.periodic_compaction_seconds: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :  Options.preclude_last_level_data_seconds: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                       Options.enable_blob_files: false
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                           Options.min_blob_size: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                          Options.blob_file_size: 268435456
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                   Options.blob_compression_type: NoCompression
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :          Options.enable_blob_garbage_collection: false
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :      Options.blob_garbage_collection_age_cutoff: 0.250000
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Options.blob_garbage_collection_force_threshold: 1.000000
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :          Options.blob_compaction_readahead_size: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                Options.blob_file_starting_level: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Options.experimental_mempurge_threshold: 0.000000
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : [/version_set.cc:5042] Recovered from manifest file:/tmp/hoodie_timeline_rocksdb/_user_hive_warehouse_stock_ticks_mor/0c7d3c4d-d593-4616-9a3a-110a03646c74/MANIFEST-000001 succeeded,manifest_file_number is 1, next_file_number is 3, last_sequence is 0, log_number is 0,prev_log_number is 0,max_column_family is 0,min_log_number_to_keep is 0

24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : [/version_set.cc:5051] Column family [default] (ID 0), log number is 0

24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : [/db_impl/db_impl_open.cc:532] DB ID: 4e55d780-6715-4b3a-9fc0-58980d18b9bd

24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : [/version_set.cc:4513] Creating manifest 5

24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : [/db_impl/db_impl_open.cc:2024] SstFileManager instance 0x7f9cc80faa60
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : DB pointer 0x7f9cc80fb440
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : [/db_impl/db_impl.cc:1070] ------- DUMPING STATS -------
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : [/db_impl/db_impl.cc:1071] 
** DB Stats **
Uptime(secs): 0.0 total, 0.0 interval
Cumulative writes: 0 writes, 0 keys, 0 commit groups, 0.0 writes per commit group, ingest: 0.00 GB, 0.00 MB/s
Cumulative WAL: 0 writes, 0 syncs, 0.00 writes per sync, written: 0.00 GB, 0.00 MB/s
Cumulative stall: 00:00:0.000 H:M:S, 0.0 percent
Interval writes: 0 writes, 0 keys, 0 commit groups, 0.0 writes per commit group, ingest: 0.00 MB, 0.00 MB/s
Interval WAL: 0 writes, 0 syncs, 0.00 writes per sync, written: 0.00 GB, 0.00 MB/s
Interval stall: 00:00:0.000 H:M:S, 0.0 percent

** Compaction Stats [default] **
Level    Files   Size     Score Read(GB)  Rn(GB) Rnp1(GB) Write(GB) Wnew(GB) Moved(GB) W-Amp Rd(MB/s) Wr(MB/s) Comp(sec) CompMergeCPU(sec) Comp(cnt) Avg(sec) KeyIn KeyDrop Rblob(GB) Wblob(GB)
------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
 Sum      0/0    0.00 KB   0.0      0.0     0.0      0.0       0.0      0.0       0.0   0.0      0.0      0.0      0.00              0.00         0    0.000       0      0       0.0       0.0
 Int      0/0    0.00 KB   0.0      0.0     0.0      0.0       0.0      0.0       0.0   0.0      0.0      0.0      0.00              0.00         0    0.000       0      0       0.0       0.0

** Compaction Stats [default] **
Priority    Files   Size     Score Read(GB)  Rn(GB) Rnp1(GB) Write(GB) Wnew(GB) Moved(GB) W-Amp Rd(MB/s) Wr(MB/s) Comp(sec) CompMergeCPU(sec) Comp(cnt) Avg(sec) KeyIn KeyDrop Rblob(GB) Wblob(GB)
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Blob file count: 0, total size: 0.0 GB, garbage size: 0.0 GB, space amp: 0.0

Uptime(secs): 0.0 total, 0.0 interval
Flush(GB): cumulative 0.000, interval 0.000
AddFile(GB): cumulative 0.000, interval 0.000
AddFile(Total Files): cumulative 0, interval 0
AddFile(L0 Files): cumulative 0, interval 0
AddFile(Keys): cumulative 0, interval 0
Cumulative compaction: 0.00 GB write, 0.00 MB/s write, 0.00 GB read, 0.00 MB/s read, 0.0 seconds
Interval compaction: 0.00 GB write, 0.00 MB/s write, 0.00 GB read, 0.00 MB/s read, 0.0 seconds
Stalls(count): 0 level0_slowdown, 0 level0_slowdown_with_compaction, 0 level0_numfiles, 0 level0_numfiles_with_compaction, 0 stop for pending_compaction_bytes, 0 slowdown for pending_compaction_bytes, 0 memtable_compaction, 0 memtable_slowdown, interval 0 total count
Block cache LRUCache@0x7f9cc80f8610#599 capacity: 8.00 MB collections: 1 last_copies: 0 last_secs: 4.9e-05 secs_since: 0
Block cache entry stats(count,size,portion): Misc(1,0.00 KB,0%)

** File Read Latency Histogram By Level [default] **

24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : [/db_impl/db_impl.cc:762] STATISTICS:
 rocksdb.block.cache.miss COUNT : 0
rocksdb.block.cache.hit COUNT : 0
rocksdb.block.cache.add COUNT : 0
rocksdb.block.cache.add.failures COUNT : 0
rocksdb.block.cache.index.miss COUNT : 0
rocksdb.block.cache.index.hit COUNT : 0
rocksdb.block.cache.index.add COUNT : 0
rocksdb.block.cache.index.bytes.insert COUNT : 0
rocksdb.block.cache.index.bytes.evict COUNT : 0
rocksdb.block.cache.filter.miss COUNT : 0
rocksdb.block.cache.filter.hit COUNT : 0
rocksdb.block.cache.filter.add COUNT : 0
rocksdb.block.cache.filter.bytes.insert COUNT : 0
rocksdb.block.cache.filter.bytes.evict COUNT : 0
rocksdb.block.cache.data.miss COUNT : 0
rocksdb.block.cache.data.hit COUNT : 0
rocksdb.block.cache.data.add COUNT : 0
rocksdb.block.cache.data.bytes.insert COUNT : 0
rocksdb.block.cache.bytes.read COUNT : 0
rocksdb.block.cache.bytes.write COUNT : 0
rocksdb.bloom.filter.useful COUNT : 0
rocksdb.bloom.filter.full.positive COUNT : 0
rocksdb.bloom.filter.full.true.positive COUNT : 0
rocksdb.bloom.filter.micros COUNT : 0
rocksdb.persistent.cache.hit COUNT : 0
rocksdb.persistent.cache.miss COUNT : 0
rocksdb.sim.block.cache.hit COUNT : 0
rocksdb.sim.block.cache.miss COUNT : 0
rocksdb.memtable.hit COUNT : 0
rocksdb.memtable.miss COUNT : 0
rocksdb.l0.hit COUNT : 0
rocksdb.l1.hit COUNT : 0
rocksdb.l2andup.hit COUNT : 0
rocksdb.compaction.key.drop.new COUNT : 0
rocksdb.compaction.key.drop.obsolete COUNT : 0
rocksdb.compaction.key.drop.range_del COUNT : 0
rocksdb.compaction.key.drop.user COUNT : 0
rocksdb.compaction.range_del.drop.obsolete COUNT : 0
rocksdb.compaction.optimized.del.drop.obsolete COUNT : 0
rocksdb.compaction.cancelled COUNT : 0
rocksdb.number.keys.written COUNT : 0
rocksdb.number.keys.read COUNT : 0
rocksdb.number.keys.updated COUNT : 0
rocksdb.bytes.written COUNT : 0
rocksdb.bytes.read COUNT : 0
rocksdb.number.db.seek COUNT : 0
rocksdb.number.db.next COUNT : 0
rocksdb.number.db.prev COUNT : 0
rocksdb.number.db.seek.found COUNT : 0
rocksdb.number.db.next.found COUNT : 0
rocksdb.number.db.prev.found COUNT : 0
rocksdb.db.iter.bytes.read COUNT : 0
rocksdb.no.file.closes COUNT : 0
rocksdb.no.file.opens COUNT : 0
rocksdb.no.file.errors COUNT : 0
rocksdb.l0.slowdown.micros COUNT : 0
rocksdb.memtable.compaction.micros COUNT : 0
rocksdb.l0.num.files.stall.micros COUNT : 0
rocksdb.stall.micros COUNT : 0
rocksdb.db.mutex.wait.micros COUNT : 0
rocksdb.rate.limit.delay.millis COUNT : 0
rocksdb.num.iterators COUNT : 0
rocksdb.number.multiget.get COUNT : 0
rocksdb.number.multiget.keys.read COUNT : 0
rocksdb.number.multiget.bytes.read COUNT : 0
rocksdb.number.deletes.filtered COUNT : 0
rocksdb.number.merge.failures COUNT : 0
rocksdb.bloom.filter.prefix.checked COUNT : 0
rocksdb.bloom.filter.prefix.useful COUNT : 0
rocksdb.number.reseeks.iteration COUNT : 0
rocksdb.getupdatessince.calls COUNT : 0
rocksdb.block.cachecompressed.miss COUNT : 0
rocksdb.block.cachecompressed.hit COUNT : 0
rocksdb.block.cachecompressed.add COUNT : 0
rocksdb.block.cachecompressed.add.failures COUNT : 0
rocksdb.wal.synced COUNT : 0
rocksdb.wal.bytes COUNT : 0
rocksdb.write.self COUNT : 0
rocksdb.write.other COUNT : 0
rocksdb.write.timeout COUNT : 0
rocksdb.write.wal COUNT : 0
rocksdb.compact.read.bytes COUNT : 0
rocksdb.compact.write.bytes COUNT : 0
rocksdb.flush.write.bytes COUNT : 0
rocksdb.compact.read.marked.bytes COUNT : 0
rocksdb.compact.read.periodic.bytes COUNT : 0
rocksdb.compact.read.ttl.bytes COUNT : 0
rocksdb.compact.write.marked.bytes COUNT : 0
rocksdb.compact.write.periodic.bytes COUNT : 0
rocksdb.compact.write.ttl.bytes COUNT : 0
rocksdb.number.direct.load.table.properties COUNT : 0
rocksdb.number.superversion_acquires COUNT : 0
rocksdb.number.superversion_releases COUNT : 0
rocksdb.number.superversion_cleanups COUNT : 0
rocksdb.number.block.compressed COUNT : 0
rocksdb.number.block.decompressed COUNT : 0
rocksdb.number.block.not_compressed COUNT : 0
rocksdb.merge.operation.time.nanos COUNT : 0
rocksdb.filter.operation.time.nanos COUNT : 0
rocksdb.row.cache.hit COUNT : 0
rocksdb.row.cache.miss COUNT : 0
rocksdb.read.amp.estimate.useful.bytes COUNT : 0
rocksdb.read.amp.total.read.bytes COUNT : 0
rocksdb.number.rate_limiter.drains COUNT : 0
rocksdb.number.iter.skip COUNT : 0
rocksdb.blobdb.num.put COUNT : 0
rocksdb.blobdb.num.write COUNT : 0
rocksdb.blobdb.num.get COUNT : 0
rocksdb.blobdb.num.multiget COUNT : 0
rocksdb.blobdb.num.seek COUNT : 0
rocksdb.blobdb.num.next COUNT : 0
rocksdb.blobdb.num.prev COUNT : 0
rocksdb.blobdb.num.keys.written COUNT : 0
rocksdb.blobdb.num.keys.read COUNT : 0
rocksdb.blobdb.bytes.written COUNT : 0
rocksdb.blobdb.bytes.read COUNT : 0
rocksdb.blobdb.write.inlined COUNT : 0
rocksdb.blobdb.write.inlined.ttl COUNT : 0
rocksdb.blobdb.write.blob COUNT : 0
rocksdb.blobdb.write.blob.ttl COUNT : 0
rocksdb.blobdb.blob.file.bytes.written COUNT : 0
rocksdb.blobdb.blob.file.bytes.read COUNT : 0
rocksdb.blobdb.blob.file.synced COUNT : 0
rocksdb.blobdb.blob.index.expired.count COUNT : 0
rocksdb.blobdb.blob.index.expired.size COUNT : 0
rocksdb.blobdb.blob.index.evicted.count COUNT : 0
rocksdb.blobdb.blob.index.evicted.size COUNT : 0
rocksdb.blobdb.gc.num.files COUNT : 0
rocksdb.blobdb.gc.num.new.files COUNT : 0
rocksdb.blobdb.gc.failures COUNT : 0
rocksdb.blobdb.gc.num.keys.overwritten COUNT : 0
rocksdb.blobdb.gc.num.keys.expired COUNT : 0
rocksdb.blobdb.gc.num.keys.relocated COUNT : 0
rocksdb.blobdb.gc.bytes.overwritten COUNT : 0
rocksdb.blobdb.gc.bytes.expired COUNT : 0
rocksdb.blobdb.gc.bytes.relocated COUNT : 0
rocksdb.blobdb.fifo.num.files.evicted COUNT : 0
rocksdb.blobdb.fifo.num.keys.evicted COUNT : 0
rocksdb.blobdb.fifo.bytes.evicted COUNT : 0
rocksdb.txn.overhead.mutex.prepare COUNT : 0
rocksdb.txn.overhead.mutex.old.commit.map COUNT : 0
rocksdb.txn.overhead.duplicate.key COUNT : 0
rocksdb.txn.overhead.mutex.snapshot COUNT : 0
rocksdb.txn.get.tryagain COUNT : 0
rocksdb.number.multiget.keys.found COUNT : 0
rocksdb.num.iterator.created COUNT : 0
rocksdb.num.iterator.deleted COUNT : 0
rocksdb.block.cache.compression.dict.miss COUNT : 0
rocksdb.block.cache.compression.dict.hit COUNT : 0
rocksdb.block.cache.compression.dict.add COUNT : 0
rocksdb.block.cache.compression.dict.bytes.insert COUNT : 0
rocksdb.block.cache.compression.dict.bytes.evict COUNT : 0
rocksdb.block.cache.add.redundant COUNT : 0
rocksdb.block.cache.index.add.redundant COUNT : 0
rocksdb.block.cache.filter.add.redundant COUNT : 0
rocksdb.block.cache.data.add.redundant COUNT : 0
rocksdb.block.cache.compression.dict.add.redundant COUNT : 0
rocksdb.files.marked.trash COUNT : 0
rocksdb.files.deleted.immediately COUNT : 0
rocksdb.error.handler.bg.errro.count COUNT : 0
rocksdb.error.handler.bg.io.errro.count COUNT : 0
rocksdb.error.handler.bg.retryable.io.errro.count COUNT : 0
rocksdb.error.handler.autoresume.count COUNT : 0
rocksdb.error.handler.autoresume.retry.total.count COUNT : 0
rocksdb.error.handler.autoresume.success.count COUNT : 0
rocksdb.memtable.payload.bytes.at.flush COUNT : 0
rocksdb.memtable.garbage.bytes.at.flush COUNT : 0
rocksdb.secondary.cache.hits COUNT : 0
rocksdb.verify_checksum.read.bytes COUNT : 0
rocksdb.backup.read.bytes COUNT : 0
rocksdb.backup.write.bytes COUNT : 0
rocksdb.remote.compact.read.bytes COUNT : 0
rocksdb.remote.compact.write.bytes COUNT : 0
rocksdb.hot.file.read.bytes COUNT : 0
rocksdb.warm.file.read.bytes COUNT : 0
rocksdb.cold.file.read.bytes COUNT : 0
rocksdb.hot.file.read.count COUNT : 0
rocksdb.warm.file.read.count COUNT : 0
rocksdb.cold.file.read.count COUNT : 0
rocksdb.last.level.read.bytes COUNT : 0
rocksdb.last.level.read.count COUNT : 0
rocksdb.non.last.level.read.bytes COUNT : 0
rocksdb.non.last.level.read.count COUNT : 0
rocksdb.block.checksum.compute.count COUNT : 0
rocksdb.multiget.coroutine.count COUNT : 0
rocksdb.blobdb.cache.miss COUNT : 0
rocksdb.blobdb.cache.hit COUNT : 0
rocksdb.blobdb.cache.add COUNT : 0
rocksdb.blobdb.cache.add.failures COUNT : 0
rocksdb.blobdb.cache.bytes.read COUNT : 0
rocksdb.blobdb.cache.bytes.write COUNT : 0
rocksdb.db.get.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.db.write.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.compaction.times.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.compaction.times.cpu_micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.subcompaction.setup.times.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.table.sync.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.compaction.outfile.sync.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.wal.file.sync.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.manifest.file.sync.micros P50 : 870.000000 P95 : 905.000000 P99 : 905.000000 P100 : 905.000000 COUNT : 2 SUM : 1756
rocksdb.table.open.io.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.db.multiget.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.read.block.compaction.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.read.block.get.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.write.raw.block.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.l0.slowdown.count P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.memtable.compaction.count P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.num.files.stall.count P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.hard.rate.limit.delay.count P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.soft.rate.limit.delay.count P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.numfiles.in.singlecompaction P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.db.seek.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.db.write.stall P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.sst.read.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.num.subcompactions.scheduled P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.bytes.per.read P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.bytes.per.write P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.bytes.per.multiget P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.bytes.compressed P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.bytes.decompressed P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.compression.times.nanos P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.decompression.times.nanos P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.read.num.merge_operands P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.blobdb.key.size P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.blobdb.value.size P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.blobdb.write.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.blobdb.get.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.blobdb.multiget.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.blobdb.seek.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.blobdb.next.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.blobdb.prev.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.blobdb.blob.file.write.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.blobdb.blob.file.read.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.blobdb.blob.file.sync.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.blobdb.gc.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.blobdb.compression.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.blobdb.decompression.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.db.flush.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.sst.batch.size P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.num.index.and.filter.blocks.read.per.level P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.num.data.blocks.read.per.level P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.num.sst.read.per.level P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.error.handler.autoresume.retry.count P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.async.read.bytes P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.poll.wait.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.prefetched.bytes.discarded P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.multiget.io.batch.size P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.num.level.read.per.multiget P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0

24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : [/column_family.cc:618] --------------- Options for column family [hudi_view__user_hive_warehouse_stock_ticks_mor]:

24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :               Options.comparator: leveldb.BytewiseComparator
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :           Options.merge_operator: None
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :        Options.compaction_filter: None
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :        Options.compaction_filter_factory: None
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :  Options.sst_partitioner_factory: None
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :         Options.memtable_factory: SkipListFactory
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :            Options.table_factory: BlockBasedTable
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :            table_factory options:   flush_block_policy_factory: FlushBlockBySizePolicyFactory (0x7f9cc8126130)
  cache_index_and_filter_blocks: 0
  cache_index_and_filter_blocks_with_high_priority: 1
  pin_l0_filter_and_index_blocks_in_cache: 0
  pin_top_level_index_and_filter: 1
  index_type: 0
  data_block_index_type: 0
  index_shortening: 1
  data_block_hash_table_util_ratio: 0.750000
  checksum: 1
  no_block_cache: 0
  block_cache: 0x7f9cc81192c0
  block_cache_name: LRUCache
  block_cache_options:
    capacity : 8388608
    num_shard_bits : 4
    strict_capacity_limit : 0
    memory_allocator : None
    high_pri_pool_ratio: 0.000
  block_cache_compressed: (nil)
  persistent_cache: (nil)
  block_size: 4096
  block_size_deviation: 10
  block_restart_interval: 16
  index_block_restart_interval: 1
  metadata_block_size: 4096
  partition_filters: 0
  use_delta_encoding: 1
  filter_policy: nullptr
  whole_key_filtering: 1
  verify_compression: 0
  read_amp_bytes_per_bit: 0
  format_version: 5
  enable_index_compression: 1
  block_align: 0
  max_auto_readahead_size: 262144
  prepopulate_block_cache: 0
  initial_auto_readahead_size: 8192

24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :        Options.write_buffer_size: 67108864
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :  Options.max_write_buffer_number: 2
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :          Options.compression: Snappy
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression: Disabled
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :       Options.prefix_extractor: nullptr
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :   Options.memtable_insert_with_hint_prefix_extractor: nullptr
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :             Options.num_levels: 7
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :        Options.min_write_buffer_number_to_merge: 1
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :     Options.max_write_buffer_number_to_maintain: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :     Options.max_write_buffer_size_to_maintain: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :            Options.bottommost_compression_opts.window_bits: -14
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression_opts.level: 32767
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :               Options.bottommost_compression_opts.strategy: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.max_dict_bytes: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.zstd_max_train_bytes: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.parallel_threads: 1
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression_opts.enabled: false
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.max_dict_buffer_bytes: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.use_zstd_dict_trainer: true
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :            Options.compression_opts.window_bits: -14
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                  Options.compression_opts.level: 32767
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :               Options.compression_opts.strategy: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.max_dict_bytes: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.zstd_max_train_bytes: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.use_zstd_dict_trainer: true
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.parallel_threads: 1
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                  Options.compression_opts.enabled: false
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.max_dict_buffer_bytes: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :      Options.level0_file_num_compaction_trigger: 4
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :          Options.level0_slowdown_writes_trigger: 20
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :              Options.level0_stop_writes_trigger: 36
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                   Options.target_file_size_base: 67108864
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :             Options.target_file_size_multiplier: 1
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                Options.max_bytes_for_level_base: 268435456
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Options.level_compaction_dynamic_level_bytes: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :          Options.max_bytes_for_level_multiplier: 10.000000
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[0]: 1
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[1]: 1
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[2]: 1
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[3]: 1
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[4]: 1
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[5]: 1
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[6]: 1
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :       Options.max_sequential_skip_in_iterations: 8
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                    Options.max_compaction_bytes: 1677721600
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                        Options.arena_block_size: 1048576
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :   Options.soft_pending_compaction_bytes_limit: 68719476736
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :   Options.hard_pending_compaction_bytes_limit: 274877906944
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                Options.disable_auto_compactions: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                        Options.compaction_style: kCompactionStyleLevel
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                          Options.compaction_pri: kMinOverlappingRatio
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.size_ratio: 1
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.min_merge_width: 2
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.max_merge_width: 4294967295
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.max_size_amplification_percent: 200
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.compression_size_percent: -1
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.stop_style: kCompactionStopStyleTotalSize
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_fifo.max_table_files_size: 1073741824
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_fifo.allow_compaction: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                   Options.table_properties_collectors: 
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                   Options.inplace_update_support: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                 Options.inplace_update_num_locks: 10000
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :               Options.memtable_prefix_bloom_size_ratio: 0.000000
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :               Options.memtable_whole_key_filtering: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :   Options.memtable_huge_page_size: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                           Options.bloom_locality: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                    Options.max_successive_merges: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                Options.optimize_filters_for_hits: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                Options.paranoid_file_checks: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                Options.force_consistency_checks: 1
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                Options.report_bg_io_stats: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                               Options.ttl: 2592000
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :          Options.periodic_compaction_seconds: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :  Options.preclude_last_level_data_seconds: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                       Options.enable_blob_files: false
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                           Options.min_blob_size: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                          Options.blob_file_size: 268435456
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                   Options.blob_compression_type: NoCompression
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :          Options.enable_blob_garbage_collection: false
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :      Options.blob_garbage_collection_age_cutoff: 0.250000
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Options.blob_garbage_collection_force_threshold: 1.000000
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :          Options.blob_compaction_readahead_size: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                Options.blob_file_starting_level: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Options.experimental_mempurge_threshold: 0.000000
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : [/db_impl/db_impl.cc:2861] Created column family [hudi_view__user_hive_warehouse_stock_ticks_mor] (ID 1)
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : [/column_family.cc:618] --------------- Options for column family [hudi_pending_compaction__user_hive_warehouse_stock_ticks_mor]:

24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :               Options.comparator: leveldb.BytewiseComparator
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :           Options.merge_operator: None
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :        Options.compaction_filter: None
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :        Options.compaction_filter_factory: None
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :  Options.sst_partitioner_factory: None
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :         Options.memtable_factory: SkipListFactory
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :            Options.table_factory: BlockBasedTable
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :            table_factory options:   flush_block_policy_factory: FlushBlockBySizePolicyFactory (0x7f9cc8123320)
  cache_index_and_filter_blocks: 0
  cache_index_and_filter_blocks_with_high_priority: 1
  pin_l0_filter_and_index_blocks_in_cache: 0
  pin_top_level_index_and_filter: 1
  index_type: 0
  data_block_index_type: 0
  index_shortening: 1
  data_block_hash_table_util_ratio: 0.750000
  checksum: 1
  no_block_cache: 0
  block_cache: 0x7f9cc81175e0
  block_cache_name: LRUCache
  block_cache_options:
    capacity : 8388608
    num_shard_bits : 4
    strict_capacity_limit : 0
    memory_allocator : None
    high_pri_pool_ratio: 0.000
  block_cache_compressed: (nil)
  persistent_cache: (nil)
  block_size: 4096
  block_size_deviation: 10
  block_restart_interval: 16
  index_block_restart_interval: 1
  metadata_block_size: 4096
  partition_filters: 0
  use_delta_encoding: 1
  filter_policy: nullptr
  whole_key_filtering: 1
  verify_compression: 0
  read_amp_bytes_per_bit: 0
  format_version: 5
  enable_index_compression: 1
  block_align: 0
  max_auto_readahead_size: 262144
  prepopulate_block_cache: 0
  initial_auto_readahead_size: 8192

24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :        Options.write_buffer_size: 67108864
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :  Options.max_write_buffer_number: 2
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :          Options.compression: Snappy
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression: Disabled
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :       Options.prefix_extractor: nullptr
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :   Options.memtable_insert_with_hint_prefix_extractor: nullptr
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :             Options.num_levels: 7
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :        Options.min_write_buffer_number_to_merge: 1
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :     Options.max_write_buffer_number_to_maintain: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :     Options.max_write_buffer_size_to_maintain: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :            Options.bottommost_compression_opts.window_bits: -14
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression_opts.level: 32767
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :               Options.bottommost_compression_opts.strategy: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.max_dict_bytes: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.zstd_max_train_bytes: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.parallel_threads: 1
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression_opts.enabled: false
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.max_dict_buffer_bytes: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.use_zstd_dict_trainer: true
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :            Options.compression_opts.window_bits: -14
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                  Options.compression_opts.level: 32767
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :               Options.compression_opts.strategy: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.max_dict_bytes: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.zstd_max_train_bytes: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.use_zstd_dict_trainer: true
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.parallel_threads: 1
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                  Options.compression_opts.enabled: false
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.max_dict_buffer_bytes: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :      Options.level0_file_num_compaction_trigger: 4
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :          Options.level0_slowdown_writes_trigger: 20
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :              Options.level0_stop_writes_trigger: 36
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                   Options.target_file_size_base: 67108864
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :             Options.target_file_size_multiplier: 1
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                Options.max_bytes_for_level_base: 268435456
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Options.level_compaction_dynamic_level_bytes: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :          Options.max_bytes_for_level_multiplier: 10.000000
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[0]: 1
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[1]: 1
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[2]: 1
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[3]: 1
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[4]: 1
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[5]: 1
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[6]: 1
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :       Options.max_sequential_skip_in_iterations: 8
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                    Options.max_compaction_bytes: 1677721600
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                        Options.arena_block_size: 1048576
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :   Options.soft_pending_compaction_bytes_limit: 68719476736
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :   Options.hard_pending_compaction_bytes_limit: 274877906944
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                Options.disable_auto_compactions: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                        Options.compaction_style: kCompactionStyleLevel
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                          Options.compaction_pri: kMinOverlappingRatio
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.size_ratio: 1
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.min_merge_width: 2
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.max_merge_width: 4294967295
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.max_size_amplification_percent: 200
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.compression_size_percent: -1
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.stop_style: kCompactionStopStyleTotalSize
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_fifo.max_table_files_size: 1073741824
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_fifo.allow_compaction: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                   Options.table_properties_collectors: 
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                   Options.inplace_update_support: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                 Options.inplace_update_num_locks: 10000
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :               Options.memtable_prefix_bloom_size_ratio: 0.000000
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :               Options.memtable_whole_key_filtering: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :   Options.memtable_huge_page_size: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                           Options.bloom_locality: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                    Options.max_successive_merges: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                Options.optimize_filters_for_hits: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                Options.paranoid_file_checks: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                Options.force_consistency_checks: 1
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                Options.report_bg_io_stats: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                               Options.ttl: 2592000
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :          Options.periodic_compaction_seconds: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :  Options.preclude_last_level_data_seconds: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                       Options.enable_blob_files: false
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                           Options.min_blob_size: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                          Options.blob_file_size: 268435456
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                   Options.blob_compression_type: NoCompression
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :          Options.enable_blob_garbage_collection: false
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :      Options.blob_garbage_collection_age_cutoff: 0.250000
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Options.blob_garbage_collection_force_threshold: 1.000000
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :          Options.blob_compaction_readahead_size: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                Options.blob_file_starting_level: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Options.experimental_mempurge_threshold: 0.000000
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : [/db_impl/db_impl.cc:2861] Created column family [hudi_pending_compaction__user_hive_warehouse_stock_ticks_mor] (ID 2)
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : [/column_family.cc:618] --------------- Options for column family [hudi_pending_log_compaction__user_hive_warehouse_stock_ticks_mor]:

24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :               Options.comparator: leveldb.BytewiseComparator
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :           Options.merge_operator: None
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :        Options.compaction_filter: None
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :        Options.compaction_filter_factory: None
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :  Options.sst_partitioner_factory: None
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :         Options.memtable_factory: SkipListFactory
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :            Options.table_factory: BlockBasedTable
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :            table_factory options:   flush_block_policy_factory: FlushBlockBySizePolicyFactory (0x7f9cc8114a10)
  cache_index_and_filter_blocks: 0
  cache_index_and_filter_blocks_with_high_priority: 1
  pin_l0_filter_and_index_blocks_in_cache: 0
  pin_top_level_index_and_filter: 1
  index_type: 0
  data_block_index_type: 0
  index_shortening: 1
  data_block_hash_table_util_ratio: 0.750000
  checksum: 1
  no_block_cache: 0
  block_cache: 0x7f9cc8114a60
  block_cache_name: LRUCache
  block_cache_options:
    capacity : 8388608
    num_shard_bits : 4
    strict_capacity_limit : 0
    memory_allocator : None
    high_pri_pool_ratio: 0.000
  block_cache_compressed: (nil)
  persistent_cache: (nil)
  block_size: 4096
  block_size_deviation: 10
  block_restart_interval: 16
  index_block_restart_interval: 1
  metadata_block_size: 4096
  partition_filters: 0
  use_delta_encoding: 1
  filter_policy: nullptr
  whole_key_filtering: 1
  verify_compression: 0
  read_amp_bytes_per_bit: 0
  format_version: 5
  enable_index_compression: 1
  block_align: 0
  max_auto_readahead_size: 262144
  prepopulate_block_cache: 0
  initial_auto_readahead_size: 8192

24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :        Options.write_buffer_size: 67108864
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :  Options.max_write_buffer_number: 2
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :          Options.compression: Snappy
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression: Disabled
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :       Options.prefix_extractor: nullptr
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :   Options.memtable_insert_with_hint_prefix_extractor: nullptr
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :             Options.num_levels: 7
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :        Options.min_write_buffer_number_to_merge: 1
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :     Options.max_write_buffer_number_to_maintain: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :     Options.max_write_buffer_size_to_maintain: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :            Options.bottommost_compression_opts.window_bits: -14
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression_opts.level: 32767
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :               Options.bottommost_compression_opts.strategy: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.max_dict_bytes: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.zstd_max_train_bytes: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.parallel_threads: 1
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression_opts.enabled: false
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.max_dict_buffer_bytes: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.use_zstd_dict_trainer: true
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :            Options.compression_opts.window_bits: -14
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                  Options.compression_opts.level: 32767
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :               Options.compression_opts.strategy: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.max_dict_bytes: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.zstd_max_train_bytes: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.use_zstd_dict_trainer: true
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.parallel_threads: 1
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                  Options.compression_opts.enabled: false
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.max_dict_buffer_bytes: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :      Options.level0_file_num_compaction_trigger: 4
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :          Options.level0_slowdown_writes_trigger: 20
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :              Options.level0_stop_writes_trigger: 36
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                   Options.target_file_size_base: 67108864
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :             Options.target_file_size_multiplier: 1
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                Options.max_bytes_for_level_base: 268435456
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Options.level_compaction_dynamic_level_bytes: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :          Options.max_bytes_for_level_multiplier: 10.000000
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[0]: 1
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[1]: 1
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[2]: 1
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[3]: 1
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[4]: 1
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[5]: 1
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[6]: 1
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :       Options.max_sequential_skip_in_iterations: 8
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                    Options.max_compaction_bytes: 1677721600
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                        Options.arena_block_size: 1048576
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :   Options.soft_pending_compaction_bytes_limit: 68719476736
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :   Options.hard_pending_compaction_bytes_limit: 274877906944
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                Options.disable_auto_compactions: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                        Options.compaction_style: kCompactionStyleLevel
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                          Options.compaction_pri: kMinOverlappingRatio
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.size_ratio: 1
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.min_merge_width: 2
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.max_merge_width: 4294967295
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.max_size_amplification_percent: 200
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.compression_size_percent: -1
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.stop_style: kCompactionStopStyleTotalSize
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_fifo.max_table_files_size: 1073741824
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_fifo.allow_compaction: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                   Options.table_properties_collectors: 
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                   Options.inplace_update_support: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                 Options.inplace_update_num_locks: 10000
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :               Options.memtable_prefix_bloom_size_ratio: 0.000000
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :               Options.memtable_whole_key_filtering: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :   Options.memtable_huge_page_size: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                           Options.bloom_locality: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                    Options.max_successive_merges: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                Options.optimize_filters_for_hits: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                Options.paranoid_file_checks: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                Options.force_consistency_checks: 1
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                Options.report_bg_io_stats: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                               Options.ttl: 2592000
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :          Options.periodic_compaction_seconds: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :  Options.preclude_last_level_data_seconds: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                       Options.enable_blob_files: false
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                           Options.min_blob_size: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                          Options.blob_file_size: 268435456
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                   Options.blob_compression_type: NoCompression
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :          Options.enable_blob_garbage_collection: false
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :      Options.blob_garbage_collection_age_cutoff: 0.250000
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Options.blob_garbage_collection_force_threshold: 1.000000
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :          Options.blob_compaction_readahead_size: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                Options.blob_file_starting_level: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Options.experimental_mempurge_threshold: 0.000000
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : [/db_impl/db_impl.cc:2861] Created column family [hudi_pending_log_compaction__user_hive_warehouse_stock_ticks_mor] (ID 3)
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : [/column_family.cc:618] --------------- Options for column family [hudi_bootstrap_basefile__user_hive_warehouse_stock_ticks_mor]:

24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :               Options.comparator: leveldb.BytewiseComparator
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :           Options.merge_operator: None
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :        Options.compaction_filter: None
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :        Options.compaction_filter_factory: None
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :  Options.sst_partitioner_factory: None
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :         Options.memtable_factory: SkipListFactory
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :            Options.table_factory: BlockBasedTable
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :            table_factory options:   flush_block_policy_factory: FlushBlockBySizePolicyFactory (0x7f9cc81593c0)
  cache_index_and_filter_blocks: 0
  cache_index_and_filter_blocks_with_high_priority: 1
  pin_l0_filter_and_index_blocks_in_cache: 0
  pin_top_level_index_and_filter: 1
  index_type: 0
  data_block_index_type: 0
  index_shortening: 1
  data_block_hash_table_util_ratio: 0.750000
  checksum: 1
  no_block_cache: 0
  block_cache: 0x7f9cc8159410
  block_cache_name: LRUCache
  block_cache_options:
    capacity : 8388608
    num_shard_bits : 4
    strict_capacity_limit : 0
    memory_allocator : None
    high_pri_pool_ratio: 0.000
  block_cache_compressed: (nil)
  persistent_cache: (nil)
  block_size: 4096
  block_size_deviation: 10
  block_restart_interval: 16
  index_block_restart_interval: 1
  metadata_block_size: 4096
  partition_filters: 0
  use_delta_encoding: 1
  filter_policy: nullptr
  whole_key_filtering: 1
  verify_compression: 0
  read_amp_bytes_per_bit: 0
  format_version: 5
  enable_index_compression: 1
  block_align: 0
  max_auto_readahead_size: 262144
  prepopulate_block_cache: 0
  initial_auto_readahead_size: 8192

24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :        Options.write_buffer_size: 67108864
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :  Options.max_write_buffer_number: 2
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :          Options.compression: Snappy
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression: Disabled
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :       Options.prefix_extractor: nullptr
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :   Options.memtable_insert_with_hint_prefix_extractor: nullptr
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :             Options.num_levels: 7
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :        Options.min_write_buffer_number_to_merge: 1
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :     Options.max_write_buffer_number_to_maintain: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :     Options.max_write_buffer_size_to_maintain: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :            Options.bottommost_compression_opts.window_bits: -14
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression_opts.level: 32767
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :               Options.bottommost_compression_opts.strategy: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.max_dict_bytes: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.zstd_max_train_bytes: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.parallel_threads: 1
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression_opts.enabled: false
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.max_dict_buffer_bytes: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.use_zstd_dict_trainer: true
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :            Options.compression_opts.window_bits: -14
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                  Options.compression_opts.level: 32767
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :               Options.compression_opts.strategy: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.max_dict_bytes: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.zstd_max_train_bytes: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.use_zstd_dict_trainer: true
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.parallel_threads: 1
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                  Options.compression_opts.enabled: false
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.max_dict_buffer_bytes: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :      Options.level0_file_num_compaction_trigger: 4
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :          Options.level0_slowdown_writes_trigger: 20
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :              Options.level0_stop_writes_trigger: 36
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                   Options.target_file_size_base: 67108864
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :             Options.target_file_size_multiplier: 1
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                Options.max_bytes_for_level_base: 268435456
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Options.level_compaction_dynamic_level_bytes: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :          Options.max_bytes_for_level_multiplier: 10.000000
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[0]: 1
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[1]: 1
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[2]: 1
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[3]: 1
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[4]: 1
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[5]: 1
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[6]: 1
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :       Options.max_sequential_skip_in_iterations: 8
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                    Options.max_compaction_bytes: 1677721600
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                        Options.arena_block_size: 1048576
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :   Options.soft_pending_compaction_bytes_limit: 68719476736
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :   Options.hard_pending_compaction_bytes_limit: 274877906944
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                Options.disable_auto_compactions: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                        Options.compaction_style: kCompactionStyleLevel
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                          Options.compaction_pri: kMinOverlappingRatio
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.size_ratio: 1
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.min_merge_width: 2
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.max_merge_width: 4294967295
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.max_size_amplification_percent: 200
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.compression_size_percent: -1
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.stop_style: kCompactionStopStyleTotalSize
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_fifo.max_table_files_size: 1073741824
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_fifo.allow_compaction: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                   Options.table_properties_collectors: 
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                   Options.inplace_update_support: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                 Options.inplace_update_num_locks: 10000
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :               Options.memtable_prefix_bloom_size_ratio: 0.000000
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :               Options.memtable_whole_key_filtering: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :   Options.memtable_huge_page_size: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                           Options.bloom_locality: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                    Options.max_successive_merges: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                Options.optimize_filters_for_hits: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                Options.paranoid_file_checks: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                Options.force_consistency_checks: 1
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                Options.report_bg_io_stats: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                               Options.ttl: 2592000
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :          Options.periodic_compaction_seconds: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :  Options.preclude_last_level_data_seconds: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                       Options.enable_blob_files: false
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                           Options.min_blob_size: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                          Options.blob_file_size: 268435456
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                   Options.blob_compression_type: NoCompression
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :          Options.enable_blob_garbage_collection: false
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :      Options.blob_garbage_collection_age_cutoff: 0.250000
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Options.blob_garbage_collection_force_threshold: 1.000000
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :          Options.blob_compaction_readahead_size: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                Options.blob_file_starting_level: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Options.experimental_mempurge_threshold: 0.000000
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : [/db_impl/db_impl.cc:2861] Created column family [hudi_bootstrap_basefile__user_hive_warehouse_stock_ticks_mor] (ID 4)
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : [/column_family.cc:618] --------------- Options for column family [hudi_partitions__user_hive_warehouse_stock_ticks_mor]:

24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :               Options.comparator: leveldb.BytewiseComparator
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :           Options.merge_operator: None
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :        Options.compaction_filter: None
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :        Options.compaction_filter_factory: None
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :  Options.sst_partitioner_factory: None
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :         Options.memtable_factory: SkipListFactory
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :            Options.table_factory: BlockBasedTable
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :            table_factory options:   flush_block_policy_factory: FlushBlockBySizePolicyFactory (0x7f9cc8163860)
  cache_index_and_filter_blocks: 0
  cache_index_and_filter_blocks_with_high_priority: 1
  pin_l0_filter_and_index_blocks_in_cache: 0
  pin_top_level_index_and_filter: 1
  index_type: 0
  data_block_index_type: 0
  index_shortening: 1
  data_block_hash_table_util_ratio: 0.750000
  checksum: 1
  no_block_cache: 0
  block_cache: 0x7f9cc81638b0
  block_cache_name: LRUCache
  block_cache_options:
    capacity : 8388608
    num_shard_bits : 4
    strict_capacity_limit : 0
    memory_allocator : None
    high_pri_pool_ratio: 0.000
  block_cache_compressed: (nil)
  persistent_cache: (nil)
  block_size: 4096
  block_size_deviation: 10
  block_restart_interval: 16
  index_block_restart_interval: 1
  metadata_block_size: 4096
  partition_filters: 0
  use_delta_encoding: 1
  filter_policy: nullptr
  whole_key_filtering: 1
  verify_compression: 0
  read_amp_bytes_per_bit: 0
  format_version: 5
  enable_index_compression: 1
  block_align: 0
  max_auto_readahead_size: 262144
  prepopulate_block_cache: 0
  initial_auto_readahead_size: 8192

24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :        Options.write_buffer_size: 67108864
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :  Options.max_write_buffer_number: 2
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :          Options.compression: Snappy
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression: Disabled
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :       Options.prefix_extractor: nullptr
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :   Options.memtable_insert_with_hint_prefix_extractor: nullptr
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :             Options.num_levels: 7
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :        Options.min_write_buffer_number_to_merge: 1
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :     Options.max_write_buffer_number_to_maintain: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :     Options.max_write_buffer_size_to_maintain: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :            Options.bottommost_compression_opts.window_bits: -14
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression_opts.level: 32767
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :               Options.bottommost_compression_opts.strategy: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.max_dict_bytes: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.zstd_max_train_bytes: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.parallel_threads: 1
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression_opts.enabled: false
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.max_dict_buffer_bytes: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.use_zstd_dict_trainer: true
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :            Options.compression_opts.window_bits: -14
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                  Options.compression_opts.level: 32767
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :               Options.compression_opts.strategy: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.max_dict_bytes: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.zstd_max_train_bytes: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.use_zstd_dict_trainer: true
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.parallel_threads: 1
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                  Options.compression_opts.enabled: false
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.max_dict_buffer_bytes: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :      Options.level0_file_num_compaction_trigger: 4
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :          Options.level0_slowdown_writes_trigger: 20
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :              Options.level0_stop_writes_trigger: 36
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                   Options.target_file_size_base: 67108864
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :             Options.target_file_size_multiplier: 1
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                Options.max_bytes_for_level_base: 268435456
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Options.level_compaction_dynamic_level_bytes: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :          Options.max_bytes_for_level_multiplier: 10.000000
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[0]: 1
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[1]: 1
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[2]: 1
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[3]: 1
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[4]: 1
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[5]: 1
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[6]: 1
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :       Options.max_sequential_skip_in_iterations: 8
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                    Options.max_compaction_bytes: 1677721600
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                        Options.arena_block_size: 1048576
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :   Options.soft_pending_compaction_bytes_limit: 68719476736
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :   Options.hard_pending_compaction_bytes_limit: 274877906944
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                Options.disable_auto_compactions: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                        Options.compaction_style: kCompactionStyleLevel
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                          Options.compaction_pri: kMinOverlappingRatio
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.size_ratio: 1
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.min_merge_width: 2
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.max_merge_width: 4294967295
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.max_size_amplification_percent: 200
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.compression_size_percent: -1
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.stop_style: kCompactionStopStyleTotalSize
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_fifo.max_table_files_size: 1073741824
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_fifo.allow_compaction: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                   Options.table_properties_collectors: 
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                   Options.inplace_update_support: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                 Options.inplace_update_num_locks: 10000
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :               Options.memtable_prefix_bloom_size_ratio: 0.000000
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :               Options.memtable_whole_key_filtering: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :   Options.memtable_huge_page_size: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                           Options.bloom_locality: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                    Options.max_successive_merges: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                Options.optimize_filters_for_hits: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                Options.paranoid_file_checks: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                Options.force_consistency_checks: 1
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                Options.report_bg_io_stats: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                               Options.ttl: 2592000
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :          Options.periodic_compaction_seconds: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :  Options.preclude_last_level_data_seconds: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                       Options.enable_blob_files: false
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                           Options.min_blob_size: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                          Options.blob_file_size: 268435456
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                   Options.blob_compression_type: NoCompression
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :          Options.enable_blob_garbage_collection: false
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :      Options.blob_garbage_collection_age_cutoff: 0.250000
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Options.blob_garbage_collection_force_threshold: 1.000000
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :          Options.blob_compaction_readahead_size: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                Options.blob_file_starting_level: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Options.experimental_mempurge_threshold: 0.000000
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : [/db_impl/db_impl.cc:2861] Created column family [hudi_partitions__user_hive_warehouse_stock_ticks_mor] (ID 5)
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : [/column_family.cc:618] --------------- Options for column family [hudi_replaced_fg_user_hive_warehouse_stock_ticks_mor]:

24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :               Options.comparator: leveldb.BytewiseComparator
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :           Options.merge_operator: None
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :        Options.compaction_filter: None
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :        Options.compaction_filter_factory: None
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :  Options.sst_partitioner_factory: None
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :         Options.memtable_factory: SkipListFactory
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :            Options.table_factory: BlockBasedTable
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :            table_factory options:   flush_block_policy_factory: FlushBlockBySizePolicyFactory (0x7f9cc81744d0)
  cache_index_and_filter_blocks: 0
  cache_index_and_filter_blocks_with_high_priority: 1
  pin_l0_filter_and_index_blocks_in_cache: 0
  pin_top_level_index_and_filter: 1
  index_type: 0
  data_block_index_type: 0
  index_shortening: 1
  data_block_hash_table_util_ratio: 0.750000
  checksum: 1
  no_block_cache: 0
  block_cache: 0x7f9cc8174520
  block_cache_name: LRUCache
  block_cache_options:
    capacity : 8388608
    num_shard_bits : 4
    strict_capacity_limit : 0
    memory_allocator : None
    high_pri_pool_ratio: 0.000
  block_cache_compressed: (nil)
  persistent_cache: (nil)
  block_size: 4096
  block_size_deviation: 10
  block_restart_interval: 16
  index_block_restart_interval: 1
  metadata_block_size: 4096
  partition_filters: 0
  use_delta_encoding: 1
  filter_policy: nullptr
  whole_key_filtering: 1
  verify_compression: 0
  read_amp_bytes_per_bit: 0
  format_version: 5
  enable_index_compression: 1
  block_align: 0
  max_auto_readahead_size: 262144
  prepopulate_block_cache: 0
  initial_auto_readahead_size: 8192

24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :        Options.write_buffer_size: 67108864
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :  Options.max_write_buffer_number: 2
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :          Options.compression: Snappy
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression: Disabled
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :       Options.prefix_extractor: nullptr
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :   Options.memtable_insert_with_hint_prefix_extractor: nullptr
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :             Options.num_levels: 7
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :        Options.min_write_buffer_number_to_merge: 1
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :     Options.max_write_buffer_number_to_maintain: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :     Options.max_write_buffer_size_to_maintain: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :            Options.bottommost_compression_opts.window_bits: -14
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression_opts.level: 32767
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :               Options.bottommost_compression_opts.strategy: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.max_dict_bytes: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.zstd_max_train_bytes: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.parallel_threads: 1
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression_opts.enabled: false
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.max_dict_buffer_bytes: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.use_zstd_dict_trainer: true
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :            Options.compression_opts.window_bits: -14
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                  Options.compression_opts.level: 32767
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :               Options.compression_opts.strategy: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.max_dict_bytes: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.zstd_max_train_bytes: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.use_zstd_dict_trainer: true
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.parallel_threads: 1
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                  Options.compression_opts.enabled: false
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.max_dict_buffer_bytes: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :      Options.level0_file_num_compaction_trigger: 4
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :          Options.level0_slowdown_writes_trigger: 20
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :              Options.level0_stop_writes_trigger: 36
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                   Options.target_file_size_base: 67108864
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :             Options.target_file_size_multiplier: 1
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                Options.max_bytes_for_level_base: 268435456
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Options.level_compaction_dynamic_level_bytes: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :          Options.max_bytes_for_level_multiplier: 10.000000
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[0]: 1
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[1]: 1
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[2]: 1
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[3]: 1
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[4]: 1
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[5]: 1
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[6]: 1
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :       Options.max_sequential_skip_in_iterations: 8
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                    Options.max_compaction_bytes: 1677721600
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                        Options.arena_block_size: 1048576
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :   Options.soft_pending_compaction_bytes_limit: 68719476736
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :   Options.hard_pending_compaction_bytes_limit: 274877906944
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                Options.disable_auto_compactions: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                        Options.compaction_style: kCompactionStyleLevel
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                          Options.compaction_pri: kMinOverlappingRatio
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.size_ratio: 1
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.min_merge_width: 2
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.max_merge_width: 4294967295
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.max_size_amplification_percent: 200
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.compression_size_percent: -1
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.stop_style: kCompactionStopStyleTotalSize
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_fifo.max_table_files_size: 1073741824
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_fifo.allow_compaction: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                   Options.table_properties_collectors: 
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                   Options.inplace_update_support: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                 Options.inplace_update_num_locks: 10000
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :               Options.memtable_prefix_bloom_size_ratio: 0.000000
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :               Options.memtable_whole_key_filtering: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :   Options.memtable_huge_page_size: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                           Options.bloom_locality: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                    Options.max_successive_merges: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                Options.optimize_filters_for_hits: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                Options.paranoid_file_checks: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                Options.force_consistency_checks: 1
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                Options.report_bg_io_stats: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                               Options.ttl: 2592000
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :          Options.periodic_compaction_seconds: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :  Options.preclude_last_level_data_seconds: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                       Options.enable_blob_files: false
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                           Options.min_blob_size: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                          Options.blob_file_size: 268435456
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                   Options.blob_compression_type: NoCompression
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :          Options.enable_blob_garbage_collection: false
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :      Options.blob_garbage_collection_age_cutoff: 0.250000
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Options.blob_garbage_collection_force_threshold: 1.000000
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :          Options.blob_compaction_readahead_size: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                Options.blob_file_starting_level: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Options.experimental_mempurge_threshold: 0.000000
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : [/db_impl/db_impl.cc:2861] Created column family [hudi_replaced_fg_user_hive_warehouse_stock_ticks_mor] (ID 6)
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : [/column_family.cc:618] --------------- Options for column family [hudi_pending_clustering_fg_user_hive_warehouse_stock_ticks_mor]:

24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :               Options.comparator: leveldb.BytewiseComparator
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :           Options.merge_operator: None
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :        Options.compaction_filter: None
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :        Options.compaction_filter_factory: None
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :  Options.sst_partitioner_factory: None
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :         Options.memtable_factory: SkipListFactory
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :            Options.table_factory: BlockBasedTable
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :            table_factory options:   flush_block_policy_factory: FlushBlockBySizePolicyFactory (0x7f9cc817ec00)
  cache_index_and_filter_blocks: 0
  cache_index_and_filter_blocks_with_high_priority: 1
  pin_l0_filter_and_index_blocks_in_cache: 0
  pin_top_level_index_and_filter: 1
  index_type: 0
  data_block_index_type: 0
  index_shortening: 1
  data_block_hash_table_util_ratio: 0.750000
  checksum: 1
  no_block_cache: 0
  block_cache: 0x7f9cc8170440
  block_cache_name: LRUCache
  block_cache_options:
    capacity : 8388608
    num_shard_bits : 4
    strict_capacity_limit : 0
    memory_allocator : None
    high_pri_pool_ratio: 0.000
  block_cache_compressed: (nil)
  persistent_cache: (nil)
  block_size: 4096
  block_size_deviation: 10
  block_restart_interval: 16
  index_block_restart_interval: 1
  metadata_block_size: 4096
  partition_filters: 0
  use_delta_encoding: 1
  filter_policy: nullptr
  whole_key_filtering: 1
  verify_compression: 0
  read_amp_bytes_per_bit: 0
  format_version: 5
  enable_index_compression: 1
  block_align: 0
  max_auto_readahead_size: 262144
  prepopulate_block_cache: 0
  initial_auto_readahead_size: 8192

24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :        Options.write_buffer_size: 67108864
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :  Options.max_write_buffer_number: 2
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :          Options.compression: Snappy
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression: Disabled
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :       Options.prefix_extractor: nullptr
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :   Options.memtable_insert_with_hint_prefix_extractor: nullptr
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :             Options.num_levels: 7
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :        Options.min_write_buffer_number_to_merge: 1
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :     Options.max_write_buffer_number_to_maintain: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :     Options.max_write_buffer_size_to_maintain: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :            Options.bottommost_compression_opts.window_bits: -14
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression_opts.level: 32767
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :               Options.bottommost_compression_opts.strategy: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.max_dict_bytes: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.zstd_max_train_bytes: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.parallel_threads: 1
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression_opts.enabled: false
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.max_dict_buffer_bytes: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.use_zstd_dict_trainer: true
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :            Options.compression_opts.window_bits: -14
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                  Options.compression_opts.level: 32767
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :               Options.compression_opts.strategy: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.max_dict_bytes: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.zstd_max_train_bytes: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.use_zstd_dict_trainer: true
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.parallel_threads: 1
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                  Options.compression_opts.enabled: false
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.max_dict_buffer_bytes: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :      Options.level0_file_num_compaction_trigger: 4
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :          Options.level0_slowdown_writes_trigger: 20
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :              Options.level0_stop_writes_trigger: 36
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                   Options.target_file_size_base: 67108864
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :             Options.target_file_size_multiplier: 1
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                Options.max_bytes_for_level_base: 268435456
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Options.level_compaction_dynamic_level_bytes: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :          Options.max_bytes_for_level_multiplier: 10.000000
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[0]: 1
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[1]: 1
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[2]: 1
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[3]: 1
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[4]: 1
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[5]: 1
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[6]: 1
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :       Options.max_sequential_skip_in_iterations: 8
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                    Options.max_compaction_bytes: 1677721600
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                        Options.arena_block_size: 1048576
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :   Options.soft_pending_compaction_bytes_limit: 68719476736
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :   Options.hard_pending_compaction_bytes_limit: 274877906944
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                Options.disable_auto_compactions: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                        Options.compaction_style: kCompactionStyleLevel
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                          Options.compaction_pri: kMinOverlappingRatio
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.size_ratio: 1
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.min_merge_width: 2
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.max_merge_width: 4294967295
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.max_size_amplification_percent: 200
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.compression_size_percent: -1
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.stop_style: kCompactionStopStyleTotalSize
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_fifo.max_table_files_size: 1073741824
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_fifo.allow_compaction: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                   Options.table_properties_collectors: 
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                   Options.inplace_update_support: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                 Options.inplace_update_num_locks: 10000
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :               Options.memtable_prefix_bloom_size_ratio: 0.000000
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :               Options.memtable_whole_key_filtering: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :   Options.memtable_huge_page_size: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                           Options.bloom_locality: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                    Options.max_successive_merges: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                Options.optimize_filters_for_hits: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                Options.paranoid_file_checks: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                Options.force_consistency_checks: 1
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                Options.report_bg_io_stats: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                               Options.ttl: 2592000
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :          Options.periodic_compaction_seconds: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :  Options.preclude_last_level_data_seconds: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                       Options.enable_blob_files: false
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                           Options.min_blob_size: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                          Options.blob_file_size: 268435456
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                   Options.blob_compression_type: NoCompression
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :          Options.enable_blob_garbage_collection: false
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :      Options.blob_garbage_collection_age_cutoff: 0.250000
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Options.blob_garbage_collection_force_threshold: 1.000000
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :          Options.blob_compaction_readahead_size: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB :                Options.blob_file_starting_level: 0
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : Options.experimental_mempurge_threshold: 0.000000
24/01/05 02:14:11 INFO collection.RocksDBDAO: From Rocks DB : [/db_impl/db_impl.cc:2861] Created column family [hudi_pending_clustering_fg_user_hive_warehouse_stock_ticks_mor] (ID 7)
24/01/05 02:14:11 INFO view.RocksDbBasedFileSystemView: Resetting replacedFileGroups to ROCKSDB based file-system view at /tmp/hoodie_timeline_rocksdb, Total file-groups=0
24/01/05 02:14:11 INFO collection.RocksDBDAO: Prefix DELETE (query=part=) on hudi_replaced_fg_user_hive_warehouse_stock_ticks_mor
24/01/05 02:14:11 INFO view.RocksDbBasedFileSystemView: Resetting replacedFileGroups to ROCKSDB based file-system view complete
24/01/05 02:14:11 INFO view.AbstractTableFileSystemView: Took 4 ms to read  0 instants, 0 replaced file groups
24/01/05 02:14:11 INFO view.RocksDbBasedFileSystemView: Initializing pending compaction operations. Count=0
24/01/05 02:14:11 INFO view.RocksDbBasedFileSystemView: Initializing pending Log compaction operations. Count=0
24/01/05 02:14:11 INFO view.RocksDbBasedFileSystemView: Initializing external data file mapping. Count=0
24/01/05 02:14:11 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:14:11 INFO view.RocksDbBasedFileSystemView: Resetting file groups in pending clustering to ROCKSDB based file-system view at /tmp/hoodie_timeline_rocksdb, Total file-groups=0
24/01/05 02:14:11 INFO collection.RocksDBDAO: Prefix DELETE (query=part=) on hudi_pending_clustering_fg_user_hive_warehouse_stock_ticks_mor
24/01/05 02:14:11 INFO view.RocksDbBasedFileSystemView: Resetting replacedFileGroups to ROCKSDB based file-system view complete
24/01/05 02:14:11 INFO view.RocksDbBasedFileSystemView: Created ROCKSDB based file-system view at /tmp/hoodie_timeline_rocksdb
24/01/05 02:14:11 INFO view.AbstractTableFileSystemView: Building file system view for partition (2018/08/31)
24/01/05 02:14:11 INFO view.RocksDbBasedFileSystemView: Resetting and adding new partition (2018/08/31) to ROCKSDB based file-system view at /tmp/hoodie_timeline_rocksdb, Total file-groups=1
24/01/05 02:14:11 INFO collection.RocksDBDAO: Prefix DELETE (query=type=slice,part=2018/08/31,id=) on hudi_view__user_hive_warehouse_stock_ticks_mor
24/01/05 02:14:11 INFO collection.RocksDBDAO: Prefix DELETE (query=type=df,part=2018/08/31,id=) on hudi_view__user_hive_warehouse_stock_ticks_mor
24/01/05 02:14:11 INFO view.RocksDbBasedFileSystemView: Finished adding new partition (2018/08/31) to ROCKSDB based file-system view at /tmp/hoodie_timeline_rocksdb, Total file-groups=1
24/01/05 02:14:11 INFO collection.RocksDBDAO: Prefix Search for (query=type=slice,part=2018/08/31,id=) on hudi_view__user_hive_warehouse_stock_ticks_mor. Total Time Taken (msec)=9. Serialization Time taken(micro)=8896, num entries=1
24/01/05 02:14:11 INFO executor.Executor: Finished task 0.0 in stage 3.0 (TID 5). 1064 bytes result sent to driver
24/01/05 02:14:11 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 3.0 (TID 5) in 934 ms on localhost (executor driver) (1/1)
24/01/05 02:14:11 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
24/01/05 02:14:11 INFO scheduler.DAGScheduler: ResultStage 3 (collect at HoodieSparkEngineContext.java:150) finished in 0.981 s
24/01/05 02:14:11 INFO scheduler.DAGScheduler: Job 1 finished: collect at HoodieSparkEngineContext.java:150, took 0.985252 s
24/01/05 02:14:11 INFO rdd.MapPartitionsRDD: Removing RDD 9 from persistence list
24/01/05 02:14:11 INFO storage.BlockManager: Removing RDD 9
24/01/05 02:14:11 INFO view.AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
24/01/05 02:14:11 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:14:11 INFO spark.SparkContext: Starting job: countByKey at HoodieJavaPairRDD.java:105
24/01/05 02:14:11 INFO scheduler.DAGScheduler: Registering RDD 10 (mapToPair at HoodieJavaRDD.java:149)
24/01/05 02:14:11 INFO scheduler.DAGScheduler: Registering RDD 20 (mapToPair at HoodieJavaRDD.java:149)
24/01/05 02:14:11 INFO scheduler.DAGScheduler: Registering RDD 28 (countByKey at HoodieJavaPairRDD.java:105)
24/01/05 02:14:11 INFO scheduler.DAGScheduler: Got job 2 (countByKey at HoodieJavaPairRDD.java:105) with 2 output partitions
24/01/05 02:14:11 INFO scheduler.DAGScheduler: Final stage: ResultStage 8 (countByKey at HoodieJavaPairRDD.java:105)
24/01/05 02:14:11 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 7)
24/01/05 02:14:11 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 7)
24/01/05 02:14:11 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 5 (MapPartitionsRDD[10] at mapToPair at HoodieJavaRDD.java:149), which has no missing parents
24/01/05 02:14:11 INFO memory.MemoryStore: Block broadcast_4 stored as values in memory (estimated size 23.7 KB, free 365.8 MB)
24/01/05 02:14:11 INFO memory.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 11.4 KB, free 365.8 MB)
24/01/05 02:14:11 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on adhoc-2:38763 (size: 11.4 KB, free: 366.2 MB)
24/01/05 02:14:11 INFO spark.SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1161
24/01/05 02:14:11 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 5 (MapPartitionsRDD[10] at mapToPair at HoodieJavaRDD.java:149) (first 15 tasks are for partitions Vector(0, 1))
24/01/05 02:14:11 INFO scheduler.TaskSchedulerImpl: Adding task set 5.0 with 2 tasks
24/01/05 02:14:11 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 6 (MapPartitionsRDD[20] at mapToPair at HoodieJavaRDD.java:149), which has no missing parents
24/01/05 02:14:11 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 5.0 (TID 6, localhost, executor driver, partition 0, PROCESS_LOCAL, 7651 bytes)
24/01/05 02:14:11 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 5.0 (TID 7, localhost, executor driver, partition 1, ANY, 7651 bytes)
24/01/05 02:14:11 INFO executor.Executor: Running task 0.0 in stage 5.0 (TID 6)
24/01/05 02:14:11 INFO executor.Executor: Running task 1.0 in stage 5.0 (TID 7)
24/01/05 02:14:11 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
24/01/05 02:14:11 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
24/01/05 02:14:11 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
24/01/05 02:14:11 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
24/01/05 02:14:11 INFO executor.Executor: Finished task 0.0 in stage 5.0 (TID 6). 1123 bytes result sent to driver
24/01/05 02:14:11 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 5.0 (TID 6) in 28 ms on localhost (executor driver) (1/2)
24/01/05 02:14:11 INFO executor.Executor: Finished task 1.0 in stage 5.0 (TID 7). 1252 bytes result sent to driver
24/01/05 02:14:11 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 5.0 (TID 7) in 36 ms on localhost (executor driver) (2/2)
24/01/05 02:14:11 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool 
24/01/05 02:14:11 INFO memory.MemoryStore: Block broadcast_5 stored as values in memory (estimated size 341.0 KB, free 365.4 MB)
24/01/05 02:14:11 INFO memory.MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 122.2 KB, free 365.3 MB)
24/01/05 02:14:11 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on adhoc-2:38763 (size: 122.2 KB, free: 366.0 MB)
24/01/05 02:14:11 INFO spark.SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1161
24/01/05 02:14:11 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[20] at mapToPair at HoodieJavaRDD.java:149) (first 15 tasks are for partitions Vector(0))
24/01/05 02:14:11 INFO scheduler.TaskSchedulerImpl: Adding task set 6.0 with 1 tasks
24/01/05 02:14:11 INFO scheduler.DAGScheduler: ShuffleMapStage 5 (mapToPair at HoodieJavaRDD.java:149) finished in 0.071 s
24/01/05 02:14:11 INFO scheduler.DAGScheduler: looking for newly runnable stages
24/01/05 02:14:11 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 6)
24/01/05 02:14:11 INFO scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 7, ResultStage 8)
24/01/05 02:14:11 INFO scheduler.DAGScheduler: failed: Set()
24/01/05 02:14:11 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 6.0 (TID 8, localhost, executor driver, partition 0, PROCESS_LOCAL, 8108 bytes)
24/01/05 02:14:11 INFO executor.Executor: Running task 0.0 in stage 6.0 (TID 8)
24/01/05 02:14:11 INFO hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 115 records.
24/01/05 02:14:11 INFO hadoop.InternalParquetRecordReader: at row 0. reading next block
24/01/05 02:14:11 INFO compress.CodecPool: Got brand-new decompressor [.gz]
24/01/05 02:14:11 INFO hadoop.InternalParquetRecordReader: block read in memory in 34 ms. row count = 115
24/01/05 02:14:11 INFO spark.ContextCleaner: Cleaned accumulator 16
24/01/05 02:14:11 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on adhoc-2:38763 in memory (size: 11.4 KB, free: 366.0 MB)
24/01/05 02:14:11 INFO spark.ContextCleaner: Cleaned accumulator 99
24/01/05 02:14:11 INFO spark.ContextCleaner: Cleaned accumulator 88
24/01/05 02:14:11 INFO spark.ContextCleaner: Cleaned accumulator 45
24/01/05 02:14:11 INFO spark.ContextCleaner: Cleaned accumulator 59
24/01/05 02:14:11 INFO spark.ContextCleaner: Cleaned accumulator 13
24/01/05 02:14:11 INFO spark.ContextCleaner: Cleaned accumulator 71
24/01/05 02:14:11 INFO spark.ContextCleaner: Cleaned accumulator 74
24/01/05 02:14:11 INFO spark.ContextCleaner: Cleaned accumulator 98
24/01/05 02:14:11 INFO spark.ContextCleaner: Cleaned accumulator 8
24/01/05 02:14:11 INFO spark.ContextCleaner: Cleaned accumulator 85
24/01/05 02:14:11 INFO spark.ContextCleaner: Cleaned accumulator 4
24/01/05 02:14:11 INFO spark.ContextCleaner: Cleaned accumulator 38
24/01/05 02:14:11 INFO spark.ContextCleaner: Cleaned accumulator 81
24/01/05 02:14:11 INFO spark.ContextCleaner: Cleaned accumulator 93
24/01/05 02:14:11 INFO spark.ContextCleaner: Cleaned accumulator 65
24/01/05 02:14:11 INFO spark.ContextCleaner: Cleaned accumulator 78
24/01/05 02:14:11 INFO spark.ContextCleaner: Cleaned accumulator 43
24/01/05 02:14:11 INFO spark.ContextCleaner: Cleaned accumulator 29
24/01/05 02:14:11 INFO spark.ContextCleaner: Cleaned accumulator 9
24/01/05 02:14:11 INFO spark.ContextCleaner: Cleaned accumulator 24
24/01/05 02:14:11 INFO spark.ContextCleaner: Cleaned accumulator 70
24/01/05 02:14:11 INFO spark.ContextCleaner: Cleaned accumulator 22
24/01/05 02:14:11 INFO spark.ContextCleaner: Cleaned accumulator 31
24/01/05 02:14:11 INFO spark.ContextCleaner: Cleaned accumulator 39
24/01/05 02:14:11 INFO spark.ContextCleaner: Cleaned accumulator 77
24/01/05 02:14:11 INFO spark.ContextCleaner: Cleaned accumulator 58
24/01/05 02:14:11 INFO spark.ContextCleaner: Cleaned accumulator 64
24/01/05 02:14:11 INFO spark.ContextCleaner: Cleaned accumulator 56
24/01/05 02:14:11 INFO spark.ContextCleaner: Cleaned accumulator 6
24/01/05 02:14:11 INFO spark.ContextCleaner: Cleaned accumulator 76
24/01/05 02:14:11 INFO spark.ContextCleaner: Cleaned accumulator 95
24/01/05 02:14:11 INFO spark.ContextCleaner: Cleaned accumulator 30
24/01/05 02:14:11 INFO spark.ContextCleaner: Cleaned accumulator 3
24/01/05 02:14:11 INFO spark.ContextCleaner: Cleaned accumulator 61
24/01/05 02:14:11 INFO spark.ContextCleaner: Cleaned accumulator 75
24/01/05 02:14:11 INFO spark.ContextCleaner: Cleaned accumulator 15
24/01/05 02:14:11 INFO spark.ContextCleaner: Cleaned accumulator 0
24/01/05 02:14:11 INFO spark.ContextCleaner: Cleaned accumulator 23
24/01/05 02:14:11 INFO spark.ContextCleaner: Cleaned accumulator 51
24/01/05 02:14:11 INFO spark.ContextCleaner: Cleaned accumulator 12
24/01/05 02:14:11 INFO spark.ContextCleaner: Cleaned accumulator 28
24/01/05 02:14:11 INFO spark.ContextCleaner: Cleaned accumulator 1
24/01/05 02:14:11 INFO spark.ContextCleaner: Cleaned accumulator 89
24/01/05 02:14:11 INFO spark.ContextCleaner: Cleaned accumulator 14
24/01/05 02:14:11 INFO spark.ContextCleaner: Cleaned accumulator 44
24/01/05 02:14:11 INFO spark.ContextCleaner: Cleaned accumulator 46
24/01/05 02:14:11 INFO spark.ContextCleaner: Cleaned accumulator 17
24/01/05 02:14:11 INFO spark.ContextCleaner: Cleaned accumulator 73
24/01/05 02:14:11 INFO spark.ContextCleaner: Cleaned accumulator 60
24/01/05 02:14:11 INFO spark.ContextCleaner: Cleaned accumulator 86
24/01/05 02:14:11 INFO spark.ContextCleaner: Cleaned accumulator 21
24/01/05 02:14:11 INFO spark.ContextCleaner: Cleaned accumulator 82
24/01/05 02:14:11 INFO spark.ContextCleaner: Cleaned accumulator 68
24/01/05 02:14:11 INFO spark.ContextCleaner: Cleaned accumulator 66
24/01/05 02:14:11 INFO spark.ContextCleaner: Cleaned accumulator 62
24/01/05 02:14:11 INFO spark.ContextCleaner: Cleaned accumulator 5
24/01/05 02:14:11 INFO spark.ContextCleaner: Cleaned accumulator 91
24/01/05 02:14:11 INFO spark.ContextCleaner: Cleaned accumulator 26
24/01/05 02:14:11 INFO spark.ContextCleaner: Cleaned accumulator 48
24/01/05 02:14:11 INFO spark.ContextCleaner: Cleaned accumulator 47
24/01/05 02:14:11 INFO spark.ContextCleaner: Cleaned accumulator 92
24/01/05 02:14:11 INFO spark.ContextCleaner: Cleaned accumulator 50
24/01/05 02:14:11 INFO spark.ContextCleaner: Cleaned accumulator 83
24/01/05 02:14:11 INFO spark.ContextCleaner: Cleaned accumulator 34
24/01/05 02:14:11 INFO spark.ContextCleaner: Cleaned accumulator 25
24/01/05 02:14:11 INFO spark.ContextCleaner: Cleaned accumulator 94
24/01/05 02:14:11 INFO spark.ContextCleaner: Cleaned accumulator 18
24/01/05 02:14:11 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on adhoc-2:38763 in memory (size: 11.6 KB, free: 366.1 MB)
24/01/05 02:14:11 INFO spark.ContextCleaner: Cleaned accumulator 20
24/01/05 02:14:11 INFO spark.ContextCleaner: Cleaned accumulator 67
24/01/05 02:14:11 INFO spark.ContextCleaner: Cleaned accumulator 90
24/01/05 02:14:11 INFO spark.ContextCleaner: Cleaned accumulator 96
24/01/05 02:14:11 INFO spark.ContextCleaner: Cleaned accumulator 7
24/01/05 02:14:11 INFO spark.ContextCleaner: Cleaned accumulator 32
24/01/05 02:14:11 INFO spark.ContextCleaner: Cleaned accumulator 36
24/01/05 02:14:11 INFO spark.ContextCleaner: Cleaned accumulator 37
24/01/05 02:14:11 INFO spark.ContextCleaner: Cleaned accumulator 10
24/01/05 02:14:11 INFO spark.ContextCleaner: Cleaned accumulator 27
24/01/05 02:14:11 INFO spark.ContextCleaner: Cleaned accumulator 97
24/01/05 02:14:11 INFO spark.ContextCleaner: Cleaned accumulator 49
24/01/05 02:14:11 INFO spark.ContextCleaner: Cleaned accumulator 69
24/01/05 02:14:11 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on adhoc-2:38763 in memory (size: 121.3 KB, free: 366.2 MB)
24/01/05 02:14:11 INFO spark.ContextCleaner: Cleaned accumulator 19
24/01/05 02:14:11 INFO spark.ContextCleaner: Cleaned accumulator 72
24/01/05 02:14:11 INFO spark.ContextCleaner: Cleaned accumulator 33
24/01/05 02:14:11 INFO spark.ContextCleaner: Cleaned accumulator 84
24/01/05 02:14:11 INFO spark.ContextCleaner: Cleaned accumulator 87
24/01/05 02:14:11 INFO spark.ContextCleaner: Cleaned accumulator 11
24/01/05 02:14:11 INFO spark.ContextCleaner: Cleaned accumulator 42
24/01/05 02:14:11 INFO storage.BlockManagerInfo: Removed broadcast_2_piece0 on adhoc-2:38763 in memory (size: 2.4 KB, free: 366.2 MB)
24/01/05 02:14:11 INFO spark.ContextCleaner: Cleaned accumulator 41
24/01/05 02:14:11 INFO spark.ContextCleaner: Cleaned accumulator 79
24/01/05 02:14:11 INFO spark.ContextCleaner: Cleaned accumulator 80
24/01/05 02:14:11 INFO spark.ContextCleaner: Cleaned accumulator 52
24/01/05 02:14:11 INFO spark.ContextCleaner: Cleaned accumulator 57
24/01/05 02:14:11 INFO spark.ContextCleaner: Cleaned accumulator 55
24/01/05 02:14:11 INFO spark.ContextCleaner: Cleaned accumulator 2
24/01/05 02:14:11 INFO spark.ContextCleaner: Cleaned accumulator 35
24/01/05 02:14:11 INFO spark.ContextCleaner: Cleaned accumulator 63
24/01/05 02:14:11 INFO spark.ContextCleaner: Cleaned accumulator 54
24/01/05 02:14:11 INFO spark.ContextCleaner: Cleaned accumulator 40
24/01/05 02:14:11 INFO spark.ContextCleaner: Cleaned accumulator 53
24/01/05 02:14:12 INFO spark.ContextCleaner: Cleaned shuffle 0
24/01/05 02:14:12 INFO collection.RocksDBDAO: From Rocks DB : [/db_impl/db_impl.cc:896] ------- PERSISTING STATS -------
24/01/05 02:14:12 INFO collection.RocksDBDAO: From Rocks DB : [/db_impl/db_impl.cc:966] [Pre-GC] In-memory stats history size: 48 bytes, slice count: 0
24/01/05 02:14:12 INFO collection.RocksDBDAO: From Rocks DB : [/db_impl/db_impl.cc:975] [Post-GC] In-memory stats history size: 48 bytes, slice count: 0
24/01/05 02:14:12 INFO executor.Executor: Finished task 0.0 in stage 6.0 (TID 8). 865 bytes result sent to driver
24/01/05 02:14:12 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 6.0 (TID 8) in 521 ms on localhost (executor driver) (1/1)
24/01/05 02:14:12 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool 
24/01/05 02:14:12 INFO scheduler.DAGScheduler: ShuffleMapStage 6 (mapToPair at HoodieJavaRDD.java:149) finished in 0.578 s
24/01/05 02:14:12 INFO scheduler.DAGScheduler: looking for newly runnable stages
24/01/05 02:14:12 INFO scheduler.DAGScheduler: running: Set()
24/01/05 02:14:12 INFO scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 7, ResultStage 8)
24/01/05 02:14:12 INFO scheduler.DAGScheduler: failed: Set()
24/01/05 02:14:12 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 7 (MapPartitionsRDD[28] at countByKey at HoodieJavaPairRDD.java:105), which has no missing parents
24/01/05 02:14:12 INFO memory.MemoryStore: Block broadcast_6 stored as values in memory (estimated size 7.7 KB, free 365.8 MB)
24/01/05 02:14:12 INFO memory.MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 4.0 KB, free 365.8 MB)
24/01/05 02:14:12 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on adhoc-2:38763 (size: 4.0 KB, free: 366.2 MB)
24/01/05 02:14:12 INFO spark.SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1161
24/01/05 02:14:12 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 7 (MapPartitionsRDD[28] at countByKey at HoodieJavaPairRDD.java:105) (first 15 tasks are for partitions Vector(0, 1))
24/01/05 02:14:12 INFO scheduler.TaskSchedulerImpl: Adding task set 7.0 with 2 tasks
24/01/05 02:14:12 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 7.0 (TID 9, localhost, executor driver, partition 0, PROCESS_LOCAL, 7714 bytes)
24/01/05 02:14:12 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 7.0 (TID 10, localhost, executor driver, partition 1, PROCESS_LOCAL, 7714 bytes)
24/01/05 02:14:12 INFO executor.Executor: Running task 0.0 in stage 7.0 (TID 9)
24/01/05 02:14:12 INFO executor.Executor: Running task 1.0 in stage 7.0 (TID 10)
24/01/05 02:14:12 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
24/01/05 02:14:12 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
24/01/05 02:14:12 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
24/01/05 02:14:12 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
24/01/05 02:14:12 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
24/01/05 02:14:12 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
24/01/05 02:14:12 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
24/01/05 02:14:12 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
24/01/05 02:14:12 INFO memory.MemoryStore: Block rdd_26_0 stored as values in memory (estimated size 0.0 B, free 365.8 MB)
24/01/05 02:14:12 INFO storage.BlockManagerInfo: Added rdd_26_0 in memory on adhoc-2:38763 (size: 0.0 B, free: 366.2 MB)
24/01/05 02:14:12 INFO executor.Executor: Finished task 0.0 in stage 7.0 (TID 9). 1166 bytes result sent to driver
24/01/05 02:14:12 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 7.0 (TID 9) in 52 ms on localhost (executor driver) (1/2)
24/01/05 02:14:12 INFO memory.MemoryStore: Block rdd_26_1 stored as values in memory (estimated size 858.0 B, free 365.8 MB)
24/01/05 02:14:12 INFO storage.BlockManagerInfo: Added rdd_26_1 in memory on adhoc-2:38763 (size: 858.0 B, free: 366.2 MB)
24/01/05 02:14:12 INFO executor.Executor: Finished task 1.0 in stage 7.0 (TID 10). 1252 bytes result sent to driver
24/01/05 02:14:12 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 7.0 (TID 10) in 75 ms on localhost (executor driver) (2/2)
24/01/05 02:14:12 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool 
24/01/05 02:14:12 INFO scheduler.DAGScheduler: ShuffleMapStage 7 (countByKey at HoodieJavaPairRDD.java:105) finished in 0.091 s
24/01/05 02:14:12 INFO scheduler.DAGScheduler: looking for newly runnable stages
24/01/05 02:14:12 INFO scheduler.DAGScheduler: running: Set()
24/01/05 02:14:12 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 8)
24/01/05 02:14:12 INFO scheduler.DAGScheduler: failed: Set()
24/01/05 02:14:12 INFO scheduler.DAGScheduler: Submitting ResultStage 8 (ShuffledRDD[29] at countByKey at HoodieJavaPairRDD.java:105), which has no missing parents
24/01/05 02:14:12 INFO memory.MemoryStore: Block broadcast_7 stored as values in memory (estimated size 3.6 KB, free 365.8 MB)
24/01/05 02:14:12 INFO memory.MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 2.1 KB, free 365.8 MB)
24/01/05 02:14:12 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on adhoc-2:38763 (size: 2.1 KB, free: 366.2 MB)
24/01/05 02:14:12 INFO spark.SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1161
24/01/05 02:14:12 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ResultStage 8 (ShuffledRDD[29] at countByKey at HoodieJavaPairRDD.java:105) (first 15 tasks are for partitions Vector(0, 1))
24/01/05 02:14:12 INFO scheduler.TaskSchedulerImpl: Adding task set 8.0 with 2 tasks
24/01/05 02:14:12 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 8.0 (TID 11, localhost, executor driver, partition 0, PROCESS_LOCAL, 7662 bytes)
24/01/05 02:14:12 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 8.0 (TID 12, localhost, executor driver, partition 1, ANY, 7662 bytes)
24/01/05 02:14:12 INFO executor.Executor: Running task 0.0 in stage 8.0 (TID 11)
24/01/05 02:14:12 INFO executor.Executor: Running task 1.0 in stage 8.0 (TID 12)
24/01/05 02:14:12 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
24/01/05 02:14:12 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
24/01/05 02:14:12 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
24/01/05 02:14:12 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
24/01/05 02:14:12 INFO executor.Executor: Finished task 0.0 in stage 8.0 (TID 11). 1098 bytes result sent to driver
24/01/05 02:14:12 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 8.0 (TID 11) in 15 ms on localhost (executor driver) (1/2)
24/01/05 02:14:12 INFO executor.Executor: Finished task 1.0 in stage 8.0 (TID 12). 1210 bytes result sent to driver
24/01/05 02:14:12 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 8.0 (TID 12) in 17 ms on localhost (executor driver) (2/2)
24/01/05 02:14:12 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool 
24/01/05 02:14:12 INFO scheduler.DAGScheduler: ResultStage 8 (countByKey at HoodieJavaPairRDD.java:105) finished in 0.028 s
24/01/05 02:14:12 INFO scheduler.DAGScheduler: Job 2 finished: countByKey at HoodieJavaPairRDD.java:105, took 0.723748 s
24/01/05 02:14:12 INFO commit.UpsertPartitioner: AvgRecordSize => 1024
24/01/05 02:14:12 INFO spark.SparkContext: Starting job: collectAsMap at UpsertPartitioner.java:282
24/01/05 02:14:12 INFO scheduler.DAGScheduler: Got job 3 (collectAsMap at UpsertPartitioner.java:282) with 1 output partitions
24/01/05 02:14:12 INFO scheduler.DAGScheduler: Final stage: ResultStage 9 (collectAsMap at UpsertPartitioner.java:282)
24/01/05 02:14:12 INFO scheduler.DAGScheduler: Parents of final stage: List()
24/01/05 02:14:12 INFO scheduler.DAGScheduler: Missing parents: List()
24/01/05 02:14:12 INFO scheduler.DAGScheduler: Submitting ResultStage 9 (MapPartitionsRDD[31] at mapToPair at UpsertPartitioner.java:281), which has no missing parents
24/01/05 02:14:12 INFO memory.MemoryStore: Block broadcast_8 stored as values in memory (estimated size 340.5 KB, free 365.5 MB)
24/01/05 02:14:12 INFO memory.MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 121.8 KB, free 365.4 MB)
24/01/05 02:14:12 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on adhoc-2:38763 (size: 121.8 KB, free: 366.1 MB)
24/01/05 02:14:12 INFO spark.SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1161
24/01/05 02:14:12 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[31] at mapToPair at UpsertPartitioner.java:281) (first 15 tasks are for partitions Vector(0))
24/01/05 02:14:12 INFO scheduler.TaskSchedulerImpl: Adding task set 9.0 with 1 tasks
24/01/05 02:14:12 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 9.0 (TID 13, localhost, executor driver, partition 0, PROCESS_LOCAL, 7735 bytes)
24/01/05 02:14:12 INFO executor.Executor: Running task 0.0 in stage 9.0 (TID 13)
24/01/05 02:14:12 INFO view.FileSystemViewManager: Creating View Manager with storage type :REMOTE_FIRST
24/01/05 02:14:12 INFO view.FileSystemViewManager: Creating remote first table view
24/01/05 02:14:12 INFO view.FileSystemViewManager: Creating remote view for basePath /user/hive/warehouse/stock_ticks_mor. Server=adhoc-2:43195, Timeout=300
24/01/05 02:14:12 INFO view.FileSystemViewManager: Creating InMemory based view for basePath /user/hive/warehouse/stock_ticks_mor
24/01/05 02:14:12 INFO view.AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
24/01/05 02:14:12 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:14:12 INFO view.RemoteHoodieTableFileSystemView: Sending request : (http://adhoc-2:43195/v1/hoodie/view/slices/beforeoron/latest/?partition=2018%2F08%2F31&maxinstant=20240105021347452&includependingcompaction=false&basepath=%2Fuser%2Fhive%2Fwarehouse%2Fstock_ticks_mor&lastinstantts=20240105021347452&timelinehash=4a3ab074d69f26b49cb5f0dfceaf1fd8f4760ea789bb800ca6cea65c8323c7a6)
24/01/05 02:14:12 INFO collection.RocksDBDAO: Prefix Search for (query=type=slice,part=2018/08/31,id=) on hudi_view__user_hive_warehouse_stock_ticks_mor. Total Time Taken (msec)=6. Serialization Time taken(micro)=5766, num entries=1
24/01/05 02:14:12 INFO executor.Executor: Finished task 0.0 in stage 9.0 (TID 13). 813 bytes result sent to driver
24/01/05 02:14:12 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 9.0 (TID 13) in 74 ms on localhost (executor driver) (1/1)
24/01/05 02:14:12 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool 
24/01/05 02:14:12 INFO scheduler.DAGScheduler: ResultStage 9 (collectAsMap at UpsertPartitioner.java:282) finished in 0.106 s
24/01/05 02:14:12 INFO scheduler.DAGScheduler: Job 3 finished: collectAsMap at UpsertPartitioner.java:282, took 0.107672 s
24/01/05 02:14:12 INFO view.AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
24/01/05 02:14:12 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:14:12 INFO commit.UpsertPartitioner: Total Buckets: 1
24/01/05 02:14:12 INFO timeline.HoodieActiveTimeline: Checking for file exists ?/user/hive/warehouse/stock_ticks_mor/.hoodie/20240105021407331.deltacommit.requested
24/01/05 02:14:12 INFO timeline.HoodieActiveTimeline: Create new file for toInstant ?/user/hive/warehouse/stock_ticks_mor/.hoodie/20240105021407331.deltacommit.inflight
24/01/05 02:14:12 INFO commit.BaseSparkCommitActionExecutor: no validators configured.
24/01/05 02:14:12 INFO commit.BaseCommitActionExecutor: Auto commit disabled for 20240105021407331
24/01/05 02:14:12 INFO spark.SparkContext: Starting job: sum at StreamSync.java:783
24/01/05 02:14:12 INFO scheduler.DAGScheduler: Registering RDD 32 (mapToPair at HoodieJavaRDD.java:149)
24/01/05 02:14:12 INFO scheduler.DAGScheduler: Got job 4 (sum at StreamSync.java:783) with 1 output partitions
24/01/05 02:14:12 INFO scheduler.DAGScheduler: Final stage: ResultStage 14 (sum at StreamSync.java:783)
24/01/05 02:14:12 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 13)
24/01/05 02:14:12 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 13)
24/01/05 02:14:12 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 13 (MapPartitionsRDD[32] at mapToPair at HoodieJavaRDD.java:149), which has no missing parents
24/01/05 02:14:12 INFO memory.MemoryStore: Block broadcast_9 stored as values in memory (estimated size 344.4 KB, free 365.0 MB)
24/01/05 02:14:12 INFO memory.MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 123.0 KB, free 364.9 MB)
24/01/05 02:14:12 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on adhoc-2:38763 (size: 123.0 KB, free: 365.9 MB)
24/01/05 02:14:12 INFO spark.SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1161
24/01/05 02:14:12 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 13 (MapPartitionsRDD[32] at mapToPair at HoodieJavaRDD.java:149) (first 15 tasks are for partitions Vector(0, 1))
24/01/05 02:14:12 INFO scheduler.TaskSchedulerImpl: Adding task set 13.0 with 2 tasks
24/01/05 02:14:12 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 13.0 (TID 14, localhost, executor driver, partition 0, PROCESS_LOCAL, 7714 bytes)
24/01/05 02:14:12 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 13.0 (TID 15, localhost, executor driver, partition 1, PROCESS_LOCAL, 7714 bytes)
24/01/05 02:14:12 INFO executor.Executor: Running task 1.0 in stage 13.0 (TID 15)
24/01/05 02:14:12 INFO executor.Executor: Running task 0.0 in stage 13.0 (TID 14)
24/01/05 02:14:12 INFO storage.BlockManager: Found block rdd_26_0 locally
24/01/05 02:14:12 INFO storage.BlockManager: Found block rdd_26_1 locally
24/01/05 02:14:12 INFO executor.Executor: Finished task 0.0 in stage 13.0 (TID 14). 735 bytes result sent to driver
24/01/05 02:14:12 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 13.0 (TID 14) in 36 ms on localhost (executor driver) (1/2)
24/01/05 02:14:12 INFO executor.Executor: Finished task 1.0 in stage 13.0 (TID 15). 907 bytes result sent to driver
24/01/05 02:14:12 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 13.0 (TID 15) in 39 ms on localhost (executor driver) (2/2)
24/01/05 02:14:12 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 13.0, whose tasks have all completed, from pool 
24/01/05 02:14:12 INFO scheduler.DAGScheduler: ShuffleMapStage 13 (mapToPair at HoodieJavaRDD.java:149) finished in 0.073 s
24/01/05 02:14:12 INFO scheduler.DAGScheduler: looking for newly runnable stages
24/01/05 02:14:12 INFO scheduler.DAGScheduler: running: Set()
24/01/05 02:14:12 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 14)
24/01/05 02:14:12 INFO scheduler.DAGScheduler: failed: Set()
24/01/05 02:14:12 INFO scheduler.DAGScheduler: Submitting ResultStage 14 (MapPartitionsRDD[37] at mapToDouble at StreamSync.java:783), which has no missing parents
24/01/05 02:14:12 INFO memory.MemoryStore: Block broadcast_10 stored as values in memory (estimated size 351.4 KB, free 364.6 MB)
24/01/05 02:14:12 INFO memory.MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 126.9 KB, free 364.5 MB)
24/01/05 02:14:12 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on adhoc-2:38763 (size: 126.9 KB, free: 365.8 MB)
24/01/05 02:14:12 INFO spark.SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1161
24/01/05 02:14:12 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 14 (MapPartitionsRDD[37] at mapToDouble at StreamSync.java:783) (first 15 tasks are for partitions Vector(0))
24/01/05 02:14:12 INFO scheduler.TaskSchedulerImpl: Adding task set 14.0 with 1 tasks
24/01/05 02:14:12 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 14.0 (TID 16, localhost, executor driver, partition 0, ANY, 7662 bytes)
24/01/05 02:14:12 INFO executor.Executor: Running task 0.0 in stage 14.0 (TID 16)
24/01/05 02:14:12 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
24/01/05 02:14:12 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
24/01/05 02:14:12 INFO deltacommit.BaseSparkDeltaCommitActionExecutor: Merging updates for commit 20240105021407331 for file 0245deea-bfb7-4901-bd55-849b7d6fa366-0
24/01/05 02:14:12 INFO view.FileSystemViewManager: Creating View Manager with storage type :REMOTE_FIRST
24/01/05 02:14:12 INFO view.FileSystemViewManager: Creating remote first table view
24/01/05 02:14:12 INFO view.FileSystemViewManager: Creating remote view for basePath /user/hive/warehouse/stock_ticks_mor. Server=adhoc-2:43195, Timeout=300
24/01/05 02:14:12 INFO view.FileSystemViewManager: Creating InMemory based view for basePath /user/hive/warehouse/stock_ticks_mor
24/01/05 02:14:12 INFO view.AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
24/01/05 02:14:12 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:14:12 INFO view.RemoteHoodieTableFileSystemView: Sending request : (http://adhoc-2:43195/v1/hoodie/view/slices/file/latest/?partition=2018%2F08%2F31&basepath=%2Fuser%2Fhive%2Fwarehouse%2Fstock_ticks_mor&fileid=0245deea-bfb7-4901-bd55-849b7d6fa366-0&lastinstantts=20240105021347452&timelinehash=4a3ab074d69f26b49cb5f0dfceaf1fd8f4760ea789bb800ca6cea65c8323c7a6)
24/01/05 02:14:12 INFO collection.RocksDBDAO: Prefix Search for (query=type=slice,part=2018/08/31,id=0245deea-bfb7-4901-bd55-849b7d6fa366-0,instant=) on hudi_view__user_hive_warehouse_stock_ticks_mor. Total Time Taken (msec)=5. Serialization Time taken(micro)=5011, num entries=1
# WARNING: Unable to attach Serviceability Agent. Unable to attach even with module exceptions: [org.apache.hudi.org.openjdk.jol.vm.sa.SASupportException: Sense failed., org.apache.hudi.org.openjdk.jol.vm.sa.SASupportException: Sense failed., org.apache.hudi.org.openjdk.jol.vm.sa.SASupportException: Sense failed.]
24/01/05 02:14:13 WARN marker.WriteMarkersFactory: Timeline-server-based markers are not supported for HDFS: base path /user/hive/warehouse/stock_ticks_mor.  Falling back to direct markers.
24/01/05 02:14:13 INFO marker.DirectWriteMarkers: Creating Marker Path=/user/hive/warehouse/stock_ticks_mor/.hoodie/.temp/20240105021407331/2018/08/31/0245deea-bfb7-4901-bd55-849b7d6fa366-0_0-14-16_20240105021347452.parquet.marker.APPEND
24/01/05 02:14:13 INFO marker.DirectWriteMarkers: [direct] Created marker file /user/hive/warehouse/stock_ticks_mor/.hoodie/.temp/20240105021407331/2018/08/31/0245deea-bfb7-4901-bd55-849b7d6fa366-0_0-14-16_20240105021347452.parquet.marker.APPEND in 12 ms
24/01/05 02:14:13 INFO log.HoodieLogFormat$WriterBuilder: Building HoodieLogFormat Writer
24/01/05 02:14:13 INFO log.HoodieLogFormat$WriterBuilder: HoodieLogFile on path /user/hive/warehouse/stock_ticks_mor/2018/08/31/.0245deea-bfb7-4901-bd55-849b7d6fa366-0_20240105021347452.log.1_0-14-16
24/01/05 02:14:13 INFO log.HoodieLogFormatWriter: HoodieLogFile{pathStr='/user/hive/warehouse/stock_ticks_mor/2018/08/31/.0245deea-bfb7-4901-bd55-849b7d6fa366-0_20240105021347452.log.1_0-14-16', fileLen=0} does not exist. Create a new file
24/01/05 02:14:13 INFO io.HoodieAppendHandle: AppendHandle for partitionPath 2018/08/31 filePath 2018/08/31/.0245deea-bfb7-4901-bd55-849b7d6fa366-0_20240105021347452.log.1_0-14-16, took 713 ms.
24/01/05 02:14:13 INFO memory.MemoryStore: Block rdd_36_0 stored as values in memory (estimated size 546.0 B, free 364.5 MB)
24/01/05 02:14:13 INFO storage.BlockManagerInfo: Added rdd_36_0 in memory on adhoc-2:38763 (size: 546.0 B, free: 365.8 MB)
24/01/05 02:14:13 INFO executor.Executor: Finished task 0.0 in stage 14.0 (TID 16). 974 bytes result sent to driver
24/01/05 02:14:13 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 14.0 (TID 16) in 762 ms on localhost (executor driver) (1/1)
24/01/05 02:14:13 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 14.0, whose tasks have all completed, from pool 
24/01/05 02:14:13 INFO scheduler.DAGScheduler: ResultStage 14 (sum at StreamSync.java:783) finished in 0.792 s
24/01/05 02:14:13 INFO scheduler.DAGScheduler: Job 4 finished: sum at StreamSync.java:783, took 0.872465 s
24/01/05 02:14:13 INFO spark.SparkContext: Starting job: sum at StreamSync.java:784
24/01/05 02:14:13 INFO scheduler.DAGScheduler: Got job 5 (sum at StreamSync.java:784) with 1 output partitions
24/01/05 02:14:13 INFO scheduler.DAGScheduler: Final stage: ResultStage 19 (sum at StreamSync.java:784)
24/01/05 02:14:13 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 18)
24/01/05 02:14:13 INFO scheduler.DAGScheduler: Missing parents: List()
24/01/05 02:14:13 INFO scheduler.DAGScheduler: Submitting ResultStage 19 (MapPartitionsRDD[39] at mapToDouble at StreamSync.java:784), which has no missing parents
24/01/05 02:14:13 INFO memory.MemoryStore: Block broadcast_11 stored as values in memory (estimated size 351.4 KB, free 364.1 MB)
24/01/05 02:14:13 INFO memory.MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 126.9 KB, free 364.0 MB)
24/01/05 02:14:13 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on adhoc-2:38763 (size: 126.9 KB, free: 365.7 MB)
24/01/05 02:14:13 INFO spark.SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1161
24/01/05 02:14:13 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 19 (MapPartitionsRDD[39] at mapToDouble at StreamSync.java:784) (first 15 tasks are for partitions Vector(0))
24/01/05 02:14:13 INFO scheduler.TaskSchedulerImpl: Adding task set 19.0 with 1 tasks
24/01/05 02:14:13 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 19.0 (TID 17, localhost, executor driver, partition 0, PROCESS_LOCAL, 7662 bytes)
24/01/05 02:14:13 INFO executor.Executor: Running task 0.0 in stage 19.0 (TID 17)
24/01/05 02:14:13 INFO storage.BlockManager: Found block rdd_36_0 locally
24/01/05 02:14:13 INFO executor.Executor: Finished task 0.0 in stage 19.0 (TID 17). 759 bytes result sent to driver
24/01/05 02:14:13 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 19.0 (TID 17) in 28 ms on localhost (executor driver) (1/1)
24/01/05 02:14:13 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 19.0, whose tasks have all completed, from pool 
24/01/05 02:14:13 INFO scheduler.DAGScheduler: ResultStage 19 (sum at StreamSync.java:784) finished in 0.059 s
24/01/05 02:14:13 INFO scheduler.DAGScheduler: Job 5 finished: sum at StreamSync.java:784, took 0.062249 s
24/01/05 02:14:13 INFO streamer.StreamSync: instantTime=20240105021407331, totalRecords=4, totalErrorRecords=0, totalSuccessfulRecords=4
24/01/05 02:14:13 INFO spark.SparkContext: Starting job: collect at SparkRDDWriteClient.java:103
24/01/05 02:14:13 INFO scheduler.DAGScheduler: Got job 6 (collect at SparkRDDWriteClient.java:103) with 1 output partitions
24/01/05 02:14:13 INFO scheduler.DAGScheduler: Final stage: ResultStage 24 (collect at SparkRDDWriteClient.java:103)
24/01/05 02:14:13 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 23)
24/01/05 02:14:13 INFO scheduler.DAGScheduler: Missing parents: List()
24/01/05 02:14:13 INFO scheduler.DAGScheduler: Submitting ResultStage 24 (MapPartitionsRDD[41] at map at SparkRDDWriteClient.java:103), which has no missing parents
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 148
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 108
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 204
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 145
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 167
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 281
24/01/05 02:14:13 INFO storage.BlockManagerInfo: Removed broadcast_9_piece0 on adhoc-2:38763 in memory (size: 123.0 KB, free: 365.8 MB)
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 213
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 266
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 125
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 253
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 106
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 200
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 163
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 134
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 198
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 135
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 188
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 150
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 143
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 102
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 158
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 243
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 196
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 126
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 132
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 265
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 237
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 260
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 185
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 206
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 282
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 197
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 256
24/01/05 02:14:13 INFO storage.BlockManagerInfo: Removed broadcast_6_piece0 on adhoc-2:38763 in memory (size: 4.0 KB, free: 365.8 MB)
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 268
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 104
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 154
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 269
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 294
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 128
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 221
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 151
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 111
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 230
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 142
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 168
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 170
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 173
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 124
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 220
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 252
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 181
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 215
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 156
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 120
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 180
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 240
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 261
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 160
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 119
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 194
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 277
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 172
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 175
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 217
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 146
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 140
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 109
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 235
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 169
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 182
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 176
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 219
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 137
24/01/05 02:14:13 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on adhoc-2:38763 in memory (size: 122.2 KB, free: 365.9 MB)
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 234
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 276
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 286
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 187
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 285
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 101
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 250
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 224
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 165
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 115
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 159
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 110
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 199
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 242
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 195
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 239
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 147
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 178
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 123
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 236
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 251
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 138
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 212
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 141
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 190
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 297
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 232
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 177
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 289
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 248
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 223
24/01/05 02:14:13 INFO storage.BlockManagerInfo: Removed broadcast_8_piece0 on adhoc-2:38763 in memory (size: 121.8 KB, free: 366.0 MB)
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 227
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 233
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 152
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 247
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 267
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 270
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 203
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 208
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 274
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 290
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 210
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 264
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 245
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 186
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 133
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 293
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 299
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 262
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 144
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 121
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 296
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 284
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 255
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 288
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 157
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 249
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 291
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 149
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 130
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 295
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 127
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 161
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 164
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 179
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 205
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 153
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 238
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 112
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 184
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 211
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 162
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 131
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 207
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 231
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 241
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 117
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 155
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 105
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 118
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 114
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 254
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 116
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 174
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 275
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 136
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 166
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 244
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 279
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 214
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 171
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 229
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 257
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 271
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 258
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 259
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 263
24/01/05 02:14:13 INFO storage.BlockManagerInfo: Removed broadcast_10_piece0 on adhoc-2:38763 in memory (size: 126.9 KB, free: 366.2 MB)
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 192
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 100
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 228
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 202
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 246
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 273
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 298
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 209
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 280
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned shuffle 4
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 222
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 278
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 218
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 272
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 283
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 193
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 129
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 287
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 139
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 107
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 183
24/01/05 02:14:13 INFO storage.BlockManagerInfo: Removed broadcast_7_piece0 on adhoc-2:38763 in memory (size: 2.1 KB, free: 366.2 MB)
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 122
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 225
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 191
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 216
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 226
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 113
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 189
24/01/05 02:14:13 INFO storage.BlockManagerInfo: Removed broadcast_11_piece0 on adhoc-2:38763 in memory (size: 126.9 KB, free: 366.3 MB)
24/01/05 02:14:13 INFO memory.MemoryStore: Block broadcast_12 stored as values in memory (estimated size 351.6 KB, free 366.0 MB)
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 103
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 201
24/01/05 02:14:13 INFO spark.ContextCleaner: Cleaned accumulator 292
24/01/05 02:14:13 INFO memory.MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 127.0 KB, free 365.8 MB)
24/01/05 02:14:13 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on adhoc-2:38763 (size: 127.0 KB, free: 366.2 MB)
24/01/05 02:14:13 INFO spark.SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1161
24/01/05 02:14:13 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 24 (MapPartitionsRDD[41] at map at SparkRDDWriteClient.java:103) (first 15 tasks are for partitions Vector(0))
24/01/05 02:14:13 INFO scheduler.TaskSchedulerImpl: Adding task set 24.0 with 1 tasks
24/01/05 02:14:13 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 24.0 (TID 18, localhost, executor driver, partition 0, PROCESS_LOCAL, 7662 bytes)
24/01/05 02:14:13 INFO executor.Executor: Running task 0.0 in stage 24.0 (TID 18)
24/01/05 02:14:13 INFO storage.BlockManager: Found block rdd_36_0 locally
24/01/05 02:14:13 INFO executor.Executor: Finished task 0.0 in stage 24.0 (TID 18). 1223 bytes result sent to driver
24/01/05 02:14:13 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 24.0 (TID 18) in 22 ms on localhost (executor driver) (1/1)
24/01/05 02:14:13 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 24.0, whose tasks have all completed, from pool 
24/01/05 02:14:13 INFO scheduler.DAGScheduler: ResultStage 24 (collect at SparkRDDWriteClient.java:103) finished in 0.067 s
24/01/05 02:14:13 INFO scheduler.DAGScheduler: Job 6 finished: collect at SparkRDDWriteClient.java:103, took 0.070076 s
24/01/05 02:14:13 INFO client.BaseHoodieWriteClient: Committing 20240105021407331 action deltacommit
24/01/05 02:14:13 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:14:13 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:14:13 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:14:13 INFO table.HoodieTableMetaClient: Loading Active commit timeline for /user/hive/warehouse/stock_ticks_mor
24/01/05 02:14:13 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[==>20240105021407331__deltacommit__INFLIGHT__20240105021412611]}
24/01/05 02:14:13 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:14:13 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:14:13 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:14:13 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:14:13 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:14:13 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:14:13 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105021347452__deltacommit__COMPLETED__20240105021356832]}
24/01/05 02:14:13 INFO view.AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
24/01/05 02:14:13 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:14:13 INFO view.FileSystemViewManager: Creating View Manager with storage type :REMOTE_FIRST
24/01/05 02:14:13 INFO view.FileSystemViewManager: Creating remote first table view
24/01/05 02:14:13 INFO util.CommitUtils: Creating  metadata for UPSERT numWriteStats:1 numReplaceFileIds:0
24/01/05 02:14:13 INFO transaction.TransactionManager: Transaction starting for Option{val=[==>20240105021407331__deltacommit__INFLIGHT]} with latest completed transaction instant Option{val=[20240105021347452__deltacommit__COMPLETED__20240105021356931]}
24/01/05 02:14:13 INFO lock.LockManager: LockProvider org.apache.hudi.client.transaction.lock.InProcessLockProvider
24/01/05 02:14:13 INFO lock.InProcessLockProvider: Base Path /user/hive/warehouse/stock_ticks_mor, Lock Instance java.util.concurrent.locks.ReentrantReadWriteLock@7254838[Write locks = 0, Read locks = 0], Thread main, In-process lock state ACQUIRING
24/01/05 02:14:13 INFO lock.InProcessLockProvider: Base Path /user/hive/warehouse/stock_ticks_mor, Lock Instance java.util.concurrent.locks.ReentrantReadWriteLock@7254838[Write locks = 1, Read locks = 0], Thread main, In-process lock state ACQUIRED
24/01/05 02:14:13 INFO transaction.TransactionManager: Transaction started for Option{val=[==>20240105021407331__deltacommit__INFLIGHT]} with latest completed transaction instant Option{val=[20240105021347452__deltacommit__COMPLETED__20240105021356931]}
24/01/05 02:14:13 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:14:13 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:14:13 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:14:13 INFO table.HoodieTableMetaClient: Loading Active commit timeline for /user/hive/warehouse/stock_ticks_mor
24/01/05 02:14:13 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[==>20240105021407331__deltacommit__INFLIGHT__20240105021412611]}
24/01/05 02:14:13 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:14:13 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:14:13 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:14:13 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:14:13 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:14:13 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:14:13 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105021347452__deltacommit__COMPLETED__20240105021356832]}
24/01/05 02:14:13 INFO view.AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
24/01/05 02:14:13 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:14:13 INFO view.FileSystemViewManager: Creating View Manager with storage type :REMOTE_FIRST
24/01/05 02:14:13 INFO view.FileSystemViewManager: Creating remote first table view
24/01/05 02:14:13 INFO client.BaseHoodieWriteClient: Committing 20240105021407331 action deltacommit
24/01/05 02:14:13 WARN marker.WriteMarkersFactory: Timeline-server-based markers are not supported for HDFS: base path /user/hive/warehouse/stock_ticks_mor.  Falling back to direct markers.
24/01/05 02:14:13 INFO spark.SparkContext: Starting job: collect at HoodieSparkEngineContext.java:150
24/01/05 02:14:13 INFO scheduler.DAGScheduler: Got job 7 (collect at HoodieSparkEngineContext.java:150) with 1 output partitions
24/01/05 02:14:13 INFO scheduler.DAGScheduler: Final stage: ResultStage 25 (collect at HoodieSparkEngineContext.java:150)
24/01/05 02:14:13 INFO scheduler.DAGScheduler: Parents of final stage: List()
24/01/05 02:14:13 INFO scheduler.DAGScheduler: Missing parents: List()
24/01/05 02:14:13 INFO scheduler.DAGScheduler: Submitting ResultStage 25 (MapPartitionsRDD[43] at flatMap at HoodieSparkEngineContext.java:150), which has no missing parents
24/01/05 02:14:13 INFO memory.MemoryStore: Block broadcast_13 stored as values in memory (estimated size 72.5 KB, free 365.8 MB)
24/01/05 02:14:13 INFO memory.MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 26.7 KB, free 365.7 MB)
24/01/05 02:14:13 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on adhoc-2:38763 (size: 26.7 KB, free: 366.1 MB)
24/01/05 02:14:13 INFO spark.SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1161
24/01/05 02:14:13 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 25 (MapPartitionsRDD[43] at flatMap at HoodieSparkEngineContext.java:150) (first 15 tasks are for partitions Vector(0))
24/01/05 02:14:13 INFO scheduler.TaskSchedulerImpl: Adding task set 25.0 with 1 tasks
24/01/05 02:14:13 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 25.0 (TID 19, localhost, executor driver, partition 0, PROCESS_LOCAL, 7820 bytes)
24/01/05 02:14:13 INFO executor.Executor: Running task 0.0 in stage 25.0 (TID 19)
24/01/05 02:14:13 INFO executor.Executor: Finished task 0.0 in stage 25.0 (TID 19). 668 bytes result sent to driver
24/01/05 02:14:13 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 25.0 (TID 19) in 21 ms on localhost (executor driver) (1/1)
24/01/05 02:14:13 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 25.0, whose tasks have all completed, from pool 
24/01/05 02:14:13 INFO scheduler.DAGScheduler: ResultStage 25 (collect at HoodieSparkEngineContext.java:150) finished in 0.032 s
24/01/05 02:14:13 INFO scheduler.DAGScheduler: Job 7 finished: collect at HoodieSparkEngineContext.java:150, took 0.033900 s
24/01/05 02:14:13 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:14:13 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:14:13 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:14:13 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:14:13 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:14:13 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:14:13 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105021347452__deltacommit__COMPLETED__20240105021356832]}
24/01/05 02:14:13 INFO metadata.HoodieBackedTableMetadataWriter: Async metadata indexing disabled and following partitions already initialized: [files]
24/01/05 02:14:13 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:14:13 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:14:13 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:14:13 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:14:13 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:14:13 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:14:13 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105021347452__deltacommit__COMPLETED__20240105021356832]}
24/01/05 02:14:13 INFO view.AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
24/01/05 02:14:13 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:14:13 INFO metadata.HoodieTableMetadataUtil: Updating at 20240105021407331 from Commit/UPSERT. #partitions_updated=2, #files_added=1
24/01/05 02:14:14 INFO view.AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
24/01/05 02:14:14 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:14:14 INFO metadata.HoodieTableMetadataUtil: Loading latest file slices for metadata table partition files
24/01/05 02:14:14 INFO view.AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
24/01/05 02:14:14 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:14:14 INFO view.AbstractTableFileSystemView: Building file system view for partition (files)
24/01/05 02:14:14 INFO client.BaseHoodieClient: Embedded Timeline Server is disabled. Not starting timeline service
24/01/05 02:14:14 INFO client.BaseHoodieClient: Embedded Timeline Server is disabled. Not starting timeline service
24/01/05 02:14:14 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:14:14 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:14:14 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:14:14 INFO table.HoodieTableMetaClient: Loading Active commit timeline for /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:14:14 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105021347452__deltacommit__COMPLETED__20240105021356832]}
24/01/05 02:14:14 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:14:14 INFO view.FileSystemViewManager: Creating View Manager with storage type :MEMORY
24/01/05 02:14:14 INFO view.FileSystemViewManager: Creating in-memory based Table View
24/01/05 02:14:14 INFO metadata.HoodieBackedTableMetadataWriter: New commit at 20240105021407331 being applied to MDT.
24/01/05 02:14:14 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:14:14 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:14:14 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:14:14 INFO table.HoodieTableMetaClient: Loading Active commit timeline for /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:14:14 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105021347452__deltacommit__COMPLETED__20240105021356832]}
24/01/05 02:14:14 INFO util.CleanerUtils: Cleaned failed attempts if any
24/01/05 02:14:14 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:14:14 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:14:14 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:14:14 INFO table.HoodieTableMetaClient: Loading Active commit timeline for /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:14:14 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105021347452__deltacommit__COMPLETED__20240105021356832]}
24/01/05 02:14:14 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:14:14 INFO view.FileSystemViewManager: Creating View Manager with storage type :MEMORY
24/01/05 02:14:14 INFO view.FileSystemViewManager: Creating in-memory based Table View
24/01/05 02:14:14 INFO client.BaseHoodieWriteClient: Generate a new instant time: 20240105021407331 action: deltacommit
24/01/05 02:14:14 INFO timeline.HoodieActiveTimeline: Creating a new instant [==>20240105021407331__deltacommit__REQUESTED]
24/01/05 02:14:14 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:14:14 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:14:14 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:14:14 INFO table.HoodieTableMetaClient: Loading Active commit timeline for /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:14:14 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[==>20240105021407331__deltacommit__REQUESTED__20240105021414073]}
24/01/05 02:14:14 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:14:14 INFO view.FileSystemViewManager: Creating View Manager with storage type :MEMORY
24/01/05 02:14:14 INFO view.FileSystemViewManager: Creating in-memory based Table View
24/01/05 02:14:14 INFO async.AsyncCleanerService: The HoodieWriteClient is not configured to auto & async clean. Async clean service will not start.
24/01/05 02:14:14 INFO async.AsyncArchiveService: The HoodieWriteClient is not configured to auto & async archive. Async archive service will not start.
24/01/05 02:14:14 INFO view.AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
24/01/05 02:14:14 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:14:14 INFO spark.SparkContext: Starting job: countByKey at HoodieJavaPairRDD.java:105
24/01/05 02:14:14 INFO scheduler.DAGScheduler: Registering RDD 52 (countByKey at HoodieJavaPairRDD.java:105)
24/01/05 02:14:14 INFO scheduler.DAGScheduler: Got job 8 (countByKey at HoodieJavaPairRDD.java:105) with 1 output partitions
24/01/05 02:14:14 INFO scheduler.DAGScheduler: Final stage: ResultStage 27 (countByKey at HoodieJavaPairRDD.java:105)
24/01/05 02:14:14 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 26)
24/01/05 02:14:14 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 26)
24/01/05 02:14:14 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 26 (MapPartitionsRDD[52] at countByKey at HoodieJavaPairRDD.java:105), which has no missing parents
24/01/05 02:14:14 INFO memory.MemoryStore: Block broadcast_14 stored as values in memory (estimated size 8.6 KB, free 365.7 MB)
24/01/05 02:14:14 INFO memory.MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 4.7 KB, free 365.7 MB)
24/01/05 02:14:14 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on adhoc-2:38763 (size: 4.7 KB, free: 366.1 MB)
24/01/05 02:14:14 INFO spark.SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1161
24/01/05 02:14:14 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 26 (MapPartitionsRDD[52] at countByKey at HoodieJavaPairRDD.java:105) (first 15 tasks are for partitions Vector(0))
24/01/05 02:14:14 INFO scheduler.TaskSchedulerImpl: Adding task set 26.0 with 1 tasks
24/01/05 02:14:14 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 26.0 (TID 20, localhost, executor driver, partition 0, PROCESS_LOCAL, 8084 bytes)
24/01/05 02:14:14 INFO executor.Executor: Running task 0.0 in stage 26.0 (TID 20)
24/01/05 02:14:14 INFO memory.MemoryStore: Block rdd_50_0 stored as values in memory (estimated size 396.0 B, free 365.7 MB)
24/01/05 02:14:14 INFO storage.BlockManagerInfo: Added rdd_50_0 in memory on adhoc-2:38763 (size: 396.0 B, free: 366.1 MB)
24/01/05 02:14:14 INFO executor.Executor: Finished task 0.0 in stage 26.0 (TID 20). 950 bytes result sent to driver
24/01/05 02:14:14 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 26.0 (TID 20) in 26 ms on localhost (executor driver) (1/1)
24/01/05 02:14:14 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 26.0, whose tasks have all completed, from pool 
24/01/05 02:14:14 INFO scheduler.DAGScheduler: ShuffleMapStage 26 (countByKey at HoodieJavaPairRDD.java:105) finished in 0.041 s
24/01/05 02:14:14 INFO scheduler.DAGScheduler: looking for newly runnable stages
24/01/05 02:14:14 INFO scheduler.DAGScheduler: running: Set()
24/01/05 02:14:14 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 27)
24/01/05 02:14:14 INFO scheduler.DAGScheduler: failed: Set()
24/01/05 02:14:14 INFO scheduler.DAGScheduler: Submitting ResultStage 27 (ShuffledRDD[53] at countByKey at HoodieJavaPairRDD.java:105), which has no missing parents
24/01/05 02:14:14 INFO memory.MemoryStore: Block broadcast_15 stored as values in memory (estimated size 3.6 KB, free 365.7 MB)
24/01/05 02:14:14 INFO memory.MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 2.1 KB, free 365.7 MB)
24/01/05 02:14:14 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on adhoc-2:38763 (size: 2.1 KB, free: 366.1 MB)
24/01/05 02:14:14 INFO spark.SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1161
24/01/05 02:14:14 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 27 (ShuffledRDD[53] at countByKey at HoodieJavaPairRDD.java:105) (first 15 tasks are for partitions Vector(0))
24/01/05 02:14:14 INFO scheduler.TaskSchedulerImpl: Adding task set 27.0 with 1 tasks
24/01/05 02:14:14 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 27.0 (TID 21, localhost, executor driver, partition 0, ANY, 7662 bytes)
24/01/05 02:14:14 INFO executor.Executor: Running task 0.0 in stage 27.0 (TID 21)
24/01/05 02:14:14 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
24/01/05 02:14:14 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
24/01/05 02:14:14 INFO executor.Executor: Finished task 0.0 in stage 27.0 (TID 21). 1179 bytes result sent to driver
24/01/05 02:14:14 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 27.0 (TID 21) in 11 ms on localhost (executor driver) (1/1)
24/01/05 02:14:14 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 27.0, whose tasks have all completed, from pool 
24/01/05 02:14:14 INFO scheduler.DAGScheduler: ResultStage 27 (countByKey at HoodieJavaPairRDD.java:105) finished in 0.018 s
24/01/05 02:14:14 INFO scheduler.DAGScheduler: Job 8 finished: countByKey at HoodieJavaPairRDD.java:105, took 0.065800 s
24/01/05 02:14:14 INFO commit.UpsertPartitioner: AvgRecordSize => 1024
24/01/05 02:14:14 INFO spark.SparkContext: Starting job: collectAsMap at UpsertPartitioner.java:282
24/01/05 02:14:14 INFO scheduler.DAGScheduler: Got job 9 (collectAsMap at UpsertPartitioner.java:282) with 1 output partitions
24/01/05 02:14:14 INFO scheduler.DAGScheduler: Final stage: ResultStage 28 (collectAsMap at UpsertPartitioner.java:282)
24/01/05 02:14:14 INFO scheduler.DAGScheduler: Parents of final stage: List()
24/01/05 02:14:14 INFO scheduler.DAGScheduler: Missing parents: List()
24/01/05 02:14:14 INFO scheduler.DAGScheduler: Submitting ResultStage 28 (MapPartitionsRDD[55] at mapToPair at UpsertPartitioner.java:281), which has no missing parents
24/01/05 02:14:14 INFO memory.MemoryStore: Block broadcast_16 stored as values in memory (estimated size 268.9 KB, free 365.5 MB)
24/01/05 02:14:14 INFO memory.MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 95.8 KB, free 365.4 MB)
24/01/05 02:14:14 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on adhoc-2:38763 (size: 95.8 KB, free: 366.0 MB)
24/01/05 02:14:14 INFO spark.SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1161
24/01/05 02:14:14 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 28 (MapPartitionsRDD[55] at mapToPair at UpsertPartitioner.java:281) (first 15 tasks are for partitions Vector(0))
24/01/05 02:14:14 INFO scheduler.TaskSchedulerImpl: Adding task set 28.0 with 1 tasks
24/01/05 02:14:14 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 28.0 (TID 22, localhost, executor driver, partition 0, PROCESS_LOCAL, 7730 bytes)
24/01/05 02:14:14 INFO executor.Executor: Running task 0.0 in stage 28.0 (TID 22)
24/01/05 02:14:14 INFO view.FileSystemViewManager: Creating View Manager with storage type :MEMORY
24/01/05 02:14:14 INFO view.FileSystemViewManager: Creating in-memory based Table View
24/01/05 02:14:14 INFO view.FileSystemViewManager: Creating InMemory based view for basePath /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:14:14 INFO view.AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
24/01/05 02:14:14 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:14:14 INFO view.AbstractTableFileSystemView: Building file system view for partition (files)
24/01/05 02:14:14 INFO executor.Executor: Finished task 0.0 in stage 28.0 (TID 22). 700 bytes result sent to driver
24/01/05 02:14:14 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 28.0 (TID 22) in 21 ms on localhost (executor driver) (1/1)
24/01/05 02:14:14 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 28.0, whose tasks have all completed, from pool 
24/01/05 02:14:14 INFO scheduler.DAGScheduler: ResultStage 28 (collectAsMap at UpsertPartitioner.java:282) finished in 0.044 s
24/01/05 02:14:14 INFO scheduler.DAGScheduler: Job 9 finished: collectAsMap at UpsertPartitioner.java:282, took 0.045501 s
24/01/05 02:14:14 INFO view.AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
24/01/05 02:14:14 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:14:14 INFO commit.UpsertPartitioner: Total Buckets: 1
24/01/05 02:14:14 INFO timeline.HoodieActiveTimeline: Checking for file exists ?/user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/20240105021407331.deltacommit.requested
24/01/05 02:14:14 INFO timeline.HoodieActiveTimeline: Create new file for toInstant ?/user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/20240105021407331.deltacommit.inflight
24/01/05 02:14:14 INFO commit.BaseSparkCommitActionExecutor: no validators configured.
24/01/05 02:14:14 INFO commit.BaseCommitActionExecutor: Auto commit enabled: Committing 20240105021407331
24/01/05 02:14:14 INFO spark.SparkContext: Starting job: collect at HoodieJavaRDD.java:177
24/01/05 02:14:14 INFO scheduler.DAGScheduler: Registering RDD 56 (mapToPair at HoodieJavaRDD.java:149)
24/01/05 02:14:14 INFO scheduler.DAGScheduler: Got job 10 (collect at HoodieJavaRDD.java:177) with 1 output partitions
24/01/05 02:14:14 INFO scheduler.DAGScheduler: Final stage: ResultStage 30 (collect at HoodieJavaRDD.java:177)
24/01/05 02:14:14 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 29)
24/01/05 02:14:14 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 29)
24/01/05 02:14:14 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 29 (MapPartitionsRDD[56] at mapToPair at HoodieJavaRDD.java:149), which has no missing parents
24/01/05 02:14:14 INFO memory.MemoryStore: Block broadcast_17 stored as values in memory (estimated size 273.7 KB, free 365.1 MB)
24/01/05 02:14:14 INFO memory.MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 95.5 KB, free 365.0 MB)
24/01/05 02:14:14 INFO storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on adhoc-2:38763 (size: 95.5 KB, free: 366.0 MB)
24/01/05 02:14:14 INFO spark.SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1161
24/01/05 02:14:14 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 29 (MapPartitionsRDD[56] at mapToPair at HoodieJavaRDD.java:149) (first 15 tasks are for partitions Vector(0))
24/01/05 02:14:14 INFO scheduler.TaskSchedulerImpl: Adding task set 29.0 with 1 tasks
24/01/05 02:14:14 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 29.0 (TID 23, localhost, executor driver, partition 0, PROCESS_LOCAL, 8084 bytes)
24/01/05 02:14:14 INFO executor.Executor: Running task 0.0 in stage 29.0 (TID 23)
24/01/05 02:14:14 INFO storage.BlockManager: Found block rdd_50_0 locally
24/01/05 02:14:14 INFO executor.Executor: Finished task 0.0 in stage 29.0 (TID 23). 907 bytes result sent to driver
24/01/05 02:14:14 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 29.0 (TID 23) in 23 ms on localhost (executor driver) (1/1)
24/01/05 02:14:14 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 29.0, whose tasks have all completed, from pool 
24/01/05 02:14:14 INFO scheduler.DAGScheduler: ShuffleMapStage 29 (mapToPair at HoodieJavaRDD.java:149) finished in 0.051 s
24/01/05 02:14:14 INFO scheduler.DAGScheduler: looking for newly runnable stages
24/01/05 02:14:14 INFO scheduler.DAGScheduler: running: Set()
24/01/05 02:14:14 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 30)
24/01/05 02:14:14 INFO scheduler.DAGScheduler: failed: Set()
24/01/05 02:14:14 INFO scheduler.DAGScheduler: Submitting ResultStage 30 (MapPartitionsRDD[61] at map at HoodieJavaRDD.java:125), which has no missing parents
24/01/05 02:14:14 INFO memory.MemoryStore: Block broadcast_18 stored as values in memory (estimated size 348.5 KB, free 364.7 MB)
24/01/05 02:14:14 INFO memory.MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 126.9 KB, free 364.5 MB)
24/01/05 02:14:14 INFO storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on adhoc-2:38763 (size: 126.9 KB, free: 365.8 MB)
24/01/05 02:14:14 INFO spark.SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1161
24/01/05 02:14:14 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 30 (MapPartitionsRDD[61] at map at HoodieJavaRDD.java:125) (first 15 tasks are for partitions Vector(0))
24/01/05 02:14:14 INFO scheduler.TaskSchedulerImpl: Adding task set 30.0 with 1 tasks
24/01/05 02:14:14 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 30.0 (TID 24, localhost, executor driver, partition 0, ANY, 7662 bytes)
24/01/05 02:14:14 INFO executor.Executor: Running task 0.0 in stage 30.0 (TID 24)
24/01/05 02:14:14 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
24/01/05 02:14:14 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
24/01/05 02:14:14 INFO deltacommit.BaseSparkDeltaCommitActionExecutor: Merging updates for commit 20240105021407331 for file files-0000-0
24/01/05 02:14:14 INFO view.FileSystemViewManager: Creating View Manager with storage type :MEMORY
24/01/05 02:14:14 INFO view.FileSystemViewManager: Creating in-memory based Table View
24/01/05 02:14:14 INFO view.FileSystemViewManager: Creating InMemory based view for basePath /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:14:14 INFO view.AbstractTableFileSystemView: Took 1 ms to read  0 instants, 0 replaced file groups
24/01/05 02:14:14 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:14:14 INFO view.AbstractTableFileSystemView: Building file system view for partition (files)
24/01/05 02:14:14 INFO marker.DirectWriteMarkers: Creating Marker Path=/user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/.temp/20240105021407331/files/files-0000-0_0-30-24_00000000000000010.hfile.marker.APPEND
24/01/05 02:14:14 INFO marker.DirectWriteMarkers: [direct] Created marker file /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/.temp/20240105021407331/files/files-0000-0_0-30-24_00000000000000010.hfile.marker.APPEND in 6 ms
24/01/05 02:14:14 INFO log.HoodieLogFormat$WriterBuilder: Building HoodieLogFormat Writer
24/01/05 02:14:14 INFO log.HoodieLogFormat$WriterBuilder: HoodieLogFile on path /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/files/.files-0000-0_00000000000000010.log.1_0-0-0
24/01/05 02:14:14 INFO log.HoodieLogFormatWriter: HoodieLogFile{pathStr='/user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/files/.files-0000-0_00000000000000010.log.1_0-0-0', fileLen=13460} exists. Appending to existing file
24/01/05 02:14:14 INFO spark.ContextCleaner: Cleaned accumulator 416
24/01/05 02:14:14 INFO spark.ContextCleaner: Cleaned accumulator 417
24/01/05 02:14:14 INFO spark.ContextCleaner: Cleaned accumulator 390
24/01/05 02:14:14 INFO spark.ContextCleaner: Cleaned accumulator 306
24/01/05 02:14:14 INFO spark.ContextCleaner: Cleaned accumulator 318
24/01/05 02:14:14 INFO spark.ContextCleaner: Cleaned accumulator 315
24/01/05 02:14:14 INFO spark.ContextCleaner: Cleaned accumulator 320
24/01/05 02:14:14 INFO spark.ContextCleaner: Cleaned accumulator 379
24/01/05 02:14:14 INFO spark.ContextCleaner: Cleaned accumulator 367
24/01/05 02:14:14 INFO spark.ContextCleaner: Cleaned accumulator 316
24/01/05 02:14:14 INFO spark.ContextCleaner: Cleaned accumulator 337
24/01/05 02:14:14 INFO spark.ContextCleaner: Cleaned accumulator 339
24/01/05 02:14:14 INFO spark.ContextCleaner: Cleaned accumulator 378
24/01/05 02:14:14 INFO spark.ContextCleaner: Cleaned accumulator 389
24/01/05 02:14:14 INFO spark.ContextCleaner: Cleaned accumulator 319
24/01/05 02:14:14 INFO spark.ContextCleaner: Cleaned accumulator 328
24/01/05 02:14:14 INFO spark.ContextCleaner: Cleaned accumulator 354
24/01/05 02:14:14 INFO spark.ContextCleaner: Cleaned accumulator 343
24/01/05 02:14:14 INFO spark.ContextCleaner: Cleaned accumulator 350
24/01/05 02:14:14 INFO storage.BlockManagerInfo: Removed broadcast_15_piece0 on adhoc-2:38763 in memory (size: 2.1 KB, free: 365.8 MB)
24/01/05 02:14:14 INFO spark.ContextCleaner: Cleaned accumulator 347
24/01/05 02:14:14 INFO spark.ContextCleaner: Cleaned accumulator 400
24/01/05 02:14:14 INFO spark.ContextCleaner: Cleaned accumulator 327
24/01/05 02:14:14 INFO spark.ContextCleaner: Cleaned accumulator 323
24/01/05 02:14:14 INFO spark.ContextCleaner: Cleaned accumulator 373
24/01/05 02:14:14 INFO spark.ContextCleaner: Cleaned accumulator 360
24/01/05 02:14:14 INFO spark.ContextCleaner: Cleaned accumulator 348
24/01/05 02:14:14 INFO spark.ContextCleaner: Cleaned accumulator 382
24/01/05 02:14:14 INFO spark.ContextCleaner: Cleaned accumulator 386
24/01/05 02:14:14 INFO spark.ContextCleaner: Cleaned accumulator 405
24/01/05 02:14:14 INFO spark.ContextCleaner: Cleaned accumulator 421
24/01/05 02:14:14 INFO storage.BlockManagerInfo: Removed broadcast_16_piece0 on adhoc-2:38763 in memory (size: 95.8 KB, free: 365.9 MB)
24/01/05 02:14:14 INFO spark.ContextCleaner: Cleaned accumulator 357
24/01/05 02:14:14 INFO spark.ContextCleaner: Cleaned accumulator 365
24/01/05 02:14:14 INFO spark.ContextCleaner: Cleaned accumulator 363
24/01/05 02:14:14 INFO spark.ContextCleaner: Cleaned accumulator 392
24/01/05 02:14:14 INFO spark.ContextCleaner: Cleaned accumulator 335
24/01/05 02:14:14 INFO spark.ContextCleaner: Cleaned shuffle 6
24/01/05 02:14:14 INFO spark.ContextCleaner: Cleaned accumulator 300
24/01/05 02:14:14 INFO spark.ContextCleaner: Cleaned accumulator 314
24/01/05 02:14:14 INFO spark.ContextCleaner: Cleaned accumulator 364
24/01/05 02:14:14 INFO spark.ContextCleaner: Cleaned accumulator 341
24/01/05 02:14:14 INFO spark.ContextCleaner: Cleaned accumulator 356
24/01/05 02:14:14 INFO spark.ContextCleaner: Cleaned accumulator 407
24/01/05 02:14:14 INFO spark.ContextCleaner: Cleaned accumulator 387
24/01/05 02:14:14 INFO storage.BlockManagerInfo: Removed broadcast_14_piece0 on adhoc-2:38763 in memory (size: 4.7 KB, free: 365.9 MB)
24/01/05 02:14:14 INFO spark.ContextCleaner: Cleaned accumulator 414
24/01/05 02:14:14 INFO spark.ContextCleaner: Cleaned accumulator 340
24/01/05 02:14:14 INFO spark.ContextCleaner: Cleaned accumulator 406
24/01/05 02:14:14 INFO spark.ContextCleaner: Cleaned accumulator 423
24/01/05 02:14:14 INFO spark.ContextCleaner: Cleaned accumulator 413
24/01/05 02:14:14 INFO spark.ContextCleaner: Cleaned accumulator 398
24/01/05 02:14:14 INFO spark.ContextCleaner: Cleaned accumulator 332
24/01/05 02:14:14 INFO spark.ContextCleaner: Cleaned accumulator 312
24/01/05 02:14:14 INFO spark.ContextCleaner: Cleaned accumulator 324
24/01/05 02:14:14 INFO spark.ContextCleaner: Cleaned accumulator 345
24/01/05 02:14:14 INFO spark.ContextCleaner: Cleaned accumulator 322
24/01/05 02:14:14 INFO spark.ContextCleaner: Cleaned accumulator 412
24/01/05 02:14:14 INFO spark.ContextCleaner: Cleaned accumulator 370
24/01/05 02:14:14 INFO spark.ContextCleaner: Cleaned accumulator 385
24/01/05 02:14:14 INFO spark.ContextCleaner: Cleaned accumulator 334
24/01/05 02:14:14 INFO spark.ContextCleaner: Cleaned accumulator 346
24/01/05 02:14:14 INFO spark.ContextCleaner: Cleaned accumulator 331
24/01/05 02:14:14 INFO spark.ContextCleaner: Cleaned accumulator 403
24/01/05 02:14:14 INFO spark.ContextCleaner: Cleaned accumulator 310
24/01/05 02:14:14 INFO spark.ContextCleaner: Cleaned accumulator 374
24/01/05 02:14:14 INFO spark.ContextCleaner: Cleaned accumulator 415
24/01/05 02:14:14 INFO spark.ContextCleaner: Cleaned accumulator 317
24/01/05 02:14:14 INFO storage.BlockManagerInfo: Removed broadcast_13_piece0 on adhoc-2:38763 in memory (size: 26.7 KB, free: 366.0 MB)
24/01/05 02:14:14 INFO spark.ContextCleaner: Cleaned accumulator 388
24/01/05 02:14:14 INFO spark.ContextCleaner: Cleaned accumulator 399
24/01/05 02:14:14 INFO spark.ContextCleaner: Cleaned accumulator 344
24/01/05 02:14:14 INFO spark.ContextCleaner: Cleaned accumulator 325
24/01/05 02:14:14 INFO storage.BlockManagerInfo: Removed broadcast_17_piece0 on adhoc-2:38763 in memory (size: 95.5 KB, free: 366.1 MB)
24/01/05 02:14:14 INFO spark.ContextCleaner: Cleaned accumulator 376
24/01/05 02:14:14 INFO spark.ContextCleaner: Cleaned accumulator 380
24/01/05 02:14:14 INFO spark.ContextCleaner: Cleaned accumulator 402
24/01/05 02:14:14 INFO spark.ContextCleaner: Cleaned accumulator 391
24/01/05 02:14:14 INFO spark.ContextCleaner: Cleaned accumulator 307
24/01/05 02:14:14 INFO spark.ContextCleaner: Cleaned accumulator 302
24/01/05 02:14:14 INFO spark.ContextCleaner: Cleaned accumulator 342
24/01/05 02:14:14 INFO spark.ContextCleaner: Cleaned accumulator 352
24/01/05 02:14:14 INFO spark.ContextCleaner: Cleaned accumulator 351
24/01/05 02:14:14 INFO spark.ContextCleaner: Cleaned accumulator 404
24/01/05 02:14:14 INFO spark.ContextCleaner: Cleaned accumulator 418
24/01/05 02:14:14 INFO spark.ContextCleaner: Cleaned accumulator 338
24/01/05 02:14:14 INFO spark.ContextCleaner: Cleaned accumulator 422
24/01/05 02:14:14 INFO spark.ContextCleaner: Cleaned accumulator 358
24/01/05 02:14:14 INFO spark.ContextCleaner: Cleaned accumulator 409
24/01/05 02:14:14 INFO spark.ContextCleaner: Cleaned accumulator 411
24/01/05 02:14:14 INFO spark.ContextCleaner: Cleaned accumulator 303
24/01/05 02:14:14 INFO spark.ContextCleaner: Cleaned accumulator 393
24/01/05 02:14:14 INFO spark.ContextCleaner: Cleaned accumulator 311
24/01/05 02:14:14 INFO spark.ContextCleaner: Cleaned accumulator 309
24/01/05 02:14:14 INFO spark.ContextCleaner: Cleaned accumulator 375
24/01/05 02:14:14 INFO spark.ContextCleaner: Cleaned accumulator 362
24/01/05 02:14:14 INFO spark.ContextCleaner: Cleaned accumulator 349
24/01/05 02:14:14 INFO spark.ContextCleaner: Cleaned accumulator 329
24/01/05 02:14:14 INFO spark.ContextCleaner: Cleaned accumulator 330
24/01/05 02:14:14 INFO spark.ContextCleaner: Cleaned accumulator 313
24/01/05 02:14:14 INFO spark.ContextCleaner: Cleaned accumulator 419
24/01/05 02:14:14 INFO spark.ContextCleaner: Cleaned accumulator 396
24/01/05 02:14:14 INFO spark.ContextCleaner: Cleaned accumulator 372
24/01/05 02:14:14 INFO spark.ContextCleaner: Cleaned accumulator 408
24/01/05 02:14:14 INFO spark.ContextCleaner: Cleaned accumulator 369
24/01/05 02:14:14 INFO spark.ContextCleaner: Cleaned accumulator 395
24/01/05 02:14:14 INFO spark.ContextCleaner: Cleaned accumulator 410
24/01/05 02:14:14 INFO spark.ContextCleaner: Cleaned accumulator 397
24/01/05 02:14:14 INFO spark.ContextCleaner: Cleaned accumulator 384
24/01/05 02:14:14 INFO spark.ContextCleaner: Cleaned accumulator 336
24/01/05 02:14:14 INFO spark.ContextCleaner: Cleaned accumulator 304
24/01/05 02:14:14 INFO spark.ContextCleaner: Cleaned accumulator 368
24/01/05 02:14:14 INFO spark.ContextCleaner: Cleaned accumulator 333
24/01/05 02:14:14 INFO spark.ContextCleaner: Cleaned accumulator 305
24/01/05 02:14:14 INFO spark.ContextCleaner: Cleaned accumulator 377
24/01/05 02:14:14 INFO spark.ContextCleaner: Cleaned accumulator 361
24/01/05 02:14:14 INFO spark.ContextCleaner: Cleaned accumulator 366
24/01/05 02:14:14 INFO spark.ContextCleaner: Cleaned accumulator 394
24/01/05 02:14:14 INFO spark.ContextCleaner: Cleaned accumulator 371
24/01/05 02:14:14 INFO spark.ContextCleaner: Cleaned accumulator 308
24/01/05 02:14:14 INFO spark.ContextCleaner: Cleaned accumulator 355
24/01/05 02:14:14 INFO storage.BlockManagerInfo: Removed broadcast_12_piece0 on adhoc-2:38763 in memory (size: 127.0 KB, free: 366.2 MB)
24/01/05 02:14:14 INFO spark.ContextCleaner: Cleaned accumulator 424
24/01/05 02:14:14 INFO spark.ContextCleaner: Cleaned accumulator 353
24/01/05 02:14:14 INFO spark.ContextCleaner: Cleaned accumulator 301
24/01/05 02:14:14 INFO spark.ContextCleaner: Cleaned accumulator 420
24/01/05 02:14:14 INFO spark.ContextCleaner: Cleaned accumulator 401
24/01/05 02:14:14 INFO spark.ContextCleaner: Cleaned accumulator 381
24/01/05 02:14:14 INFO spark.ContextCleaner: Cleaned accumulator 383
24/01/05 02:14:14 INFO spark.ContextCleaner: Cleaned accumulator 326
24/01/05 02:14:14 INFO spark.ContextCleaner: Cleaned accumulator 321
24/01/05 02:14:14 INFO spark.ContextCleaner: Cleaned accumulator 359
24/01/05 02:14:14 INFO impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
24/01/05 02:14:14 INFO impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
24/01/05 02:14:14 INFO impl.MetricsSystemImpl: HBase metrics system started
24/01/05 02:14:14 INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.hudi.org.apache.hadoop.hbase.metrics.impl.MetricRegistriesImpl
24/01/05 02:14:15 INFO compress.CodecPool: Got brand-new compressor [.gz]
24/01/05 02:14:15 INFO compress.CodecPool: Got brand-new compressor [.gz]
24/01/05 02:14:15 INFO io.HoodieAppendHandle: AppendHandle for partitionPath files filePath files/.files-0000-0_00000000000000010.log.1_0-0-0, took 627 ms.
24/01/05 02:14:15 INFO memory.MemoryStore: Block rdd_60_0 stored as values in memory (estimated size 438.0 B, free 365.8 MB)
24/01/05 02:14:15 INFO storage.BlockManagerInfo: Added rdd_60_0 in memory on adhoc-2:38763 (size: 438.0 B, free: 366.2 MB)
24/01/05 02:14:15 INFO executor.Executor: Finished task 0.0 in stage 30.0 (TID 24). 1486 bytes result sent to driver
24/01/05 02:14:15 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 30.0 (TID 24) in 678 ms on localhost (executor driver) (1/1)
24/01/05 02:14:15 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 30.0, whose tasks have all completed, from pool 
24/01/05 02:14:15 INFO scheduler.DAGScheduler: ResultStage 30 (collect at HoodieJavaRDD.java:177) finished in 0.717 s
24/01/05 02:14:15 INFO scheduler.DAGScheduler: Job 10 finished: collect at HoodieJavaRDD.java:177, took 0.773244 s
24/01/05 02:14:15 INFO util.CommitUtils: Creating  metadata for UPSERT_PREPPED numWriteStats:1 numReplaceFileIds:0
24/01/05 02:14:15 INFO commit.BaseSparkCommitActionExecutor: Committing 20240105021407331, action Type deltacommit, operation Type UPSERT_PREPPED
24/01/05 02:14:15 INFO spark.SparkContext: Starting job: collect at HoodieSparkEngineContext.java:150
24/01/05 02:14:15 INFO scheduler.DAGScheduler: Got job 11 (collect at HoodieSparkEngineContext.java:150) with 1 output partitions
24/01/05 02:14:15 INFO scheduler.DAGScheduler: Final stage: ResultStage 31 (collect at HoodieSparkEngineContext.java:150)
24/01/05 02:14:15 INFO scheduler.DAGScheduler: Parents of final stage: List()
24/01/05 02:14:15 INFO scheduler.DAGScheduler: Missing parents: List()
24/01/05 02:14:15 INFO scheduler.DAGScheduler: Submitting ResultStage 31 (MapPartitionsRDD[63] at flatMap at HoodieSparkEngineContext.java:150), which has no missing parents
24/01/05 02:14:15 INFO memory.MemoryStore: Block broadcast_19 stored as values in memory (estimated size 72.5 KB, free 365.8 MB)
24/01/05 02:14:15 INFO memory.MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 26.7 KB, free 365.7 MB)
24/01/05 02:14:15 INFO storage.BlockManagerInfo: Added broadcast_19_piece0 in memory on adhoc-2:38763 (size: 26.7 KB, free: 366.1 MB)
24/01/05 02:14:15 INFO spark.SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1161
24/01/05 02:14:15 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 31 (MapPartitionsRDD[63] at flatMap at HoodieSparkEngineContext.java:150) (first 15 tasks are for partitions Vector(0))
24/01/05 02:14:15 INFO scheduler.TaskSchedulerImpl: Adding task set 31.0 with 1 tasks
24/01/05 02:14:15 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 31.0 (TID 25, localhost, executor driver, partition 0, PROCESS_LOCAL, 7838 bytes)
24/01/05 02:14:15 INFO executor.Executor: Running task 0.0 in stage 31.0 (TID 25)
24/01/05 02:14:15 INFO executor.Executor: Finished task 0.0 in stage 31.0 (TID 25). 668 bytes result sent to driver
24/01/05 02:14:15 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 31.0 (TID 25) in 11 ms on localhost (executor driver) (1/1)
24/01/05 02:14:15 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 31.0, whose tasks have all completed, from pool 
24/01/05 02:14:15 INFO scheduler.DAGScheduler: ResultStage 31 (collect at HoodieSparkEngineContext.java:150) finished in 0.027 s
24/01/05 02:14:15 INFO scheduler.DAGScheduler: Job 11 finished: collect at HoodieSparkEngineContext.java:150, took 0.028743 s
24/01/05 02:14:15 INFO timeline.HoodieActiveTimeline: Marking instant complete [==>20240105021407331__deltacommit__INFLIGHT]
24/01/05 02:14:15 INFO timeline.HoodieActiveTimeline: Checking for file exists ?/user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/20240105021407331.deltacommit.inflight
24/01/05 02:14:15 INFO timeline.HoodieActiveTimeline: Create new file for toInstant ?/user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/20240105021407331.deltacommit
24/01/05 02:14:15 INFO timeline.HoodieActiveTimeline: Completed [==>20240105021407331__deltacommit__INFLIGHT]
24/01/05 02:14:15 INFO commit.BaseSparkCommitActionExecutor: Committed 20240105021407331
24/01/05 02:14:15 INFO spark.SparkContext: Starting job: collectAsMap at HoodieSparkEngineContext.java:164
24/01/05 02:14:15 INFO scheduler.DAGScheduler: Got job 12 (collectAsMap at HoodieSparkEngineContext.java:164) with 1 output partitions
24/01/05 02:14:15 INFO scheduler.DAGScheduler: Final stage: ResultStage 32 (collectAsMap at HoodieSparkEngineContext.java:164)
24/01/05 02:14:15 INFO scheduler.DAGScheduler: Parents of final stage: List()
24/01/05 02:14:15 INFO scheduler.DAGScheduler: Missing parents: List()
24/01/05 02:14:15 INFO scheduler.DAGScheduler: Submitting ResultStage 32 (MapPartitionsRDD[65] at mapToPair at HoodieSparkEngineContext.java:161), which has no missing parents
24/01/05 02:14:15 INFO memory.MemoryStore: Block broadcast_20 stored as values in memory (estimated size 72.8 KB, free 365.7 MB)
24/01/05 02:14:15 INFO memory.MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 26.8 KB, free 365.6 MB)
24/01/05 02:14:15 INFO storage.BlockManagerInfo: Added broadcast_20_piece0 in memory on adhoc-2:38763 (size: 26.8 KB, free: 366.1 MB)
24/01/05 02:14:15 INFO spark.SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:1161
24/01/05 02:14:15 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 32 (MapPartitionsRDD[65] at mapToPair at HoodieSparkEngineContext.java:161) (first 15 tasks are for partitions Vector(0))
24/01/05 02:14:15 INFO scheduler.TaskSchedulerImpl: Adding task set 32.0 with 1 tasks
24/01/05 02:14:15 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 32.0 (TID 26, localhost, executor driver, partition 0, PROCESS_LOCAL, 7838 bytes)
24/01/05 02:14:15 INFO executor.Executor: Running task 0.0 in stage 32.0 (TID 26)
24/01/05 02:14:15 INFO executor.Executor: Finished task 0.0 in stage 32.0 (TID 26). 787 bytes result sent to driver
24/01/05 02:14:15 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 32.0 (TID 26) in 15 ms on localhost (executor driver) (1/1)
24/01/05 02:14:15 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 32.0, whose tasks have all completed, from pool 
24/01/05 02:14:15 INFO scheduler.DAGScheduler: ResultStage 32 (collectAsMap at HoodieSparkEngineContext.java:164) finished in 0.026 s
24/01/05 02:14:15 INFO scheduler.DAGScheduler: Job 12 finished: collectAsMap at HoodieSparkEngineContext.java:164, took 0.028279 s
24/01/05 02:14:15 INFO fs.FSUtils: Removed directory at /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/.temp/20240105021407331
24/01/05 02:14:15 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:14:15 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:14:15 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:14:15 INFO table.HoodieTableMetaClient: Loading Active commit timeline for /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:14:15 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105021407331__deltacommit__COMPLETED__20240105021415246]}
24/01/05 02:14:15 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:14:15 INFO view.FileSystemViewManager: Creating View Manager with storage type :MEMORY
24/01/05 02:14:15 INFO view.FileSystemViewManager: Creating in-memory based Table View
24/01/05 02:14:15 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105021407331__deltacommit__COMPLETED__20240105021415246]}
24/01/05 02:14:15 INFO timeline.HoodieActiveTimeline: Marking instant complete [==>20240105021407331__deltacommit__INFLIGHT]
24/01/05 02:14:15 INFO timeline.HoodieActiveTimeline: Checking for file exists ?/user/hive/warehouse/stock_ticks_mor/.hoodie/20240105021407331.deltacommit.inflight
24/01/05 02:14:15 INFO timeline.HoodieActiveTimeline: Create new file for toInstant ?/user/hive/warehouse/stock_ticks_mor/.hoodie/20240105021407331.deltacommit
24/01/05 02:14:15 INFO timeline.HoodieActiveTimeline: Completed [==>20240105021407331__deltacommit__INFLIGHT]
24/01/05 02:14:15 WARN marker.WriteMarkersFactory: Timeline-server-based markers are not supported for HDFS: base path /user/hive/warehouse/stock_ticks_mor.  Falling back to direct markers.
24/01/05 02:14:15 INFO spark.SparkContext: Starting job: collectAsMap at HoodieSparkEngineContext.java:164
24/01/05 02:14:15 INFO scheduler.DAGScheduler: Got job 13 (collectAsMap at HoodieSparkEngineContext.java:164) with 1 output partitions
24/01/05 02:14:15 INFO scheduler.DAGScheduler: Final stage: ResultStage 33 (collectAsMap at HoodieSparkEngineContext.java:164)
24/01/05 02:14:15 INFO scheduler.DAGScheduler: Parents of final stage: List()
24/01/05 02:14:15 INFO scheduler.DAGScheduler: Missing parents: List()
24/01/05 02:14:15 INFO scheduler.DAGScheduler: Submitting ResultStage 33 (MapPartitionsRDD[67] at mapToPair at HoodieSparkEngineContext.java:161), which has no missing parents
24/01/05 02:14:15 INFO memory.MemoryStore: Block broadcast_21 stored as values in memory (estimated size 72.8 KB, free 365.6 MB)
24/01/05 02:14:15 INFO memory.MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 26.8 KB, free 365.5 MB)
24/01/05 02:14:15 INFO storage.BlockManagerInfo: Added broadcast_21_piece0 in memory on adhoc-2:38763 (size: 26.8 KB, free: 366.1 MB)
24/01/05 02:14:15 INFO spark.SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:1161
24/01/05 02:14:15 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 33 (MapPartitionsRDD[67] at mapToPair at HoodieSparkEngineContext.java:161) (first 15 tasks are for partitions Vector(0))
24/01/05 02:14:15 INFO scheduler.TaskSchedulerImpl: Adding task set 33.0 with 1 tasks
24/01/05 02:14:15 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 33.0 (TID 27, localhost, executor driver, partition 0, PROCESS_LOCAL, 7820 bytes)
24/01/05 02:14:15 INFO executor.Executor: Running task 0.0 in stage 33.0 (TID 27)
24/01/05 02:14:15 INFO executor.Executor: Finished task 0.0 in stage 33.0 (TID 27). 769 bytes result sent to driver
24/01/05 02:14:15 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 33.0 (TID 27) in 13 ms on localhost (executor driver) (1/1)
24/01/05 02:14:15 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 33.0, whose tasks have all completed, from pool 
24/01/05 02:14:15 INFO scheduler.DAGScheduler: ResultStage 33 (collectAsMap at HoodieSparkEngineContext.java:164) finished in 0.029 s
24/01/05 02:14:15 INFO scheduler.DAGScheduler: Job 13 finished: collectAsMap at HoodieSparkEngineContext.java:164, took 0.030995 s
24/01/05 02:14:15 INFO fs.FSUtils: Removed directory at /user/hive/warehouse/stock_ticks_mor/.hoodie/.temp/20240105021407331
24/01/05 02:14:15 INFO client.BaseHoodieWriteClient: Committed 20240105021407331
24/01/05 02:14:15 INFO rdd.MapPartitionsRDD: Removing RDD 26 from persistence list
24/01/05 02:14:15 INFO storage.BlockManager: Removing RDD 26
24/01/05 02:14:15 INFO rdd.MapPartitionsRDD: Removing RDD 36 from persistence list
24/01/05 02:14:15 INFO storage.BlockManager: Removing RDD 36
24/01/05 02:14:15 INFO rdd.UnionRDD: Removing RDD 50 from persistence list
24/01/05 02:14:15 INFO storage.BlockManager: Removing RDD 50
24/01/05 02:14:15 INFO rdd.MapPartitionsRDD: Removing RDD 60 from persistence list
24/01/05 02:14:15 INFO storage.BlockManager: Removing RDD 60
24/01/05 02:14:15 INFO transaction.TransactionManager: Transaction ending with transaction owner Option{val=[==>20240105021407331__deltacommit__INFLIGHT]}
24/01/05 02:14:15 INFO lock.InProcessLockProvider: Base Path /user/hive/warehouse/stock_ticks_mor, Lock Instance java.util.concurrent.locks.ReentrantReadWriteLock@7254838[Write locks = 1, Read locks = 0], Thread main, In-process lock state RELEASING
24/01/05 02:14:15 INFO lock.InProcessLockProvider: Base Path /user/hive/warehouse/stock_ticks_mor, Lock Instance java.util.concurrent.locks.ReentrantReadWriteLock@7254838[Write locks = 0, Read locks = 0], Thread main, In-process lock state RELEASED
24/01/05 02:14:15 INFO lock.InProcessLockProvider: Base Path /user/hive/warehouse/stock_ticks_mor, Lock Instance java.util.concurrent.locks.ReentrantReadWriteLock@7254838[Write locks = 0, Read locks = 0], Thread main, In-process lock state ALREADY_RELEASED
24/01/05 02:14:15 INFO lock.LockManager: Released connection created for acquiring lock
24/01/05 02:14:15 INFO transaction.TransactionManager: Transaction ended with transaction owner Option{val=[==>20240105021407331__deltacommit__INFLIGHT]}
24/01/05 02:14:15 INFO client.BaseHoodieWriteClient: Start to clean synchronously.
24/01/05 02:14:15 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:14:15 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:14:15 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:14:15 INFO table.HoodieTableMetaClient: Loading Active commit timeline for /user/hive/warehouse/stock_ticks_mor
24/01/05 02:14:15 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105021407331__deltacommit__COMPLETED__20240105021415726]}
24/01/05 02:14:15 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:14:15 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:14:15 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:14:15 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:14:15 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:14:15 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:14:15 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105021407331__deltacommit__COMPLETED__20240105021415246]}
24/01/05 02:14:15 INFO view.AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
24/01/05 02:14:15 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:14:15 INFO view.FileSystemViewManager: Creating View Manager with storage type :REMOTE_FIRST
24/01/05 02:14:15 INFO view.FileSystemViewManager: Creating remote first table view
24/01/05 02:14:15 INFO client.BaseHoodieWriteClient: Cleaner started
24/01/05 02:14:15 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:14:15 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:14:15 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:14:15 INFO table.HoodieTableMetaClient: Loading Active commit timeline for /user/hive/warehouse/stock_ticks_mor
24/01/05 02:14:15 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105021407331__deltacommit__COMPLETED__20240105021415726]}
24/01/05 02:14:15 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:14:15 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:14:15 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:14:15 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:14:15 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:14:15 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:14:15 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105021407331__deltacommit__COMPLETED__20240105021415246]}
24/01/05 02:14:15 INFO view.AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
24/01/05 02:14:15 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:14:15 INFO view.FileSystemViewManager: Creating View Manager with storage type :REMOTE_FIRST
24/01/05 02:14:15 INFO view.FileSystemViewManager: Creating remote first table view
24/01/05 02:14:15 INFO client.BaseHoodieWriteClient: Scheduling cleaning at instant time :20240105021415814
24/01/05 02:14:15 INFO view.FileSystemViewManager: Creating remote view for basePath /user/hive/warehouse/stock_ticks_mor. Server=adhoc-2:43195, Timeout=300
24/01/05 02:14:15 INFO view.FileSystemViewManager: Creating InMemory based view for basePath /user/hive/warehouse/stock_ticks_mor
24/01/05 02:14:15 INFO view.AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
24/01/05 02:14:15 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:14:15 INFO view.RemoteHoodieTableFileSystemView: Sending request : (http://adhoc-2:43195/v1/hoodie/view/compactions/pending/?basepath=%2Fuser%2Fhive%2Fwarehouse%2Fstock_ticks_mor&lastinstantts=20240105021407331&timelinehash=b271aa6d866dd95e491ac1c4d3c452644fd5c7bef1f4894a286a392c43b0a50d)
24/01/05 02:14:15 INFO service.RequestHandler: Syncing view as client passed last known instant 20240105021407331 as last known instant but server has the following last instant on timeline :Option{val=[20240105021347452__deltacommit__COMPLETED__20240105021356931]}
24/01/05 02:14:15 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105021407331__deltacommit__COMPLETED__20240105021415726]}
24/01/05 02:14:15 INFO view.RocksDbBasedFileSystemView: Deleting all rocksdb data associated with table filesystem view
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : [/db_impl/db_impl.cc:478] Shutdown: canceling all background work
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : [/db_impl/db_impl.cc:677] Shutdown complete
24/01/05 02:14:15 INFO collection.RocksDBDAO: DELETING RocksDB persisted at /tmp/hoodie_timeline_rocksdb/_user_hive_warehouse_stock_ticks_mor/2f87ad92-f70f-4020-9118-9022e1d3b401
24/01/05 02:14:15 INFO collection.RocksDBDAO: No column family found. Loading default
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : RocksDB version: 7.5.3

24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Compile date 2022-09-02 09:50:20
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : DB SUMMARY

24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : DB Session ID:  JQIOZABS498AA5408TNQ

24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : SST files in /tmp/hoodie_timeline_rocksdb/_user_hive_warehouse_stock_ticks_mor/2f87ad92-f70f-4020-9118-9022e1d3b401 dir, Total Num: 0, files: 

24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Write Ahead Log file in /tmp/hoodie_timeline_rocksdb/_user_hive_warehouse_stock_ticks_mor/2f87ad92-f70f-4020-9118-9022e1d3b401: 

24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                         Options.error_if_exists: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                       Options.create_if_missing: 1
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                         Options.paranoid_checks: 1
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :             Options.flush_verify_memtable_count: 1
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                               Options.track_and_verify_wals_in_manifest: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :        Options.verify_sst_unique_id_in_manifest: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                                     Options.env: 0x7f9da2f945e0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                                      Options.fs: PosixFileSystem
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                                Options.info_log: 0x7f9cc003b358
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                Options.max_file_opening_threads: 16
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                              Options.statistics: 0x7f9cc003b6c0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                               Options.use_fsync: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                       Options.max_log_file_size: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                  Options.max_manifest_file_size: 1073741824
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                   Options.log_file_time_to_roll: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                       Options.keep_log_file_num: 1000
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                    Options.recycle_log_file_num: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                         Options.allow_fallocate: 1
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                        Options.allow_mmap_reads: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                       Options.allow_mmap_writes: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                        Options.use_direct_reads: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                        Options.use_direct_io_for_flush_and_compaction: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :          Options.create_missing_column_families: 1
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                              Options.db_log_dir: 
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                                 Options.wal_dir: 
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                Options.table_cache_numshardbits: 6
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                         Options.WAL_ttl_seconds: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                       Options.WAL_size_limit_MB: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                        Options.max_write_batch_group_size_bytes: 1048576
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :             Options.manifest_preallocation_size: 4194304
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                     Options.is_fd_close_on_exec: 1
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                   Options.advise_random_on_open: 1
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                    Options.db_write_buffer_size: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                    Options.write_buffer_manager: 0x7f9cc0042130
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :         Options.access_hint_on_compaction_start: 1
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :           Options.random_access_max_buffer_size: 1048576
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                      Options.use_adaptive_mutex: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                            Options.rate_limiter: (nil)
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :     Options.sst_file_manager.rate_bytes_per_sec: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                       Options.wal_recovery_mode: 2
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                  Options.enable_thread_tracking: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                  Options.enable_pipelined_write: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                  Options.unordered_write: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :         Options.allow_concurrent_memtable_write: 1
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :      Options.enable_write_thread_adaptive_yield: 1
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :             Options.write_thread_max_yield_usec: 100
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :            Options.write_thread_slow_yield_usec: 3
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                               Options.row_cache: None
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                              Options.wal_filter: None
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :             Options.avoid_flush_during_recovery: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :             Options.allow_ingest_behind: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :             Options.two_write_queues: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :             Options.manual_wal_flush: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :             Options.wal_compression: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :             Options.atomic_flush: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :             Options.avoid_unnecessary_blocking_io: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                 Options.persist_stats_to_disk: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                 Options.write_dbid_to_manifest: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                 Options.log_readahead_size: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                 Options.file_checksum_gen_factory: Unknown
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                 Options.best_efforts_recovery: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                Options.max_bgerror_resume_count: 2147483647
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :            Options.bgerror_resume_retry_interval: 1000000
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :             Options.allow_data_in_errors: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :             Options.db_host_id: __hostname__
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :             Options.enforce_single_del_contracts: true
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :             Options.max_background_jobs: 2
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :             Options.max_background_compactions: -1
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :             Options.max_subcompactions: 1
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :             Options.avoid_flush_during_shutdown: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :           Options.writable_file_max_buffer_size: 1048576
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :             Options.delayed_write_rate : 16777216
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :             Options.max_total_wal_size: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :             Options.delete_obsolete_files_period_micros: 21600000000
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                   Options.stats_dump_period_sec: 300
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                 Options.stats_persist_period_sec: 600
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                 Options.stats_history_buffer_size: 1048576
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                          Options.max_open_files: -1
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                          Options.bytes_per_sync: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                      Options.wal_bytes_per_sync: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                   Options.strict_bytes_per_sync: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :       Options.compaction_readahead_size: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                  Options.max_background_flushes: -1
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Compression algorithms supported:
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : 	kZSTDNotFinalCompression supported: 1
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : 	kZSTD supported: 1
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : 	kXpressCompression supported: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : 	kLZ4HCCompression supported: 1
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : 	kLZ4Compression supported: 1
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : 	kBZip2Compression supported: 1
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : 	kZlibCompression supported: 1
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : 	kSnappyCompression supported: 1
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Fast CRC32 supported: Not supported on x86
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : DMutex implementation: pthread_mutex_t
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : [/db_impl/db_impl_open.cc:313] Creating manifest 1 

24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : [/version_set.cc:4994] Recovering from manifest file: /tmp/hoodie_timeline_rocksdb/_user_hive_warehouse_stock_ticks_mor/2f87ad92-f70f-4020-9118-9022e1d3b401/MANIFEST-000001

24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : [/column_family.cc:618] --------------- Options for column family [default]:

24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :               Options.comparator: leveldb.BytewiseComparator
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :           Options.merge_operator: None
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :        Options.compaction_filter: None
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :        Options.compaction_filter_factory: None
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :  Options.sst_partitioner_factory: None
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :         Options.memtable_factory: SkipListFactory
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :            Options.table_factory: BlockBasedTable
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :            table_factory options:   flush_block_policy_factory: FlushBlockBySizePolicyFactory (0x7f9cc003e840)
  cache_index_and_filter_blocks: 0
  cache_index_and_filter_blocks_with_high_priority: 1
  pin_l0_filter_and_index_blocks_in_cache: 0
  pin_top_level_index_and_filter: 1
  index_type: 0
  data_block_index_type: 0
  index_shortening: 1
  data_block_hash_table_util_ratio: 0.750000
  checksum: 1
  no_block_cache: 0
  block_cache: 0x7f9cc003e890
  block_cache_name: LRUCache
  block_cache_options:
    capacity : 8388608
    num_shard_bits : 4
    strict_capacity_limit : 0
    memory_allocator : None
    high_pri_pool_ratio: 0.000
  block_cache_compressed: (nil)
  persistent_cache: (nil)
  block_size: 4096
  block_size_deviation: 10
  block_restart_interval: 16
  index_block_restart_interval: 1
  metadata_block_size: 4096
  partition_filters: 0
  use_delta_encoding: 1
  filter_policy: nullptr
  whole_key_filtering: 1
  verify_compression: 0
  read_amp_bytes_per_bit: 0
  format_version: 5
  enable_index_compression: 1
  block_align: 0
  max_auto_readahead_size: 262144
  prepopulate_block_cache: 0
  initial_auto_readahead_size: 8192

24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :        Options.write_buffer_size: 67108864
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :  Options.max_write_buffer_number: 2
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :          Options.compression: Snappy
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression: Disabled
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :       Options.prefix_extractor: nullptr
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :   Options.memtable_insert_with_hint_prefix_extractor: nullptr
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :             Options.num_levels: 7
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :        Options.min_write_buffer_number_to_merge: 1
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :     Options.max_write_buffer_number_to_maintain: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :     Options.max_write_buffer_size_to_maintain: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :            Options.bottommost_compression_opts.window_bits: -14
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression_opts.level: 32767
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :               Options.bottommost_compression_opts.strategy: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.max_dict_bytes: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.zstd_max_train_bytes: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.parallel_threads: 1
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression_opts.enabled: false
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.max_dict_buffer_bytes: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.use_zstd_dict_trainer: true
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :            Options.compression_opts.window_bits: -14
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                  Options.compression_opts.level: 32767
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :               Options.compression_opts.strategy: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.max_dict_bytes: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.zstd_max_train_bytes: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.use_zstd_dict_trainer: true
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.parallel_threads: 1
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                  Options.compression_opts.enabled: false
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.max_dict_buffer_bytes: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :      Options.level0_file_num_compaction_trigger: 4
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :          Options.level0_slowdown_writes_trigger: 20
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :              Options.level0_stop_writes_trigger: 36
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                   Options.target_file_size_base: 67108864
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :             Options.target_file_size_multiplier: 1
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                Options.max_bytes_for_level_base: 268435456
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Options.level_compaction_dynamic_level_bytes: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :          Options.max_bytes_for_level_multiplier: 10.000000
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[0]: 1
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[1]: 1
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[2]: 1
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[3]: 1
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[4]: 1
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[5]: 1
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[6]: 1
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :       Options.max_sequential_skip_in_iterations: 8
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                    Options.max_compaction_bytes: 1677721600
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                        Options.arena_block_size: 1048576
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :   Options.soft_pending_compaction_bytes_limit: 68719476736
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :   Options.hard_pending_compaction_bytes_limit: 274877906944
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                Options.disable_auto_compactions: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                        Options.compaction_style: kCompactionStyleLevel
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                          Options.compaction_pri: kMinOverlappingRatio
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.size_ratio: 1
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.min_merge_width: 2
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.max_merge_width: 4294967295
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.max_size_amplification_percent: 200
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.compression_size_percent: -1
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.stop_style: kCompactionStopStyleTotalSize
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_fifo.max_table_files_size: 1073741824
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_fifo.allow_compaction: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                   Options.table_properties_collectors: 
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                   Options.inplace_update_support: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                 Options.inplace_update_num_locks: 10000
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :               Options.memtable_prefix_bloom_size_ratio: 0.000000
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :               Options.memtable_whole_key_filtering: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :   Options.memtable_huge_page_size: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                           Options.bloom_locality: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                    Options.max_successive_merges: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                Options.optimize_filters_for_hits: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                Options.paranoid_file_checks: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                Options.force_consistency_checks: 1
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                Options.report_bg_io_stats: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                               Options.ttl: 2592000
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :          Options.periodic_compaction_seconds: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :  Options.preclude_last_level_data_seconds: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                       Options.enable_blob_files: false
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                           Options.min_blob_size: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                          Options.blob_file_size: 268435456
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                   Options.blob_compression_type: NoCompression
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :          Options.enable_blob_garbage_collection: false
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :      Options.blob_garbage_collection_age_cutoff: 0.250000
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Options.blob_garbage_collection_force_threshold: 1.000000
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :          Options.blob_compaction_readahead_size: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                Options.blob_file_starting_level: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Options.experimental_mempurge_threshold: 0.000000
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : [/version_set.cc:5042] Recovered from manifest file:/tmp/hoodie_timeline_rocksdb/_user_hive_warehouse_stock_ticks_mor/2f87ad92-f70f-4020-9118-9022e1d3b401/MANIFEST-000001 succeeded,manifest_file_number is 1, next_file_number is 3, last_sequence is 0, log_number is 0,prev_log_number is 0,max_column_family is 0,min_log_number_to_keep is 0

24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : [/version_set.cc:5051] Column family [default] (ID 0), log number is 0

24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : [/db_impl/db_impl_open.cc:532] DB ID: 41ca331f-d35f-4291-8de3-8dee71932a16

24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : [/version_set.cc:4513] Creating manifest 5

24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : [/db_impl/db_impl_open.cc:2024] SstFileManager instance 0x7f9cc0042210
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : DB pointer 0x7f9cc0040940
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : [/column_family.cc:618] --------------- Options for column family [hudi_view__user_hive_warehouse_stock_ticks_mor]:

24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :               Options.comparator: leveldb.BytewiseComparator
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :           Options.merge_operator: None
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :        Options.compaction_filter: None
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :        Options.compaction_filter_factory: None
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :  Options.sst_partitioner_factory: None
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :         Options.memtable_factory: SkipListFactory
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :            Options.table_factory: BlockBasedTable
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :            table_factory options:   flush_block_policy_factory: FlushBlockBySizePolicyFactory (0x7f9cc00d7e30)
  cache_index_and_filter_blocks: 0
  cache_index_and_filter_blocks_with_high_priority: 1
  pin_l0_filter_and_index_blocks_in_cache: 0
  pin_top_level_index_and_filter: 1
  index_type: 0
  data_block_index_type: 0
  index_shortening: 1
  data_block_hash_table_util_ratio: 0.750000
  checksum: 1
  no_block_cache: 0
  block_cache: 0x7f9cc00d7940
  block_cache_name: LRUCache
  block_cache_options:
    capacity : 8388608
    num_shard_bits : 4
    strict_capacity_limit : 0
    memory_allocator : None
    high_pri_pool_ratio: 0.000
  block_cache_compressed: (nil)
  persistent_cache: (nil)
  block_size: 4096
  block_size_deviation: 10
  block_restart_interval: 16
  index_block_restart_interval: 1
  metadata_block_size: 4096
  partition_filters: 0
  use_delta_encoding: 1
  filter_policy: nullptr
  whole_key_filtering: 1
  verify_compression: 0
  read_amp_bytes_per_bit: 0
  format_version: 5
  enable_index_compression: 1
  block_align: 0
  max_auto_readahead_size: 262144
  prepopulate_block_cache: 0
  initial_auto_readahead_size: 8192

24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :        Options.write_buffer_size: 67108864
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :  Options.max_write_buffer_number: 2
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :          Options.compression: Snappy
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression: Disabled
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :       Options.prefix_extractor: nullptr
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :   Options.memtable_insert_with_hint_prefix_extractor: nullptr
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :             Options.num_levels: 7
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :        Options.min_write_buffer_number_to_merge: 1
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :     Options.max_write_buffer_number_to_maintain: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :     Options.max_write_buffer_size_to_maintain: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :            Options.bottommost_compression_opts.window_bits: -14
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression_opts.level: 32767
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :               Options.bottommost_compression_opts.strategy: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.max_dict_bytes: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.zstd_max_train_bytes: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.parallel_threads: 1
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression_opts.enabled: false
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.max_dict_buffer_bytes: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.use_zstd_dict_trainer: true
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :            Options.compression_opts.window_bits: -14
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                  Options.compression_opts.level: 32767
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :               Options.compression_opts.strategy: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.max_dict_bytes: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.zstd_max_train_bytes: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.use_zstd_dict_trainer: true
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.parallel_threads: 1
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                  Options.compression_opts.enabled: false
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.max_dict_buffer_bytes: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :      Options.level0_file_num_compaction_trigger: 4
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :          Options.level0_slowdown_writes_trigger: 20
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :              Options.level0_stop_writes_trigger: 36
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                   Options.target_file_size_base: 67108864
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :             Options.target_file_size_multiplier: 1
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                Options.max_bytes_for_level_base: 268435456
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Options.level_compaction_dynamic_level_bytes: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :          Options.max_bytes_for_level_multiplier: 10.000000
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[0]: 1
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[1]: 1
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[2]: 1
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[3]: 1
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[4]: 1
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[5]: 1
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[6]: 1
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :       Options.max_sequential_skip_in_iterations: 8
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                    Options.max_compaction_bytes: 1677721600
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                        Options.arena_block_size: 1048576
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :   Options.soft_pending_compaction_bytes_limit: 68719476736
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :   Options.hard_pending_compaction_bytes_limit: 274877906944
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                Options.disable_auto_compactions: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                        Options.compaction_style: kCompactionStyleLevel
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                          Options.compaction_pri: kMinOverlappingRatio
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.size_ratio: 1
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.min_merge_width: 2
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.max_merge_width: 4294967295
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.max_size_amplification_percent: 200
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.compression_size_percent: -1
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.stop_style: kCompactionStopStyleTotalSize
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_fifo.max_table_files_size: 1073741824
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_fifo.allow_compaction: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                   Options.table_properties_collectors: 
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                   Options.inplace_update_support: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                 Options.inplace_update_num_locks: 10000
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :               Options.memtable_prefix_bloom_size_ratio: 0.000000
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :               Options.memtable_whole_key_filtering: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :   Options.memtable_huge_page_size: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                           Options.bloom_locality: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                    Options.max_successive_merges: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                Options.optimize_filters_for_hits: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                Options.paranoid_file_checks: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                Options.force_consistency_checks: 1
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                Options.report_bg_io_stats: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                               Options.ttl: 2592000
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :          Options.periodic_compaction_seconds: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :  Options.preclude_last_level_data_seconds: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                       Options.enable_blob_files: false
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                           Options.min_blob_size: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                          Options.blob_file_size: 268435456
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                   Options.blob_compression_type: NoCompression
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :          Options.enable_blob_garbage_collection: false
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :      Options.blob_garbage_collection_age_cutoff: 0.250000
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Options.blob_garbage_collection_force_threshold: 1.000000
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :          Options.blob_compaction_readahead_size: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                Options.blob_file_starting_level: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Options.experimental_mempurge_threshold: 0.000000
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : [/db_impl/db_impl.cc:2861] Created column family [hudi_view__user_hive_warehouse_stock_ticks_mor] (ID 1)
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : [/column_family.cc:618] --------------- Options for column family [hudi_pending_compaction__user_hive_warehouse_stock_ticks_mor]:

24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :               Options.comparator: leveldb.BytewiseComparator
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :           Options.merge_operator: None
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :        Options.compaction_filter: None
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :        Options.compaction_filter_factory: None
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :  Options.sst_partitioner_factory: None
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :         Options.memtable_factory: SkipListFactory
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :            Options.table_factory: BlockBasedTable
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :            table_factory options:   flush_block_policy_factory: FlushBlockBySizePolicyFactory (0x7f9cc00d5ff0)
  cache_index_and_filter_blocks: 0
  cache_index_and_filter_blocks_with_high_priority: 1
  pin_l0_filter_and_index_blocks_in_cache: 0
  pin_top_level_index_and_filter: 1
  index_type: 0
  data_block_index_type: 0
  index_shortening: 1
  data_block_hash_table_util_ratio: 0.750000
  checksum: 1
  no_block_cache: 0
  block_cache: 0x7f9cc00c8150
  block_cache_name: LRUCache
  block_cache_options:
    capacity : 8388608
    num_shard_bits : 4
    strict_capacity_limit : 0
    memory_allocator : None
    high_pri_pool_ratio: 0.000
  block_cache_compressed: (nil)
  persistent_cache: (nil)
  block_size: 4096
  block_size_deviation: 10
  block_restart_interval: 16
  index_block_restart_interval: 1
  metadata_block_size: 4096
  partition_filters: 0
  use_delta_encoding: 1
  filter_policy: nullptr
  whole_key_filtering: 1
  verify_compression: 0
  read_amp_bytes_per_bit: 0
  format_version: 5
  enable_index_compression: 1
  block_align: 0
  max_auto_readahead_size: 262144
  prepopulate_block_cache: 0
  initial_auto_readahead_size: 8192

24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :        Options.write_buffer_size: 67108864
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :  Options.max_write_buffer_number: 2
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :          Options.compression: Snappy
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression: Disabled
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :       Options.prefix_extractor: nullptr
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :   Options.memtable_insert_with_hint_prefix_extractor: nullptr
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :             Options.num_levels: 7
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :        Options.min_write_buffer_number_to_merge: 1
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :     Options.max_write_buffer_number_to_maintain: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :     Options.max_write_buffer_size_to_maintain: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :            Options.bottommost_compression_opts.window_bits: -14
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression_opts.level: 32767
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :               Options.bottommost_compression_opts.strategy: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.max_dict_bytes: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.zstd_max_train_bytes: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.parallel_threads: 1
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression_opts.enabled: false
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.max_dict_buffer_bytes: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.use_zstd_dict_trainer: true
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :            Options.compression_opts.window_bits: -14
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                  Options.compression_opts.level: 32767
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :               Options.compression_opts.strategy: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.max_dict_bytes: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.zstd_max_train_bytes: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.use_zstd_dict_trainer: true
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.parallel_threads: 1
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                  Options.compression_opts.enabled: false
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.max_dict_buffer_bytes: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :      Options.level0_file_num_compaction_trigger: 4
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :          Options.level0_slowdown_writes_trigger: 20
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :              Options.level0_stop_writes_trigger: 36
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                   Options.target_file_size_base: 67108864
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :             Options.target_file_size_multiplier: 1
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                Options.max_bytes_for_level_base: 268435456
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Options.level_compaction_dynamic_level_bytes: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :          Options.max_bytes_for_level_multiplier: 10.000000
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[0]: 1
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[1]: 1
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[2]: 1
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[3]: 1
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[4]: 1
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[5]: 1
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[6]: 1
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :       Options.max_sequential_skip_in_iterations: 8
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                    Options.max_compaction_bytes: 1677721600
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                        Options.arena_block_size: 1048576
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :   Options.soft_pending_compaction_bytes_limit: 68719476736
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :   Options.hard_pending_compaction_bytes_limit: 274877906944
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                Options.disable_auto_compactions: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                        Options.compaction_style: kCompactionStyleLevel
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                          Options.compaction_pri: kMinOverlappingRatio
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.size_ratio: 1
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.min_merge_width: 2
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.max_merge_width: 4294967295
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.max_size_amplification_percent: 200
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.compression_size_percent: -1
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.stop_style: kCompactionStopStyleTotalSize
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_fifo.max_table_files_size: 1073741824
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_fifo.allow_compaction: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                   Options.table_properties_collectors: 
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                   Options.inplace_update_support: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                 Options.inplace_update_num_locks: 10000
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :               Options.memtable_prefix_bloom_size_ratio: 0.000000
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :               Options.memtable_whole_key_filtering: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :   Options.memtable_huge_page_size: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                           Options.bloom_locality: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                    Options.max_successive_merges: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                Options.optimize_filters_for_hits: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                Options.paranoid_file_checks: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                Options.force_consistency_checks: 1
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                Options.report_bg_io_stats: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                               Options.ttl: 2592000
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :          Options.periodic_compaction_seconds: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :  Options.preclude_last_level_data_seconds: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                       Options.enable_blob_files: false
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                           Options.min_blob_size: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                          Options.blob_file_size: 268435456
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                   Options.blob_compression_type: NoCompression
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :          Options.enable_blob_garbage_collection: false
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :      Options.blob_garbage_collection_age_cutoff: 0.250000
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Options.blob_garbage_collection_force_threshold: 1.000000
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :          Options.blob_compaction_readahead_size: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                Options.blob_file_starting_level: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Options.experimental_mempurge_threshold: 0.000000
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : [/db_impl/db_impl.cc:2861] Created column family [hudi_pending_compaction__user_hive_warehouse_stock_ticks_mor] (ID 2)
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : [/column_family.cc:618] --------------- Options for column family [hudi_pending_log_compaction__user_hive_warehouse_stock_ticks_mor]:

24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :               Options.comparator: leveldb.BytewiseComparator
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :           Options.merge_operator: None
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :        Options.compaction_filter: None
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :        Options.compaction_filter_factory: None
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :  Options.sst_partitioner_factory: None
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :         Options.memtable_factory: SkipListFactory
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :            Options.table_factory: BlockBasedTable
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :            table_factory options:   flush_block_policy_factory: FlushBlockBySizePolicyFactory (0x7f9cc0100d80)
  cache_index_and_filter_blocks: 0
  cache_index_and_filter_blocks_with_high_priority: 1
  pin_l0_filter_and_index_blocks_in_cache: 0
  pin_top_level_index_and_filter: 1
  index_type: 0
  data_block_index_type: 0
  index_shortening: 1
  data_block_hash_table_util_ratio: 0.750000
  checksum: 1
  no_block_cache: 0
  block_cache: 0x7f9cc0100bd0
  block_cache_name: LRUCache
  block_cache_options:
    capacity : 8388608
    num_shard_bits : 4
    strict_capacity_limit : 0
    memory_allocator : None
    high_pri_pool_ratio: 0.000
  block_cache_compressed: (nil)
  persistent_cache: (nil)
  block_size: 4096
  block_size_deviation: 10
  block_restart_interval: 16
  index_block_restart_interval: 1
  metadata_block_size: 4096
  partition_filters: 0
  use_delta_encoding: 1
  filter_policy: nullptr
  whole_key_filtering: 1
  verify_compression: 0
  read_amp_bytes_per_bit: 0
  format_version: 5
  enable_index_compression: 1
  block_align: 0
  max_auto_readahead_size: 262144
  prepopulate_block_cache: 0
  initial_auto_readahead_size: 8192

24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :        Options.write_buffer_size: 67108864
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :  Options.max_write_buffer_number: 2
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :          Options.compression: Snappy
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression: Disabled
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :       Options.prefix_extractor: nullptr
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :   Options.memtable_insert_with_hint_prefix_extractor: nullptr
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :             Options.num_levels: 7
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :        Options.min_write_buffer_number_to_merge: 1
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :     Options.max_write_buffer_number_to_maintain: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :     Options.max_write_buffer_size_to_maintain: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :            Options.bottommost_compression_opts.window_bits: -14
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression_opts.level: 32767
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :               Options.bottommost_compression_opts.strategy: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.max_dict_bytes: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.zstd_max_train_bytes: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.parallel_threads: 1
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression_opts.enabled: false
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.max_dict_buffer_bytes: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.use_zstd_dict_trainer: true
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :            Options.compression_opts.window_bits: -14
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                  Options.compression_opts.level: 32767
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :               Options.compression_opts.strategy: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.max_dict_bytes: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.zstd_max_train_bytes: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.use_zstd_dict_trainer: true
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.parallel_threads: 1
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                  Options.compression_opts.enabled: false
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.max_dict_buffer_bytes: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :      Options.level0_file_num_compaction_trigger: 4
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :          Options.level0_slowdown_writes_trigger: 20
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :              Options.level0_stop_writes_trigger: 36
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                   Options.target_file_size_base: 67108864
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :             Options.target_file_size_multiplier: 1
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                Options.max_bytes_for_level_base: 268435456
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Options.level_compaction_dynamic_level_bytes: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :          Options.max_bytes_for_level_multiplier: 10.000000
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[0]: 1
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[1]: 1
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[2]: 1
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[3]: 1
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[4]: 1
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[5]: 1
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[6]: 1
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :       Options.max_sequential_skip_in_iterations: 8
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                    Options.max_compaction_bytes: 1677721600
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                        Options.arena_block_size: 1048576
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :   Options.soft_pending_compaction_bytes_limit: 68719476736
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :   Options.hard_pending_compaction_bytes_limit: 274877906944
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                Options.disable_auto_compactions: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                        Options.compaction_style: kCompactionStyleLevel
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                          Options.compaction_pri: kMinOverlappingRatio
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.size_ratio: 1
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.min_merge_width: 2
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.max_merge_width: 4294967295
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.max_size_amplification_percent: 200
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.compression_size_percent: -1
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.stop_style: kCompactionStopStyleTotalSize
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_fifo.max_table_files_size: 1073741824
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_fifo.allow_compaction: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                   Options.table_properties_collectors: 
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                   Options.inplace_update_support: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                 Options.inplace_update_num_locks: 10000
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :               Options.memtable_prefix_bloom_size_ratio: 0.000000
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :               Options.memtable_whole_key_filtering: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :   Options.memtable_huge_page_size: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                           Options.bloom_locality: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                    Options.max_successive_merges: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                Options.optimize_filters_for_hits: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                Options.paranoid_file_checks: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                Options.force_consistency_checks: 1
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                Options.report_bg_io_stats: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                               Options.ttl: 2592000
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :          Options.periodic_compaction_seconds: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :  Options.preclude_last_level_data_seconds: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                       Options.enable_blob_files: false
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                           Options.min_blob_size: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                          Options.blob_file_size: 268435456
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                   Options.blob_compression_type: NoCompression
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :          Options.enable_blob_garbage_collection: false
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :      Options.blob_garbage_collection_age_cutoff: 0.250000
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Options.blob_garbage_collection_force_threshold: 1.000000
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :          Options.blob_compaction_readahead_size: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                Options.blob_file_starting_level: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Options.experimental_mempurge_threshold: 0.000000
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : [/db_impl/db_impl.cc:2861] Created column family [hudi_pending_log_compaction__user_hive_warehouse_stock_ticks_mor] (ID 3)
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : [/column_family.cc:618] --------------- Options for column family [hudi_bootstrap_basefile__user_hive_warehouse_stock_ticks_mor]:

24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :               Options.comparator: leveldb.BytewiseComparator
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :           Options.merge_operator: None
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :        Options.compaction_filter: None
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :        Options.compaction_filter_factory: None
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :  Options.sst_partitioner_factory: None
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :         Options.memtable_factory: SkipListFactory
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :            Options.table_factory: BlockBasedTable
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :            table_factory options:   flush_block_policy_factory: FlushBlockBySizePolicyFactory (0x7f9cc01199f0)
  cache_index_and_filter_blocks: 0
  cache_index_and_filter_blocks_with_high_priority: 1
  pin_l0_filter_and_index_blocks_in_cache: 0
  pin_top_level_index_and_filter: 1
  index_type: 0
  data_block_index_type: 0
  index_shortening: 1
  data_block_hash_table_util_ratio: 0.750000
  checksum: 1
  no_block_cache: 0
  block_cache: 0x7f9cc010e590
  block_cache_name: LRUCache
  block_cache_options:
    capacity : 8388608
    num_shard_bits : 4
    strict_capacity_limit : 0
    memory_allocator : None
    high_pri_pool_ratio: 0.000
  block_cache_compressed: (nil)
  persistent_cache: (nil)
  block_size: 4096
  block_size_deviation: 10
  block_restart_interval: 16
  index_block_restart_interval: 1
  metadata_block_size: 4096
  partition_filters: 0
  use_delta_encoding: 1
  filter_policy: nullptr
  whole_key_filtering: 1
  verify_compression: 0
  read_amp_bytes_per_bit: 0
  format_version: 5
  enable_index_compression: 1
  block_align: 0
  max_auto_readahead_size: 262144
  prepopulate_block_cache: 0
  initial_auto_readahead_size: 8192

24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :        Options.write_buffer_size: 67108864
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :  Options.max_write_buffer_number: 2
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :          Options.compression: Snappy
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression: Disabled
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :       Options.prefix_extractor: nullptr
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :   Options.memtable_insert_with_hint_prefix_extractor: nullptr
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :             Options.num_levels: 7
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :        Options.min_write_buffer_number_to_merge: 1
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :     Options.max_write_buffer_number_to_maintain: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :     Options.max_write_buffer_size_to_maintain: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :            Options.bottommost_compression_opts.window_bits: -14
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression_opts.level: 32767
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :               Options.bottommost_compression_opts.strategy: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.max_dict_bytes: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.zstd_max_train_bytes: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.parallel_threads: 1
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression_opts.enabled: false
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.max_dict_buffer_bytes: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.use_zstd_dict_trainer: true
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :            Options.compression_opts.window_bits: -14
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                  Options.compression_opts.level: 32767
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :               Options.compression_opts.strategy: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.max_dict_bytes: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.zstd_max_train_bytes: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.use_zstd_dict_trainer: true
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.parallel_threads: 1
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                  Options.compression_opts.enabled: false
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.max_dict_buffer_bytes: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :      Options.level0_file_num_compaction_trigger: 4
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :          Options.level0_slowdown_writes_trigger: 20
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :              Options.level0_stop_writes_trigger: 36
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                   Options.target_file_size_base: 67108864
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :             Options.target_file_size_multiplier: 1
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                Options.max_bytes_for_level_base: 268435456
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Options.level_compaction_dynamic_level_bytes: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :          Options.max_bytes_for_level_multiplier: 10.000000
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[0]: 1
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[1]: 1
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[2]: 1
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[3]: 1
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[4]: 1
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[5]: 1
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[6]: 1
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :       Options.max_sequential_skip_in_iterations: 8
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                    Options.max_compaction_bytes: 1677721600
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                        Options.arena_block_size: 1048576
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :   Options.soft_pending_compaction_bytes_limit: 68719476736
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :   Options.hard_pending_compaction_bytes_limit: 274877906944
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                Options.disable_auto_compactions: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                        Options.compaction_style: kCompactionStyleLevel
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                          Options.compaction_pri: kMinOverlappingRatio
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.size_ratio: 1
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.min_merge_width: 2
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.max_merge_width: 4294967295
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.max_size_amplification_percent: 200
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.compression_size_percent: -1
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.stop_style: kCompactionStopStyleTotalSize
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_fifo.max_table_files_size: 1073741824
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_fifo.allow_compaction: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                   Options.table_properties_collectors: 
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                   Options.inplace_update_support: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                 Options.inplace_update_num_locks: 10000
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :               Options.memtable_prefix_bloom_size_ratio: 0.000000
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :               Options.memtable_whole_key_filtering: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :   Options.memtable_huge_page_size: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                           Options.bloom_locality: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                    Options.max_successive_merges: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                Options.optimize_filters_for_hits: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                Options.paranoid_file_checks: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                Options.force_consistency_checks: 1
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                Options.report_bg_io_stats: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                               Options.ttl: 2592000
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :          Options.periodic_compaction_seconds: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :  Options.preclude_last_level_data_seconds: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                       Options.enable_blob_files: false
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                           Options.min_blob_size: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                          Options.blob_file_size: 268435456
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                   Options.blob_compression_type: NoCompression
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :          Options.enable_blob_garbage_collection: false
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :      Options.blob_garbage_collection_age_cutoff: 0.250000
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Options.blob_garbage_collection_force_threshold: 1.000000
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :          Options.blob_compaction_readahead_size: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                Options.blob_file_starting_level: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Options.experimental_mempurge_threshold: 0.000000
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : [/db_impl/db_impl.cc:2861] Created column family [hudi_bootstrap_basefile__user_hive_warehouse_stock_ticks_mor] (ID 4)
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : [/column_family.cc:618] --------------- Options for column family [hudi_partitions__user_hive_warehouse_stock_ticks_mor]:

24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :               Options.comparator: leveldb.BytewiseComparator
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :           Options.merge_operator: None
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :        Options.compaction_filter: None
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :        Options.compaction_filter_factory: None
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :  Options.sst_partitioner_factory: None
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :         Options.memtable_factory: SkipListFactory
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :            Options.table_factory: BlockBasedTable
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :            table_factory options:   flush_block_policy_factory: FlushBlockBySizePolicyFactory (0x7f9cc011c170)
  cache_index_and_filter_blocks: 0
  cache_index_and_filter_blocks_with_high_priority: 1
  pin_l0_filter_and_index_blocks_in_cache: 0
  pin_top_level_index_and_filter: 1
  index_type: 0
  data_block_index_type: 0
  index_shortening: 1
  data_block_hash_table_util_ratio: 0.750000
  checksum: 1
  no_block_cache: 0
  block_cache: 0x7f9cc011c010
  block_cache_name: LRUCache
  block_cache_options:
    capacity : 8388608
    num_shard_bits : 4
    strict_capacity_limit : 0
    memory_allocator : None
    high_pri_pool_ratio: 0.000
  block_cache_compressed: (nil)
  persistent_cache: (nil)
  block_size: 4096
  block_size_deviation: 10
  block_restart_interval: 16
  index_block_restart_interval: 1
  metadata_block_size: 4096
  partition_filters: 0
  use_delta_encoding: 1
  filter_policy: nullptr
  whole_key_filtering: 1
  verify_compression: 0
  read_amp_bytes_per_bit: 0
  format_version: 5
  enable_index_compression: 1
  block_align: 0
  max_auto_readahead_size: 262144
  prepopulate_block_cache: 0
  initial_auto_readahead_size: 8192

24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :        Options.write_buffer_size: 67108864
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :  Options.max_write_buffer_number: 2
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :          Options.compression: Snappy
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression: Disabled
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :       Options.prefix_extractor: nullptr
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :   Options.memtable_insert_with_hint_prefix_extractor: nullptr
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :             Options.num_levels: 7
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :        Options.min_write_buffer_number_to_merge: 1
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :     Options.max_write_buffer_number_to_maintain: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :     Options.max_write_buffer_size_to_maintain: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :            Options.bottommost_compression_opts.window_bits: -14
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression_opts.level: 32767
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :               Options.bottommost_compression_opts.strategy: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.max_dict_bytes: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.zstd_max_train_bytes: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.parallel_threads: 1
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression_opts.enabled: false
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.max_dict_buffer_bytes: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.use_zstd_dict_trainer: true
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :            Options.compression_opts.window_bits: -14
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                  Options.compression_opts.level: 32767
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :               Options.compression_opts.strategy: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.max_dict_bytes: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.zstd_max_train_bytes: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.use_zstd_dict_trainer: true
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.parallel_threads: 1
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                  Options.compression_opts.enabled: false
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.max_dict_buffer_bytes: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :      Options.level0_file_num_compaction_trigger: 4
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :          Options.level0_slowdown_writes_trigger: 20
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :              Options.level0_stop_writes_trigger: 36
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                   Options.target_file_size_base: 67108864
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :             Options.target_file_size_multiplier: 1
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                Options.max_bytes_for_level_base: 268435456
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Options.level_compaction_dynamic_level_bytes: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :          Options.max_bytes_for_level_multiplier: 10.000000
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[0]: 1
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[1]: 1
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[2]: 1
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[3]: 1
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[4]: 1
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[5]: 1
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[6]: 1
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :       Options.max_sequential_skip_in_iterations: 8
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                    Options.max_compaction_bytes: 1677721600
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                        Options.arena_block_size: 1048576
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :   Options.soft_pending_compaction_bytes_limit: 68719476736
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :   Options.hard_pending_compaction_bytes_limit: 274877906944
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                Options.disable_auto_compactions: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                        Options.compaction_style: kCompactionStyleLevel
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                          Options.compaction_pri: kMinOverlappingRatio
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.size_ratio: 1
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.min_merge_width: 2
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.max_merge_width: 4294967295
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.max_size_amplification_percent: 200
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.compression_size_percent: -1
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.stop_style: kCompactionStopStyleTotalSize
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_fifo.max_table_files_size: 1073741824
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_fifo.allow_compaction: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                   Options.table_properties_collectors: 
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                   Options.inplace_update_support: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                 Options.inplace_update_num_locks: 10000
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :               Options.memtable_prefix_bloom_size_ratio: 0.000000
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :               Options.memtable_whole_key_filtering: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :   Options.memtable_huge_page_size: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                           Options.bloom_locality: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                    Options.max_successive_merges: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                Options.optimize_filters_for_hits: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                Options.paranoid_file_checks: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                Options.force_consistency_checks: 1
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                Options.report_bg_io_stats: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                               Options.ttl: 2592000
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :          Options.periodic_compaction_seconds: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :  Options.preclude_last_level_data_seconds: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                       Options.enable_blob_files: false
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                           Options.min_blob_size: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                          Options.blob_file_size: 268435456
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                   Options.blob_compression_type: NoCompression
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :          Options.enable_blob_garbage_collection: false
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :      Options.blob_garbage_collection_age_cutoff: 0.250000
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Options.blob_garbage_collection_force_threshold: 1.000000
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :          Options.blob_compaction_readahead_size: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                Options.blob_file_starting_level: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Options.experimental_mempurge_threshold: 0.000000
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : [/db_impl/db_impl.cc:2861] Created column family [hudi_partitions__user_hive_warehouse_stock_ticks_mor] (ID 5)
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : [/column_family.cc:618] --------------- Options for column family [hudi_replaced_fg_user_hive_warehouse_stock_ticks_mor]:

24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :               Options.comparator: leveldb.BytewiseComparator
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :           Options.merge_operator: None
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :        Options.compaction_filter: None
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :        Options.compaction_filter_factory: None
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :  Options.sst_partitioner_factory: None
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :         Options.memtable_factory: SkipListFactory
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :            Options.table_factory: BlockBasedTable
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :            table_factory options:   flush_block_policy_factory: FlushBlockBySizePolicyFactory (0x7f9cc0134d30)
  cache_index_and_filter_blocks: 0
  cache_index_and_filter_blocks_with_high_priority: 1
  pin_l0_filter_and_index_blocks_in_cache: 0
  pin_top_level_index_and_filter: 1
  index_type: 0
  data_block_index_type: 0
  index_shortening: 1
  data_block_hash_table_util_ratio: 0.750000
  checksum: 1
  no_block_cache: 0
  block_cache: 0x7f9cc0129990
  block_cache_name: LRUCache
  block_cache_options:
    capacity : 8388608
    num_shard_bits : 4
    strict_capacity_limit : 0
    memory_allocator : None
    high_pri_pool_ratio: 0.000
  block_cache_compressed: (nil)
  persistent_cache: (nil)
  block_size: 4096
  block_size_deviation: 10
  block_restart_interval: 16
  index_block_restart_interval: 1
  metadata_block_size: 4096
  partition_filters: 0
  use_delta_encoding: 1
  filter_policy: nullptr
  whole_key_filtering: 1
  verify_compression: 0
  read_amp_bytes_per_bit: 0
  format_version: 5
  enable_index_compression: 1
  block_align: 0
  max_auto_readahead_size: 262144
  prepopulate_block_cache: 0
  initial_auto_readahead_size: 8192

24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :        Options.write_buffer_size: 67108864
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :  Options.max_write_buffer_number: 2
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :          Options.compression: Snappy
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression: Disabled
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :       Options.prefix_extractor: nullptr
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :   Options.memtable_insert_with_hint_prefix_extractor: nullptr
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :             Options.num_levels: 7
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :        Options.min_write_buffer_number_to_merge: 1
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :     Options.max_write_buffer_number_to_maintain: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :     Options.max_write_buffer_size_to_maintain: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :            Options.bottommost_compression_opts.window_bits: -14
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression_opts.level: 32767
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :               Options.bottommost_compression_opts.strategy: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.max_dict_bytes: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.zstd_max_train_bytes: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.parallel_threads: 1
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression_opts.enabled: false
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.max_dict_buffer_bytes: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.use_zstd_dict_trainer: true
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :            Options.compression_opts.window_bits: -14
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                  Options.compression_opts.level: 32767
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :               Options.compression_opts.strategy: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.max_dict_bytes: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.zstd_max_train_bytes: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.use_zstd_dict_trainer: true
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.parallel_threads: 1
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                  Options.compression_opts.enabled: false
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.max_dict_buffer_bytes: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :      Options.level0_file_num_compaction_trigger: 4
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :          Options.level0_slowdown_writes_trigger: 20
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :              Options.level0_stop_writes_trigger: 36
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                   Options.target_file_size_base: 67108864
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :             Options.target_file_size_multiplier: 1
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                Options.max_bytes_for_level_base: 268435456
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Options.level_compaction_dynamic_level_bytes: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :          Options.max_bytes_for_level_multiplier: 10.000000
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[0]: 1
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[1]: 1
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[2]: 1
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[3]: 1
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[4]: 1
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[5]: 1
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[6]: 1
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :       Options.max_sequential_skip_in_iterations: 8
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                    Options.max_compaction_bytes: 1677721600
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                        Options.arena_block_size: 1048576
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :   Options.soft_pending_compaction_bytes_limit: 68719476736
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :   Options.hard_pending_compaction_bytes_limit: 274877906944
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                Options.disable_auto_compactions: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                        Options.compaction_style: kCompactionStyleLevel
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                          Options.compaction_pri: kMinOverlappingRatio
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.size_ratio: 1
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.min_merge_width: 2
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.max_merge_width: 4294967295
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.max_size_amplification_percent: 200
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.compression_size_percent: -1
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.stop_style: kCompactionStopStyleTotalSize
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_fifo.max_table_files_size: 1073741824
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_fifo.allow_compaction: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                   Options.table_properties_collectors: 
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                   Options.inplace_update_support: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                 Options.inplace_update_num_locks: 10000
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :               Options.memtable_prefix_bloom_size_ratio: 0.000000
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :               Options.memtable_whole_key_filtering: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :   Options.memtable_huge_page_size: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                           Options.bloom_locality: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                    Options.max_successive_merges: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                Options.optimize_filters_for_hits: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                Options.paranoid_file_checks: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                Options.force_consistency_checks: 1
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                Options.report_bg_io_stats: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                               Options.ttl: 2592000
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :          Options.periodic_compaction_seconds: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :  Options.preclude_last_level_data_seconds: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                       Options.enable_blob_files: false
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                           Options.min_blob_size: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                          Options.blob_file_size: 268435456
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                   Options.blob_compression_type: NoCompression
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :          Options.enable_blob_garbage_collection: false
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :      Options.blob_garbage_collection_age_cutoff: 0.250000
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Options.blob_garbage_collection_force_threshold: 1.000000
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :          Options.blob_compaction_readahead_size: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                Options.blob_file_starting_level: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Options.experimental_mempurge_threshold: 0.000000
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : [/db_impl/db_impl.cc:2861] Created column family [hudi_replaced_fg_user_hive_warehouse_stock_ticks_mor] (ID 6)
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : [/column_family.cc:618] --------------- Options for column family [hudi_pending_clustering_fg_user_hive_warehouse_stock_ticks_mor]:

24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :               Options.comparator: leveldb.BytewiseComparator
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :           Options.merge_operator: None
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :        Options.compaction_filter: None
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :        Options.compaction_filter_factory: None
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :  Options.sst_partitioner_factory: None
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :         Options.memtable_factory: SkipListFactory
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :            Options.table_factory: BlockBasedTable
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :            table_factory options:   flush_block_policy_factory: FlushBlockBySizePolicyFactory (0x7f9cc0142750)
  cache_index_and_filter_blocks: 0
  cache_index_and_filter_blocks_with_high_priority: 1
  pin_l0_filter_and_index_blocks_in_cache: 0
  pin_top_level_index_and_filter: 1
  index_type: 0
  data_block_index_type: 0
  index_shortening: 1
  data_block_hash_table_util_ratio: 0.750000
  checksum: 1
  no_block_cache: 0
  block_cache: 0x7f9cc0137360
  block_cache_name: LRUCache
  block_cache_options:
    capacity : 8388608
    num_shard_bits : 4
    strict_capacity_limit : 0
    memory_allocator : None
    high_pri_pool_ratio: 0.000
  block_cache_compressed: (nil)
  persistent_cache: (nil)
  block_size: 4096
  block_size_deviation: 10
  block_restart_interval: 16
  index_block_restart_interval: 1
  metadata_block_size: 4096
  partition_filters: 0
  use_delta_encoding: 1
  filter_policy: nullptr
  whole_key_filtering: 1
  verify_compression: 0
  read_amp_bytes_per_bit: 0
  format_version: 5
  enable_index_compression: 1
  block_align: 0
  max_auto_readahead_size: 262144
  prepopulate_block_cache: 0
  initial_auto_readahead_size: 8192

24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :        Options.write_buffer_size: 67108864
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :  Options.max_write_buffer_number: 2
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :          Options.compression: Snappy
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression: Disabled
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :       Options.prefix_extractor: nullptr
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :   Options.memtable_insert_with_hint_prefix_extractor: nullptr
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :             Options.num_levels: 7
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :        Options.min_write_buffer_number_to_merge: 1
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :     Options.max_write_buffer_number_to_maintain: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :     Options.max_write_buffer_size_to_maintain: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :            Options.bottommost_compression_opts.window_bits: -14
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression_opts.level: 32767
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :               Options.bottommost_compression_opts.strategy: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.max_dict_bytes: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.zstd_max_train_bytes: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.parallel_threads: 1
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression_opts.enabled: false
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.max_dict_buffer_bytes: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.use_zstd_dict_trainer: true
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :            Options.compression_opts.window_bits: -14
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                  Options.compression_opts.level: 32767
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :               Options.compression_opts.strategy: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.max_dict_bytes: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.zstd_max_train_bytes: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.use_zstd_dict_trainer: true
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.parallel_threads: 1
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                  Options.compression_opts.enabled: false
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.max_dict_buffer_bytes: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :      Options.level0_file_num_compaction_trigger: 4
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :          Options.level0_slowdown_writes_trigger: 20
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :              Options.level0_stop_writes_trigger: 36
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                   Options.target_file_size_base: 67108864
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :             Options.target_file_size_multiplier: 1
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                Options.max_bytes_for_level_base: 268435456
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Options.level_compaction_dynamic_level_bytes: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :          Options.max_bytes_for_level_multiplier: 10.000000
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[0]: 1
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[1]: 1
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[2]: 1
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[3]: 1
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[4]: 1
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[5]: 1
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[6]: 1
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :       Options.max_sequential_skip_in_iterations: 8
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                    Options.max_compaction_bytes: 1677721600
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                        Options.arena_block_size: 1048576
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :   Options.soft_pending_compaction_bytes_limit: 68719476736
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :   Options.hard_pending_compaction_bytes_limit: 274877906944
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                Options.disable_auto_compactions: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                        Options.compaction_style: kCompactionStyleLevel
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                          Options.compaction_pri: kMinOverlappingRatio
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.size_ratio: 1
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.min_merge_width: 2
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.max_merge_width: 4294967295
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.max_size_amplification_percent: 200
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.compression_size_percent: -1
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.stop_style: kCompactionStopStyleTotalSize
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_fifo.max_table_files_size: 1073741824
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_fifo.allow_compaction: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                   Options.table_properties_collectors: 
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                   Options.inplace_update_support: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                 Options.inplace_update_num_locks: 10000
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :               Options.memtable_prefix_bloom_size_ratio: 0.000000
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :               Options.memtable_whole_key_filtering: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :   Options.memtable_huge_page_size: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                           Options.bloom_locality: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                    Options.max_successive_merges: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                Options.optimize_filters_for_hits: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                Options.paranoid_file_checks: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                Options.force_consistency_checks: 1
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                Options.report_bg_io_stats: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                               Options.ttl: 2592000
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :          Options.periodic_compaction_seconds: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :  Options.preclude_last_level_data_seconds: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                       Options.enable_blob_files: false
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                           Options.min_blob_size: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                          Options.blob_file_size: 268435456
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                   Options.blob_compression_type: NoCompression
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :          Options.enable_blob_garbage_collection: false
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :      Options.blob_garbage_collection_age_cutoff: 0.250000
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Options.blob_garbage_collection_force_threshold: 1.000000
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :          Options.blob_compaction_readahead_size: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB :                Options.blob_file_starting_level: 0
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : Options.experimental_mempurge_threshold: 0.000000
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : [/db_impl/db_impl.cc:2861] Created column family [hudi_pending_clustering_fg_user_hive_warehouse_stock_ticks_mor] (ID 7)
24/01/05 02:14:15 INFO view.RocksDbBasedFileSystemView: Resetting replacedFileGroups to ROCKSDB based file-system view at /tmp/hoodie_timeline_rocksdb, Total file-groups=0
24/01/05 02:14:15 INFO collection.RocksDBDAO: Prefix DELETE (query=part=) on hudi_replaced_fg_user_hive_warehouse_stock_ticks_mor
24/01/05 02:14:15 INFO view.RocksDbBasedFileSystemView: Resetting replacedFileGroups to ROCKSDB based file-system view complete
24/01/05 02:14:15 INFO view.AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
24/01/05 02:14:15 INFO view.RocksDbBasedFileSystemView: Initializing pending compaction operations. Count=0
24/01/05 02:14:15 INFO view.RocksDbBasedFileSystemView: Initializing pending Log compaction operations. Count=0
24/01/05 02:14:15 INFO view.RocksDbBasedFileSystemView: Initializing external data file mapping. Count=0
24/01/05 02:14:15 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:14:15 INFO view.RocksDbBasedFileSystemView: Resetting file groups in pending clustering to ROCKSDB based file-system view at /tmp/hoodie_timeline_rocksdb, Total file-groups=0
24/01/05 02:14:15 INFO collection.RocksDBDAO: Prefix DELETE (query=part=) on hudi_pending_clustering_fg_user_hive_warehouse_stock_ticks_mor
24/01/05 02:14:15 INFO view.RocksDbBasedFileSystemView: Resetting replacedFileGroups to ROCKSDB based file-system view complete
24/01/05 02:14:15 INFO view.RocksDbBasedFileSystemView: Created ROCKSDB based file-system view at /tmp/hoodie_timeline_rocksdb
24/01/05 02:14:15 INFO collection.RocksDBDAO: Prefix Search for (query=) on hudi_pending_compaction__user_hive_warehouse_stock_ticks_mor. Total Time Taken (msec)=0. Serialization Time taken(micro)=0, num entries=0
24/01/05 02:14:15 INFO view.RemoteHoodieTableFileSystemView: Sending request : (http://adhoc-2:43195/v1/hoodie/view/logcompactions/pending/?basepath=%2Fuser%2Fhive%2Fwarehouse%2Fstock_ticks_mor&lastinstantts=20240105021407331&timelinehash=b271aa6d866dd95e491ac1c4d3c452644fd5c7bef1f4894a286a392c43b0a50d)
24/01/05 02:14:15 INFO collection.RocksDBDAO: Prefix Search for (query=) on hudi_pending_log_compaction__user_hive_warehouse_stock_ticks_mor. Total Time Taken (msec)=0. Serialization Time taken(micro)=0, num entries=0
24/01/05 02:14:15 INFO clean.CleanPlanner: No earliest commit to retain. No need to scan partitions !!
24/01/05 02:14:15 INFO clean.CleanPlanActionExecutor: Nothing to clean here. It is already clean
24/01/05 02:14:15 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105021407331__deltacommit__COMPLETED__20240105021415726]}
24/01/05 02:14:15 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:14:15 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:14:15 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:14:15 INFO table.HoodieTableMetaClient: Loading Active commit timeline for /user/hive/warehouse/stock_ticks_mor
24/01/05 02:14:15 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105021407331__deltacommit__COMPLETED__20240105021415726]}
24/01/05 02:14:15 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:14:15 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:14:15 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:14:15 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:14:15 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:14:15 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:14:15 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105021407331__deltacommit__COMPLETED__20240105021415246]}
24/01/05 02:14:15 INFO view.AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
24/01/05 02:14:15 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:14:15 INFO view.FileSystemViewManager: Creating View Manager with storage type :REMOTE_FIRST
24/01/05 02:14:15 INFO view.FileSystemViewManager: Creating remote first table view
24/01/05 02:14:15 INFO client.BaseHoodieWriteClient: Start to archive synchronously.
24/01/05 02:14:15 INFO transaction.TransactionManager: Transaction starting for Optional.empty with latest completed transaction instant Optional.empty
24/01/05 02:14:15 INFO lock.LockManager: LockProvider org.apache.hudi.client.transaction.lock.InProcessLockProvider
24/01/05 02:14:15 INFO lock.InProcessLockProvider: Base Path /user/hive/warehouse/stock_ticks_mor, Lock Instance java.util.concurrent.locks.ReentrantReadWriteLock@7254838[Write locks = 0, Read locks = 0], Thread main, In-process lock state ACQUIRING
24/01/05 02:14:15 INFO lock.InProcessLockProvider: Base Path /user/hive/warehouse/stock_ticks_mor, Lock Instance java.util.concurrent.locks.ReentrantReadWriteLock@7254838[Write locks = 1, Read locks = 0], Thread main, In-process lock state ACQUIRED
24/01/05 02:14:15 INFO transaction.TransactionManager: Transaction started for Optional.empty with latest completed transaction instant Optional.empty
24/01/05 02:14:15 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105021407331__deltacommit__COMPLETED__20240105021415726]}
24/01/05 02:14:15 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:14:15 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:14:15 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:14:15 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:14:15 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:14:15 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:14:15 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105021407331__deltacommit__COMPLETED__20240105021415246]}
24/01/05 02:14:15 INFO view.AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
24/01/05 02:14:15 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:14:15 INFO client.HoodieTimelineArchiver: Not archiving as there is no compaction yet on the metadata table
24/01/05 02:14:15 INFO client.HoodieTimelineArchiver: No Instants to archive
24/01/05 02:14:15 INFO transaction.TransactionManager: Transaction ending with transaction owner Optional.empty
24/01/05 02:14:15 INFO lock.InProcessLockProvider: Base Path /user/hive/warehouse/stock_ticks_mor, Lock Instance java.util.concurrent.locks.ReentrantReadWriteLock@7254838[Write locks = 1, Read locks = 0], Thread main, In-process lock state RELEASING
24/01/05 02:14:15 INFO lock.InProcessLockProvider: Base Path /user/hive/warehouse/stock_ticks_mor, Lock Instance java.util.concurrent.locks.ReentrantReadWriteLock@7254838[Write locks = 0, Read locks = 0], Thread main, In-process lock state RELEASED
24/01/05 02:14:15 INFO lock.InProcessLockProvider: Base Path /user/hive/warehouse/stock_ticks_mor, Lock Instance java.util.concurrent.locks.ReentrantReadWriteLock@7254838[Write locks = 0, Read locks = 0], Thread main, In-process lock state ALREADY_RELEASED
24/01/05 02:14:15 INFO lock.LockManager: Released connection created for acquiring lock
24/01/05 02:14:15 INFO transaction.TransactionManager: Transaction ended with transaction owner Optional.empty
24/01/05 02:14:15 INFO view.FileSystemViewManager: Creating remote view for basePath /user/hive/warehouse/stock_ticks_mor. Server=adhoc-2:43195, Timeout=300
24/01/05 02:14:15 INFO view.FileSystemViewManager: Creating InMemory based view for basePath /user/hive/warehouse/stock_ticks_mor
24/01/05 02:14:15 INFO view.AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
24/01/05 02:14:15 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:14:15 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105021407331__deltacommit__COMPLETED__20240105021415726]}
24/01/05 02:14:15 INFO view.RemoteHoodieTableFileSystemView: Sending request : (http://adhoc-2:43195/v1/hoodie/view/refresh/?basepath=%2Fuser%2Fhive%2Fwarehouse%2Fstock_ticks_mor&lastinstantts=20240105021407331&timelinehash=b271aa6d866dd95e491ac1c4d3c452644fd5c7bef1f4894a286a392c43b0a50d)
24/01/05 02:14:15 INFO view.RocksDbBasedFileSystemView: Closing Rocksdb !!
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : [/db_impl/db_impl.cc:478] Shutdown: canceling all background work
24/01/05 02:14:15 INFO collection.RocksDBDAO: From Rocks DB : [/db_impl/db_impl.cc:677] Shutdown complete
24/01/05 02:14:15 INFO view.RocksDbBasedFileSystemView: Closed Rocksdb !!
24/01/05 02:14:16 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105021407331__deltacommit__COMPLETED__20240105021415726]}
24/01/05 02:14:16 INFO view.AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
24/01/05 02:14:16 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:14:16 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105021407331__deltacommit__COMPLETED__20240105021415726]}
24/01/05 02:14:16 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105021407331__deltacommit__COMPLETED__20240105021415246]}
24/01/05 02:14:16 INFO view.AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
24/01/05 02:14:16 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:14:16 INFO streamer.StreamSync: Commit 20240105021407331 successful!
24/01/05 02:14:16 INFO transaction.TransactionManager: Transaction manager closed
24/01/05 02:14:16 INFO transaction.TransactionManager: Transaction manager closed
24/01/05 02:14:16 INFO streamer.StreamSync: Shutting down embedded timeline server
24/01/05 02:14:16 INFO embedded.EmbeddedTimelineService: Closing Timeline server
24/01/05 02:14:16 INFO service.TimelineService: Closing Timeline Service
24/01/05 02:14:16 INFO javalin.Javalin: Stopping Javalin ...
24/01/05 02:14:16 INFO javalin.Javalin: Javalin has stopped
24/01/05 02:14:16 INFO service.TimelineService: Closed Timeline Service
24/01/05 02:14:16 INFO embedded.EmbeddedTimelineService: Closed Timeline server
24/01/05 02:14:16 INFO ingestion.HoodieIngestionService: Ingestion service (run-once mode) has been shut down.
24/01/05 02:14:16 INFO server.AbstractConnector: Stopped Spark@61019f59{HTTP/1.1,[http/1.1]}{0.0.0.0:8090}
24/01/05 02:14:16 INFO ui.SparkUI: Stopped Spark web UI at http://adhoc-2:8090
24/01/05 02:14:16 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
24/01/05 02:14:16 INFO memory.MemoryStore: MemoryStore cleared
24/01/05 02:14:16 INFO storage.BlockManager: BlockManager stopped
24/01/05 02:14:16 INFO storage.BlockManagerMaster: BlockManagerMaster stopped
24/01/05 02:14:16 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
24/01/05 02:14:16 INFO spark.SparkContext: Successfully stopped SparkContext
24/01/05 02:14:16 INFO util.ShutdownHookManager: Shutdown hook called
24/01/05 02:14:16 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-9b9a75cf-f642-424d-b46a-089bb7383b50
24/01/05 02:14:16 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-17eacc4f-209c-4e6d-8b63-007c3ed26ca8
+ docker exec -i adhoc-1 hadoop --config /etc/hadoop fs -ls -R '/user/hive/warehouse/stock_ticks*/20*'
drwxr-xr-x   - root supergroup          0 2024-01-05 02:13 /user/hive/warehouse/stock_ticks_mor/2018/08
drwxr-xr-x   - root supergroup          0 2024-01-05 02:14 /user/hive/warehouse/stock_ticks_mor/2018/08/31
-rw-r--r--   1 root supergroup       1786 2024-01-05 02:14 /user/hive/warehouse/stock_ticks_mor/2018/08/31/.0245deea-bfb7-4901-bd55-849b7d6fa366-0_20240105021347452.log.1_0-14-16
-rw-r--r--   1 root supergroup         96 2024-01-05 02:13 /user/hive/warehouse/stock_ticks_mor/2018/08/31/.hoodie_partition_metadata
-rw-r--r--   1 root supergroup     441503 2024-01-05 02:13 /user/hive/warehouse/stock_ticks_mor/2018/08/31/0245deea-bfb7-4901-bd55-849b7d6fa366-0_0-23-24_20240105021347452.parquet
