+ set -u
+ n=1866
+ shift
+ bash /home/alex/github/alexttx/hudi/docker/setup_demo.sh
 Container adhoc-2  Stopping
 Container spark-worker-1  Stopping
 Container trino-worker-1  Stopping
 Container adhoc-1  Stopping
 Container datanode1  Stopping
 Container spark-worker-1  Stopped
 Container spark-worker-1  Removing
 Container adhoc-2  Stopped
 Container adhoc-2  Removing
 Container datanode1  Stopped
 Container datanode1  Removing
 Container adhoc-1  Stopped
 Container adhoc-1  Removing
 Container trino-worker-1  Stopped
 Container trino-worker-1  Removing
 Container adhoc-2  Removed
 Container adhoc-1  Removed
 Container presto-coordinator-1  Stopping
 Container spark-worker-1  Removed
 Container sparkmaster  Stopping
 Container datanode1  Removed
 Container historyserver  Stopping
 Container trino-worker-1  Removed
 Container trino-coordinator-1  Stopping
 Container presto-coordinator-1  Stopped
 Container presto-coordinator-1  Removing
 Container presto-coordinator-1  Removed
 Container historyserver  Stopped
 Container historyserver  Removing
 Container historyserver  Removed
 Container trino-coordinator-1  Stopped
 Container trino-coordinator-1  Removing
 Container trino-coordinator-1  Removed
 Container sparkmaster  Stopped
 Container sparkmaster  Removing
 Container sparkmaster  Removed
 Container hiveserver  Stopping
 Container hiveserver  Stopped
 Container hiveserver  Removing
 Container hiveserver  Removed
 Container hivemetastore  Stopping
 Container hivemetastore  Stopped
 Container hivemetastore  Removing
 Container hivemetastore  Removed
 Container namenode  Stopping
 Container hive-metastore-postgresql  Stopping
 Container hive-metastore-postgresql  Stopped
 Container hive-metastore-postgresql  Removing
 Container hive-metastore-postgresql  Removed
 Container namenode  Stopped
 Container namenode  Removing
 Container namenode  Removed
 Network hudi  Removing
 Network hudi  Removed
Pulling docker demo images ...
 adhoc-2 Skipped - Image is already being pulled by adhoc-1 
 hiveserver Skipped - Image is already being pulled by hivemetastore 
 graphite Pulling 
 presto-coordinator-1 Skipped - Image is already being pulled by presto-worker-1 
 adhoc-1 Pulling 
 trino-coordinator-1 Pulling 
 sparkmaster Pulling 
 trino-worker-1 Pulling 
 namenode Pulling 
 zookeeper Pulling 
 hive-metastore-postgresql Pulling 
 datanode1 Pulling 
 historyserver Pulling 
 presto-worker-1 Pulling 
 kafka Pulling 
 hivemetastore Pulling 
 spark-worker-1 Pulling 
 hive-metastore-postgresql Pulled 
 zookeeper Pulled 
 kafka Pulled 
 graphite Pulled 
 namenode Pulled 
 hivemetastore Pulled 
 historyserver Pulled 
 spark-worker-1 Pulled 
 sparkmaster Pulled 
 adhoc-1 Pulled 
 trino-coordinator-1 Pulled 
 datanode1 Pulled 
 trino-worker-1 Pulled 
 presto-worker-1 Pulled 
 Network hudi  Creating
 Network hudi  Created
 Container namenode  Creating
 Container hive-metastore-postgresql  Creating
 Container zookeeper  Creating
 Container kafkabroker  Creating
 Container graphite  Creating
 Container zookeeper  Created
 Container kafkabroker  Created
 Container hive-metastore-postgresql  Created
 Container graphite  Created
 Container namenode  Created
 Container hivemetastore  Creating
 Container historyserver  Creating
 Container hivemetastore  Created
 Container hiveserver  Creating
 Container presto-coordinator-1  Creating
 Container trino-coordinator-1  Creating
 Container historyserver  Created
 Container datanode1  Creating
 Container presto-coordinator-1  Created
 Container hiveserver  Created
 Container presto-worker-1  Creating
 Container sparkmaster  Creating
 Container datanode1  Created
 Container trino-coordinator-1  Created
 Container trino-worker-1  Creating
 Container presto-worker-1  Created
 Container sparkmaster  Created
 Container adhoc-1  Creating
 Container adhoc-2  Creating
 Container spark-worker-1  Creating
 Container trino-worker-1  Created
 Container spark-worker-1  Created
 Container adhoc-1  Created
 Container adhoc-2  Created
 Container zookeeper  Starting
 Container graphite  Starting
 Container namenode  Starting
 Container hive-metastore-postgresql  Starting
 Container kafkabroker  Starting
 Container zookeeper  Started
 Container kafkabroker  Started
 Container graphite  Started
 Container hive-metastore-postgresql  Started
 Container namenode  Started
 Container hivemetastore  Starting
 Container historyserver  Starting
 Container hivemetastore  Started
 Container trino-coordinator-1  Starting
 Container hiveserver  Starting
 Container presto-coordinator-1  Starting
 Container historyserver  Started
 Container datanode1  Starting
 Container presto-coordinator-1  Started
 Container hiveserver  Started
 Container presto-worker-1  Starting
 Container sparkmaster  Starting
 Container trino-coordinator-1  Started
 Container trino-worker-1  Starting
 Container datanode1  Started
 Container presto-worker-1  Started
 Container trino-worker-1  Started
 Container sparkmaster  Started
 Container spark-worker-1  Starting
 Container adhoc-1  Starting
 Container adhoc-2  Starting
 Container adhoc-1  Started
 Container adhoc-2  Started
 Container spark-worker-1  Started
Copying spark default config and setting up configs
Copying spark default config and setting up configs
+ cat /home/alex/github/alexttx/hudi/docker/demo/data/batch_1.json
+ head -n1866
+ kcat -b kafkabroker -t stock_ticks -P
+ kcat -b kafkabroker -L -J
+ jq -C .
[1;39m{
  [0m[34;1m"originating_broker"[0m[1;39m: [0m[1;39m{
    [0m[34;1m"id"[0m[1;39m: [0m[0;39m1001[0m[1;39m,
    [0m[34;1m"name"[0m[1;39m: [0m[0;32m"kafkabroker:9092/1001"[0m[1;39m
  [1;39m}[0m[1;39m,
  [0m[34;1m"query"[0m[1;39m: [0m[1;39m{
    [0m[34;1m"topic"[0m[1;39m: [0m[0;32m"*"[0m[1;39m
  [1;39m}[0m[1;39m,
  [0m[34;1m"controllerid"[0m[1;39m: [0m[0;39m1001[0m[1;39m,
  [0m[34;1m"brokers"[0m[1;39m: [0m[1;39m[
    [1;39m{
      [0m[34;1m"id"[0m[1;39m: [0m[0;39m1001[0m[1;39m,
      [0m[34;1m"name"[0m[1;39m: [0m[0;32m"kafkabroker:9092"[0m[1;39m
    [1;39m}[0m[1;39m
  [1;39m][0m[1;39m,
  [0m[34;1m"topics"[0m[1;39m: [0m[1;39m[
    [1;39m{
      [0m[34;1m"topic"[0m[1;39m: [0m[0;32m"stock_ticks"[0m[1;39m,
      [0m[34;1m"partitions"[0m[1;39m: [0m[1;39m[
        [1;39m{
          [0m[34;1m"partition"[0m[1;39m: [0m[0;39m0[0m[1;39m,
          [0m[34;1m"leader"[0m[1;39m: [0m[0;39m1001[0m[1;39m,
          [0m[34;1m"replicas"[0m[1;39m: [0m[1;39m[
            [1;39m{
              [0m[34;1m"id"[0m[1;39m: [0m[0;39m1001[0m[1;39m
            [1;39m}[0m[1;39m
          [1;39m][0m[1;39m,
          [0m[34;1m"isrs"[0m[1;39m: [0m[1;39m[
            [1;39m{
              [0m[34;1m"id"[0m[1;39m: [0m[0;39m1001[0m[1;39m
            [1;39m}[0m[1;39m
          [1;39m][0m[1;39m
        [1;39m}[0m[1;39m
      [1;39m][0m[1;39m
    [1;39m}[0m[1;39m
  [1;39m][0m[1;39m
[1;39m}[0m
+ docker exec -i adhoc-1 /bin/bash -x
+ spark-submit --class org.apache.hudi.utilities.streamer.HoodieStreamer /var/hoodie/ws/docker/hoodie/hadoop/hive_base/target/hoodie-utilities.jar --table-type MERGE_ON_READ --source-class org.apache.hudi.utilities.sources.JsonKafkaSource --source-ordering-field ts --target-base-path /user/hive/warehouse/stock_ticks_mor --target-table stock_ticks_mor --props /var/demo/config/kafka-source.properties --schemaprovider-class org.apache.hudi.utilities.schema.FilebasedSchemaProvider --disable-compaction
24/01/05 02:43:40 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
24/01/05 02:43:40 WARN streamer.SchedulerConfGenerator: Job Scheduling Configs will not be in effect as spark.scheduler.mode is not set to FAIR at instantiation time. Continuing without scheduling configs
24/01/05 02:43:40 INFO spark.SparkContext: Running Spark version 2.4.4
24/01/05 02:43:40 INFO spark.SparkContext: Submitted application: streamer-stock_ticks_mor
24/01/05 02:43:40 INFO spark.SecurityManager: Changing view acls to: root
24/01/05 02:43:40 INFO spark.SecurityManager: Changing modify acls to: root
24/01/05 02:43:40 INFO spark.SecurityManager: Changing view acls groups to: 
24/01/05 02:43:40 INFO spark.SecurityManager: Changing modify acls groups to: 
24/01/05 02:43:40 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
24/01/05 02:43:40 INFO Configuration.deprecation: mapred.output.compression.codec is deprecated. Instead, use mapreduce.output.fileoutputformat.compress.codec
24/01/05 02:43:40 INFO Configuration.deprecation: mapred.output.compress is deprecated. Instead, use mapreduce.output.fileoutputformat.compress
24/01/05 02:43:40 INFO Configuration.deprecation: mapred.output.compression.type is deprecated. Instead, use mapreduce.output.fileoutputformat.compress.type
24/01/05 02:43:41 INFO util.Utils: Successfully started service 'sparkDriver' on port 37679.
24/01/05 02:43:41 INFO spark.SparkEnv: Registering MapOutputTracker
24/01/05 02:43:41 INFO spark.SparkEnv: Registering BlockManagerMaster
24/01/05 02:43:41 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
24/01/05 02:43:41 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
24/01/05 02:43:41 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-24aa21e1-07d1-4dae-a7a2-b7efb7eb5fab
24/01/05 02:43:41 INFO memory.MemoryStore: MemoryStore started with capacity 366.3 MB
24/01/05 02:43:41 INFO spark.SparkEnv: Registering OutputCommitCoordinator
24/01/05 02:43:41 INFO util.log: Logging initialized @1607ms
24/01/05 02:43:41 INFO server.Server: jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
24/01/05 02:43:41 INFO server.Server: Started @1684ms
24/01/05 02:43:41 INFO server.AbstractConnector: Started ServerConnector@61019f59{HTTP/1.1,[http/1.1]}{0.0.0.0:8090}
24/01/05 02:43:41 INFO util.Utils: Successfully started service 'SparkUI' on port 8090.
24/01/05 02:43:41 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3e1162e7{/jobs,null,AVAILABLE,@Spark}
24/01/05 02:43:41 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@17f460bb{/jobs/json,null,AVAILABLE,@Spark}
24/01/05 02:43:41 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@64a1923a{/jobs/job,null,AVAILABLE,@Spark}
24/01/05 02:43:41 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@18ca3c62{/jobs/job/json,null,AVAILABLE,@Spark}
24/01/05 02:43:41 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2c0f7678{/stages,null,AVAILABLE,@Spark}
24/01/05 02:43:41 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@44d70181{/stages/json,null,AVAILABLE,@Spark}
24/01/05 02:43:41 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6aa648b9{/stages/stage,null,AVAILABLE,@Spark}
24/01/05 02:43:41 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@88a8218{/stages/stage/json,null,AVAILABLE,@Spark}
24/01/05 02:43:41 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@50b1f030{/stages/pool,null,AVAILABLE,@Spark}
24/01/05 02:43:41 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4163f1cd{/stages/pool/json,null,AVAILABLE,@Spark}
24/01/05 02:43:41 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5fa05212{/storage,null,AVAILABLE,@Spark}
24/01/05 02:43:41 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3e681bc{/storage/json,null,AVAILABLE,@Spark}
24/01/05 02:43:41 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5c09d180{/storage/rdd,null,AVAILABLE,@Spark}
24/01/05 02:43:41 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@23aae55{/storage/rdd/json,null,AVAILABLE,@Spark}
24/01/05 02:43:41 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5f574cc2{/environment,null,AVAILABLE,@Spark}
24/01/05 02:43:41 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@680bddf5{/environment/json,null,AVAILABLE,@Spark}
24/01/05 02:43:41 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7a9c84a5{/executors,null,AVAILABLE,@Spark}
24/01/05 02:43:41 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2d83c5a5{/executors/json,null,AVAILABLE,@Spark}
24/01/05 02:43:41 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@48d7ad8b{/executors/threadDump,null,AVAILABLE,@Spark}
24/01/05 02:43:41 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7e053511{/executors/threadDump/json,null,AVAILABLE,@Spark}
24/01/05 02:43:41 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@60222fd8{/static,null,AVAILABLE,@Spark}
24/01/05 02:43:41 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@37ff4054{/,null,AVAILABLE,@Spark}
24/01/05 02:43:41 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@894858{/api,null,AVAILABLE,@Spark}
24/01/05 02:43:41 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@74cf8b28{/jobs/job/kill,null,AVAILABLE,@Spark}
24/01/05 02:43:41 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@36c54a56{/stages/stage/kill,null,AVAILABLE,@Spark}
24/01/05 02:43:41 INFO ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://adhoc-1:8090
24/01/05 02:43:41 INFO spark.SparkContext: Added JAR file:/var/hoodie/ws/docker/hoodie/hadoop/hive_base/target/hoodie-utilities.jar at spark://adhoc-1:37679/jars/hoodie-utilities.jar with timestamp 1704422621377
24/01/05 02:43:41 INFO executor.Executor: Starting executor ID driver on host localhost
24/01/05 02:43:41 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43055.
24/01/05 02:43:41 INFO netty.NettyBlockTransferService: Server created on adhoc-1:43055
24/01/05 02:43:41 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
24/01/05 02:43:41 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, adhoc-1, 43055, None)
24/01/05 02:43:41 INFO storage.BlockManagerMasterEndpoint: Registering block manager adhoc-1:43055 with 366.3 MB RAM, BlockManagerId(driver, adhoc-1, 43055, None)
24/01/05 02:43:41 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, adhoc-1, 43055, None)
24/01/05 02:43:41 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, adhoc-1, 43055, None)
24/01/05 02:43:41 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@62e6a3ec{/metrics/json,null,AVAILABLE,@Spark}
24/01/05 02:43:42 WARN config.DFSPropertiesConfiguration: Cannot find HUDI_CONF_DIR, please set it as the dir of hudi-defaults.conf
24/01/05 02:43:42 WARN config.DFSPropertiesConfiguration: Properties file file:/etc/hudi/conf/hudi-defaults.conf not found. Ignoring to load props file
24/01/05 02:43:42 INFO internal.SharedState: loading hive config file: file:/opt/hadoop-2.8.4/etc/hadoop/hive-site.xml
24/01/05 02:43:42 INFO internal.SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/opt/spark-warehouse').
24/01/05 02:43:42 INFO internal.SharedState: Warehouse path is 'file:/opt/spark-warehouse'.
24/01/05 02:43:42 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@423c5404{/SQL,null,AVAILABLE,@Spark}
24/01/05 02:43:42 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5853ca50{/SQL/json,null,AVAILABLE,@Spark}
24/01/05 02:43:42 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3c79088e{/SQL/execution,null,AVAILABLE,@Spark}
24/01/05 02:43:42 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4a37191a{/SQL/execution/json,null,AVAILABLE,@Spark}
24/01/05 02:43:42 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@24eb65e3{/static/sql,null,AVAILABLE,@Spark}
24/01/05 02:43:42 INFO state.StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
24/01/05 02:43:42 WARN sql.SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
24/01/05 02:43:42 INFO streamer.HoodieStreamer: Creating Hudi Streamer with configs:
auto.offset.reset: earliest
bootstrap.servers: kafkabroker:9092
hoodie.auto.adjust.lock.configs: true
hoodie.bulkinsert.shuffle.parallelism: 2
hoodie.compact.inline: false
hoodie.datasource.write.partitionpath.field: date
hoodie.datasource.write.reconcile.schema: false
hoodie.datasource.write.recordkey.field: key
hoodie.delete.shuffle.parallelism: 2
hoodie.embed.timeline.server: true
hoodie.filesystem.view.type: EMBEDDED_KV_STORE
hoodie.insert.shuffle.parallelism: 2
hoodie.streamer.schemaprovider.source.schema.file: /var/demo/config/schema.avsc
hoodie.streamer.schemaprovider.target.schema.file: /var/demo/config/schema.avsc
hoodie.streamer.source.kafka.topic: stock_ticks
hoodie.table.type: MERGE_ON_READ
hoodie.upsert.shuffle.parallelism: 2

24/01/05 02:43:42 INFO fs.FSUtils: Resolving file /var/demo/config/schema.avscto be a remote file.
24/01/05 02:43:42 INFO factory.HoodieSparkKeyGeneratorFactory: The value of hoodie.datasource.write.keygenerator.type is empty; inferred to be SIMPLE
24/01/05 02:43:42 INFO factory.HoodieSparkKeyGeneratorFactory: The value of hoodie.datasource.write.keygenerator.type is empty; inferred to be SIMPLE
24/01/05 02:43:42 INFO table.HoodieTableMetaClient: Initializing /user/hive/warehouse/stock_ticks_mor as hoodie table /user/hive/warehouse/stock_ticks_mor
24/01/05 02:43:43 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:43:43 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:43:43 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:43:43 INFO table.HoodieTableMetaClient: Finished initializing Table of type MERGE_ON_READ from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:43:43 INFO config.HoodieWriteConfig: Automatically set hoodie.write.lock.provider=org.apache.hudi.client.transaction.lock.InProcessLockProvider since user has not set the lock provider for single writer with async table services
24/01/05 02:43:43 INFO config.HoodieWriteConfig: Automatically set hoodie.write.lock.provider=org.apache.hudi.client.transaction.lock.InProcessLockProvider since user has not set the lock provider for single writer with async table services
24/01/05 02:43:43 INFO ingestion.HoodieIngestionService: Ingestion service starts running in run-once mode
24/01/05 02:43:43 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:43:43 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:43:43 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:43:43 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Optional.empty
24/01/05 02:43:43 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:43:43 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:43:43 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:43:43 INFO streamer.StreamSync: Checkpoint to resume from : Optional.empty
24/01/05 02:43:43 INFO consumer.ConsumerConfig: ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [kafkabroker:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

24/01/05 02:43:43 INFO utils.AppInfoParser: Kafka version : 2.0.0
24/01/05 02:43:43 INFO utils.AppInfoParser: Kafka commitId : 3402a8361b734732
24/01/05 02:43:43 INFO clients.Metadata: Cluster ID: XFgecpgeR0meVK58zP5FxA
24/01/05 02:43:43 INFO helpers.KafkaOffsetGen: SourceLimit not configured, set numEvents to default value : 5000000
24/01/05 02:43:43 INFO helpers.KafkaOffsetGen: getNextOffsetRanges set config hoodie.streamer.source.kafka.minPartitions to 0
24/01/05 02:43:43 INFO sources.KafkaSource: About to read 1866 from Kafka for topic :stock_ticks
24/01/05 02:43:43 WARN kafka010.KafkaUtils: overriding enable.auto.commit to false for executor
24/01/05 02:43:43 WARN kafka010.KafkaUtils: overriding auto.offset.reset to none for executor
24/01/05 02:43:43 ERROR kafka010.KafkaUtils: group.id is null, you should probably set it
24/01/05 02:43:43 WARN kafka010.KafkaUtils: overriding executor group.id to spark-executor-null
24/01/05 02:43:43 WARN kafka010.KafkaUtils: overriding receive.buffer.bytes to 65536 see KAFKA-3135
24/01/05 02:43:43 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Optional.empty
24/01/05 02:43:43 INFO factory.HoodieSparkKeyGeneratorFactory: The value of hoodie.datasource.write.keygenerator.type is empty; inferred to be SIMPLE
24/01/05 02:43:43 INFO streamer.StreamSync: Setting up new Hoodie Write Client
24/01/05 02:43:43 INFO config.HoodieWriteConfig: Automatically set hoodie.write.lock.provider=org.apache.hudi.client.transaction.lock.InProcessLockProvider since user has not set the lock provider for single writer with async table services
24/01/05 02:43:43 INFO embedded.EmbeddedTimelineService: Overriding hostIp to (adhoc-1) found in spark-conf. It was null
24/01/05 02:43:43 INFO view.FileSystemViewManager: Creating View Manager with storage type :EMBEDDED_KV_STORE
24/01/05 02:43:43 INFO view.FileSystemViewManager: Creating embedded rocks-db based Table View
24/01/05 02:43:43 INFO util.log: Logging initialized @4239ms to org.apache.hudi.org.eclipse.jetty.util.log.Slf4jLog
24/01/05 02:43:44 INFO javalin.Javalin: 
       __                      __ _            __ __
      / /____ _ _   __ ____ _ / /(_)____      / // /
 __  / // __ `/| | / // __ `// // // __ \    / // /_
/ /_/ // /_/ / | |/ // /_/ // // // / / /   /__  __/
\____/ \__,_/  |___/ \__,_//_//_//_/ /_/      /_/

          https://javalin.io/documentation

24/01/05 02:43:44 INFO javalin.Javalin: Starting Javalin ...
24/01/05 02:43:44 INFO javalin.Javalin: You are running Javalin 4.6.7 (released October 24, 2022. Your Javalin version is 437 days old. Consider checking for a newer version.).
24/01/05 02:43:44 INFO server.Server: jetty-9.4.48.v20220622; built: 2022-06-21T20:42:25.880Z; git: 6b67c5719d1f4371b33655ff2d047d24e171e49a; jvm 1.8.0_212-b04
24/01/05 02:43:44 INFO server.Server: Started @4627ms
24/01/05 02:43:44 INFO javalin.Javalin: Listening on http://localhost:37525/
24/01/05 02:43:44 INFO javalin.Javalin: Javalin started in 159ms \o/
24/01/05 02:43:44 INFO service.TimelineService: Starting Timeline server on port :37525
24/01/05 02:43:44 INFO embedded.EmbeddedTimelineService: Started embedded timeline server at adhoc-1:37525
24/01/05 02:43:44 INFO client.BaseHoodieClient: Timeline Server already running. Not restarting the service
24/01/05 02:43:44 INFO client.BaseHoodieClient: Timeline Server already running. Not restarting the service
24/01/05 02:43:44 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:43:44 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:43:44 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:43:44 INFO table.HoodieTableMetaClient: Loading Active commit timeline for /user/hive/warehouse/stock_ticks_mor
24/01/05 02:43:44 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Optional.empty
24/01/05 02:43:44 INFO util.CleanerUtils: Cleaned failed attempts if any
24/01/05 02:43:44 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:43:44 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:43:44 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:43:44 INFO table.HoodieTableMetaClient: Loading Active commit timeline for /user/hive/warehouse/stock_ticks_mor
24/01/05 02:43:44 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Optional.empty
24/01/05 02:43:44 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:43:44 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:43:44 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:43:44 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:43:44 INFO view.FileSystemViewManager: Creating View Manager with storage type :REMOTE_FIRST
24/01/05 02:43:44 INFO view.FileSystemViewManager: Creating remote first table view
24/01/05 02:43:44 INFO client.BaseHoodieWriteClient: Generate a new instant time: 20240105024343265 action: deltacommit
24/01/05 02:43:44 INFO timeline.HoodieActiveTimeline: Creating a new instant [==>20240105024343265__deltacommit__REQUESTED]
24/01/05 02:43:44 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:43:44 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:43:44 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:43:44 INFO table.HoodieTableMetaClient: Loading Active commit timeline for /user/hive/warehouse/stock_ticks_mor
24/01/05 02:43:44 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[==>20240105024343265__deltacommit__REQUESTED__20240105024344334]}
24/01/05 02:43:44 INFO transaction.TransactionManager: Transaction starting for Option{val=[==>20240105024343265__deltacommit__INFLIGHT]} with latest completed transaction instant Optional.empty
24/01/05 02:43:44 INFO lock.LockManager: LockProvider org.apache.hudi.client.transaction.lock.InProcessLockProvider
24/01/05 02:43:44 INFO lock.InProcessLockProvider: Base Path /user/hive/warehouse/stock_ticks_mor, Lock Instance java.util.concurrent.locks.ReentrantReadWriteLock@592a1882[Write locks = 0, Read locks = 0], Thread main, In-process lock state ACQUIRING
24/01/05 02:43:44 INFO lock.InProcessLockProvider: Base Path /user/hive/warehouse/stock_ticks_mor, Lock Instance java.util.concurrent.locks.ReentrantReadWriteLock@592a1882[Write locks = 1, Read locks = 0], Thread main, In-process lock state ACQUIRED
24/01/05 02:43:44 INFO transaction.TransactionManager: Transaction started for Option{val=[==>20240105024343265__deltacommit__INFLIGHT]} with latest completed transaction instant Optional.empty
24/01/05 02:43:44 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:43:44 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:43:44 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:43:44 INFO metadata.HoodieBackedTableMetadataWriter: Async metadata indexing disabled and following partitions already initialized: []
24/01/05 02:43:44 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[==>20240105024343265__deltacommit__REQUESTED__20240105024344334]}
24/01/05 02:43:44 INFO table.HoodieTableMetaClient: Initializing /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata as hoodie table /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:43:44 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:43:44 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:43:44 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:43:44 INFO table.HoodieTableMetaClient: Finished initializing Table of type MERGE_ON_READ from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:43:44 INFO spark.SparkContext: Starting job: collect at HoodieSparkEngineContext.java:116
24/01/05 02:43:44 INFO scheduler.DAGScheduler: Got job 0 (collect at HoodieSparkEngineContext.java:116) with 1 output partitions
24/01/05 02:43:44 INFO scheduler.DAGScheduler: Final stage: ResultStage 0 (collect at HoodieSparkEngineContext.java:116)
24/01/05 02:43:44 INFO scheduler.DAGScheduler: Parents of final stage: List()
24/01/05 02:43:44 INFO scheduler.DAGScheduler: Missing parents: List()
24/01/05 02:43:44 INFO scheduler.DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[8] at map at HoodieSparkEngineContext.java:116), which has no missing parents
24/01/05 02:43:44 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 72.2 KB, free 366.2 MB)
24/01/05 02:43:44 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 26.3 KB, free 366.2 MB)
24/01/05 02:43:44 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on adhoc-1:43055 (size: 26.3 KB, free: 366.3 MB)
24/01/05 02:43:44 INFO spark.SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1161
24/01/05 02:43:44 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[8] at map at HoodieSparkEngineContext.java:116) (first 15 tasks are for partitions Vector(0))
24/01/05 02:43:44 INFO scheduler.TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
24/01/05 02:43:44 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7842 bytes)
24/01/05 02:43:44 INFO executor.Executor: Running task 0.0 in stage 0.0 (TID 0)
24/01/05 02:43:44 INFO executor.Executor: Fetching spark://adhoc-1:37679/jars/hoodie-utilities.jar with timestamp 1704422621377
24/01/05 02:43:44 INFO client.TransportClientFactory: Successfully created connection to adhoc-1/172.31.0.16:37679 after 28 ms (0 ms spent in bootstraps)
24/01/05 02:43:44 INFO util.Utils: Fetching spark://adhoc-1:37679/jars/hoodie-utilities.jar to /tmp/spark-f001e7e0-823a-4ddc-a6da-c40036711c21/userFiles-baa53d07-3a29-4805-a477-59c3cfcfe940/fetchFileTemp8726144252042235058.tmp
24/01/05 02:43:45 INFO executor.Executor: Adding file:/tmp/spark-f001e7e0-823a-4ddc-a6da-c40036711c21/userFiles-baa53d07-3a29-4805-a477-59c3cfcfe940/hoodie-utilities.jar to class loader
24/01/05 02:43:45 INFO executor.Executor: Finished task 0.0 in stage 0.0 (TID 0). 833 bytes result sent to driver
24/01/05 02:43:45 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 613 ms on localhost (executor driver) (1/1)
24/01/05 02:43:45 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
24/01/05 02:43:45 INFO scheduler.DAGScheduler: ResultStage 0 (collect at HoodieSparkEngineContext.java:116) finished in 0.901 s
24/01/05 02:43:45 INFO scheduler.DAGScheduler: Job 0 finished: collect at HoodieSparkEngineContext.java:116, took 0.937476 s
24/01/05 02:43:45 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Optional.empty
24/01/05 02:43:45 INFO metadata.HoodieBackedTableMetadataWriter: Initializing MDT partition FILES at instant 00000000000000010
24/01/05 02:43:45 INFO metadata.HoodieBackedTableMetadataWriter: Committing total 0 partitions and 0 files to metadata
24/01/05 02:43:45 INFO spark.SparkContext: Starting job: count at HoodieJavaRDD.java:115
24/01/05 02:43:45 INFO scheduler.DAGScheduler: Got job 1 (count at HoodieJavaRDD.java:115) with 1 output partitions
24/01/05 02:43:45 INFO scheduler.DAGScheduler: Final stage: ResultStage 1 (count at HoodieJavaRDD.java:115)
24/01/05 02:43:45 INFO scheduler.DAGScheduler: Parents of final stage: List()
24/01/05 02:43:45 INFO scheduler.DAGScheduler: Missing parents: List()
24/01/05 02:43:45 INFO scheduler.DAGScheduler: Submitting ResultStage 1 (ParallelCollectionRDD[9] at parallelize at HoodieSparkEngineContext.java:111), which has no missing parents
24/01/05 02:43:45 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 1512.0 B, free 366.2 MB)
24/01/05 02:43:45 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 1046.0 B, free 366.2 MB)
24/01/05 02:43:45 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on adhoc-1:43055 (size: 1046.0 B, free: 366.3 MB)
24/01/05 02:43:45 INFO spark.SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1161
24/01/05 02:43:45 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (ParallelCollectionRDD[9] at parallelize at HoodieSparkEngineContext.java:111) (first 15 tasks are for partitions Vector(0))
24/01/05 02:43:45 INFO scheduler.TaskSchedulerImpl: Adding task set 1.0 with 1 tasks
24/01/05 02:43:45 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 7801 bytes)
24/01/05 02:43:45 INFO executor.Executor: Running task 0.0 in stage 1.0 (TID 1)
24/01/05 02:43:45 INFO executor.Executor: Finished task 0.0 in stage 1.0 (TID 1). 666 bytes result sent to driver
24/01/05 02:43:45 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 28 ms on localhost (executor driver) (1/1)
24/01/05 02:43:45 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
24/01/05 02:43:45 INFO scheduler.DAGScheduler: ResultStage 1 (count at HoodieJavaRDD.java:115) finished in 0.039 s
24/01/05 02:43:45 INFO scheduler.DAGScheduler: Job 1 finished: count at HoodieJavaRDD.java:115, took 0.042419 s
24/01/05 02:43:45 INFO metadata.HoodieBackedTableMetadataWriter: Initializing FILES index with 1 mappings and 1 file groups.
24/01/05 02:43:45 INFO metadata.HoodieBackedTableMetadataWriter: Creating 1 file groups for partition files with base fileId files- at instant time 00000000000000010
24/01/05 02:43:45 INFO spark.SparkContext: Starting job: foreach at HoodieSparkEngineContext.java:155
24/01/05 02:43:45 INFO scheduler.DAGScheduler: Got job 2 (foreach at HoodieSparkEngineContext.java:155) with 1 output partitions
24/01/05 02:43:45 INFO scheduler.DAGScheduler: Final stage: ResultStage 2 (foreach at HoodieSparkEngineContext.java:155)
24/01/05 02:43:45 INFO scheduler.DAGScheduler: Parents of final stage: List()
24/01/05 02:43:45 INFO scheduler.DAGScheduler: Missing parents: List()
24/01/05 02:43:45 INFO scheduler.DAGScheduler: Submitting ResultStage 2 (ParallelCollectionRDD[10] at parallelize at HoodieSparkEngineContext.java:155), which has no missing parents
24/01/05 02:43:45 INFO memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 296.9 KB, free 365.9 MB)
24/01/05 02:43:45 INFO memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 100.2 KB, free 365.8 MB)
24/01/05 02:43:45 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on adhoc-1:43055 (size: 100.2 KB, free: 366.2 MB)
24/01/05 02:43:45 INFO spark.SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1161
24/01/05 02:43:45 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (ParallelCollectionRDD[10] at parallelize at HoodieSparkEngineContext.java:155) (first 15 tasks are for partitions Vector(0))
24/01/05 02:43:45 INFO scheduler.TaskSchedulerImpl: Adding task set 2.0 with 1 tasks
24/01/05 02:43:45 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, PROCESS_LOCAL, 7737 bytes)
24/01/05 02:43:45 INFO executor.Executor: Running task 0.0 in stage 2.0 (TID 2)
24/01/05 02:43:45 INFO log.HoodieLogFormat$WriterBuilder: Building HoodieLogFormat Writer
24/01/05 02:43:45 INFO log.HoodieLogFormat$WriterBuilder: HoodieLogFile on path /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/files/.files-0000-0_00000000000000010.log.1_0-0-0
24/01/05 02:43:45 INFO log.HoodieLogFormatWriter: HoodieLogFile{pathStr='/user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/files/.files-0000-0_00000000000000010.log.1_0-0-0', fileLen=0} does not exist. Create a new file
24/01/05 02:43:45 INFO storage.BlockManagerInfo: Removed broadcast_0_piece0 on adhoc-1:43055 in memory (size: 26.3 KB, free: 366.2 MB)
24/01/05 02:43:45 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on adhoc-1:43055 in memory (size: 1046.0 B, free: 366.2 MB)
24/01/05 02:43:45 INFO executor.Executor: Finished task 0.0 in stage 2.0 (TID 2). 709 bytes result sent to driver
24/01/05 02:43:45 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 155 ms on localhost (executor driver) (1/1)
24/01/05 02:43:45 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
24/01/05 02:43:45 INFO scheduler.DAGScheduler: ResultStage 2 (foreach at HoodieSparkEngineContext.java:155) finished in 0.205 s
24/01/05 02:43:45 INFO scheduler.DAGScheduler: Job 2 finished: foreach at HoodieSparkEngineContext.java:155, took 0.209056 s
24/01/05 02:43:45 INFO view.AbstractTableFileSystemView: Took 2 ms to read  0 instants, 0 replaced file groups
24/01/05 02:43:45 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:43:45 INFO metadata.HoodieTableMetadataUtil: Loading latest file slices for metadata table partition files
24/01/05 02:43:45 INFO view.AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
24/01/05 02:43:45 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:43:45 INFO view.AbstractTableFileSystemView: Building file system view for partition (files)
24/01/05 02:43:45 INFO client.BaseHoodieClient: Embedded Timeline Server is disabled. Not starting timeline service
24/01/05 02:43:45 INFO client.BaseHoodieClient: Embedded Timeline Server is disabled. Not starting timeline service
24/01/05 02:43:45 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:43:45 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:43:45 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:43:45 INFO table.HoodieTableMetaClient: Loading Active commit timeline for /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:43:45 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Optional.empty
24/01/05 02:43:45 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:43:45 INFO view.FileSystemViewManager: Creating View Manager with storage type :MEMORY
24/01/05 02:43:45 INFO view.FileSystemViewManager: Creating in-memory based Table View
24/01/05 02:43:45 INFO metadata.HoodieBackedTableMetadataWriter: New commit at 00000000000000010 being applied to MDT.
24/01/05 02:43:45 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:43:45 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:43:45 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:43:45 INFO table.HoodieTableMetaClient: Loading Active commit timeline for /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:43:45 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Optional.empty
24/01/05 02:43:45 INFO util.CleanerUtils: Cleaned failed attempts if any
24/01/05 02:43:45 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:43:45 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:43:45 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:43:45 INFO table.HoodieTableMetaClient: Loading Active commit timeline for /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:43:45 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Optional.empty
24/01/05 02:43:45 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:43:45 INFO view.FileSystemViewManager: Creating View Manager with storage type :MEMORY
24/01/05 02:43:45 INFO view.FileSystemViewManager: Creating in-memory based Table View
24/01/05 02:43:45 INFO client.BaseHoodieWriteClient: Generate a new instant time: 00000000000000010 action: deltacommit
24/01/05 02:43:45 INFO timeline.HoodieActiveTimeline: Creating a new instant [==>00000000000000010__deltacommit__REQUESTED]
24/01/05 02:43:46 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:43:46 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:43:46 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:43:46 INFO table.HoodieTableMetaClient: Loading Active commit timeline for /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:43:46 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[==>00000000000000010__deltacommit__REQUESTED__20240105024346000]}
24/01/05 02:43:46 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:43:46 INFO view.FileSystemViewManager: Creating View Manager with storage type :MEMORY
24/01/05 02:43:46 INFO view.FileSystemViewManager: Creating in-memory based Table View
24/01/05 02:43:46 INFO async.AsyncCleanerService: The HoodieWriteClient is not configured to auto & async clean. Async clean service will not start.
24/01/05 02:43:46 INFO async.AsyncArchiveService: The HoodieWriteClient is not configured to auto & async archive. Async archive service will not start.
24/01/05 02:43:46 INFO timeline.HoodieActiveTimeline: Checking for file exists ?/user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/00000000000000010.deltacommit.requested
24/01/05 02:43:46 INFO timeline.HoodieActiveTimeline: Create new file for toInstant ?/user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/00000000000000010.deltacommit.inflight
24/01/05 02:43:46 INFO spark.SparkContext: Starting job: collect at SparkHoodieMetadataBulkInsertPartitioner.java:95
24/01/05 02:43:46 INFO scheduler.DAGScheduler: Registering RDD 14 (keyBy at SparkHoodieMetadataBulkInsertPartitioner.java:74)
24/01/05 02:43:46 INFO scheduler.DAGScheduler: Got job 3 (collect at SparkHoodieMetadataBulkInsertPartitioner.java:95) with 1 output partitions
24/01/05 02:43:46 INFO scheduler.DAGScheduler: Final stage: ResultStage 4 (collect at SparkHoodieMetadataBulkInsertPartitioner.java:95)
24/01/05 02:43:46 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)
24/01/05 02:43:46 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 3)
24/01/05 02:43:46 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 3 (MapPartitionsRDD[14] at keyBy at SparkHoodieMetadataBulkInsertPartitioner.java:74), which has no missing parents
24/01/05 02:43:46 INFO memory.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 7.0 KB, free 365.9 MB)
24/01/05 02:43:46 INFO memory.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 4.0 KB, free 365.9 MB)
24/01/05 02:43:46 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on adhoc-1:43055 (size: 4.0 KB, free: 366.2 MB)
24/01/05 02:43:46 INFO spark.SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1161
24/01/05 02:43:46 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[14] at keyBy at SparkHoodieMetadataBulkInsertPartitioner.java:74) (first 15 tasks are for partitions Vector(0))
24/01/05 02:43:46 INFO scheduler.TaskSchedulerImpl: Adding task set 3.0 with 1 tasks
24/01/05 02:43:46 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3, localhost, executor driver, partition 0, PROCESS_LOCAL, 7899 bytes)
24/01/05 02:43:46 INFO executor.Executor: Running task 0.0 in stage 3.0 (TID 3)
24/01/05 02:43:46 INFO executor.Executor: Finished task 0.0 in stage 3.0 (TID 3). 821 bytes result sent to driver
24/01/05 02:43:46 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 46 ms on localhost (executor driver) (1/1)
24/01/05 02:43:46 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
24/01/05 02:43:46 INFO scheduler.DAGScheduler: ShuffleMapStage 3 (keyBy at SparkHoodieMetadataBulkInsertPartitioner.java:74) finished in 0.069 s
24/01/05 02:43:46 INFO scheduler.DAGScheduler: looking for newly runnable stages
24/01/05 02:43:46 INFO scheduler.DAGScheduler: running: Set()
24/01/05 02:43:46 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 4)
24/01/05 02:43:46 INFO scheduler.DAGScheduler: failed: Set()
24/01/05 02:43:46 INFO scheduler.DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[17] at mapPartitions at SparkHoodieMetadataBulkInsertPartitioner.java:81), which has no missing parents
24/01/05 02:43:46 INFO memory.MemoryStore: Block broadcast_4 stored as values in memory (estimated size 5.4 KB, free 365.9 MB)
24/01/05 02:43:46 INFO memory.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 3.0 KB, free 365.9 MB)
24/01/05 02:43:46 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on adhoc-1:43055 (size: 3.0 KB, free: 366.2 MB)
24/01/05 02:43:46 INFO spark.SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1161
24/01/05 02:43:46 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[17] at mapPartitions at SparkHoodieMetadataBulkInsertPartitioner.java:81) (first 15 tasks are for partitions Vector(0))
24/01/05 02:43:46 INFO scheduler.TaskSchedulerImpl: Adding task set 4.0 with 1 tasks
24/01/05 02:43:46 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4, localhost, executor driver, partition 0, ANY, 7662 bytes)
24/01/05 02:43:46 INFO executor.Executor: Running task 0.0 in stage 4.0 (TID 4)
24/01/05 02:43:46 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
24/01/05 02:43:46 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 6 ms
24/01/05 02:43:46 INFO executor.Executor: Finished task 0.0 in stage 4.0 (TID 4). 1110 bytes result sent to driver
24/01/05 02:43:46 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 73 ms on localhost (executor driver) (1/1)
24/01/05 02:43:46 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 
24/01/05 02:43:46 INFO scheduler.DAGScheduler: ResultStage 4 (collect at SparkHoodieMetadataBulkInsertPartitioner.java:95) finished in 0.085 s
24/01/05 02:43:46 INFO scheduler.DAGScheduler: Job 3 finished: collect at SparkHoodieMetadataBulkInsertPartitioner.java:95, took 0.178324 s
24/01/05 02:43:46 INFO commit.BaseSparkCommitActionExecutor: no validators configured.
24/01/05 02:43:46 INFO commit.BaseCommitActionExecutor: Auto commit enabled: Committing 00000000000000010
24/01/05 02:43:46 INFO spark.SparkContext: Starting job: collect at HoodieJavaRDD.java:177
24/01/05 02:43:46 INFO scheduler.DAGScheduler: Got job 4 (collect at HoodieJavaRDD.java:177) with 1 output partitions
24/01/05 02:43:46 INFO scheduler.DAGScheduler: Final stage: ResultStage 6 (collect at HoodieJavaRDD.java:177)
24/01/05 02:43:46 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 5)
24/01/05 02:43:46 INFO scheduler.DAGScheduler: Missing parents: List()
24/01/05 02:43:46 INFO scheduler.DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[20] at map at HoodieJavaRDD.java:125), which has no missing parents
24/01/05 02:43:46 INFO memory.MemoryStore: Block broadcast_5 stored as values in memory (estimated size 273.1 KB, free 365.6 MB)
24/01/05 02:43:46 INFO memory.MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 96.5 KB, free 365.5 MB)
24/01/05 02:43:46 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on adhoc-1:43055 (size: 96.5 KB, free: 366.1 MB)
24/01/05 02:43:46 INFO spark.SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1161
24/01/05 02:43:46 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[20] at map at HoodieJavaRDD.java:125) (first 15 tasks are for partitions Vector(0))
24/01/05 02:43:46 INFO scheduler.TaskSchedulerImpl: Adding task set 6.0 with 1 tasks
24/01/05 02:43:46 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 6.0 (TID 5, localhost, executor driver, partition 0, ANY, 7662 bytes)
24/01/05 02:43:46 INFO executor.Executor: Running task 0.0 in stage 6.0 (TID 5)
24/01/05 02:43:46 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
24/01/05 02:43:46 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
24/01/05 02:43:46 INFO queue.SimpleExecutor: Starting consumer, consuming records from the records iterator directly
24/01/05 02:43:46 INFO marker.DirectWriteMarkers: Creating Marker Path=/user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/.temp/00000000000000010/files/files-0000-0_0-6-5_00000000000000010.hfile.marker.CREATE
24/01/05 02:43:46 INFO marker.DirectWriteMarkers: [direct] Created marker file /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/.temp/00000000000000010/files/files-0000-0_0-6-5_00000000000000010.hfile.marker.CREATE in 7 ms
24/01/05 02:43:46 INFO impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
24/01/05 02:43:46 INFO impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
24/01/05 02:43:46 INFO impl.MetricsSystemImpl: HBase metrics system started
24/01/05 02:43:46 INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.hudi.org.apache.hadoop.hbase.metrics.impl.MetricRegistriesImpl
24/01/05 02:43:46 INFO compress.CodecPool: Got brand-new compressor [.gz]
24/01/05 02:43:46 INFO compress.CodecPool: Got brand-new compressor [.gz]
24/01/05 02:43:46 INFO io.HoodieCreateHandle: New CreateHandle for partition :files with fileId files-0000-0
24/01/05 02:43:46 INFO io.HoodieCreateHandle: Closing the file files-0000-0 as we are done with all the records 1
24/01/05 02:43:47 INFO io.HoodieCreateHandle: CreateHandle for partitionPath files fileID files-0000-0, took 559 ms.
24/01/05 02:43:47 INFO memory.MemoryStore: Block rdd_19_0 stored as values in memory (estimated size 319.0 B, free 365.5 MB)
24/01/05 02:43:47 INFO storage.BlockManagerInfo: Added rdd_19_0 in memory on adhoc-1:43055 (size: 319.0 B, free: 366.1 MB)
24/01/05 02:43:47 INFO executor.Executor: Finished task 0.0 in stage 6.0 (TID 5). 1302 bytes result sent to driver
24/01/05 02:43:47 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 6.0 (TID 5) in 641 ms on localhost (executor driver) (1/1)
24/01/05 02:43:47 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool 
24/01/05 02:43:47 INFO scheduler.DAGScheduler: ResultStage 6 (collect at HoodieJavaRDD.java:177) finished in 0.667 s
24/01/05 02:43:47 INFO scheduler.DAGScheduler: Job 4 finished: collect at HoodieJavaRDD.java:177, took 0.672507 s
24/01/05 02:43:47 INFO util.CommitUtils: Creating  metadata for BULK_INSERT numWriteStats:1 numReplaceFileIds:0
24/01/05 02:43:47 INFO commit.BaseSparkCommitActionExecutor: Committing 00000000000000010, action Type deltacommit, operation Type BULK_INSERT
24/01/05 02:43:47 INFO spark.SparkContext: Starting job: collect at HoodieSparkEngineContext.java:150
24/01/05 02:43:47 INFO scheduler.DAGScheduler: Got job 5 (collect at HoodieSparkEngineContext.java:150) with 1 output partitions
24/01/05 02:43:47 INFO scheduler.DAGScheduler: Final stage: ResultStage 7 (collect at HoodieSparkEngineContext.java:150)
24/01/05 02:43:47 INFO scheduler.DAGScheduler: Parents of final stage: List()
24/01/05 02:43:47 INFO scheduler.DAGScheduler: Missing parents: List()
24/01/05 02:43:47 INFO scheduler.DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[22] at flatMap at HoodieSparkEngineContext.java:150), which has no missing parents
24/01/05 02:43:47 INFO memory.MemoryStore: Block broadcast_6 stored as values in memory (estimated size 72.5 KB, free 365.5 MB)
24/01/05 02:43:47 INFO memory.MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 26.7 KB, free 365.4 MB)
24/01/05 02:43:47 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on adhoc-1:43055 (size: 26.7 KB, free: 366.1 MB)
24/01/05 02:43:47 INFO spark.SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1161
24/01/05 02:43:47 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[22] at flatMap at HoodieSparkEngineContext.java:150) (first 15 tasks are for partitions Vector(0))
24/01/05 02:43:47 INFO scheduler.TaskSchedulerImpl: Adding task set 7.0 with 1 tasks
24/01/05 02:43:47 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 7.0 (TID 6, localhost, executor driver, partition 0, PROCESS_LOCAL, 7838 bytes)
24/01/05 02:43:47 INFO executor.Executor: Running task 0.0 in stage 7.0 (TID 6)
24/01/05 02:43:47 INFO executor.Executor: Finished task 0.0 in stage 7.0 (TID 6). 718 bytes result sent to driver
24/01/05 02:43:47 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 7.0 (TID 6) in 19 ms on localhost (executor driver) (1/1)
24/01/05 02:43:47 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool 
24/01/05 02:43:47 INFO scheduler.DAGScheduler: ResultStage 7 (collect at HoodieSparkEngineContext.java:150) finished in 0.038 s
24/01/05 02:43:47 INFO scheduler.DAGScheduler: Job 5 finished: collect at HoodieSparkEngineContext.java:150, took 0.040283 s
24/01/05 02:43:47 INFO timeline.HoodieActiveTimeline: Marking instant complete [==>00000000000000010__deltacommit__INFLIGHT]
24/01/05 02:43:47 INFO timeline.HoodieActiveTimeline: Checking for file exists ?/user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/00000000000000010.deltacommit.inflight
24/01/05 02:43:47 INFO timeline.HoodieActiveTimeline: Create new file for toInstant ?/user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/00000000000000010.deltacommit
24/01/05 02:43:47 INFO timeline.HoodieActiveTimeline: Completed [==>00000000000000010__deltacommit__INFLIGHT]
24/01/05 02:43:47 INFO commit.BaseSparkCommitActionExecutor: Committed 00000000000000010
24/01/05 02:43:47 INFO spark.SparkContext: Starting job: collectAsMap at HoodieSparkEngineContext.java:164
24/01/05 02:43:47 INFO scheduler.DAGScheduler: Got job 6 (collectAsMap at HoodieSparkEngineContext.java:164) with 1 output partitions
24/01/05 02:43:47 INFO scheduler.DAGScheduler: Final stage: ResultStage 8 (collectAsMap at HoodieSparkEngineContext.java:164)
24/01/05 02:43:47 INFO scheduler.DAGScheduler: Parents of final stage: List()
24/01/05 02:43:47 INFO scheduler.DAGScheduler: Missing parents: List()
24/01/05 02:43:47 INFO scheduler.DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[24] at mapToPair at HoodieSparkEngineContext.java:161), which has no missing parents
24/01/05 02:43:47 INFO memory.MemoryStore: Block broadcast_7 stored as values in memory (estimated size 72.8 KB, free 365.4 MB)
24/01/05 02:43:47 INFO memory.MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 26.8 KB, free 365.3 MB)
24/01/05 02:43:47 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on adhoc-1:43055 (size: 26.8 KB, free: 366.0 MB)
24/01/05 02:43:47 INFO spark.SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1161
24/01/05 02:43:47 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[24] at mapToPair at HoodieSparkEngineContext.java:161) (first 15 tasks are for partitions Vector(0))
24/01/05 02:43:47 INFO scheduler.TaskSchedulerImpl: Adding task set 8.0 with 1 tasks
24/01/05 02:43:47 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 8.0 (TID 7, localhost, executor driver, partition 0, PROCESS_LOCAL, 7838 bytes)
24/01/05 02:43:47 INFO executor.Executor: Running task 0.0 in stage 8.0 (TID 7)
24/01/05 02:43:47 INFO spark.ContextCleaner: Cleaned accumulator 76
24/01/05 02:43:47 INFO spark.ContextCleaner: Cleaned accumulator 155
24/01/05 02:43:47 INFO spark.ContextCleaner: Cleaned accumulator 152
24/01/05 02:43:47 INFO spark.ContextCleaner: Cleaned accumulator 153
24/01/05 02:43:47 INFO spark.ContextCleaner: Cleaned accumulator 150
24/01/05 02:43:47 INFO spark.ContextCleaner: Cleaned accumulator 160
24/01/05 02:43:47 INFO spark.ContextCleaner: Cleaned accumulator 118
24/01/05 02:43:47 INFO spark.ContextCleaner: Cleaned accumulator 77
24/01/05 02:43:47 INFO storage.BlockManagerInfo: Removed broadcast_2_piece0 on adhoc-1:43055 in memory (size: 100.2 KB, free: 366.1 MB)
24/01/05 02:43:47 INFO spark.ContextCleaner: Cleaned accumulator 147
24/01/05 02:43:47 INFO spark.ContextCleaner: Cleaned accumulator 94
24/01/05 02:43:47 INFO spark.ContextCleaner: Cleaned accumulator 157
24/01/05 02:43:47 INFO spark.ContextCleaner: Cleaned accumulator 75
24/01/05 02:43:47 INFO spark.ContextCleaner: Cleaned accumulator 137
24/01/05 02:43:47 INFO spark.ContextCleaner: Cleaned accumulator 139
24/01/05 02:43:47 INFO spark.ContextCleaner: Cleaned accumulator 168
24/01/05 02:43:47 INFO spark.ContextCleaner: Cleaned accumulator 161
24/01/05 02:43:47 INFO spark.ContextCleaner: Cleaned accumulator 114
24/01/05 02:43:47 INFO spark.ContextCleaner: Cleaned accumulator 126
24/01/05 02:43:47 INFO spark.ContextCleaner: Cleaned accumulator 129
24/01/05 02:43:47 INFO spark.ContextCleaner: Cleaned accumulator 148
24/01/05 02:43:47 INFO spark.ContextCleaner: Cleaned accumulator 169
24/01/05 02:43:47 INFO spark.ContextCleaner: Cleaned accumulator 145
24/01/05 02:43:47 INFO executor.Executor: Finished task 0.0 in stage 8.0 (TID 7). 830 bytes result sent to driver
24/01/05 02:43:47 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on adhoc-1:43055 in memory (size: 3.0 KB, free: 366.1 MB)
24/01/05 02:43:47 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 8.0 (TID 7) in 46 ms on localhost (executor driver) (1/1)
24/01/05 02:43:47 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool 
24/01/05 02:43:47 INFO scheduler.DAGScheduler: ResultStage 8 (collectAsMap at HoodieSparkEngineContext.java:164) finished in 0.060 s
24/01/05 02:43:47 INFO scheduler.DAGScheduler: Job 6 finished: collectAsMap at HoodieSparkEngineContext.java:164, took 0.062794 s
24/01/05 02:43:47 INFO spark.ContextCleaner: Cleaned accumulator 101
24/01/05 02:43:47 INFO spark.ContextCleaner: Cleaned accumulator 93
24/01/05 02:43:47 INFO spark.ContextCleaner: Cleaned accumulator 162
24/01/05 02:43:47 INFO spark.ContextCleaner: Cleaned accumulator 156
24/01/05 02:43:47 INFO spark.ContextCleaner: Cleaned accumulator 84
24/01/05 02:43:47 INFO spark.ContextCleaner: Cleaned accumulator 82
24/01/05 02:43:47 INFO spark.ContextCleaner: Cleaned accumulator 104
24/01/05 02:43:47 INFO spark.ContextCleaner: Cleaned accumulator 97
24/01/05 02:43:47 INFO spark.ContextCleaner: Cleaned accumulator 172
24/01/05 02:43:47 INFO spark.ContextCleaner: Cleaned accumulator 111
24/01/05 02:43:47 INFO spark.ContextCleaner: Cleaned accumulator 134
24/01/05 02:43:47 INFO spark.ContextCleaner: Cleaned accumulator 130
24/01/05 02:43:47 INFO spark.ContextCleaner: Cleaned accumulator 120
24/01/05 02:43:47 INFO spark.ContextCleaner: Cleaned accumulator 87
24/01/05 02:43:47 INFO storage.BlockManagerInfo: Removed broadcast_6_piece0 on adhoc-1:43055 in memory (size: 26.7 KB, free: 366.2 MB)
24/01/05 02:43:47 INFO fs.FSUtils: Removed directory at /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/.temp/00000000000000010
24/01/05 02:43:47 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:43:47 INFO spark.ContextCleaner: Cleaned accumulator 158
24/01/05 02:43:47 INFO spark.ContextCleaner: Cleaned accumulator 170
24/01/05 02:43:47 INFO spark.ContextCleaner: Cleaned accumulator 124
24/01/05 02:43:47 INFO spark.ContextCleaner: Cleaned accumulator 149
24/01/05 02:43:47 INFO spark.ContextCleaner: Cleaned accumulator 133
24/01/05 02:43:47 INFO spark.ContextCleaner: Cleaned accumulator 85
24/01/05 02:43:47 INFO spark.ContextCleaner: Cleaned accumulator 136
24/01/05 02:43:47 INFO spark.ContextCleaner: Cleaned accumulator 99
24/01/05 02:43:47 INFO spark.ContextCleaner: Cleaned accumulator 90
24/01/05 02:43:47 INFO spark.ContextCleaner: Cleaned accumulator 173
24/01/05 02:43:47 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:43:47 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on adhoc-1:43055 in memory (size: 4.0 KB, free: 366.2 MB)
24/01/05 02:43:47 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:43:47 INFO table.HoodieTableMetaClient: Loading Active commit timeline for /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:43:47 INFO spark.ContextCleaner: Cleaned accumulator 110
24/01/05 02:43:47 INFO spark.ContextCleaner: Cleaned accumulator 107
24/01/05 02:43:47 INFO spark.ContextCleaner: Cleaned accumulator 112
24/01/05 02:43:47 INFO spark.ContextCleaner: Cleaned accumulator 140
24/01/05 02:43:47 INFO spark.ContextCleaner: Cleaned accumulator 166
24/01/05 02:43:47 INFO spark.ContextCleaner: Cleaned accumulator 119
24/01/05 02:43:47 INFO spark.ContextCleaner: Cleaned accumulator 142
24/01/05 02:43:47 INFO spark.ContextCleaner: Cleaned accumulator 103
24/01/05 02:43:47 INFO spark.ContextCleaner: Cleaned accumulator 86
24/01/05 02:43:47 INFO spark.ContextCleaner: Cleaned accumulator 117
24/01/05 02:43:47 INFO spark.ContextCleaner: Cleaned accumulator 92
24/01/05 02:43:47 INFO spark.ContextCleaner: Cleaned accumulator 89
24/01/05 02:43:47 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20240105024347141]}
24/01/05 02:43:47 INFO spark.ContextCleaner: Cleaned accumulator 96
24/01/05 02:43:47 INFO spark.ContextCleaner: Cleaned accumulator 98
24/01/05 02:43:47 INFO spark.ContextCleaner: Cleaned accumulator 91
24/01/05 02:43:47 INFO spark.ContextCleaner: Cleaned accumulator 109
24/01/05 02:43:47 INFO spark.ContextCleaner: Cleaned accumulator 141
24/01/05 02:43:47 INFO spark.ContextCleaner: Cleaned accumulator 95
24/01/05 02:43:47 INFO spark.ContextCleaner: Cleaned accumulator 154
24/01/05 02:43:47 INFO spark.ContextCleaner: Cleaned accumulator 171
24/01/05 02:43:47 INFO spark.ContextCleaner: Cleaned accumulator 106
24/01/05 02:43:47 INFO spark.ContextCleaner: Cleaned accumulator 167
24/01/05 02:43:47 INFO spark.ContextCleaner: Cleaned accumulator 121
24/01/05 02:43:47 INFO spark.ContextCleaner: Cleaned accumulator 143
24/01/05 02:43:47 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:43:47 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on adhoc-1:43055 in memory (size: 96.5 KB, free: 366.3 MB)
24/01/05 02:43:47 INFO view.FileSystemViewManager: Creating View Manager with storage type :MEMORY
24/01/05 02:43:47 INFO view.FileSystemViewManager: Creating in-memory based Table View
24/01/05 02:43:47 INFO spark.ContextCleaner: Cleaned accumulator 165
24/01/05 02:43:47 INFO spark.ContextCleaner: Cleaned accumulator 102
24/01/05 02:43:47 INFO spark.ContextCleaner: Cleaned accumulator 128
24/01/05 02:43:47 INFO spark.ContextCleaner: Cleaned accumulator 123
24/01/05 02:43:47 INFO spark.ContextCleaner: Cleaned accumulator 113
24/01/05 02:43:47 INFO spark.ContextCleaner: Cleaned accumulator 83
24/01/05 02:43:47 INFO spark.ContextCleaner: Cleaned accumulator 127
24/01/05 02:43:47 INFO spark.ContextCleaner: Cleaned accumulator 174
24/01/05 02:43:47 INFO spark.ContextCleaner: Cleaned accumulator 146
24/01/05 02:43:47 INFO spark.ContextCleaner: Cleaned accumulator 131
24/01/05 02:43:47 INFO spark.ContextCleaner: Cleaned accumulator 88
24/01/05 02:43:47 INFO spark.ContextCleaner: Cleaned accumulator 116
24/01/05 02:43:47 INFO spark.ContextCleaner: Cleaned accumulator 108
24/01/05 02:43:47 INFO spark.ContextCleaner: Cleaned accumulator 79
24/01/05 02:43:47 INFO spark.ContextCleaner: Cleaned accumulator 163
24/01/05 02:43:47 INFO spark.ContextCleaner: Cleaned accumulator 151
24/01/05 02:43:47 INFO spark.ContextCleaner: Cleaned accumulator 159
24/01/05 02:43:47 INFO spark.ContextCleaner: Cleaned accumulator 132
24/01/05 02:43:47 INFO spark.ContextCleaner: Cleaned accumulator 122
24/01/05 02:43:47 INFO spark.ContextCleaner: Cleaned accumulator 78
24/01/05 02:43:47 INFO spark.ContextCleaner: Cleaned accumulator 80
24/01/05 02:43:47 INFO spark.ContextCleaner: Cleaned accumulator 115
24/01/05 02:43:47 INFO spark.ContextCleaner: Cleaned accumulator 144
24/01/05 02:43:47 INFO spark.ContextCleaner: Cleaned accumulator 135
24/01/05 02:43:47 INFO spark.ContextCleaner: Cleaned accumulator 100
24/01/05 02:43:47 INFO spark.ContextCleaner: Cleaned accumulator 164
24/01/05 02:43:47 INFO spark.ContextCleaner: Cleaned accumulator 105
24/01/05 02:43:47 INFO spark.ContextCleaner: Cleaned accumulator 125
24/01/05 02:43:47 INFO spark.ContextCleaner: Cleaned accumulator 138
24/01/05 02:43:47 INFO spark.ContextCleaner: Cleaned accumulator 81
24/01/05 02:43:47 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20240105024347141]}
24/01/05 02:43:47 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20240105024347141]}
24/01/05 02:43:47 INFO table.HoodieTableConfig: MDT /user/hive/warehouse/stock_ticks_mor partition FILES has been enabled
24/01/05 02:43:47 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:43:47 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:43:47 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:43:47 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:43:47 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:43:47 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:43:47 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20240105024347141]}
24/01/05 02:43:47 INFO view.AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
24/01/05 02:43:47 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:43:47 INFO metadata.HoodieBackedTableMetadataWriter: Initializing FILES index in metadata table took 1742 in ms
24/01/05 02:43:47 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:43:47 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:43:47 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:43:47 INFO table.HoodieTableMetaClient: Loading Active commit timeline for /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:43:47 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20240105024347141]}
24/01/05 02:43:47 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:43:47 INFO view.FileSystemViewManager: Creating View Manager with storage type :MEMORY
24/01/05 02:43:47 INFO view.FileSystemViewManager: Creating in-memory based Table View
24/01/05 02:43:47 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:43:47 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:43:47 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:43:47 INFO table.HoodieTableMetaClient: Loading Active commit timeline for /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:43:47 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20240105024347141]}
24/01/05 02:43:47 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:43:47 INFO view.FileSystemViewManager: Creating View Manager with storage type :MEMORY
24/01/05 02:43:47 INFO view.FileSystemViewManager: Creating in-memory based Table View
24/01/05 02:43:47 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20240105024347141]}
24/01/05 02:43:47 INFO metadata.HoodieBackedTableMetadataWriter: Latest deltacommit time found is 00000000000000010, running clean operations.
24/01/05 02:43:47 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20240105024347141]}
24/01/05 02:43:47 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:43:47 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:43:47 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:43:47 INFO table.HoodieTableMetaClient: Loading Active commit timeline for /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:43:47 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20240105024347141]}
24/01/05 02:43:47 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:43:47 INFO view.FileSystemViewManager: Creating View Manager with storage type :MEMORY
24/01/05 02:43:47 INFO view.FileSystemViewManager: Creating in-memory based Table View
24/01/05 02:43:47 INFO client.BaseHoodieWriteClient: Cleaner started
24/01/05 02:43:47 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:43:47 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:43:47 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:43:47 INFO table.HoodieTableMetaClient: Loading Active commit timeline for /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:43:47 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20240105024347141]}
24/01/05 02:43:47 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:43:47 INFO view.FileSystemViewManager: Creating View Manager with storage type :MEMORY
24/01/05 02:43:47 INFO view.FileSystemViewManager: Creating in-memory based Table View
24/01/05 02:43:47 INFO client.BaseHoodieWriteClient: Scheduling cleaning at instant time :00000000000000010002
24/01/05 02:43:47 INFO view.FileSystemViewManager: Creating InMemory based view for basePath /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:43:47 INFO view.AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
24/01/05 02:43:47 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:43:47 INFO clean.CleanPlanner: No earliest commit to retain. No need to scan partitions !!
24/01/05 02:43:47 INFO clean.CleanPlanActionExecutor: Nothing to clean here. It is already clean
24/01/05 02:43:47 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20240105024347141]}
24/01/05 02:43:47 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:43:47 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:43:47 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:43:47 INFO table.HoodieTableMetaClient: Loading Active commit timeline for /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:43:47 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20240105024347141]}
24/01/05 02:43:47 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:43:47 INFO view.FileSystemViewManager: Creating View Manager with storage type :MEMORY
24/01/05 02:43:47 INFO view.FileSystemViewManager: Creating in-memory based Table View
24/01/05 02:43:47 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[==>20240105024343265__deltacommit__REQUESTED__20240105024344334]}
24/01/05 02:43:47 INFO client.BaseHoodieWriteClient: Scheduling table service COMPACT
24/01/05 02:43:47 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:43:47 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:43:47 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:43:47 INFO table.HoodieTableMetaClient: Loading Active commit timeline for /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:43:47 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20240105024347141]}
24/01/05 02:43:47 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:43:47 INFO view.FileSystemViewManager: Creating View Manager with storage type :MEMORY
24/01/05 02:43:47 INFO view.FileSystemViewManager: Creating in-memory based Table View
24/01/05 02:43:47 INFO client.BaseHoodieWriteClient: Scheduling compaction at instant time :00000000000000010001
24/01/05 02:43:47 INFO compact.ScheduleCompactionActionExecutor: Checking if compaction needs to be run on /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:43:47 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:43:47 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:43:47 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:43:47 INFO table.HoodieTableMetaClient: Loading Active commit timeline for /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:43:47 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20240105024347141]}
24/01/05 02:43:47 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:43:47 INFO view.FileSystemViewManager: Creating View Manager with storage type :MEMORY
24/01/05 02:43:47 INFO view.FileSystemViewManager: Creating in-memory based Table View
24/01/05 02:43:47 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20240105024347141]}
24/01/05 02:43:47 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:43:47 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:43:47 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:43:47 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[==>20240105024343265__deltacommit__REQUESTED__20240105024344334]}
24/01/05 02:43:47 INFO client.HoodieTimelineArchiver: No Instants to archive
24/01/05 02:43:47 INFO metadata.HoodieBackedTableMetadataWriter: All the table services operations on MDT completed successfully
24/01/05 02:43:47 INFO transaction.TransactionManager: Transaction ending with transaction owner Option{val=[==>20240105024343265__deltacommit__INFLIGHT]}
24/01/05 02:43:47 INFO lock.InProcessLockProvider: Base Path /user/hive/warehouse/stock_ticks_mor, Lock Instance java.util.concurrent.locks.ReentrantReadWriteLock@592a1882[Write locks = 1, Read locks = 0], Thread main, In-process lock state RELEASING
24/01/05 02:43:47 INFO lock.InProcessLockProvider: Base Path /user/hive/warehouse/stock_ticks_mor, Lock Instance java.util.concurrent.locks.ReentrantReadWriteLock@592a1882[Write locks = 0, Read locks = 0], Thread main, In-process lock state RELEASED
24/01/05 02:43:47 INFO lock.InProcessLockProvider: Base Path /user/hive/warehouse/stock_ticks_mor, Lock Instance java.util.concurrent.locks.ReentrantReadWriteLock@592a1882[Write locks = 0, Read locks = 0], Thread main, In-process lock state ALREADY_RELEASED
24/01/05 02:43:47 INFO lock.LockManager: Released connection created for acquiring lock
24/01/05 02:43:47 INFO transaction.TransactionManager: Transaction ended with transaction owner Option{val=[==>20240105024343265__deltacommit__INFLIGHT]}
24/01/05 02:43:47 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:43:47 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:43:47 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:43:47 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:43:47 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:43:47 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:43:47 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20240105024347141]}
24/01/05 02:43:47 INFO view.AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
24/01/05 02:43:47 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:43:47 INFO view.FileSystemViewManager: Creating View Manager with storage type :REMOTE_FIRST
24/01/05 02:43:47 INFO view.FileSystemViewManager: Creating remote first table view
24/01/05 02:43:47 INFO async.AsyncCleanerService: The HoodieWriteClient is not configured to auto & async clean. Async clean service will not start.
24/01/05 02:43:47 INFO async.AsyncArchiveService: The HoodieWriteClient is not configured to auto & async archive. Async archive service will not start.
24/01/05 02:43:47 INFO spark.SparkContext: Starting job: collect at HoodieJavaRDD.java:177
24/01/05 02:43:47 INFO scheduler.DAGScheduler: Registering RDD 25 (mapToPair at HoodieJavaRDD.java:149)
24/01/05 02:43:47 INFO scheduler.DAGScheduler: Registering RDD 31 (distinct at HoodieJavaRDD.java:157)
24/01/05 02:43:47 INFO scheduler.DAGScheduler: Got job 7 (collect at HoodieJavaRDD.java:177) with 2 output partitions
24/01/05 02:43:47 INFO scheduler.DAGScheduler: Final stage: ResultStage 11 (collect at HoodieJavaRDD.java:177)
24/01/05 02:43:47 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 10)
24/01/05 02:43:47 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 10)
24/01/05 02:43:47 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 9 (MapPartitionsRDD[25] at mapToPair at HoodieJavaRDD.java:149), which has no missing parents
24/01/05 02:43:47 INFO memory.MemoryStore: Block broadcast_8 stored as values in memory (estimated size 29.5 KB, free 366.2 MB)
24/01/05 02:43:47 INFO memory.MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 13.9 KB, free 366.2 MB)
24/01/05 02:43:47 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on adhoc-1:43055 (size: 13.9 KB, free: 366.3 MB)
24/01/05 02:43:47 INFO spark.SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1161
24/01/05 02:43:47 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 9 (MapPartitionsRDD[25] at mapToPair at HoodieJavaRDD.java:149) (first 15 tasks are for partitions Vector(0))
24/01/05 02:43:47 INFO scheduler.TaskSchedulerImpl: Adding task set 9.0 with 1 tasks
24/01/05 02:43:47 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 9.0 (TID 8, localhost, executor driver, partition 0, PROCESS_LOCAL, 7739 bytes)
24/01/05 02:43:47 INFO executor.Executor: Running task 0.0 in stage 9.0 (TID 8)
24/01/05 02:43:47 INFO kafka010.KafkaRDD: Computing topic stock_ticks, partition 0 offsets 0 -> 1866
24/01/05 02:43:47 INFO kafka010.KafkaDataConsumer: Initializing cache 16 64 0.75
24/01/05 02:43:47 INFO consumer.ConsumerConfig: ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [kafkabroker:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-executor-null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

24/01/05 02:43:47 INFO utils.AppInfoParser: Kafka version : 2.0.0
24/01/05 02:43:47 INFO utils.AppInfoParser: Kafka commitId : 3402a8361b734732
24/01/05 02:43:47 INFO factory.HoodieSparkKeyGeneratorFactory: The value of hoodie.datasource.write.keygenerator.type is empty; inferred to be SIMPLE
24/01/05 02:43:47 INFO kafka010.InternalKafkaConsumer: Initial fetch for spark-executor-null stock_ticks-0 0
24/01/05 02:43:47 INFO clients.Metadata: Cluster ID: XFgecpgeR0meVK58zP5FxA
24/01/05 02:43:47 INFO executor.Executor: Finished task 0.0 in stage 9.0 (TID 8). 951 bytes result sent to driver
24/01/05 02:43:47 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 9.0 (TID 8) in 350 ms on localhost (executor driver) (1/1)
24/01/05 02:43:47 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool 
24/01/05 02:43:47 INFO scheduler.DAGScheduler: ShuffleMapStage 9 (mapToPair at HoodieJavaRDD.java:149) finished in 0.362 s
24/01/05 02:43:47 INFO scheduler.DAGScheduler: looking for newly runnable stages
24/01/05 02:43:47 INFO scheduler.DAGScheduler: running: Set()
24/01/05 02:43:47 INFO scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 10, ResultStage 11)
24/01/05 02:43:47 INFO scheduler.DAGScheduler: failed: Set()
24/01/05 02:43:47 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 10 (MapPartitionsRDD[31] at distinct at HoodieJavaRDD.java:157), which has no missing parents
24/01/05 02:43:47 INFO memory.MemoryStore: Block broadcast_9 stored as values in memory (estimated size 24.3 KB, free 366.1 MB)
24/01/05 02:43:47 INFO memory.MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 11.5 KB, free 366.1 MB)
24/01/05 02:43:47 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on adhoc-1:43055 (size: 11.5 KB, free: 366.2 MB)
24/01/05 02:43:47 INFO spark.SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1161
24/01/05 02:43:47 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 10 (MapPartitionsRDD[31] at distinct at HoodieJavaRDD.java:157) (first 15 tasks are for partitions Vector(0, 1))
24/01/05 02:43:47 INFO scheduler.TaskSchedulerImpl: Adding task set 10.0 with 2 tasks
24/01/05 02:43:47 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 10.0 (TID 9, localhost, executor driver, partition 0, ANY, 7651 bytes)
24/01/05 02:43:47 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 10.0 (TID 10, localhost, executor driver, partition 1, ANY, 7651 bytes)
24/01/05 02:43:47 INFO executor.Executor: Running task 0.0 in stage 10.0 (TID 9)
24/01/05 02:43:47 INFO executor.Executor: Running task 1.0 in stage 10.0 (TID 10)
24/01/05 02:43:47 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
24/01/05 02:43:47 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
24/01/05 02:43:47 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
24/01/05 02:43:47 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
24/01/05 02:43:47 INFO memory.MemoryStore: Block rdd_27_0 stored as values in memory (estimated size 6.2 KB, free 366.1 MB)
24/01/05 02:43:47 INFO storage.BlockManagerInfo: Added rdd_27_0 in memory on adhoc-1:43055 (size: 6.2 KB, free: 366.2 MB)
24/01/05 02:43:47 INFO memory.MemoryStore: Block rdd_27_1 stored as values in memory (estimated size 9.6 KB, free 366.1 MB)
24/01/05 02:43:47 INFO storage.BlockManagerInfo: Added rdd_27_1 in memory on adhoc-1:43055 (size: 9.6 KB, free: 366.2 MB)
24/01/05 02:43:47 INFO executor.Executor: Finished task 0.0 in stage 10.0 (TID 9). 1252 bytes result sent to driver
24/01/05 02:43:47 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 10.0 (TID 9) in 54 ms on localhost (executor driver) (1/2)
24/01/05 02:43:47 INFO executor.Executor: Finished task 1.0 in stage 10.0 (TID 10). 1252 bytes result sent to driver
24/01/05 02:43:47 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 10.0 (TID 10) in 56 ms on localhost (executor driver) (2/2)
24/01/05 02:43:47 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool 
24/01/05 02:43:47 INFO scheduler.DAGScheduler: ShuffleMapStage 10 (distinct at HoodieJavaRDD.java:157) finished in 0.067 s
24/01/05 02:43:47 INFO scheduler.DAGScheduler: looking for newly runnable stages
24/01/05 02:43:47 INFO scheduler.DAGScheduler: running: Set()
24/01/05 02:43:47 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 11)
24/01/05 02:43:47 INFO scheduler.DAGScheduler: failed: Set()
24/01/05 02:43:47 INFO scheduler.DAGScheduler: Submitting ResultStage 11 (MapPartitionsRDD[33] at distinct at HoodieJavaRDD.java:157), which has no missing parents
24/01/05 02:43:47 INFO memory.MemoryStore: Block broadcast_10 stored as values in memory (estimated size 4.1 KB, free 366.1 MB)
24/01/05 02:43:47 INFO memory.MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 2.4 KB, free 366.1 MB)
24/01/05 02:43:47 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on adhoc-1:43055 (size: 2.4 KB, free: 366.2 MB)
24/01/05 02:43:47 INFO spark.SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1161
24/01/05 02:43:47 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ResultStage 11 (MapPartitionsRDD[33] at distinct at HoodieJavaRDD.java:157) (first 15 tasks are for partitions Vector(0, 1))
24/01/05 02:43:47 INFO scheduler.TaskSchedulerImpl: Adding task set 11.0 with 2 tasks
24/01/05 02:43:47 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 11.0 (TID 11, localhost, executor driver, partition 0, PROCESS_LOCAL, 7662 bytes)
24/01/05 02:43:47 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 11.0 (TID 12, localhost, executor driver, partition 1, ANY, 7662 bytes)
24/01/05 02:43:47 INFO executor.Executor: Running task 1.0 in stage 11.0 (TID 12)
24/01/05 02:43:47 INFO executor.Executor: Running task 0.0 in stage 11.0 (TID 11)
24/01/05 02:43:47 INFO storage.ShuffleBlockFetcherIterator: Getting 2 non-empty blocks including 2 local blocks and 0 remote blocks
24/01/05 02:43:47 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
24/01/05 02:43:47 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
24/01/05 02:43:47 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
24/01/05 02:43:47 INFO executor.Executor: Finished task 0.0 in stage 11.0 (TID 11). 1098 bytes result sent to driver
24/01/05 02:43:47 INFO executor.Executor: Finished task 1.0 in stage 11.0 (TID 12). 1110 bytes result sent to driver
24/01/05 02:43:47 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 11.0 (TID 11) in 14 ms on localhost (executor driver) (1/2)
24/01/05 02:43:48 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 11.0 (TID 12) in 13 ms on localhost (executor driver) (2/2)
24/01/05 02:43:48 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool 
24/01/05 02:43:48 INFO scheduler.DAGScheduler: ResultStage 11 (collect at HoodieJavaRDD.java:177) finished in 0.023 s
24/01/05 02:43:48 INFO scheduler.DAGScheduler: Job 7 finished: collect at HoodieJavaRDD.java:177, took 0.471343 s
24/01/05 02:43:48 INFO spark.SparkContext: Starting job: collect at HoodieSparkEngineContext.java:150
24/01/05 02:43:48 INFO scheduler.DAGScheduler: Got job 8 (collect at HoodieSparkEngineContext.java:150) with 1 output partitions
24/01/05 02:43:48 INFO scheduler.DAGScheduler: Final stage: ResultStage 12 (collect at HoodieSparkEngineContext.java:150)
24/01/05 02:43:48 INFO scheduler.DAGScheduler: Parents of final stage: List()
24/01/05 02:43:48 INFO scheduler.DAGScheduler: Missing parents: List()
24/01/05 02:43:48 INFO scheduler.DAGScheduler: Submitting ResultStage 12 (MapPartitionsRDD[35] at flatMap at HoodieSparkEngineContext.java:150), which has no missing parents
24/01/05 02:43:48 INFO memory.MemoryStore: Block broadcast_11 stored as values in memory (estimated size 338.9 KB, free 365.8 MB)
24/01/05 02:43:48 INFO memory.MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 121.2 KB, free 365.7 MB)
24/01/05 02:43:48 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on adhoc-1:43055 (size: 121.2 KB, free: 366.1 MB)
24/01/05 02:43:48 INFO spark.SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1161
24/01/05 02:43:48 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 12 (MapPartitionsRDD[35] at flatMap at HoodieSparkEngineContext.java:150) (first 15 tasks are for partitions Vector(0))
24/01/05 02:43:48 INFO scheduler.TaskSchedulerImpl: Adding task set 12.0 with 1 tasks
24/01/05 02:43:48 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 12.0 (TID 13, localhost, executor driver, partition 0, PROCESS_LOCAL, 7735 bytes)
24/01/05 02:43:48 INFO executor.Executor: Running task 0.0 in stage 12.0 (TID 13)
24/01/05 02:43:48 INFO executor.Executor: Finished task 0.0 in stage 12.0 (TID 13). 668 bytes result sent to driver
24/01/05 02:43:48 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 12.0 (TID 13) in 30 ms on localhost (executor driver) (1/1)
24/01/05 02:43:48 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool 
24/01/05 02:43:48 INFO scheduler.DAGScheduler: ResultStage 12 (collect at HoodieSparkEngineContext.java:150) finished in 0.061 s
24/01/05 02:43:48 INFO scheduler.DAGScheduler: Job 8 finished: collect at HoodieSparkEngineContext.java:150, took 0.063489 s
24/01/05 02:43:48 INFO rdd.MapPartitionsRDD: Removing RDD 27 from persistence list
24/01/05 02:43:48 INFO storage.BlockManager: Removing RDD 27
24/01/05 02:43:48 INFO view.AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
24/01/05 02:43:48 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:43:48 INFO spark.SparkContext: Starting job: countByKey at HoodieJavaPairRDD.java:105
24/01/05 02:43:48 INFO scheduler.DAGScheduler: Registering RDD 38 (mapToPair at HoodieJavaRDD.java:149)
24/01/05 02:43:48 INFO scheduler.DAGScheduler: Registering RDD 28 (mapToPair at HoodieJavaRDD.java:149)
24/01/05 02:43:48 INFO scheduler.DAGScheduler: Registering RDD 46 (countByKey at HoodieJavaPairRDD.java:105)
24/01/05 02:43:48 INFO scheduler.DAGScheduler: Got job 9 (countByKey at HoodieJavaPairRDD.java:105) with 2 output partitions
24/01/05 02:43:48 INFO scheduler.DAGScheduler: Final stage: ResultStage 17 (countByKey at HoodieJavaPairRDD.java:105)
24/01/05 02:43:48 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 16)
24/01/05 02:43:48 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 16)
24/01/05 02:43:48 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 14 (MapPartitionsRDD[38] at mapToPair at HoodieJavaRDD.java:149), which has no missing parents
24/01/05 02:43:48 INFO memory.MemoryStore: Block broadcast_12 stored as values in memory (estimated size 340.7 KB, free 365.3 MB)
24/01/05 02:43:48 INFO memory.MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 122.1 KB, free 365.2 MB)
24/01/05 02:43:48 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on adhoc-1:43055 (size: 122.1 KB, free: 366.0 MB)
24/01/05 02:43:48 INFO spark.SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1161
24/01/05 02:43:48 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 14 (MapPartitionsRDD[38] at mapToPair at HoodieJavaRDD.java:149) (first 15 tasks are for partitions Vector(0))
24/01/05 02:43:48 INFO scheduler.TaskSchedulerImpl: Adding task set 14.0 with 1 tasks
24/01/05 02:43:48 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 15 (MapPartitionsRDD[28] at mapToPair at HoodieJavaRDD.java:149), which has no missing parents
24/01/05 02:43:48 INFO memory.MemoryStore: Block broadcast_13 stored as values in memory (estimated size 23.7 KB, free 365.2 MB)
24/01/05 02:43:48 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 14.0 (TID 14, localhost, executor driver, partition 0, PROCESS_LOCAL, 7712 bytes)
24/01/05 02:43:48 INFO executor.Executor: Running task 0.0 in stage 14.0 (TID 14)
24/01/05 02:43:48 INFO memory.MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 11.4 KB, free 365.2 MB)
24/01/05 02:43:48 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on adhoc-1:43055 (size: 11.4 KB, free: 366.0 MB)
24/01/05 02:43:48 INFO spark.SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1161
24/01/05 02:43:48 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 15 (MapPartitionsRDD[28] at mapToPair at HoodieJavaRDD.java:149) (first 15 tasks are for partitions Vector(0, 1))
24/01/05 02:43:48 INFO scheduler.TaskSchedulerImpl: Adding task set 15.0 with 2 tasks
24/01/05 02:43:48 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 15.0 (TID 15, localhost, executor driver, partition 0, ANY, 7651 bytes)
24/01/05 02:43:48 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 15.0 (TID 16, localhost, executor driver, partition 1, ANY, 7651 bytes)
24/01/05 02:43:48 INFO executor.Executor: Running task 0.0 in stage 15.0 (TID 15)
24/01/05 02:43:48 INFO executor.Executor: Running task 1.0 in stage 15.0 (TID 16)
24/01/05 02:43:48 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
24/01/05 02:43:48 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
24/01/05 02:43:48 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
24/01/05 02:43:48 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
24/01/05 02:43:48 INFO executor.Executor: Finished task 0.0 in stage 14.0 (TID 14). 693 bytes result sent to driver
24/01/05 02:43:48 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 14.0 (TID 14) in 32 ms on localhost (executor driver) (1/1)
24/01/05 02:43:48 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 14.0, whose tasks have all completed, from pool 
24/01/05 02:43:48 INFO scheduler.DAGScheduler: ShuffleMapStage 14 (mapToPair at HoodieJavaRDD.java:149) finished in 0.063 s
24/01/05 02:43:48 INFO scheduler.DAGScheduler: looking for newly runnable stages
24/01/05 02:43:48 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 15)
24/01/05 02:43:48 INFO scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 16, ResultStage 17)
24/01/05 02:43:48 INFO scheduler.DAGScheduler: failed: Set()
24/01/05 02:43:48 INFO executor.Executor: Finished task 0.0 in stage 15.0 (TID 15). 1252 bytes result sent to driver
24/01/05 02:43:48 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 15.0 (TID 15) in 28 ms on localhost (executor driver) (1/2)
24/01/05 02:43:48 INFO executor.Executor: Finished task 1.0 in stage 15.0 (TID 16). 1252 bytes result sent to driver
24/01/05 02:43:48 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 15.0 (TID 16) in 30 ms on localhost (executor driver) (2/2)
24/01/05 02:43:48 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 15.0, whose tasks have all completed, from pool 
24/01/05 02:43:48 INFO scheduler.DAGScheduler: ShuffleMapStage 15 (mapToPair at HoodieJavaRDD.java:149) finished in 0.040 s
24/01/05 02:43:48 INFO scheduler.DAGScheduler: looking for newly runnable stages
24/01/05 02:43:48 INFO scheduler.DAGScheduler: running: Set()
24/01/05 02:43:48 INFO scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 16, ResultStage 17)
24/01/05 02:43:48 INFO scheduler.DAGScheduler: failed: Set()
24/01/05 02:43:48 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 16 (MapPartitionsRDD[46] at countByKey at HoodieJavaPairRDD.java:105), which has no missing parents
24/01/05 02:43:48 INFO memory.MemoryStore: Block broadcast_14 stored as values in memory (estimated size 7.7 KB, free 365.2 MB)
24/01/05 02:43:48 INFO memory.MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 4.0 KB, free 365.2 MB)
24/01/05 02:43:48 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on adhoc-1:43055 (size: 4.0 KB, free: 366.0 MB)
24/01/05 02:43:48 INFO spark.SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1161
24/01/05 02:43:48 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 16 (MapPartitionsRDD[46] at countByKey at HoodieJavaPairRDD.java:105) (first 15 tasks are for partitions Vector(0, 1))
24/01/05 02:43:48 INFO scheduler.TaskSchedulerImpl: Adding task set 16.0 with 2 tasks
24/01/05 02:43:48 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 16.0 (TID 17, localhost, executor driver, partition 0, PROCESS_LOCAL, 7714 bytes)
24/01/05 02:43:48 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 16.0 (TID 18, localhost, executor driver, partition 1, PROCESS_LOCAL, 7714 bytes)
24/01/05 02:43:48 INFO executor.Executor: Running task 1.0 in stage 16.0 (TID 18)
24/01/05 02:43:48 INFO executor.Executor: Running task 0.0 in stage 16.0 (TID 17)
24/01/05 02:43:48 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
24/01/05 02:43:48 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
24/01/05 02:43:48 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
24/01/05 02:43:48 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
24/01/05 02:43:48 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
24/01/05 02:43:48 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
24/01/05 02:43:48 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
24/01/05 02:43:48 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
24/01/05 02:43:48 INFO memory.MemoryStore: Block rdd_44_0 stored as values in memory (estimated size 6.2 KB, free 365.2 MB)
24/01/05 02:43:48 INFO storage.BlockManagerInfo: Added rdd_44_0 in memory on adhoc-1:43055 (size: 6.2 KB, free: 366.0 MB)
24/01/05 02:43:48 INFO executor.Executor: Finished task 0.0 in stage 16.0 (TID 17). 1252 bytes result sent to driver
24/01/05 02:43:48 INFO memory.MemoryStore: Block rdd_44_1 stored as values in memory (estimated size 9.6 KB, free 365.2 MB)
24/01/05 02:43:48 INFO storage.BlockManagerInfo: Added rdd_44_1 in memory on adhoc-1:43055 (size: 9.6 KB, free: 366.0 MB)
24/01/05 02:43:48 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 16.0 (TID 17) in 41 ms on localhost (executor driver) (1/2)
24/01/05 02:43:48 INFO executor.Executor: Finished task 1.0 in stage 16.0 (TID 18). 1252 bytes result sent to driver
24/01/05 02:43:48 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 16.0 (TID 18) in 50 ms on localhost (executor driver) (2/2)
24/01/05 02:43:48 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 16.0, whose tasks have all completed, from pool 
24/01/05 02:43:48 INFO scheduler.DAGScheduler: ShuffleMapStage 16 (countByKey at HoodieJavaPairRDD.java:105) finished in 0.058 s
24/01/05 02:43:48 INFO scheduler.DAGScheduler: looking for newly runnable stages
24/01/05 02:43:48 INFO scheduler.DAGScheduler: running: Set()
24/01/05 02:43:48 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 17)
24/01/05 02:43:48 INFO scheduler.DAGScheduler: failed: Set()
24/01/05 02:43:48 INFO scheduler.DAGScheduler: Submitting ResultStage 17 (ShuffledRDD[47] at countByKey at HoodieJavaPairRDD.java:105), which has no missing parents
24/01/05 02:43:48 INFO memory.MemoryStore: Block broadcast_15 stored as values in memory (estimated size 3.6 KB, free 365.2 MB)
24/01/05 02:43:48 INFO memory.MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 2.1 KB, free 365.2 MB)
24/01/05 02:43:48 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on adhoc-1:43055 (size: 2.1 KB, free: 366.0 MB)
24/01/05 02:43:48 INFO spark.SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1161
24/01/05 02:43:48 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ResultStage 17 (ShuffledRDD[47] at countByKey at HoodieJavaPairRDD.java:105) (first 15 tasks are for partitions Vector(0, 1))
24/01/05 02:43:48 INFO scheduler.TaskSchedulerImpl: Adding task set 17.0 with 2 tasks
24/01/05 02:43:48 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 17.0 (TID 19, localhost, executor driver, partition 0, PROCESS_LOCAL, 7662 bytes)
24/01/05 02:43:48 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 17.0 (TID 20, localhost, executor driver, partition 1, ANY, 7662 bytes)
24/01/05 02:43:48 INFO executor.Executor: Running task 1.0 in stage 17.0 (TID 20)
24/01/05 02:43:48 INFO executor.Executor: Running task 0.0 in stage 17.0 (TID 19)
24/01/05 02:43:48 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
24/01/05 02:43:48 INFO storage.ShuffleBlockFetcherIterator: Getting 2 non-empty blocks including 2 local blocks and 0 remote blocks
24/01/05 02:43:48 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
24/01/05 02:43:48 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
24/01/05 02:43:48 INFO executor.Executor: Finished task 0.0 in stage 17.0 (TID 19). 1098 bytes result sent to driver
24/01/05 02:43:48 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 17.0 (TID 19) in 10 ms on localhost (executor driver) (1/2)
24/01/05 02:43:48 INFO executor.Executor: Finished task 1.0 in stage 17.0 (TID 20). 1155 bytes result sent to driver
24/01/05 02:43:48 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 17.0 (TID 20) in 12 ms on localhost (executor driver) (2/2)
24/01/05 02:43:48 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 17.0, whose tasks have all completed, from pool 
24/01/05 02:43:48 INFO scheduler.DAGScheduler: ResultStage 17 (countByKey at HoodieJavaPairRDD.java:105) finished in 0.019 s
24/01/05 02:43:48 INFO scheduler.DAGScheduler: Job 9 finished: countByKey at HoodieJavaPairRDD.java:105, took 0.155371 s
24/01/05 02:43:48 INFO commit.UpsertPartitioner: AvgRecordSize => 1024
24/01/05 02:43:48 INFO spark.SparkContext: Starting job: collectAsMap at UpsertPartitioner.java:282
24/01/05 02:43:48 INFO scheduler.DAGScheduler: Got job 10 (collectAsMap at UpsertPartitioner.java:282) with 1 output partitions
24/01/05 02:43:48 INFO scheduler.DAGScheduler: Final stage: ResultStage 18 (collectAsMap at UpsertPartitioner.java:282)
24/01/05 02:43:48 INFO scheduler.DAGScheduler: Parents of final stage: List()
24/01/05 02:43:48 INFO scheduler.DAGScheduler: Missing parents: List()
24/01/05 02:43:48 INFO scheduler.DAGScheduler: Submitting ResultStage 18 (MapPartitionsRDD[49] at mapToPair at UpsertPartitioner.java:281), which has no missing parents
24/01/05 02:43:48 INFO memory.MemoryStore: Block broadcast_16 stored as values in memory (estimated size 339.7 KB, free 364.8 MB)
24/01/05 02:43:48 INFO memory.MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 121.4 KB, free 364.7 MB)
24/01/05 02:43:48 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on adhoc-1:43055 (size: 121.4 KB, free: 365.9 MB)
24/01/05 02:43:48 INFO spark.SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1161
24/01/05 02:43:48 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 18 (MapPartitionsRDD[49] at mapToPair at UpsertPartitioner.java:281) (first 15 tasks are for partitions Vector(0))
24/01/05 02:43:48 INFO scheduler.TaskSchedulerImpl: Adding task set 18.0 with 1 tasks
24/01/05 02:43:48 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 18.0 (TID 21, localhost, executor driver, partition 0, PROCESS_LOCAL, 7735 bytes)
24/01/05 02:43:48 INFO executor.Executor: Running task 0.0 in stage 18.0 (TID 21)
24/01/05 02:43:48 INFO executor.Executor: Finished task 0.0 in stage 18.0 (TID 21). 716 bytes result sent to driver
24/01/05 02:43:48 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 18.0 (TID 21) in 23 ms on localhost (executor driver) (1/1)
24/01/05 02:43:48 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 18.0, whose tasks have all completed, from pool 
24/01/05 02:43:48 INFO scheduler.DAGScheduler: ResultStage 18 (collectAsMap at UpsertPartitioner.java:282) finished in 0.048 s
24/01/05 02:43:48 INFO scheduler.DAGScheduler: Job 10 finished: collectAsMap at UpsertPartitioner.java:282, took 0.050443 s
24/01/05 02:43:48 INFO view.AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
24/01/05 02:43:48 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:43:48 INFO commit.UpsertPartitioner: Total Buckets: 1
24/01/05 02:43:48 INFO timeline.HoodieActiveTimeline: Checking for file exists ?/user/hive/warehouse/stock_ticks_mor/.hoodie/20240105024343265.deltacommit.requested
24/01/05 02:43:48 INFO timeline.HoodieActiveTimeline: Create new file for toInstant ?/user/hive/warehouse/stock_ticks_mor/.hoodie/20240105024343265.deltacommit.inflight
24/01/05 02:43:49 INFO commit.BaseSparkCommitActionExecutor: no validators configured.
24/01/05 02:43:49 INFO commit.BaseCommitActionExecutor: Auto commit disabled for 20240105024343265
24/01/05 02:43:49 INFO spark.SparkContext: Starting job: sum at StreamSync.java:783
24/01/05 02:43:49 INFO scheduler.DAGScheduler: Registering RDD 50 (mapToPair at HoodieJavaRDD.java:149)
24/01/05 02:43:49 INFO scheduler.DAGScheduler: Got job 11 (sum at StreamSync.java:783) with 1 output partitions
24/01/05 02:43:49 INFO scheduler.DAGScheduler: Final stage: ResultStage 23 (sum at StreamSync.java:783)
24/01/05 02:43:49 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 22)
24/01/05 02:43:49 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 22)
24/01/05 02:43:49 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 22 (MapPartitionsRDD[50] at mapToPair at HoodieJavaRDD.java:149), which has no missing parents
24/01/05 02:43:49 INFO memory.MemoryStore: Block broadcast_17 stored as values in memory (estimated size 344.3 KB, free 364.4 MB)
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 208
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 312
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 213
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 416
24/01/05 02:43:49 INFO storage.BlockManagerInfo: Removed broadcast_12_piece0 on adhoc-1:43055 in memory (size: 122.1 KB, free: 366.0 MB)
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 401
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 256
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 266
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 207
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 415
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 419
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 268
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 320
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 367
24/01/05 02:43:49 INFO memory.MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 123.0 KB, free 364.7 MB)
24/01/05 02:43:49 INFO storage.BlockManagerInfo: Removed broadcast_14_piece0 on adhoc-1:43055 in memory (size: 4.0 KB, free: 366.0 MB)
24/01/05 02:43:49 INFO storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on adhoc-1:43055 (size: 123.0 KB, free: 365.9 MB)
24/01/05 02:43:49 INFO spark.SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1161
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 388
24/01/05 02:43:49 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 22 (MapPartitionsRDD[50] at mapToPair at HoodieJavaRDD.java:149) (first 15 tasks are for partitions Vector(0, 1))
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 421
24/01/05 02:43:49 INFO scheduler.TaskSchedulerImpl: Adding task set 22.0 with 2 tasks
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 235
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 260
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 418
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 197
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 274
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 232
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 271
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 281
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 344
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 246
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 195
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 283
24/01/05 02:43:49 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 22.0 (TID 22, localhost, executor driver, partition 0, PROCESS_LOCAL, 7714 bytes)
24/01/05 02:43:49 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 22.0 (TID 23, localhost, executor driver, partition 1, PROCESS_LOCAL, 7714 bytes)
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned shuffle 0
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 346
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 188
24/01/05 02:43:49 INFO executor.Executor: Running task 0.0 in stage 22.0 (TID 22)
24/01/05 02:43:49 INFO executor.Executor: Running task 1.0 in stage 22.0 (TID 23)
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 362
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 288
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 307
24/01/05 02:43:49 INFO storage.BlockManager: Removing RDD 19
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned RDD 19
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 389
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 261
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 368
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 348
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 308
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 369
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 225
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 177
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 365
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 175
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 379
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 220
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 366
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 317
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 231
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 241
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 392
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 397
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 224
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 296
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 354
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 382
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 407
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 192
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 423
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 223
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 310
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 337
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 240
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 267
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 279
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 406
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 325
24/01/05 02:43:49 INFO storage.BlockManagerInfo: Removed broadcast_10_piece0 on adhoc-1:43055 in memory (size: 2.4 KB, free: 365.9 MB)
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 378
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 383
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 215
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 193
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 319
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 321
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 290
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 340
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 259
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 345
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 284
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 323
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 381
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 277
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 411
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 318
24/01/05 02:43:49 INFO storage.BlockManagerInfo: Removed broadcast_8_piece0 on adhoc-1:43055 in memory (size: 13.9 KB, free: 365.9 MB)
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 371
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 336
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 311
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 198
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 209
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 285
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 276
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 250
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 356
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 402
24/01/05 02:43:49 INFO storage.BlockManagerInfo: Removed broadcast_7_piece0 on adhoc-1:43055 in memory (size: 26.8 KB, free: 365.9 MB)
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned shuffle 5
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 390
24/01/05 02:43:49 INFO storage.BlockManagerInfo: Removed broadcast_16_piece0 on adhoc-1:43055 in memory (size: 121.4 KB, free: 366.0 MB)
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 265
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 293
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 211
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 227
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 324
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 294
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 291
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 303
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 359
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 184
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 328
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 297
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 424
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 230
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 217
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 243
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 263
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 326
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 196
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 236
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 343
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 338
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 360
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 380
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 420
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 322
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 350
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 408
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 214
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 289
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 226
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 396
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 387
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 414
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 239
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 302
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 245
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 222
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 349
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 251
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 180
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 182
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 306
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 203
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 249
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 270
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 242
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 342
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 331
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 181
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 254
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 315
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 199
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 287
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 212
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 404
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 405
24/01/05 02:43:49 INFO storage.BlockManager: Found block rdd_44_0 locally
24/01/05 02:43:49 INFO storage.BlockManager: Found block rdd_44_1 locally
24/01/05 02:43:49 INFO storage.BlockManagerInfo: Removed broadcast_11_piece0 on adhoc-1:43055 in memory (size: 121.2 KB, free: 366.1 MB)
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 352
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 398
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 216
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 327
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 210
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 309
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 409
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned shuffle 1
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 330
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 412
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 347
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 361
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 314
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 205
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 299
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 358
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 185
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 385
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 400
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 252
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 355
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 298
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 364
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 339
24/01/05 02:43:49 INFO storage.BlockManagerInfo: Removed broadcast_15_piece0 on adhoc-1:43055 in memory (size: 2.1 KB, free: 366.1 MB)
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 384
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 394
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 410
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 417
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 304
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 218
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 189
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 229
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 206
24/01/05 02:43:49 INFO storage.BlockManagerInfo: Removed broadcast_13_piece0 on adhoc-1:43055 in memory (size: 11.4 KB, free: 366.2 MB)
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 316
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 234
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 183
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 393
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 272
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 264
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 370
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 403
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 237
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 186
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 194
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 257
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 200
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 334
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 292
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 413
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 191
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 374
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 176
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 341
24/01/05 02:43:49 INFO storage.BlockManagerInfo: Removed broadcast_9_piece0 on adhoc-1:43055 in memory (size: 11.5 KB, free: 366.2 MB)
24/01/05 02:43:49 INFO executor.Executor: Finished task 0.0 in stage 22.0 (TID 22). 907 bytes result sent to driver
24/01/05 02:43:49 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 22.0 (TID 22) in 41 ms on localhost (executor driver) (1/2)
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 386
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 255
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 333
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 313
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 248
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 357
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 247
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 204
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 258
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 295
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 300
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 190
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 275
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 228
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 233
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 273
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 377
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 201
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 373
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 376
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 253
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 375
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 244
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 332
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 286
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 221
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 238
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 280
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 391
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 335
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 219
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 301
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 363
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 329
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 395
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 187
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 202
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 178
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 422
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 353
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 305
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 262
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 399
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 372
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 278
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 269
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 282
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 179
24/01/05 02:43:49 INFO spark.ContextCleaner: Cleaned accumulator 351
24/01/05 02:43:49 INFO executor.Executor: Finished task 1.0 in stage 22.0 (TID 23). 950 bytes result sent to driver
24/01/05 02:43:49 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 22.0 (TID 23) in 44 ms on localhost (executor driver) (2/2)
24/01/05 02:43:49 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 22.0, whose tasks have all completed, from pool 
24/01/05 02:43:49 INFO scheduler.DAGScheduler: ShuffleMapStage 22 (mapToPair at HoodieJavaRDD.java:149) finished in 0.102 s
24/01/05 02:43:49 INFO scheduler.DAGScheduler: looking for newly runnable stages
24/01/05 02:43:49 INFO scheduler.DAGScheduler: running: Set()
24/01/05 02:43:49 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 23)
24/01/05 02:43:49 INFO scheduler.DAGScheduler: failed: Set()
24/01/05 02:43:49 INFO scheduler.DAGScheduler: Submitting ResultStage 23 (MapPartitionsRDD[55] at mapToDouble at StreamSync.java:783), which has no missing parents
24/01/05 02:43:49 INFO memory.MemoryStore: Block broadcast_18 stored as values in memory (estimated size 351.4 KB, free 365.5 MB)
24/01/05 02:43:49 INFO memory.MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 127.2 KB, free 365.4 MB)
24/01/05 02:43:49 INFO storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on adhoc-1:43055 (size: 127.2 KB, free: 366.0 MB)
24/01/05 02:43:49 INFO spark.SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1161
24/01/05 02:43:49 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 23 (MapPartitionsRDD[55] at mapToDouble at StreamSync.java:783) (first 15 tasks are for partitions Vector(0))
24/01/05 02:43:49 INFO scheduler.TaskSchedulerImpl: Adding task set 23.0 with 1 tasks
24/01/05 02:43:49 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 23.0 (TID 24, localhost, executor driver, partition 0, ANY, 7662 bytes)
24/01/05 02:43:49 INFO executor.Executor: Running task 0.0 in stage 23.0 (TID 24)
24/01/05 02:43:49 INFO storage.ShuffleBlockFetcherIterator: Getting 2 non-empty blocks including 2 local blocks and 0 remote blocks
24/01/05 02:43:49 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
24/01/05 02:43:49 INFO queue.SimpleExecutor: Starting consumer, consuming records from the records iterator directly
24/01/05 02:43:49 WARN marker.WriteMarkersFactory: Timeline-server-based markers are not supported for HDFS: base path /user/hive/warehouse/stock_ticks_mor.  Falling back to direct markers.
24/01/05 02:43:49 INFO marker.DirectWriteMarkers: Creating Marker Path=/user/hive/warehouse/stock_ticks_mor/.hoodie/.temp/20240105024343265/2018/08/31/6ad87b7b-db22-4d47-afc3-cb2b288006ba-0_0-23-24_20240105024343265.parquet.marker.CREATE
24/01/05 02:43:49 INFO marker.DirectWriteMarkers: [direct] Created marker file /user/hive/warehouse/stock_ticks_mor/.hoodie/.temp/20240105024343265/2018/08/31/6ad87b7b-db22-4d47-afc3-cb2b288006ba-0_0-23-24_20240105024343265.parquet.marker.CREATE in 7 ms
24/01/05 02:43:49 INFO compress.CodecPool: Got brand-new compressor [.gz]
24/01/05 02:43:49 INFO io.HoodieCreateHandle: New CreateHandle for partition :2018/08/31 with fileId 6ad87b7b-db22-4d47-afc3-cb2b288006ba-0
24/01/05 02:43:49 INFO io.HoodieCreateHandle: Closing the file 6ad87b7b-db22-4d47-afc3-cb2b288006ba-0 as we are done with all the records 102
24/01/05 02:43:49 INFO hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 19221
24/01/05 02:43:49 INFO io.HoodieCreateHandle: CreateHandle for partitionPath 2018/08/31 fileID 6ad87b7b-db22-4d47-afc3-cb2b288006ba-0, took 523 ms.
24/01/05 02:43:49 INFO memory.MemoryStore: Block rdd_54_0 stored as values in memory (estimated size 376.0 B, free 365.4 MB)
24/01/05 02:43:49 INFO storage.BlockManagerInfo: Added rdd_54_0 in memory on adhoc-1:43055 (size: 376.0 B, free: 366.0 MB)
24/01/05 02:43:49 INFO executor.Executor: Finished task 0.0 in stage 23.0 (TID 24). 974 bytes result sent to driver
24/01/05 02:43:49 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 23.0 (TID 24) in 555 ms on localhost (executor driver) (1/1)
24/01/05 02:43:49 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 23.0, whose tasks have all completed, from pool 
24/01/05 02:43:49 INFO scheduler.DAGScheduler: ResultStage 23 (sum at StreamSync.java:783) finished in 0.591 s
24/01/05 02:43:49 INFO scheduler.DAGScheduler: Job 11 finished: sum at StreamSync.java:783, took 0.699029 s
24/01/05 02:43:49 INFO spark.SparkContext: Starting job: sum at StreamSync.java:784
24/01/05 02:43:49 INFO scheduler.DAGScheduler: Got job 12 (sum at StreamSync.java:784) with 1 output partitions
24/01/05 02:43:49 INFO scheduler.DAGScheduler: Final stage: ResultStage 28 (sum at StreamSync.java:784)
24/01/05 02:43:49 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 27)
24/01/05 02:43:49 INFO scheduler.DAGScheduler: Missing parents: List()
24/01/05 02:43:49 INFO scheduler.DAGScheduler: Submitting ResultStage 28 (MapPartitionsRDD[57] at mapToDouble at StreamSync.java:784), which has no missing parents
24/01/05 02:43:49 INFO memory.MemoryStore: Block broadcast_19 stored as values in memory (estimated size 351.3 KB, free 365.0 MB)
24/01/05 02:43:49 INFO memory.MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 127.2 KB, free 364.9 MB)
24/01/05 02:43:49 INFO storage.BlockManagerInfo: Added broadcast_19_piece0 in memory on adhoc-1:43055 (size: 127.2 KB, free: 365.9 MB)
24/01/05 02:43:49 INFO spark.SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1161
24/01/05 02:43:49 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 28 (MapPartitionsRDD[57] at mapToDouble at StreamSync.java:784) (first 15 tasks are for partitions Vector(0))
24/01/05 02:43:49 INFO scheduler.TaskSchedulerImpl: Adding task set 28.0 with 1 tasks
24/01/05 02:43:49 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 28.0 (TID 25, localhost, executor driver, partition 0, PROCESS_LOCAL, 7662 bytes)
24/01/05 02:43:49 INFO executor.Executor: Running task 0.0 in stage 28.0 (TID 25)
24/01/05 02:43:49 INFO storage.BlockManager: Found block rdd_54_0 locally
24/01/05 02:43:49 INFO executor.Executor: Finished task 0.0 in stage 28.0 (TID 25). 759 bytes result sent to driver
24/01/05 02:43:49 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 28.0 (TID 25) in 38 ms on localhost (executor driver) (1/1)
24/01/05 02:43:49 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 28.0, whose tasks have all completed, from pool 
24/01/05 02:43:49 INFO scheduler.DAGScheduler: ResultStage 28 (sum at StreamSync.java:784) finished in 0.070 s
24/01/05 02:43:49 INFO scheduler.DAGScheduler: Job 12 finished: sum at StreamSync.java:784, took 0.072534 s
24/01/05 02:43:49 INFO streamer.StreamSync: instantTime=20240105024343265, totalRecords=102, totalErrorRecords=0, totalSuccessfulRecords=102
24/01/05 02:43:49 INFO spark.SparkContext: Starting job: collect at SparkRDDWriteClient.java:103
24/01/05 02:43:49 INFO scheduler.DAGScheduler: Got job 13 (collect at SparkRDDWriteClient.java:103) with 1 output partitions
24/01/05 02:43:49 INFO scheduler.DAGScheduler: Final stage: ResultStage 33 (collect at SparkRDDWriteClient.java:103)
24/01/05 02:43:49 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 32)
24/01/05 02:43:49 INFO scheduler.DAGScheduler: Missing parents: List()
24/01/05 02:43:49 INFO scheduler.DAGScheduler: Submitting ResultStage 33 (MapPartitionsRDD[59] at map at SparkRDDWriteClient.java:103), which has no missing parents
24/01/05 02:43:49 INFO memory.MemoryStore: Block broadcast_20 stored as values in memory (estimated size 351.6 KB, free 364.5 MB)
24/01/05 02:43:49 INFO memory.MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 127.3 KB, free 364.4 MB)
24/01/05 02:43:49 INFO storage.BlockManagerInfo: Added broadcast_20_piece0 in memory on adhoc-1:43055 (size: 127.3 KB, free: 365.8 MB)
24/01/05 02:43:49 INFO spark.SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:1161
24/01/05 02:43:49 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 33 (MapPartitionsRDD[59] at map at SparkRDDWriteClient.java:103) (first 15 tasks are for partitions Vector(0))
24/01/05 02:43:49 INFO scheduler.TaskSchedulerImpl: Adding task set 33.0 with 1 tasks
24/01/05 02:43:49 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 33.0 (TID 26, localhost, executor driver, partition 0, PROCESS_LOCAL, 7662 bytes)
24/01/05 02:43:49 INFO executor.Executor: Running task 0.0 in stage 33.0 (TID 26)
24/01/05 02:43:49 INFO storage.BlockManager: Found block rdd_54_0 locally
24/01/05 02:43:49 INFO executor.Executor: Finished task 0.0 in stage 33.0 (TID 26). 1033 bytes result sent to driver
24/01/05 02:43:49 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 33.0 (TID 26) in 17 ms on localhost (executor driver) (1/1)
24/01/05 02:43:49 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 33.0, whose tasks have all completed, from pool 
24/01/05 02:43:49 INFO scheduler.DAGScheduler: ResultStage 33 (collect at SparkRDDWriteClient.java:103) finished in 0.047 s
24/01/05 02:43:49 INFO scheduler.DAGScheduler: Job 13 finished: collect at SparkRDDWriteClient.java:103, took 0.049249 s
24/01/05 02:43:49 INFO client.BaseHoodieWriteClient: Committing 20240105024343265 action deltacommit
24/01/05 02:43:49 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:43:49 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:43:49 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:43:49 INFO table.HoodieTableMetaClient: Loading Active commit timeline for /user/hive/warehouse/stock_ticks_mor
24/01/05 02:43:49 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[==>20240105024343265__deltacommit__INFLIGHT__20240105024348971]}
24/01/05 02:43:49 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:43:49 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:43:49 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:43:49 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:43:49 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:43:49 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:43:49 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20240105024347141]}
24/01/05 02:43:49 INFO view.AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
24/01/05 02:43:49 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:43:49 INFO view.FileSystemViewManager: Creating View Manager with storage type :REMOTE_FIRST
24/01/05 02:43:49 INFO view.FileSystemViewManager: Creating remote first table view
24/01/05 02:43:49 INFO util.CommitUtils: Creating  metadata for UPSERT numWriteStats:1 numReplaceFileIds:0
24/01/05 02:43:49 INFO transaction.TransactionManager: Transaction starting for Option{val=[==>20240105024343265__deltacommit__INFLIGHT]} with latest completed transaction instant Optional.empty
24/01/05 02:43:49 INFO lock.LockManager: LockProvider org.apache.hudi.client.transaction.lock.InProcessLockProvider
24/01/05 02:43:49 INFO lock.InProcessLockProvider: Base Path /user/hive/warehouse/stock_ticks_mor, Lock Instance java.util.concurrent.locks.ReentrantReadWriteLock@592a1882[Write locks = 0, Read locks = 0], Thread main, In-process lock state ACQUIRING
24/01/05 02:43:49 INFO lock.InProcessLockProvider: Base Path /user/hive/warehouse/stock_ticks_mor, Lock Instance java.util.concurrent.locks.ReentrantReadWriteLock@592a1882[Write locks = 1, Read locks = 0], Thread main, In-process lock state ACQUIRED
24/01/05 02:43:49 INFO transaction.TransactionManager: Transaction started for Option{val=[==>20240105024343265__deltacommit__INFLIGHT]} with latest completed transaction instant Optional.empty
24/01/05 02:43:49 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:43:49 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:43:49 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:43:49 INFO table.HoodieTableMetaClient: Loading Active commit timeline for /user/hive/warehouse/stock_ticks_mor
24/01/05 02:43:49 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[==>20240105024343265__deltacommit__INFLIGHT__20240105024348971]}
24/01/05 02:43:49 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:43:49 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:43:50 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:43:50 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:43:50 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:43:50 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:43:50 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20240105024347141]}
24/01/05 02:43:50 INFO view.AbstractTableFileSystemView: Took 1 ms to read  0 instants, 0 replaced file groups
24/01/05 02:43:50 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:43:50 INFO view.FileSystemViewManager: Creating View Manager with storage type :REMOTE_FIRST
24/01/05 02:43:50 INFO view.FileSystemViewManager: Creating remote first table view
24/01/05 02:43:50 INFO client.BaseHoodieWriteClient: Committing 20240105024343265 action deltacommit
24/01/05 02:43:50 WARN marker.WriteMarkersFactory: Timeline-server-based markers are not supported for HDFS: base path /user/hive/warehouse/stock_ticks_mor.  Falling back to direct markers.
24/01/05 02:43:50 INFO spark.SparkContext: Starting job: collect at HoodieSparkEngineContext.java:150
24/01/05 02:43:50 INFO scheduler.DAGScheduler: Got job 14 (collect at HoodieSparkEngineContext.java:150) with 1 output partitions
24/01/05 02:43:50 INFO scheduler.DAGScheduler: Final stage: ResultStage 34 (collect at HoodieSparkEngineContext.java:150)
24/01/05 02:43:50 INFO scheduler.DAGScheduler: Parents of final stage: List()
24/01/05 02:43:50 INFO scheduler.DAGScheduler: Missing parents: List()
24/01/05 02:43:50 INFO scheduler.DAGScheduler: Submitting ResultStage 34 (MapPartitionsRDD[61] at flatMap at HoodieSparkEngineContext.java:150), which has no missing parents
24/01/05 02:43:50 INFO memory.MemoryStore: Block broadcast_21 stored as values in memory (estimated size 72.5 KB, free 364.4 MB)
24/01/05 02:43:50 INFO memory.MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 26.7 KB, free 364.3 MB)
24/01/05 02:43:50 INFO storage.BlockManagerInfo: Added broadcast_21_piece0 in memory on adhoc-1:43055 (size: 26.7 KB, free: 365.8 MB)
24/01/05 02:43:50 INFO spark.SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:1161
24/01/05 02:43:50 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 34 (MapPartitionsRDD[61] at flatMap at HoodieSparkEngineContext.java:150) (first 15 tasks are for partitions Vector(0))
24/01/05 02:43:50 INFO scheduler.TaskSchedulerImpl: Adding task set 34.0 with 1 tasks
24/01/05 02:43:50 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 34.0 (TID 27, localhost, executor driver, partition 0, PROCESS_LOCAL, 7820 bytes)
24/01/05 02:43:50 INFO executor.Executor: Running task 0.0 in stage 34.0 (TID 27)
24/01/05 02:43:50 INFO executor.Executor: Finished task 0.0 in stage 34.0 (TID 27). 755 bytes result sent to driver
24/01/05 02:43:50 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 34.0 (TID 27) in 12 ms on localhost (executor driver) (1/1)
24/01/05 02:43:50 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 34.0, whose tasks have all completed, from pool 
24/01/05 02:43:50 INFO scheduler.DAGScheduler: ResultStage 34 (collect at HoodieSparkEngineContext.java:150) finished in 0.024 s
24/01/05 02:43:50 INFO scheduler.DAGScheduler: Job 14 finished: collect at HoodieSparkEngineContext.java:150, took 0.025364 s
24/01/05 02:43:50 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:43:50 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:43:50 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:43:50 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:43:50 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:43:50 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:43:50 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20240105024347141]}
24/01/05 02:43:50 INFO metadata.HoodieBackedTableMetadataWriter: Async metadata indexing disabled and following partitions already initialized: [files]
24/01/05 02:43:50 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:43:50 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:43:50 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:43:50 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:43:50 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:43:50 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:43:50 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20240105024347141]}
24/01/05 02:43:50 INFO view.AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
24/01/05 02:43:50 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:43:50 INFO metadata.HoodieTableMetadataUtil: Updating at 20240105024343265 from Commit/UPSERT. #partitions_updated=2, #files_added=1
24/01/05 02:43:50 INFO view.AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
24/01/05 02:43:50 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:43:50 INFO metadata.HoodieTableMetadataUtil: Loading latest file slices for metadata table partition files
24/01/05 02:43:50 INFO view.AbstractTableFileSystemView: Took 1 ms to read  0 instants, 0 replaced file groups
24/01/05 02:43:50 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:43:50 INFO view.AbstractTableFileSystemView: Building file system view for partition (files)
24/01/05 02:43:50 INFO client.BaseHoodieClient: Embedded Timeline Server is disabled. Not starting timeline service
24/01/05 02:43:50 INFO client.BaseHoodieClient: Embedded Timeline Server is disabled. Not starting timeline service
24/01/05 02:43:50 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:43:50 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:43:50 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:43:50 INFO table.HoodieTableMetaClient: Loading Active commit timeline for /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:43:50 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20240105024347141]}
24/01/05 02:43:50 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:43:50 INFO view.FileSystemViewManager: Creating View Manager with storage type :MEMORY
24/01/05 02:43:50 INFO view.FileSystemViewManager: Creating in-memory based Table View
24/01/05 02:43:50 INFO metadata.HoodieBackedTableMetadataWriter: New commit at 20240105024343265 being applied to MDT.
24/01/05 02:43:50 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:43:50 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:43:50 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:43:50 INFO table.HoodieTableMetaClient: Loading Active commit timeline for /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:43:50 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20240105024347141]}
24/01/05 02:43:50 INFO util.CleanerUtils: Cleaned failed attempts if any
24/01/05 02:43:50 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:43:50 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:43:50 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:43:50 INFO table.HoodieTableMetaClient: Loading Active commit timeline for /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:43:50 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20240105024347141]}
24/01/05 02:43:50 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:43:50 INFO view.FileSystemViewManager: Creating View Manager with storage type :MEMORY
24/01/05 02:43:50 INFO view.FileSystemViewManager: Creating in-memory based Table View
24/01/05 02:43:50 INFO client.BaseHoodieWriteClient: Generate a new instant time: 20240105024343265 action: deltacommit
24/01/05 02:43:50 INFO timeline.HoodieActiveTimeline: Creating a new instant [==>20240105024343265__deltacommit__REQUESTED]
24/01/05 02:43:50 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:43:50 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:43:50 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:43:50 INFO table.HoodieTableMetaClient: Loading Active commit timeline for /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:43:50 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[==>20240105024343265__deltacommit__REQUESTED__20240105024350145]}
24/01/05 02:43:50 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:43:50 INFO view.FileSystemViewManager: Creating View Manager with storage type :MEMORY
24/01/05 02:43:50 INFO view.FileSystemViewManager: Creating in-memory based Table View
24/01/05 02:43:50 INFO async.AsyncCleanerService: The HoodieWriteClient is not configured to auto & async clean. Async clean service will not start.
24/01/05 02:43:50 INFO async.AsyncArchiveService: The HoodieWriteClient is not configured to auto & async archive. Async archive service will not start.
24/01/05 02:43:50 INFO view.AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
24/01/05 02:43:50 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:43:50 INFO spark.SparkContext: Starting job: countByKey at HoodieJavaPairRDD.java:105
24/01/05 02:43:50 INFO scheduler.DAGScheduler: Registering RDD 70 (countByKey at HoodieJavaPairRDD.java:105)
24/01/05 02:43:50 INFO scheduler.DAGScheduler: Got job 15 (countByKey at HoodieJavaPairRDD.java:105) with 1 output partitions
24/01/05 02:43:50 INFO scheduler.DAGScheduler: Final stage: ResultStage 36 (countByKey at HoodieJavaPairRDD.java:105)
24/01/05 02:43:50 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 35)
24/01/05 02:43:50 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 35)
24/01/05 02:43:50 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 35 (MapPartitionsRDD[70] at countByKey at HoodieJavaPairRDD.java:105), which has no missing parents
24/01/05 02:43:50 INFO memory.MemoryStore: Block broadcast_22 stored as values in memory (estimated size 8.6 KB, free 364.3 MB)
24/01/05 02:43:50 INFO memory.MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 4.7 KB, free 364.3 MB)
24/01/05 02:43:50 INFO storage.BlockManagerInfo: Added broadcast_22_piece0 in memory on adhoc-1:43055 (size: 4.7 KB, free: 365.8 MB)
24/01/05 02:43:50 INFO spark.SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:1161
24/01/05 02:43:50 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 35 (MapPartitionsRDD[70] at countByKey at HoodieJavaPairRDD.java:105) (first 15 tasks are for partitions Vector(0))
24/01/05 02:43:50 INFO scheduler.TaskSchedulerImpl: Adding task set 35.0 with 1 tasks
24/01/05 02:43:50 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 35.0 (TID 28, localhost, executor driver, partition 0, PROCESS_LOCAL, 8086 bytes)
24/01/05 02:43:50 INFO executor.Executor: Running task 0.0 in stage 35.0 (TID 28)
24/01/05 02:43:50 INFO memory.MemoryStore: Block rdd_68_0 stored as values in memory (estimated size 398.0 B, free 364.3 MB)
24/01/05 02:43:50 INFO storage.BlockManagerInfo: Added rdd_68_0 in memory on adhoc-1:43055 (size: 398.0 B, free: 365.8 MB)
24/01/05 02:43:50 INFO executor.Executor: Finished task 0.0 in stage 35.0 (TID 28). 950 bytes result sent to driver
24/01/05 02:43:50 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 35.0 (TID 28) in 21 ms on localhost (executor driver) (1/1)
24/01/05 02:43:50 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 35.0, whose tasks have all completed, from pool 
24/01/05 02:43:50 INFO scheduler.DAGScheduler: ShuffleMapStage 35 (countByKey at HoodieJavaPairRDD.java:105) finished in 0.028 s
24/01/05 02:43:50 INFO scheduler.DAGScheduler: looking for newly runnable stages
24/01/05 02:43:50 INFO scheduler.DAGScheduler: running: Set()
24/01/05 02:43:50 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 36)
24/01/05 02:43:50 INFO scheduler.DAGScheduler: failed: Set()
24/01/05 02:43:50 INFO scheduler.DAGScheduler: Submitting ResultStage 36 (ShuffledRDD[71] at countByKey at HoodieJavaPairRDD.java:105), which has no missing parents
24/01/05 02:43:50 INFO memory.MemoryStore: Block broadcast_23 stored as values in memory (estimated size 3.6 KB, free 364.3 MB)
24/01/05 02:43:50 INFO memory.MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 2.1 KB, free 364.3 MB)
24/01/05 02:43:50 INFO storage.BlockManagerInfo: Added broadcast_23_piece0 in memory on adhoc-1:43055 (size: 2.1 KB, free: 365.8 MB)
24/01/05 02:43:50 INFO spark.SparkContext: Created broadcast 23 from broadcast at DAGScheduler.scala:1161
24/01/05 02:43:50 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 36 (ShuffledRDD[71] at countByKey at HoodieJavaPairRDD.java:105) (first 15 tasks are for partitions Vector(0))
24/01/05 02:43:50 INFO scheduler.TaskSchedulerImpl: Adding task set 36.0 with 1 tasks
24/01/05 02:43:50 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 36.0 (TID 29, localhost, executor driver, partition 0, ANY, 7662 bytes)
24/01/05 02:43:50 INFO executor.Executor: Running task 0.0 in stage 36.0 (TID 29)
24/01/05 02:43:50 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
24/01/05 02:43:50 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
24/01/05 02:43:50 INFO executor.Executor: Finished task 0.0 in stage 36.0 (TID 29). 1136 bytes result sent to driver
24/01/05 02:43:50 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 36.0 (TID 29) in 12 ms on localhost (executor driver) (1/1)
24/01/05 02:43:50 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 36.0, whose tasks have all completed, from pool 
24/01/05 02:43:50 INFO scheduler.DAGScheduler: ResultStage 36 (countByKey at HoodieJavaPairRDD.java:105) finished in 0.018 s
24/01/05 02:43:50 INFO scheduler.DAGScheduler: Job 15 finished: countByKey at HoodieJavaPairRDD.java:105, took 0.048139 s
24/01/05 02:43:50 INFO commit.UpsertPartitioner: AvgRecordSize => 1024
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 594
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 431
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 439
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 520
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 440
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 429
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 506
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 450
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 464
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 581
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 508
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 433
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 514
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 575
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 446
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 503
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 430
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 427
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 434
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 512
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 560
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 443
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 588
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 595
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 515
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 540
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 507
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 449
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 473
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 553
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 552
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 461
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 571
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 528
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 479
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 555
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 576
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 436
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 564
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 445
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 526
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 441
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 487
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 518
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 453
24/01/05 02:43:50 INFO storage.BlockManagerInfo: Removed broadcast_23_piece0 on adhoc-1:43055 in memory (size: 2.1 KB, free: 365.8 MB)
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 432
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 435
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 546
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 448
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 589
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 437
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 463
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 572
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 527
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 574
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 444
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 545
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 562
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 596
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 459
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 578
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 519
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 583
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 529
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 592
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 559
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 497
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 541
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 566
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 428
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 577
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 481
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 502
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 569
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 537
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 585
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 455
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 472
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 493
24/01/05 02:43:50 INFO storage.BlockManagerInfo: Removed broadcast_20_piece0 on adhoc-1:43055 in memory (size: 127.3 KB, free: 365.9 MB)
24/01/05 02:43:50 INFO storage.BlockManagerInfo: Removed broadcast_21_piece0 on adhoc-1:43055 in memory (size: 26.7 KB, free: 365.9 MB)
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 471
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 475
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 524
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 523
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 452
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 484
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 573
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 534
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 535
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 522
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 510
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 549
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 597
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 495
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 516
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 474
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 500
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 470
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 451
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 561
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 557
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 547
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 580
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 554
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 425
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 467
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 482
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 531
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 485
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 496
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 476
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 521
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 570
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 511
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 498
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 568
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 442
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 426
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 458
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 567
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 550
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 591
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 536
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 486
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 488
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 491
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned shuffle 7
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 505
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 460
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 533
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 586
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 478
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 456
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 551
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 539
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 447
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 543
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 599
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 454
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 542
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 544
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 438
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 469
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 499
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 590
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 477
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 480
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 457
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 483
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 468
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 598
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 513
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 490
24/01/05 02:43:50 INFO storage.BlockManagerInfo: Removed broadcast_19_piece0 on adhoc-1:43055 in memory (size: 127.2 KB, free: 366.0 MB)
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 530
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 462
24/01/05 02:43:50 INFO storage.BlockManagerInfo: Removed broadcast_18_piece0 on adhoc-1:43055 in memory (size: 127.2 KB, free: 366.2 MB)
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 465
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 548
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 582
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 563
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 466
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 517
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 579
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 538
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 504
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 525
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 556
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 587
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 532
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 558
24/01/05 02:43:50 INFO storage.BlockManagerInfo: Removed broadcast_22_piece0 on adhoc-1:43055 in memory (size: 4.7 KB, free: 366.2 MB)
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 494
24/01/05 02:43:50 INFO storage.BlockManagerInfo: Removed broadcast_17_piece0 on adhoc-1:43055 in memory (size: 123.0 KB, free: 366.3 MB)
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 501
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 509
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 593
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 584
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 565
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 489
24/01/05 02:43:50 INFO spark.ContextCleaner: Cleaned accumulator 492
24/01/05 02:43:50 INFO spark.SparkContext: Starting job: collectAsMap at UpsertPartitioner.java:282
24/01/05 02:43:50 INFO scheduler.DAGScheduler: Got job 16 (collectAsMap at UpsertPartitioner.java:282) with 1 output partitions
24/01/05 02:43:50 INFO scheduler.DAGScheduler: Final stage: ResultStage 37 (collectAsMap at UpsertPartitioner.java:282)
24/01/05 02:43:50 INFO scheduler.DAGScheduler: Parents of final stage: List()
24/01/05 02:43:50 INFO scheduler.DAGScheduler: Missing parents: List()
24/01/05 02:43:50 INFO scheduler.DAGScheduler: Submitting ResultStage 37 (MapPartitionsRDD[73] at mapToPair at UpsertPartitioner.java:281), which has no missing parents
24/01/05 02:43:50 INFO memory.MemoryStore: Block broadcast_24 stored as values in memory (estimated size 268.9 KB, free 366.0 MB)
24/01/05 02:43:50 INFO memory.MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 95.8 KB, free 365.9 MB)
24/01/05 02:43:50 INFO storage.BlockManagerInfo: Added broadcast_24_piece0 in memory on adhoc-1:43055 (size: 95.8 KB, free: 366.2 MB)
24/01/05 02:43:50 INFO spark.SparkContext: Created broadcast 24 from broadcast at DAGScheduler.scala:1161
24/01/05 02:43:50 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 37 (MapPartitionsRDD[73] at mapToPair at UpsertPartitioner.java:281) (first 15 tasks are for partitions Vector(0))
24/01/05 02:43:50 INFO scheduler.TaskSchedulerImpl: Adding task set 37.0 with 1 tasks
24/01/05 02:43:50 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 37.0 (TID 30, localhost, executor driver, partition 0, PROCESS_LOCAL, 7730 bytes)
24/01/05 02:43:50 INFO executor.Executor: Running task 0.0 in stage 37.0 (TID 30)
24/01/05 02:43:50 INFO view.FileSystemViewManager: Creating View Manager with storage type :MEMORY
24/01/05 02:43:50 INFO view.FileSystemViewManager: Creating in-memory based Table View
24/01/05 02:43:50 INFO view.FileSystemViewManager: Creating InMemory based view for basePath /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:43:50 INFO view.AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
24/01/05 02:43:50 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:43:50 INFO view.AbstractTableFileSystemView: Building file system view for partition (files)
24/01/05 02:43:50 INFO executor.Executor: Finished task 0.0 in stage 37.0 (TID 30). 700 bytes result sent to driver
24/01/05 02:43:50 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 37.0 (TID 30) in 20 ms on localhost (executor driver) (1/1)
24/01/05 02:43:50 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 37.0, whose tasks have all completed, from pool 
24/01/05 02:43:50 INFO scheduler.DAGScheduler: ResultStage 37 (collectAsMap at UpsertPartitioner.java:282) finished in 0.043 s
24/01/05 02:43:50 INFO scheduler.DAGScheduler: Job 16 finished: collectAsMap at UpsertPartitioner.java:282, took 0.045206 s
24/01/05 02:43:50 INFO view.AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
24/01/05 02:43:50 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:43:50 INFO commit.UpsertPartitioner: Total Buckets: 1
24/01/05 02:43:50 INFO timeline.HoodieActiveTimeline: Checking for file exists ?/user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/20240105024343265.deltacommit.requested
24/01/05 02:43:50 INFO timeline.HoodieActiveTimeline: Create new file for toInstant ?/user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/20240105024343265.deltacommit.inflight
24/01/05 02:43:50 INFO commit.BaseSparkCommitActionExecutor: no validators configured.
24/01/05 02:43:50 INFO commit.BaseCommitActionExecutor: Auto commit enabled: Committing 20240105024343265
24/01/05 02:43:50 INFO spark.SparkContext: Starting job: collect at HoodieJavaRDD.java:177
24/01/05 02:43:50 INFO scheduler.DAGScheduler: Registering RDD 74 (mapToPair at HoodieJavaRDD.java:149)
24/01/05 02:43:50 INFO scheduler.DAGScheduler: Got job 17 (collect at HoodieJavaRDD.java:177) with 1 output partitions
24/01/05 02:43:50 INFO scheduler.DAGScheduler: Final stage: ResultStage 39 (collect at HoodieJavaRDD.java:177)
24/01/05 02:43:50 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 38)
24/01/05 02:43:50 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 38)
24/01/05 02:43:50 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 38 (MapPartitionsRDD[74] at mapToPair at HoodieJavaRDD.java:149), which has no missing parents
24/01/05 02:43:50 INFO memory.MemoryStore: Block broadcast_25 stored as values in memory (estimated size 273.6 KB, free 365.7 MB)
24/01/05 02:43:50 INFO memory.MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 95.5 KB, free 365.6 MB)
24/01/05 02:43:50 INFO storage.BlockManagerInfo: Added broadcast_25_piece0 in memory on adhoc-1:43055 (size: 95.5 KB, free: 366.1 MB)
24/01/05 02:43:50 INFO spark.SparkContext: Created broadcast 25 from broadcast at DAGScheduler.scala:1161
24/01/05 02:43:50 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 38 (MapPartitionsRDD[74] at mapToPair at HoodieJavaRDD.java:149) (first 15 tasks are for partitions Vector(0))
24/01/05 02:43:50 INFO scheduler.TaskSchedulerImpl: Adding task set 38.0 with 1 tasks
24/01/05 02:43:50 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 38.0 (TID 31, localhost, executor driver, partition 0, PROCESS_LOCAL, 8086 bytes)
24/01/05 02:43:50 INFO executor.Executor: Running task 0.0 in stage 38.0 (TID 31)
24/01/05 02:43:50 INFO storage.BlockManager: Found block rdd_68_0 locally
24/01/05 02:43:50 INFO executor.Executor: Finished task 0.0 in stage 38.0 (TID 31). 907 bytes result sent to driver
24/01/05 02:43:50 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 38.0 (TID 31) in 28 ms on localhost (executor driver) (1/1)
24/01/05 02:43:50 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 38.0, whose tasks have all completed, from pool 
24/01/05 02:43:50 INFO scheduler.DAGScheduler: ShuffleMapStage 38 (mapToPair at HoodieJavaRDD.java:149) finished in 0.051 s
24/01/05 02:43:50 INFO scheduler.DAGScheduler: looking for newly runnable stages
24/01/05 02:43:50 INFO scheduler.DAGScheduler: running: Set()
24/01/05 02:43:50 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 39)
24/01/05 02:43:50 INFO scheduler.DAGScheduler: failed: Set()
24/01/05 02:43:50 INFO scheduler.DAGScheduler: Submitting ResultStage 39 (MapPartitionsRDD[79] at map at HoodieJavaRDD.java:125), which has no missing parents
24/01/05 02:43:50 INFO memory.MemoryStore: Block broadcast_26 stored as values in memory (estimated size 348.5 KB, free 365.2 MB)
24/01/05 02:43:50 INFO memory.MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 126.8 KB, free 365.1 MB)
24/01/05 02:43:50 INFO storage.BlockManagerInfo: Added broadcast_26_piece0 in memory on adhoc-1:43055 (size: 126.8 KB, free: 366.0 MB)
24/01/05 02:43:50 INFO spark.SparkContext: Created broadcast 26 from broadcast at DAGScheduler.scala:1161
24/01/05 02:43:50 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 39 (MapPartitionsRDD[79] at map at HoodieJavaRDD.java:125) (first 15 tasks are for partitions Vector(0))
24/01/05 02:43:50 INFO scheduler.TaskSchedulerImpl: Adding task set 39.0 with 1 tasks
24/01/05 02:43:50 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 39.0 (TID 32, localhost, executor driver, partition 0, ANY, 7662 bytes)
24/01/05 02:43:50 INFO executor.Executor: Running task 0.0 in stage 39.0 (TID 32)
24/01/05 02:43:50 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
24/01/05 02:43:50 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
24/01/05 02:43:50 INFO deltacommit.BaseSparkDeltaCommitActionExecutor: Merging updates for commit 20240105024343265 for file files-0000-0
24/01/05 02:43:50 INFO view.FileSystemViewManager: Creating View Manager with storage type :MEMORY
24/01/05 02:43:50 INFO view.FileSystemViewManager: Creating in-memory based Table View
24/01/05 02:43:50 INFO view.FileSystemViewManager: Creating InMemory based view for basePath /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:43:50 INFO view.AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
24/01/05 02:43:50 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:43:50 INFO view.AbstractTableFileSystemView: Building file system view for partition (files)
# WARNING: Unable to attach Serviceability Agent. Unable to attach even with module exceptions: [org.apache.hudi.org.openjdk.jol.vm.sa.SASupportException: Sense failed., org.apache.hudi.org.openjdk.jol.vm.sa.SASupportException: Sense failed., org.apache.hudi.org.openjdk.jol.vm.sa.SASupportException: Sense failed.]
24/01/05 02:43:51 INFO marker.DirectWriteMarkers: Creating Marker Path=/user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/.temp/20240105024343265/files/files-0000-0_0-39-32_00000000000000010.hfile.marker.APPEND
24/01/05 02:43:51 INFO marker.DirectWriteMarkers: [direct] Created marker file /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/.temp/20240105024343265/files/files-0000-0_0-39-32_00000000000000010.hfile.marker.APPEND in 5 ms
24/01/05 02:43:51 INFO log.HoodieLogFormat$WriterBuilder: Building HoodieLogFormat Writer
24/01/05 02:43:51 INFO log.HoodieLogFormat$WriterBuilder: HoodieLogFile on path /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/files/.files-0000-0_00000000000000010.log.1_0-0-0
24/01/05 02:43:51 INFO log.HoodieLogFormatWriter: HoodieLogFile{pathStr='/user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/files/.files-0000-0_00000000000000010.log.1_0-0-0', fileLen=80} exists. Appending to existing file
24/01/05 02:43:51 INFO compress.CodecPool: Got brand-new compressor [.gz]
24/01/05 02:43:51 INFO compress.CodecPool: Got brand-new compressor [.gz]
24/01/05 02:43:51 INFO io.HoodieAppendHandle: AppendHandle for partitionPath files filePath files/.files-0000-0_00000000000000010.log.1_0-0-0, took 692 ms.
24/01/05 02:43:51 INFO memory.MemoryStore: Block rdd_78_0 stored as values in memory (estimated size 437.0 B, free 365.1 MB)
24/01/05 02:43:51 INFO storage.BlockManagerInfo: Added rdd_78_0 in memory on adhoc-1:43055 (size: 437.0 B, free: 366.0 MB)
24/01/05 02:43:51 INFO executor.Executor: Finished task 0.0 in stage 39.0 (TID 32). 1442 bytes result sent to driver
24/01/05 02:43:51 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 39.0 (TID 32) in 733 ms on localhost (executor driver) (1/1)
24/01/05 02:43:51 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 39.0, whose tasks have all completed, from pool 
24/01/05 02:43:51 INFO scheduler.DAGScheduler: ResultStage 39 (collect at HoodieJavaRDD.java:177) finished in 0.761 s
24/01/05 02:43:51 INFO scheduler.DAGScheduler: Job 17 finished: collect at HoodieJavaRDD.java:177, took 0.815810 s
24/01/05 02:43:51 INFO util.CommitUtils: Creating  metadata for UPSERT_PREPPED numWriteStats:1 numReplaceFileIds:0
24/01/05 02:43:51 INFO commit.BaseSparkCommitActionExecutor: Committing 20240105024343265, action Type deltacommit, operation Type UPSERT_PREPPED
24/01/05 02:43:51 INFO spark.SparkContext: Starting job: collect at HoodieSparkEngineContext.java:150
24/01/05 02:43:51 INFO scheduler.DAGScheduler: Got job 18 (collect at HoodieSparkEngineContext.java:150) with 1 output partitions
24/01/05 02:43:51 INFO scheduler.DAGScheduler: Final stage: ResultStage 40 (collect at HoodieSparkEngineContext.java:150)
24/01/05 02:43:51 INFO scheduler.DAGScheduler: Parents of final stage: List()
24/01/05 02:43:51 INFO scheduler.DAGScheduler: Missing parents: List()
24/01/05 02:43:51 INFO scheduler.DAGScheduler: Submitting ResultStage 40 (MapPartitionsRDD[81] at flatMap at HoodieSparkEngineContext.java:150), which has no missing parents
24/01/05 02:43:51 INFO memory.MemoryStore: Block broadcast_27 stored as values in memory (estimated size 72.5 KB, free 365.0 MB)
24/01/05 02:43:51 INFO memory.MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 26.7 KB, free 365.0 MB)
24/01/05 02:43:51 INFO storage.BlockManagerInfo: Added broadcast_27_piece0 in memory on adhoc-1:43055 (size: 26.7 KB, free: 365.9 MB)
24/01/05 02:43:51 INFO spark.SparkContext: Created broadcast 27 from broadcast at DAGScheduler.scala:1161
24/01/05 02:43:51 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 40 (MapPartitionsRDD[81] at flatMap at HoodieSparkEngineContext.java:150) (first 15 tasks are for partitions Vector(0))
24/01/05 02:43:51 INFO scheduler.TaskSchedulerImpl: Adding task set 40.0 with 1 tasks
24/01/05 02:43:51 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 40.0 (TID 33, localhost, executor driver, partition 0, PROCESS_LOCAL, 7838 bytes)
24/01/05 02:43:51 INFO executor.Executor: Running task 0.0 in stage 40.0 (TID 33)
24/01/05 02:43:51 INFO executor.Executor: Finished task 0.0 in stage 40.0 (TID 33). 668 bytes result sent to driver
24/01/05 02:43:51 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 40.0 (TID 33) in 12 ms on localhost (executor driver) (1/1)
24/01/05 02:43:51 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 40.0, whose tasks have all completed, from pool 
24/01/05 02:43:51 INFO scheduler.DAGScheduler: ResultStage 40 (collect at HoodieSparkEngineContext.java:150) finished in 0.022 s
24/01/05 02:43:51 INFO scheduler.DAGScheduler: Job 18 finished: collect at HoodieSparkEngineContext.java:150, took 0.023477 s
24/01/05 02:43:51 INFO timeline.HoodieActiveTimeline: Marking instant complete [==>20240105024343265__deltacommit__INFLIGHT]
24/01/05 02:43:51 INFO timeline.HoodieActiveTimeline: Checking for file exists ?/user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/20240105024343265.deltacommit.inflight
24/01/05 02:43:52 INFO timeline.HoodieActiveTimeline: Create new file for toInstant ?/user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/20240105024343265.deltacommit
24/01/05 02:43:52 INFO timeline.HoodieActiveTimeline: Completed [==>20240105024343265__deltacommit__INFLIGHT]
24/01/05 02:43:52 INFO commit.BaseSparkCommitActionExecutor: Committed 20240105024343265
24/01/05 02:43:52 INFO spark.SparkContext: Starting job: collectAsMap at HoodieSparkEngineContext.java:164
24/01/05 02:43:52 INFO scheduler.DAGScheduler: Got job 19 (collectAsMap at HoodieSparkEngineContext.java:164) with 1 output partitions
24/01/05 02:43:52 INFO scheduler.DAGScheduler: Final stage: ResultStage 41 (collectAsMap at HoodieSparkEngineContext.java:164)
24/01/05 02:43:52 INFO scheduler.DAGScheduler: Parents of final stage: List()
24/01/05 02:43:52 INFO scheduler.DAGScheduler: Missing parents: List()
24/01/05 02:43:52 INFO scheduler.DAGScheduler: Submitting ResultStage 41 (MapPartitionsRDD[83] at mapToPair at HoodieSparkEngineContext.java:161), which has no missing parents
24/01/05 02:43:52 INFO memory.MemoryStore: Block broadcast_28 stored as values in memory (estimated size 72.8 KB, free 364.9 MB)
24/01/05 02:43:52 INFO memory.MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 26.8 KB, free 364.9 MB)
24/01/05 02:43:52 INFO storage.BlockManagerInfo: Added broadcast_28_piece0 in memory on adhoc-1:43055 (size: 26.8 KB, free: 365.9 MB)
24/01/05 02:43:52 INFO spark.SparkContext: Created broadcast 28 from broadcast at DAGScheduler.scala:1161
24/01/05 02:43:52 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 41 (MapPartitionsRDD[83] at mapToPair at HoodieSparkEngineContext.java:161) (first 15 tasks are for partitions Vector(0))
24/01/05 02:43:52 INFO scheduler.TaskSchedulerImpl: Adding task set 41.0 with 1 tasks
24/01/05 02:43:52 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 41.0 (TID 34, localhost, executor driver, partition 0, PROCESS_LOCAL, 7838 bytes)
24/01/05 02:43:52 INFO executor.Executor: Running task 0.0 in stage 41.0 (TID 34)
24/01/05 02:43:52 INFO executor.Executor: Finished task 0.0 in stage 41.0 (TID 34). 787 bytes result sent to driver
24/01/05 02:43:52 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 41.0 (TID 34) in 10 ms on localhost (executor driver) (1/1)
24/01/05 02:43:52 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 41.0, whose tasks have all completed, from pool 
24/01/05 02:43:52 INFO scheduler.DAGScheduler: ResultStage 41 (collectAsMap at HoodieSparkEngineContext.java:164) finished in 0.024 s
24/01/05 02:43:52 INFO scheduler.DAGScheduler: Job 19 finished: collectAsMap at HoodieSparkEngineContext.java:164, took 0.025037 s
24/01/05 02:43:52 INFO fs.FSUtils: Removed directory at /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/.temp/20240105024343265
24/01/05 02:43:52 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:43:52 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:43:52 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:43:52 INFO table.HoodieTableMetaClient: Loading Active commit timeline for /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:43:52 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105024343265__deltacommit__COMPLETED__20240105024352164]}
24/01/05 02:43:52 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:43:52 INFO view.FileSystemViewManager: Creating View Manager with storage type :MEMORY
24/01/05 02:43:52 INFO view.FileSystemViewManager: Creating in-memory based Table View
24/01/05 02:43:52 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105024343265__deltacommit__COMPLETED__20240105024352164]}
24/01/05 02:43:52 INFO timeline.HoodieActiveTimeline: Marking instant complete [==>20240105024343265__deltacommit__INFLIGHT]
24/01/05 02:43:52 INFO timeline.HoodieActiveTimeline: Checking for file exists ?/user/hive/warehouse/stock_ticks_mor/.hoodie/20240105024343265.deltacommit.inflight
24/01/05 02:43:52 INFO timeline.HoodieActiveTimeline: Create new file for toInstant ?/user/hive/warehouse/stock_ticks_mor/.hoodie/20240105024343265.deltacommit
24/01/05 02:43:52 INFO timeline.HoodieActiveTimeline: Completed [==>20240105024343265__deltacommit__INFLIGHT]
24/01/05 02:43:52 WARN marker.WriteMarkersFactory: Timeline-server-based markers are not supported for HDFS: base path /user/hive/warehouse/stock_ticks_mor.  Falling back to direct markers.
24/01/05 02:43:52 INFO spark.SparkContext: Starting job: collectAsMap at HoodieSparkEngineContext.java:164
24/01/05 02:43:52 INFO scheduler.DAGScheduler: Got job 20 (collectAsMap at HoodieSparkEngineContext.java:164) with 1 output partitions
24/01/05 02:43:52 INFO scheduler.DAGScheduler: Final stage: ResultStage 42 (collectAsMap at HoodieSparkEngineContext.java:164)
24/01/05 02:43:52 INFO scheduler.DAGScheduler: Parents of final stage: List()
24/01/05 02:43:52 INFO scheduler.DAGScheduler: Missing parents: List()
24/01/05 02:43:52 INFO scheduler.DAGScheduler: Submitting ResultStage 42 (MapPartitionsRDD[85] at mapToPair at HoodieSparkEngineContext.java:161), which has no missing parents
24/01/05 02:43:52 INFO memory.MemoryStore: Block broadcast_29 stored as values in memory (estimated size 72.8 KB, free 364.8 MB)
24/01/05 02:43:52 INFO memory.MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 26.8 KB, free 364.8 MB)
24/01/05 02:43:52 INFO storage.BlockManagerInfo: Added broadcast_29_piece0 in memory on adhoc-1:43055 (size: 26.8 KB, free: 365.9 MB)
24/01/05 02:43:52 INFO spark.SparkContext: Created broadcast 29 from broadcast at DAGScheduler.scala:1161
24/01/05 02:43:52 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 42 (MapPartitionsRDD[85] at mapToPair at HoodieSparkEngineContext.java:161) (first 15 tasks are for partitions Vector(0))
24/01/05 02:43:52 INFO scheduler.TaskSchedulerImpl: Adding task set 42.0 with 1 tasks
24/01/05 02:43:52 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 42.0 (TID 35, localhost, executor driver, partition 0, PROCESS_LOCAL, 7820 bytes)
24/01/05 02:43:52 INFO executor.Executor: Running task 0.0 in stage 42.0 (TID 35)
24/01/05 02:43:52 INFO executor.Executor: Finished task 0.0 in stage 42.0 (TID 35). 769 bytes result sent to driver
24/01/05 02:43:52 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 42.0 (TID 35) in 12 ms on localhost (executor driver) (1/1)
24/01/05 02:43:52 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 42.0, whose tasks have all completed, from pool 
24/01/05 02:43:52 INFO scheduler.DAGScheduler: ResultStage 42 (collectAsMap at HoodieSparkEngineContext.java:164) finished in 0.022 s
24/01/05 02:43:52 INFO scheduler.DAGScheduler: Job 20 finished: collectAsMap at HoodieSparkEngineContext.java:164, took 0.023873 s
24/01/05 02:43:52 INFO fs.FSUtils: Removed directory at /user/hive/warehouse/stock_ticks_mor/.hoodie/.temp/20240105024343265
24/01/05 02:43:52 INFO client.BaseHoodieWriteClient: Committed 20240105024343265
24/01/05 02:43:52 INFO rdd.MapPartitionsRDD: Removing RDD 44 from persistence list
24/01/05 02:43:52 INFO storage.BlockManager: Removing RDD 44
24/01/05 02:43:52 INFO rdd.MapPartitionsRDD: Removing RDD 54 from persistence list
24/01/05 02:43:52 INFO storage.BlockManager: Removing RDD 54
24/01/05 02:43:52 INFO rdd.UnionRDD: Removing RDD 68 from persistence list
24/01/05 02:43:52 INFO storage.BlockManager: Removing RDD 68
24/01/05 02:43:52 INFO rdd.MapPartitionsRDD: Removing RDD 78 from persistence list
24/01/05 02:43:52 INFO storage.BlockManager: Removing RDD 78
24/01/05 02:43:52 INFO transaction.TransactionManager: Transaction ending with transaction owner Option{val=[==>20240105024343265__deltacommit__INFLIGHT]}
24/01/05 02:43:52 INFO lock.InProcessLockProvider: Base Path /user/hive/warehouse/stock_ticks_mor, Lock Instance java.util.concurrent.locks.ReentrantReadWriteLock@592a1882[Write locks = 1, Read locks = 0], Thread main, In-process lock state RELEASING
24/01/05 02:43:52 INFO lock.InProcessLockProvider: Base Path /user/hive/warehouse/stock_ticks_mor, Lock Instance java.util.concurrent.locks.ReentrantReadWriteLock@592a1882[Write locks = 0, Read locks = 0], Thread main, In-process lock state RELEASED
24/01/05 02:43:52 INFO lock.InProcessLockProvider: Base Path /user/hive/warehouse/stock_ticks_mor, Lock Instance java.util.concurrent.locks.ReentrantReadWriteLock@592a1882[Write locks = 0, Read locks = 0], Thread main, In-process lock state ALREADY_RELEASED
24/01/05 02:43:52 INFO lock.LockManager: Released connection created for acquiring lock
24/01/05 02:43:52 INFO transaction.TransactionManager: Transaction ended with transaction owner Option{val=[==>20240105024343265__deltacommit__INFLIGHT]}
24/01/05 02:43:52 INFO client.BaseHoodieWriteClient: Start to clean synchronously.
24/01/05 02:43:52 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:43:52 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:43:52 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:43:52 INFO table.HoodieTableMetaClient: Loading Active commit timeline for /user/hive/warehouse/stock_ticks_mor
24/01/05 02:43:52 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105024343265__deltacommit__COMPLETED__20240105024352675]}
24/01/05 02:43:52 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:43:52 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:43:52 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:43:52 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:43:52 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:43:52 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:43:52 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105024343265__deltacommit__COMPLETED__20240105024352164]}
24/01/05 02:43:52 INFO view.AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
24/01/05 02:43:52 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:43:52 INFO view.FileSystemViewManager: Creating View Manager with storage type :REMOTE_FIRST
24/01/05 02:43:52 INFO view.FileSystemViewManager: Creating remote first table view
24/01/05 02:43:52 INFO client.BaseHoodieWriteClient: Cleaner started
24/01/05 02:43:52 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:43:52 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:43:52 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:43:52 INFO table.HoodieTableMetaClient: Loading Active commit timeline for /user/hive/warehouse/stock_ticks_mor
24/01/05 02:43:52 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105024343265__deltacommit__COMPLETED__20240105024352675]}
24/01/05 02:43:52 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:43:52 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:43:52 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:43:52 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:43:52 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:43:52 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:43:52 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105024343265__deltacommit__COMPLETED__20240105024352164]}
24/01/05 02:43:52 INFO view.AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
24/01/05 02:43:52 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:43:52 INFO view.FileSystemViewManager: Creating View Manager with storage type :REMOTE_FIRST
24/01/05 02:43:52 INFO view.FileSystemViewManager: Creating remote first table view
24/01/05 02:43:52 INFO client.BaseHoodieWriteClient: Scheduling cleaning at instant time :20240105024352757
24/01/05 02:43:52 INFO view.FileSystemViewManager: Creating remote view for basePath /user/hive/warehouse/stock_ticks_mor. Server=adhoc-1:37525, Timeout=300
24/01/05 02:43:52 INFO view.FileSystemViewManager: Creating InMemory based view for basePath /user/hive/warehouse/stock_ticks_mor
24/01/05 02:43:52 INFO view.AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
24/01/05 02:43:52 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:43:52 INFO view.RemoteHoodieTableFileSystemView: Sending request : (http://adhoc-1:37525/v1/hoodie/view/compactions/pending/?basepath=%2Fuser%2Fhive%2Fwarehouse%2Fstock_ticks_mor&lastinstantts=20240105024343265&timelinehash=7a0b5e1482c059272c847fc6668f12da532a637ce11f6f599d2728792005a6a1)
24/01/05 02:43:53 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:43:53 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:43:53 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:43:53 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105024343265__deltacommit__COMPLETED__20240105024352675]}
24/01/05 02:43:53 INFO collection.RocksDBDAO: DELETING RocksDB persisted at /tmp/hoodie_timeline_rocksdb/_user_hive_warehouse_stock_ticks_mor/0b7fa480-638a-4a39-bdc3-9dedc7527982
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 49
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 72
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 47
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 17
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 66
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 40
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 42
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 8
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 23
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 67
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 11
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 5
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 65
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 27
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 10
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 12
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 46
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 33
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 62
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 37
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 13
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 32
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 30
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 71
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 41
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 22
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 53
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 0
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 56
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 68
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 61
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 6
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 7
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 64
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 44
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 60
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 20
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 63
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 54
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 74
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 2
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 19
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 14
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 28
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 25
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 45
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 38
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 55
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 31
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 36
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 43
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 24
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 9
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 1
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 18
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 57
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 70
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 73
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 34
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 26
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 21
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 29
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 16
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 15
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 48
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 69
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 3
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 59
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 50
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 35
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 51
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 39
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 52
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 58
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 4
24/01/05 02:43:53 INFO collection.RocksDBDAO: No column family found. Loading default
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : RocksDB version: 7.5.3

24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Compile date 2022-09-02 09:50:20
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : DB SUMMARY

24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : DB Session ID:  S1KH7XQI06Q6BTM0HT1U

24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : SST files in /tmp/hoodie_timeline_rocksdb/_user_hive_warehouse_stock_ticks_mor/0b7fa480-638a-4a39-bdc3-9dedc7527982 dir, Total Num: 0, files: 

24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Write Ahead Log file in /tmp/hoodie_timeline_rocksdb/_user_hive_warehouse_stock_ticks_mor/0b7fa480-638a-4a39-bdc3-9dedc7527982: 

24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                         Options.error_if_exists: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                       Options.create_if_missing: 1
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                         Options.paranoid_checks: 1
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :             Options.flush_verify_memtable_count: 1
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                               Options.track_and_verify_wals_in_manifest: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :        Options.verify_sst_unique_id_in_manifest: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                                     Options.env: 0x7feb50d655e0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                                      Options.fs: PosixFileSystem
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                                Options.info_log: 0x7feb64091f38
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                Options.max_file_opening_threads: 16
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                              Options.statistics: 0x7feb64091fd0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                               Options.use_fsync: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                       Options.max_log_file_size: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                  Options.max_manifest_file_size: 1073741824
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                   Options.log_file_time_to_roll: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                       Options.keep_log_file_num: 1000
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                    Options.recycle_log_file_num: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                         Options.allow_fallocate: 1
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                        Options.allow_mmap_reads: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                       Options.allow_mmap_writes: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                        Options.use_direct_reads: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                        Options.use_direct_io_for_flush_and_compaction: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :          Options.create_missing_column_families: 1
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                              Options.db_log_dir: 
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                                 Options.wal_dir: 
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                Options.table_cache_numshardbits: 6
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                         Options.WAL_ttl_seconds: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                       Options.WAL_size_limit_MB: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                        Options.max_write_batch_group_size_bytes: 1048576
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :             Options.manifest_preallocation_size: 4194304
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                     Options.is_fd_close_on_exec: 1
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                   Options.advise_random_on_open: 1
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                    Options.db_write_buffer_size: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                    Options.write_buffer_manager: 0x7feb64108280
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :         Options.access_hint_on_compaction_start: 1
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :           Options.random_access_max_buffer_size: 1048576
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                      Options.use_adaptive_mutex: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                            Options.rate_limiter: (nil)
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :     Options.sst_file_manager.rate_bytes_per_sec: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                       Options.wal_recovery_mode: 2
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                  Options.enable_thread_tracking: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                  Options.enable_pipelined_write: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                  Options.unordered_write: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :         Options.allow_concurrent_memtable_write: 1
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :      Options.enable_write_thread_adaptive_yield: 1
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :             Options.write_thread_max_yield_usec: 100
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :            Options.write_thread_slow_yield_usec: 3
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                               Options.row_cache: None
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                              Options.wal_filter: None
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :             Options.avoid_flush_during_recovery: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :             Options.allow_ingest_behind: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :             Options.two_write_queues: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :             Options.manual_wal_flush: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :             Options.wal_compression: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :             Options.atomic_flush: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :             Options.avoid_unnecessary_blocking_io: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                 Options.persist_stats_to_disk: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                 Options.write_dbid_to_manifest: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                 Options.log_readahead_size: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                 Options.file_checksum_gen_factory: Unknown
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                 Options.best_efforts_recovery: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                Options.max_bgerror_resume_count: 2147483647
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :            Options.bgerror_resume_retry_interval: 1000000
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :             Options.allow_data_in_errors: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :             Options.db_host_id: __hostname__
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :             Options.enforce_single_del_contracts: true
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :             Options.max_background_jobs: 2
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :             Options.max_background_compactions: -1
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :             Options.max_subcompactions: 1
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :             Options.avoid_flush_during_shutdown: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :           Options.writable_file_max_buffer_size: 1048576
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :             Options.delayed_write_rate : 16777216
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :             Options.max_total_wal_size: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :             Options.delete_obsolete_files_period_micros: 21600000000
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                   Options.stats_dump_period_sec: 300
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                 Options.stats_persist_period_sec: 600
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                 Options.stats_history_buffer_size: 1048576
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                          Options.max_open_files: -1
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                          Options.bytes_per_sync: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                      Options.wal_bytes_per_sync: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                   Options.strict_bytes_per_sync: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :       Options.compaction_readahead_size: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                  Options.max_background_flushes: -1
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Compression algorithms supported:
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : 	kZSTDNotFinalCompression supported: 1
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : 	kZSTD supported: 1
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : 	kXpressCompression supported: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : 	kLZ4HCCompression supported: 1
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : 	kLZ4Compression supported: 1
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : 	kBZip2Compression supported: 1
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : 	kZlibCompression supported: 1
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : 	kSnappyCompression supported: 1
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Fast CRC32 supported: Not supported on x86
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : DMutex implementation: pthread_mutex_t
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 611
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 600
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 702
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 618
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 701
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 674
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 685
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned shuffle 8
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 656
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 673
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 724
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 654
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 727
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 720
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 625
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 607
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 635
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 735
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 648
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 610
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 690
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 609
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 660
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 675
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 644
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 696
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 730
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 737
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : [/db_impl/db_impl_open.cc:313] Creating manifest 1 

24/01/05 02:43:53 INFO storage.BlockManagerInfo: Removed broadcast_26_piece0 on adhoc-1:43055 in memory (size: 126.8 KB, free: 366.0 MB)
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 629
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 653
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 602
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 620
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 621
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 682
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 657
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 670
24/01/05 02:43:53 INFO storage.BlockManagerInfo: Removed broadcast_29_piece0 on adhoc-1:43055 in memory (size: 26.8 KB, free: 366.1 MB)
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 718
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 640
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 672
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 712
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 740
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 606
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 729
24/01/05 02:43:53 INFO storage.BlockManagerInfo: Removed broadcast_28_piece0 on adhoc-1:43055 in memory (size: 26.8 KB, free: 366.1 MB)
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : [/version_set.cc:4994] Recovering from manifest file: /tmp/hoodie_timeline_rocksdb/_user_hive_warehouse_stock_ticks_mor/0b7fa480-638a-4a39-bdc3-9dedc7527982/MANIFEST-000001

24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : [/column_family.cc:618] --------------- Options for column family [default]:

24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :               Options.comparator: leveldb.BytewiseComparator
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :           Options.merge_operator: None
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :        Options.compaction_filter: None
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :        Options.compaction_filter_factory: None
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :  Options.sst_partitioner_factory: None
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :         Options.memtable_factory: SkipListFactory
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :            Options.table_factory: BlockBasedTable
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 723
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 605
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :            table_factory options:   flush_block_policy_factory: FlushBlockBySizePolicyFactory (0x7feb641061c0)
  cache_index_and_filter_blocks: 0
  cache_index_and_filter_blocks_with_high_priority: 1
  pin_l0_filter_and_index_blocks_in_cache: 0
  pin_top_level_index_and_filter: 1
  index_type: 0
  data_block_index_type: 0
  index_shortening: 1
  data_block_hash_table_util_ratio: 0.750000
  checksum: 1
  no_block_cache: 0
  block_cache: 0x7feb641061f0
  block_cache_name: LRUCache
  block_cache_options:
    capacity : 8388608
    num_shard_bits : 4
    strict_capacity_limit : 0
    memory_allocator : None
    high_pri_pool_ratio: 0.000
  block_cache_compressed: (nil)
  persistent_cache: (nil)
  block_size: 4096
  block_size_deviation: 10
  block_restart_interval: 16
  index_block_restart_interval: 1
  metadata_block_size: 4096
  partition_filters: 0
  use_delta_encoding: 1
  filter_policy: nullptr
  whole_key_filtering: 1
  verify_compression: 0
  read_amp_bytes_per_bit: 0
  format_version: 5
  enable_index_compression: 1
  block_align: 0
  max_auto_readahead_size: 262144
  prepopulate_block_cache: 0
  initial_auto_readahead_size: 8192

24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 665
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 655
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 671
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 612
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 604
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 642
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 733
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 739
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 608
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 725
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 749
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 728
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 628
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 666
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 695
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 714
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 747
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 649
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 691
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 726
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 689
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 692
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 647
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :        Options.write_buffer_size: 67108864
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 626
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :  Options.max_write_buffer_number: 2
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 603
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :          Options.compression: Snappy
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression: Disabled
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :       Options.prefix_extractor: nullptr
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :   Options.memtable_insert_with_hint_prefix_extractor: nullptr
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 736
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :             Options.num_levels: 7
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 623
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 630
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 624
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 651
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 741
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :        Options.min_write_buffer_number_to_merge: 1
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 645
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 676
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :     Options.max_write_buffer_number_to_maintain: 0
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 678
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :     Options.max_write_buffer_size_to_maintain: 0
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 646
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 601
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :            Options.bottommost_compression_opts.window_bits: -14
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 742
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression_opts.level: 32767
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 664
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :               Options.bottommost_compression_opts.strategy: 0
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 687
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.max_dict_bytes: 0
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 637
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.zstd_max_train_bytes: 0
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 631
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.parallel_threads: 1
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 688
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression_opts.enabled: false
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.max_dict_buffer_bytes: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.use_zstd_dict_trainer: true
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :            Options.compression_opts.window_bits: -14
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                  Options.compression_opts.level: 32767
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :               Options.compression_opts.strategy: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.max_dict_bytes: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.zstd_max_train_bytes: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.use_zstd_dict_trainer: true
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.parallel_threads: 1
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                  Options.compression_opts.enabled: false
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.max_dict_buffer_bytes: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :      Options.level0_file_num_compaction_trigger: 4
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :          Options.level0_slowdown_writes_trigger: 20
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :              Options.level0_stop_writes_trigger: 36
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                   Options.target_file_size_base: 67108864
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :             Options.target_file_size_multiplier: 1
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                Options.max_bytes_for_level_base: 268435456
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Options.level_compaction_dynamic_level_bytes: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :          Options.max_bytes_for_level_multiplier: 10.000000
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[0]: 1
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[1]: 1
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[2]: 1
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[3]: 1
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[4]: 1
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[5]: 1
24/01/05 02:43:53 INFO storage.BlockManager: Removing RDD 68
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[6]: 1
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :       Options.max_sequential_skip_in_iterations: 8
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                    Options.max_compaction_bytes: 1677721600
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                        Options.arena_block_size: 1048576
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :   Options.soft_pending_compaction_bytes_limit: 68719476736
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :   Options.hard_pending_compaction_bytes_limit: 274877906944
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                Options.disable_auto_compactions: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                        Options.compaction_style: kCompactionStyleLevel
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                          Options.compaction_pri: kMinOverlappingRatio
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.size_ratio: 1
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.min_merge_width: 2
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.max_merge_width: 4294967295
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.max_size_amplification_percent: 200
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.compression_size_percent: -1
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.stop_style: kCompactionStopStyleTotalSize
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_fifo.max_table_files_size: 1073741824
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_fifo.allow_compaction: 0
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned RDD 68
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                   Options.table_properties_collectors: 
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 683
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                   Options.inplace_update_support: 0
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 715
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                 Options.inplace_update_num_locks: 10000
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 717
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :               Options.memtable_prefix_bloom_size_ratio: 0.000000
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 667
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :               Options.memtable_whole_key_filtering: 0
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 638
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :   Options.memtable_huge_page_size: 0
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 680
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                           Options.bloom_locality: 0
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 677
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                    Options.max_successive_merges: 0
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 697
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                Options.optimize_filters_for_hits: 0
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 719
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                Options.paranoid_file_checks: 0
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 738
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                Options.force_consistency_checks: 1
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                Options.report_bg_io_stats: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                               Options.ttl: 2592000
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :          Options.periodic_compaction_seconds: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :  Options.preclude_last_level_data_seconds: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                       Options.enable_blob_files: false
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                           Options.min_blob_size: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                          Options.blob_file_size: 268435456
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                   Options.blob_compression_type: NoCompression
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :          Options.enable_blob_garbage_collection: false
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :      Options.blob_garbage_collection_age_cutoff: 0.250000
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Options.blob_garbage_collection_force_threshold: 1.000000
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :          Options.blob_compaction_readahead_size: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                Options.blob_file_starting_level: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Options.experimental_mempurge_threshold: 0.000000
24/01/05 02:43:53 INFO storage.BlockManagerInfo: Removed broadcast_25_piece0 on adhoc-1:43055 in memory (size: 95.5 KB, free: 366.2 MB)
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : [/version_set.cc:5042] Recovered from manifest file:/tmp/hoodie_timeline_rocksdb/_user_hive_warehouse_stock_ticks_mor/0b7fa480-638a-4a39-bdc3-9dedc7527982/MANIFEST-000001 succeeded,manifest_file_number is 1, next_file_number is 3, last_sequence is 0, log_number is 0,prev_log_number is 0,max_column_family is 0,min_log_number_to_keep is 0

24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : [/version_set.cc:5051] Column family [default] (ID 0), log number is 0

24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 619
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 699
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 703
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 639
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 658
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 713
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : [/db_impl/db_impl_open.cc:532] DB ID: 70cd8a46-3c3f-4cc5-b1fc-7ddde5505f7f

24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 622
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 661
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 650
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 744
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 694
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : [/version_set.cc:4513] Creating manifest 5

24/01/05 02:43:53 INFO storage.BlockManagerInfo: Removed broadcast_24_piece0 on adhoc-1:43055 in memory (size: 95.8 KB, free: 366.3 MB)
24/01/05 02:43:53 INFO storage.BlockManager: Removing RDD 78
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned RDD 78
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 693
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 643
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 711
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 746
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 614
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 616
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 686
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 710
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 704
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 748
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 707
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 716
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 745
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 743
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 627
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 700
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 698
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 722
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 708
24/01/05 02:43:53 INFO storage.BlockManagerInfo: Removed broadcast_27_piece0 on adhoc-1:43055 in memory (size: 26.7 KB, free: 366.3 MB)
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 731
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 706
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 659
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 663
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 632
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 615
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 679
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 636
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 613
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 641
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 681
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 709
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 662
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 652
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 617
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 633
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 734
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 668
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 732
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 684
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 669
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 705
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 634
24/01/05 02:43:53 INFO spark.ContextCleaner: Cleaned accumulator 721
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : [/db_impl/db_impl_open.cc:2024] SstFileManager instance 0x7feb64108760
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : DB pointer 0x7feb64109080
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : [/db_impl/db_impl.cc:1070] ------- DUMPING STATS -------
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : [/db_impl/db_impl.cc:1071] 
** DB Stats **
Uptime(secs): 0.0 total, 0.0 interval
Cumulative writes: 0 writes, 0 keys, 0 commit groups, 0.0 writes per commit group, ingest: 0.00 GB, 0.00 MB/s
Cumulative WAL: 0 writes, 0 syncs, 0.00 writes per sync, written: 0.00 GB, 0.00 MB/s
Cumulative stall: 00:00:0.000 H:M:S, 0.0 percent
Interval writes: 0 writes, 0 keys, 0 commit groups, 0.0 writes per commit group, ingest: 0.00 MB, 0.00 MB/s
Interval WAL: 0 writes, 0 syncs, 0.00 writes per sync, written: 0.00 GB, 0.00 MB/s
Interval stall: 00:00:0.000 H:M:S, 0.0 percent

** Compaction Stats [default] **
Level    Files   Size     Score Read(GB)  Rn(GB) Rnp1(GB) Write(GB) Wnew(GB) Moved(GB) W-Amp Rd(MB/s) Wr(MB/s) Comp(sec) CompMergeCPU(sec) Comp(cnt) Avg(sec) KeyIn KeyDrop Rblob(GB) Wblob(GB)
------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
 Sum      0/0    0.00 KB   0.0      0.0     0.0      0.0       0.0      0.0       0.0   0.0      0.0      0.0      0.00              0.00         0    0.000       0      0       0.0       0.0
 Int      0/0    0.00 KB   0.0      0.0     0.0      0.0       0.0      0.0       0.0   0.0      0.0      0.0      0.00              0.00         0    0.000       0      0       0.0       0.0

** Compaction Stats [default] **
Priority    Files   Size     Score Read(GB)  Rn(GB) Rnp1(GB) Write(GB) Wnew(GB) Moved(GB) W-Amp Rd(MB/s) Wr(MB/s) Comp(sec) CompMergeCPU(sec) Comp(cnt) Avg(sec) KeyIn KeyDrop Rblob(GB) Wblob(GB)
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Blob file count: 0, total size: 0.0 GB, garbage size: 0.0 GB, space amp: 0.0

Uptime(secs): 0.0 total, 0.0 interval
Flush(GB): cumulative 0.000, interval 0.000
AddFile(GB): cumulative 0.000, interval 0.000
AddFile(Total Files): cumulative 0, interval 0
AddFile(L0 Files): cumulative 0, interval 0
AddFile(Keys): cumulative 0, interval 0
Cumulative compaction: 0.00 GB write, 0.00 MB/s write, 0.00 GB read, 0.00 MB/s read, 0.0 seconds
Interval compaction: 0.00 GB write, 0.00 MB/s write, 0.00 GB read, 0.00 MB/s read, 0.0 seconds
Stalls(count): 0 level0_slowdown, 0 level0_slowdown_with_compaction, 0 level0_numfiles, 0 level0_numfiles_with_compaction, 0 stop for pending_compaction_bytes, 0 slowdown for pending_compaction_bytes, 0 memtable_compaction, 0 memtable_slowdown, interval 0 total count
Block cache LRUCache@0x7feb641061f0#598 capacity: 8.00 MB collections: 1 last_copies: 0 last_secs: 3.6e-05 secs_since: 0
Block cache entry stats(count,size,portion): Misc(1,0.00 KB,0%)

** File Read Latency Histogram By Level [default] **

24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : [/db_impl/db_impl.cc:762] STATISTICS:
 rocksdb.block.cache.miss COUNT : 0
rocksdb.block.cache.hit COUNT : 0
rocksdb.block.cache.add COUNT : 0
rocksdb.block.cache.add.failures COUNT : 0
rocksdb.block.cache.index.miss COUNT : 0
rocksdb.block.cache.index.hit COUNT : 0
rocksdb.block.cache.index.add COUNT : 0
rocksdb.block.cache.index.bytes.insert COUNT : 0
rocksdb.block.cache.index.bytes.evict COUNT : 0
rocksdb.block.cache.filter.miss COUNT : 0
rocksdb.block.cache.filter.hit COUNT : 0
rocksdb.block.cache.filter.add COUNT : 0
rocksdb.block.cache.filter.bytes.insert COUNT : 0
rocksdb.block.cache.filter.bytes.evict COUNT : 0
rocksdb.block.cache.data.miss COUNT : 0
rocksdb.block.cache.data.hit COUNT : 0
rocksdb.block.cache.data.add COUNT : 0
rocksdb.block.cache.data.bytes.insert COUNT : 0
rocksdb.block.cache.bytes.read COUNT : 0
rocksdb.block.cache.bytes.write COUNT : 0
rocksdb.bloom.filter.useful COUNT : 0
rocksdb.bloom.filter.full.positive COUNT : 0
rocksdb.bloom.filter.full.true.positive COUNT : 0
rocksdb.bloom.filter.micros COUNT : 0
rocksdb.persistent.cache.hit COUNT : 0
rocksdb.persistent.cache.miss COUNT : 0
rocksdb.sim.block.cache.hit COUNT : 0
rocksdb.sim.block.cache.miss COUNT : 0
rocksdb.memtable.hit COUNT : 0
rocksdb.memtable.miss COUNT : 0
rocksdb.l0.hit COUNT : 0
rocksdb.l1.hit COUNT : 0
rocksdb.l2andup.hit COUNT : 0
rocksdb.compaction.key.drop.new COUNT : 0
rocksdb.compaction.key.drop.obsolete COUNT : 0
rocksdb.compaction.key.drop.range_del COUNT : 0
rocksdb.compaction.key.drop.user COUNT : 0
rocksdb.compaction.range_del.drop.obsolete COUNT : 0
rocksdb.compaction.optimized.del.drop.obsolete COUNT : 0
rocksdb.compaction.cancelled COUNT : 0
rocksdb.number.keys.written COUNT : 0
rocksdb.number.keys.read COUNT : 0
rocksdb.number.keys.updated COUNT : 0
rocksdb.bytes.written COUNT : 0
rocksdb.bytes.read COUNT : 0
rocksdb.number.db.seek COUNT : 0
rocksdb.number.db.next COUNT : 0
rocksdb.number.db.prev COUNT : 0
rocksdb.number.db.seek.found COUNT : 0
rocksdb.number.db.next.found COUNT : 0
rocksdb.number.db.prev.found COUNT : 0
rocksdb.db.iter.bytes.read COUNT : 0
rocksdb.no.file.closes COUNT : 0
rocksdb.no.file.opens COUNT : 0
rocksdb.no.file.errors COUNT : 0
rocksdb.l0.slowdown.micros COUNT : 0
rocksdb.memtable.compaction.micros COUNT : 0
rocksdb.l0.num.files.stall.micros COUNT : 0
rocksdb.stall.micros COUNT : 0
rocksdb.db.mutex.wait.micros COUNT : 0
rocksdb.rate.limit.delay.millis COUNT : 0
rocksdb.num.iterators COUNT : 0
rocksdb.number.multiget.get COUNT : 0
rocksdb.number.multiget.keys.read COUNT : 0
rocksdb.number.multiget.bytes.read COUNT : 0
rocksdb.number.deletes.filtered COUNT : 0
rocksdb.number.merge.failures COUNT : 0
rocksdb.bloom.filter.prefix.checked COUNT : 0
rocksdb.bloom.filter.prefix.useful COUNT : 0
rocksdb.number.reseeks.iteration COUNT : 0
rocksdb.getupdatessince.calls COUNT : 0
rocksdb.block.cachecompressed.miss COUNT : 0
rocksdb.block.cachecompressed.hit COUNT : 0
rocksdb.block.cachecompressed.add COUNT : 0
rocksdb.block.cachecompressed.add.failures COUNT : 0
rocksdb.wal.synced COUNT : 0
rocksdb.wal.bytes COUNT : 0
rocksdb.write.self COUNT : 0
rocksdb.write.other COUNT : 0
rocksdb.write.timeout COUNT : 0
rocksdb.write.wal COUNT : 0
rocksdb.compact.read.bytes COUNT : 0
rocksdb.compact.write.bytes COUNT : 0
rocksdb.flush.write.bytes COUNT : 0
rocksdb.compact.read.marked.bytes COUNT : 0
rocksdb.compact.read.periodic.bytes COUNT : 0
rocksdb.compact.read.ttl.bytes COUNT : 0
rocksdb.compact.write.marked.bytes COUNT : 0
rocksdb.compact.write.periodic.bytes COUNT : 0
rocksdb.compact.write.ttl.bytes COUNT : 0
rocksdb.number.direct.load.table.properties COUNT : 0
rocksdb.number.superversion_acquires COUNT : 0
rocksdb.number.superversion_releases COUNT : 0
rocksdb.number.superversion_cleanups COUNT : 0
rocksdb.number.block.compressed COUNT : 0
rocksdb.number.block.decompressed COUNT : 0
rocksdb.number.block.not_compressed COUNT : 0
rocksdb.merge.operation.time.nanos COUNT : 0
rocksdb.filter.operation.time.nanos COUNT : 0
rocksdb.row.cache.hit COUNT : 0
rocksdb.row.cache.miss COUNT : 0
rocksdb.read.amp.estimate.useful.bytes COUNT : 0
rocksdb.read.amp.total.read.bytes COUNT : 0
rocksdb.number.rate_limiter.drains COUNT : 0
rocksdb.number.iter.skip COUNT : 0
rocksdb.blobdb.num.put COUNT : 0
rocksdb.blobdb.num.write COUNT : 0
rocksdb.blobdb.num.get COUNT : 0
rocksdb.blobdb.num.multiget COUNT : 0
rocksdb.blobdb.num.seek COUNT : 0
rocksdb.blobdb.num.next COUNT : 0
rocksdb.blobdb.num.prev COUNT : 0
rocksdb.blobdb.num.keys.written COUNT : 0
rocksdb.blobdb.num.keys.read COUNT : 0
rocksdb.blobdb.bytes.written COUNT : 0
rocksdb.blobdb.bytes.read COUNT : 0
rocksdb.blobdb.write.inlined COUNT : 0
rocksdb.blobdb.write.inlined.ttl COUNT : 0
rocksdb.blobdb.write.blob COUNT : 0
rocksdb.blobdb.write.blob.ttl COUNT : 0
rocksdb.blobdb.blob.file.bytes.written COUNT : 0
rocksdb.blobdb.blob.file.bytes.read COUNT : 0
rocksdb.blobdb.blob.file.synced COUNT : 0
rocksdb.blobdb.blob.index.expired.count COUNT : 0
rocksdb.blobdb.blob.index.expired.size COUNT : 0
rocksdb.blobdb.blob.index.evicted.count COUNT : 0
rocksdb.blobdb.blob.index.evicted.size COUNT : 0
rocksdb.blobdb.gc.num.files COUNT : 0
rocksdb.blobdb.gc.num.new.files COUNT : 0
rocksdb.blobdb.gc.failures COUNT : 0
rocksdb.blobdb.gc.num.keys.overwritten COUNT : 0
rocksdb.blobdb.gc.num.keys.expired COUNT : 0
rocksdb.blobdb.gc.num.keys.relocated COUNT : 0
rocksdb.blobdb.gc.bytes.overwritten COUNT : 0
rocksdb.blobdb.gc.bytes.expired COUNT : 0
rocksdb.blobdb.gc.bytes.relocated COUNT : 0
rocksdb.blobdb.fifo.num.files.evicted COUNT : 0
rocksdb.blobdb.fifo.num.keys.evicted COUNT : 0
rocksdb.blobdb.fifo.bytes.evicted COUNT : 0
rocksdb.txn.overhead.mutex.prepare COUNT : 0
rocksdb.txn.overhead.mutex.old.commit.map COUNT : 0
rocksdb.txn.overhead.duplicate.key COUNT : 0
rocksdb.txn.overhead.mutex.snapshot COUNT : 0
rocksdb.txn.get.tryagain COUNT : 0
rocksdb.number.multiget.keys.found COUNT : 0
rocksdb.num.iterator.created COUNT : 0
rocksdb.num.iterator.deleted COUNT : 0
rocksdb.block.cache.compression.dict.miss COUNT : 0
rocksdb.block.cache.compression.dict.hit COUNT : 0
rocksdb.block.cache.compression.dict.add COUNT : 0
rocksdb.block.cache.compression.dict.bytes.insert COUNT : 0
rocksdb.block.cache.compression.dict.bytes.evict COUNT : 0
rocksdb.block.cache.add.redundant COUNT : 0
rocksdb.block.cache.index.add.redundant COUNT : 0
rocksdb.block.cache.filter.add.redundant COUNT : 0
rocksdb.block.cache.data.add.redundant COUNT : 0
rocksdb.block.cache.compression.dict.add.redundant COUNT : 0
rocksdb.files.marked.trash COUNT : 0
rocksdb.files.deleted.immediately COUNT : 0
rocksdb.error.handler.bg.errro.count COUNT : 0
rocksdb.error.handler.bg.io.errro.count COUNT : 0
rocksdb.error.handler.bg.retryable.io.errro.count COUNT : 0
rocksdb.error.handler.autoresume.count COUNT : 0
rocksdb.error.handler.autoresume.retry.total.count COUNT : 0
rocksdb.error.handler.autoresume.success.count COUNT : 0
rocksdb.memtable.payload.bytes.at.flush COUNT : 0
rocksdb.memtable.garbage.bytes.at.flush COUNT : 0
rocksdb.secondary.cache.hits COUNT : 0
rocksdb.verify_checksum.read.bytes COUNT : 0
rocksdb.backup.read.bytes COUNT : 0
rocksdb.backup.write.bytes COUNT : 0
rocksdb.remote.compact.read.bytes COUNT : 0
rocksdb.remote.compact.write.bytes COUNT : 0
rocksdb.hot.file.read.bytes COUNT : 0
rocksdb.warm.file.read.bytes COUNT : 0
rocksdb.cold.file.read.bytes COUNT : 0
rocksdb.hot.file.read.count COUNT : 0
rocksdb.warm.file.read.count COUNT : 0
rocksdb.cold.file.read.count COUNT : 0
rocksdb.last.level.read.bytes COUNT : 0
rocksdb.last.level.read.count COUNT : 0
rocksdb.non.last.level.read.bytes COUNT : 0
rocksdb.non.last.level.read.count COUNT : 0
rocksdb.block.checksum.compute.count COUNT : 0
rocksdb.multiget.coroutine.count COUNT : 0
rocksdb.blobdb.cache.miss COUNT : 0
rocksdb.blobdb.cache.hit COUNT : 0
rocksdb.blobdb.cache.add COUNT : 0
rocksdb.blobdb.cache.add.failures COUNT : 0
rocksdb.blobdb.cache.bytes.read COUNT : 0
rocksdb.blobdb.cache.bytes.write COUNT : 0
rocksdb.db.get.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.db.write.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.compaction.times.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.compaction.times.cpu_micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.subcompaction.setup.times.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.table.sync.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.compaction.outfile.sync.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.wal.file.sync.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.manifest.file.sync.micros P50 : 831.000000 P95 : 855.500000 P99 : 867.100000 P100 : 870.000000 COUNT : 2 SUM : 1701
rocksdb.table.open.io.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.db.multiget.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.read.block.compaction.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.read.block.get.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.write.raw.block.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.l0.slowdown.count P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.memtable.compaction.count P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.num.files.stall.count P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.hard.rate.limit.delay.count P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.soft.rate.limit.delay.count P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.numfiles.in.singlecompaction P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.db.seek.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.db.write.stall P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.sst.read.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.num.subcompactions.scheduled P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.bytes.per.read P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.bytes.per.write P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.bytes.per.multiget P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.bytes.compressed P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.bytes.decompressed P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.compression.times.nanos P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.decompression.times.nanos P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.read.num.merge_operands P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.blobdb.key.size P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.blobdb.value.size P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.blobdb.write.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.blobdb.get.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.blobdb.multiget.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.blobdb.seek.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.blobdb.next.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.blobdb.prev.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.blobdb.blob.file.write.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.blobdb.blob.file.read.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.blobdb.blob.file.sync.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.blobdb.gc.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.blobdb.compression.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.blobdb.decompression.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.db.flush.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.sst.batch.size P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.num.index.and.filter.blocks.read.per.level P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.num.data.blocks.read.per.level P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.num.sst.read.per.level P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.error.handler.autoresume.retry.count P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.async.read.bytes P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.poll.wait.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.prefetched.bytes.discarded P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.multiget.io.batch.size P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.num.level.read.per.multiget P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0

24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : [/column_family.cc:618] --------------- Options for column family [hudi_view__user_hive_warehouse_stock_ticks_mor]:

24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :               Options.comparator: leveldb.BytewiseComparator
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :           Options.merge_operator: None
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :        Options.compaction_filter: None
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :        Options.compaction_filter_factory: None
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :  Options.sst_partitioner_factory: None
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :         Options.memtable_factory: SkipListFactory
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :            Options.table_factory: BlockBasedTable
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :            table_factory options:   flush_block_policy_factory: FlushBlockBySizePolicyFactory (0x7feb641081b0)
  cache_index_and_filter_blocks: 0
  cache_index_and_filter_blocks_with_high_priority: 1
  pin_l0_filter_and_index_blocks_in_cache: 0
  pin_top_level_index_and_filter: 1
  index_type: 0
  data_block_index_type: 0
  index_shortening: 1
  data_block_hash_table_util_ratio: 0.750000
  checksum: 1
  no_block_cache: 0
  block_cache: 0x7feb64133c80
  block_cache_name: LRUCache
  block_cache_options:
    capacity : 8388608
    num_shard_bits : 4
    strict_capacity_limit : 0
    memory_allocator : None
    high_pri_pool_ratio: 0.000
  block_cache_compressed: (nil)
  persistent_cache: (nil)
  block_size: 4096
  block_size_deviation: 10
  block_restart_interval: 16
  index_block_restart_interval: 1
  metadata_block_size: 4096
  partition_filters: 0
  use_delta_encoding: 1
  filter_policy: nullptr
  whole_key_filtering: 1
  verify_compression: 0
  read_amp_bytes_per_bit: 0
  format_version: 5
  enable_index_compression: 1
  block_align: 0
  max_auto_readahead_size: 262144
  prepopulate_block_cache: 0
  initial_auto_readahead_size: 8192

24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :        Options.write_buffer_size: 67108864
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :  Options.max_write_buffer_number: 2
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :          Options.compression: Snappy
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression: Disabled
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :       Options.prefix_extractor: nullptr
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :   Options.memtable_insert_with_hint_prefix_extractor: nullptr
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :             Options.num_levels: 7
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :        Options.min_write_buffer_number_to_merge: 1
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :     Options.max_write_buffer_number_to_maintain: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :     Options.max_write_buffer_size_to_maintain: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :            Options.bottommost_compression_opts.window_bits: -14
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression_opts.level: 32767
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :               Options.bottommost_compression_opts.strategy: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.max_dict_bytes: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.zstd_max_train_bytes: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.parallel_threads: 1
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression_opts.enabled: false
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.max_dict_buffer_bytes: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.use_zstd_dict_trainer: true
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :            Options.compression_opts.window_bits: -14
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                  Options.compression_opts.level: 32767
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :               Options.compression_opts.strategy: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.max_dict_bytes: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.zstd_max_train_bytes: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.use_zstd_dict_trainer: true
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.parallel_threads: 1
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                  Options.compression_opts.enabled: false
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.max_dict_buffer_bytes: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :      Options.level0_file_num_compaction_trigger: 4
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :          Options.level0_slowdown_writes_trigger: 20
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :              Options.level0_stop_writes_trigger: 36
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                   Options.target_file_size_base: 67108864
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :             Options.target_file_size_multiplier: 1
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                Options.max_bytes_for_level_base: 268435456
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Options.level_compaction_dynamic_level_bytes: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :          Options.max_bytes_for_level_multiplier: 10.000000
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[0]: 1
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[1]: 1
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[2]: 1
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[3]: 1
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[4]: 1
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[5]: 1
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[6]: 1
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :       Options.max_sequential_skip_in_iterations: 8
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                    Options.max_compaction_bytes: 1677721600
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                        Options.arena_block_size: 1048576
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :   Options.soft_pending_compaction_bytes_limit: 68719476736
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :   Options.hard_pending_compaction_bytes_limit: 274877906944
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                Options.disable_auto_compactions: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                        Options.compaction_style: kCompactionStyleLevel
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                          Options.compaction_pri: kMinOverlappingRatio
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.size_ratio: 1
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.min_merge_width: 2
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.max_merge_width: 4294967295
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.max_size_amplification_percent: 200
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.compression_size_percent: -1
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.stop_style: kCompactionStopStyleTotalSize
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_fifo.max_table_files_size: 1073741824
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_fifo.allow_compaction: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                   Options.table_properties_collectors: 
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                   Options.inplace_update_support: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                 Options.inplace_update_num_locks: 10000
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :               Options.memtable_prefix_bloom_size_ratio: 0.000000
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :               Options.memtable_whole_key_filtering: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :   Options.memtable_huge_page_size: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                           Options.bloom_locality: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                    Options.max_successive_merges: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                Options.optimize_filters_for_hits: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                Options.paranoid_file_checks: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                Options.force_consistency_checks: 1
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                Options.report_bg_io_stats: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                               Options.ttl: 2592000
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :          Options.periodic_compaction_seconds: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :  Options.preclude_last_level_data_seconds: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                       Options.enable_blob_files: false
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                           Options.min_blob_size: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                          Options.blob_file_size: 268435456
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                   Options.blob_compression_type: NoCompression
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :          Options.enable_blob_garbage_collection: false
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :      Options.blob_garbage_collection_age_cutoff: 0.250000
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Options.blob_garbage_collection_force_threshold: 1.000000
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :          Options.blob_compaction_readahead_size: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                Options.blob_file_starting_level: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Options.experimental_mempurge_threshold: 0.000000
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : [/db_impl/db_impl.cc:2861] Created column family [hudi_view__user_hive_warehouse_stock_ticks_mor] (ID 1)
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : [/column_family.cc:618] --------------- Options for column family [hudi_pending_compaction__user_hive_warehouse_stock_ticks_mor]:

24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :               Options.comparator: leveldb.BytewiseComparator
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :           Options.merge_operator: None
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :        Options.compaction_filter: None
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :        Options.compaction_filter_factory: None
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :  Options.sst_partitioner_factory: None
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :         Options.memtable_factory: SkipListFactory
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :            Options.table_factory: BlockBasedTable
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :            table_factory options:   flush_block_policy_factory: FlushBlockBySizePolicyFactory (0x7feb64135ea0)
  cache_index_and_filter_blocks: 0
  cache_index_and_filter_blocks_with_high_priority: 1
  pin_l0_filter_and_index_blocks_in_cache: 0
  pin_top_level_index_and_filter: 1
  index_type: 0
  data_block_index_type: 0
  index_shortening: 1
  data_block_hash_table_util_ratio: 0.750000
  checksum: 1
  no_block_cache: 0
  block_cache: 0x7feb641f2a90
  block_cache_name: LRUCache
  block_cache_options:
    capacity : 8388608
    num_shard_bits : 4
    strict_capacity_limit : 0
    memory_allocator : None
    high_pri_pool_ratio: 0.000
  block_cache_compressed: (nil)
  persistent_cache: (nil)
  block_size: 4096
  block_size_deviation: 10
  block_restart_interval: 16
  index_block_restart_interval: 1
  metadata_block_size: 4096
  partition_filters: 0
  use_delta_encoding: 1
  filter_policy: nullptr
  whole_key_filtering: 1
  verify_compression: 0
  read_amp_bytes_per_bit: 0
  format_version: 5
  enable_index_compression: 1
  block_align: 0
  max_auto_readahead_size: 262144
  prepopulate_block_cache: 0
  initial_auto_readahead_size: 8192

24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :        Options.write_buffer_size: 67108864
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :  Options.max_write_buffer_number: 2
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :          Options.compression: Snappy
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression: Disabled
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :       Options.prefix_extractor: nullptr
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :   Options.memtable_insert_with_hint_prefix_extractor: nullptr
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :             Options.num_levels: 7
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :        Options.min_write_buffer_number_to_merge: 1
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :     Options.max_write_buffer_number_to_maintain: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :     Options.max_write_buffer_size_to_maintain: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :            Options.bottommost_compression_opts.window_bits: -14
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression_opts.level: 32767
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :               Options.bottommost_compression_opts.strategy: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.max_dict_bytes: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.zstd_max_train_bytes: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.parallel_threads: 1
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression_opts.enabled: false
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.max_dict_buffer_bytes: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.use_zstd_dict_trainer: true
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :            Options.compression_opts.window_bits: -14
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                  Options.compression_opts.level: 32767
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :               Options.compression_opts.strategy: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.max_dict_bytes: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.zstd_max_train_bytes: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.use_zstd_dict_trainer: true
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.parallel_threads: 1
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                  Options.compression_opts.enabled: false
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.max_dict_buffer_bytes: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :      Options.level0_file_num_compaction_trigger: 4
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :          Options.level0_slowdown_writes_trigger: 20
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :              Options.level0_stop_writes_trigger: 36
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                   Options.target_file_size_base: 67108864
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :             Options.target_file_size_multiplier: 1
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                Options.max_bytes_for_level_base: 268435456
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Options.level_compaction_dynamic_level_bytes: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :          Options.max_bytes_for_level_multiplier: 10.000000
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[0]: 1
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[1]: 1
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[2]: 1
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[3]: 1
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[4]: 1
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[5]: 1
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[6]: 1
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :       Options.max_sequential_skip_in_iterations: 8
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                    Options.max_compaction_bytes: 1677721600
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                        Options.arena_block_size: 1048576
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :   Options.soft_pending_compaction_bytes_limit: 68719476736
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :   Options.hard_pending_compaction_bytes_limit: 274877906944
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                Options.disable_auto_compactions: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                        Options.compaction_style: kCompactionStyleLevel
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                          Options.compaction_pri: kMinOverlappingRatio
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.size_ratio: 1
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.min_merge_width: 2
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.max_merge_width: 4294967295
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.max_size_amplification_percent: 200
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.compression_size_percent: -1
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.stop_style: kCompactionStopStyleTotalSize
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_fifo.max_table_files_size: 1073741824
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_fifo.allow_compaction: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                   Options.table_properties_collectors: 
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                   Options.inplace_update_support: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                 Options.inplace_update_num_locks: 10000
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :               Options.memtable_prefix_bloom_size_ratio: 0.000000
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :               Options.memtable_whole_key_filtering: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :   Options.memtable_huge_page_size: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                           Options.bloom_locality: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                    Options.max_successive_merges: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                Options.optimize_filters_for_hits: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                Options.paranoid_file_checks: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                Options.force_consistency_checks: 1
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                Options.report_bg_io_stats: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                               Options.ttl: 2592000
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :          Options.periodic_compaction_seconds: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :  Options.preclude_last_level_data_seconds: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                       Options.enable_blob_files: false
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                           Options.min_blob_size: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                          Options.blob_file_size: 268435456
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                   Options.blob_compression_type: NoCompression
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :          Options.enable_blob_garbage_collection: false
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :      Options.blob_garbage_collection_age_cutoff: 0.250000
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Options.blob_garbage_collection_force_threshold: 1.000000
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :          Options.blob_compaction_readahead_size: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                Options.blob_file_starting_level: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Options.experimental_mempurge_threshold: 0.000000
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : [/db_impl/db_impl.cc:2861] Created column family [hudi_pending_compaction__user_hive_warehouse_stock_ticks_mor] (ID 2)
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : [/column_family.cc:618] --------------- Options for column family [hudi_pending_log_compaction__user_hive_warehouse_stock_ticks_mor]:

24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :               Options.comparator: leveldb.BytewiseComparator
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :           Options.merge_operator: None
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :        Options.compaction_filter: None
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :        Options.compaction_filter_factory: None
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :  Options.sst_partitioner_factory: None
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :         Options.memtable_factory: SkipListFactory
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :            Options.table_factory: BlockBasedTable
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :            table_factory options:   flush_block_policy_factory: FlushBlockBySizePolicyFactory (0x7feb641351a0)
  cache_index_and_filter_blocks: 0
  cache_index_and_filter_blocks_with_high_priority: 1
  pin_l0_filter_and_index_blocks_in_cache: 0
  pin_top_level_index_and_filter: 1
  index_type: 0
  data_block_index_type: 0
  index_shortening: 1
  data_block_hash_table_util_ratio: 0.750000
  checksum: 1
  no_block_cache: 0
  block_cache: 0x7feb641351f0
  block_cache_name: LRUCache
  block_cache_options:
    capacity : 8388608
    num_shard_bits : 4
    strict_capacity_limit : 0
    memory_allocator : None
    high_pri_pool_ratio: 0.000
  block_cache_compressed: (nil)
  persistent_cache: (nil)
  block_size: 4096
  block_size_deviation: 10
  block_restart_interval: 16
  index_block_restart_interval: 1
  metadata_block_size: 4096
  partition_filters: 0
  use_delta_encoding: 1
  filter_policy: nullptr
  whole_key_filtering: 1
  verify_compression: 0
  read_amp_bytes_per_bit: 0
  format_version: 5
  enable_index_compression: 1
  block_align: 0
  max_auto_readahead_size: 262144
  prepopulate_block_cache: 0
  initial_auto_readahead_size: 8192

24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :        Options.write_buffer_size: 67108864
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :  Options.max_write_buffer_number: 2
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :          Options.compression: Snappy
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression: Disabled
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :       Options.prefix_extractor: nullptr
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :   Options.memtable_insert_with_hint_prefix_extractor: nullptr
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :             Options.num_levels: 7
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :        Options.min_write_buffer_number_to_merge: 1
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :     Options.max_write_buffer_number_to_maintain: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :     Options.max_write_buffer_size_to_maintain: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :            Options.bottommost_compression_opts.window_bits: -14
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression_opts.level: 32767
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :               Options.bottommost_compression_opts.strategy: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.max_dict_bytes: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.zstd_max_train_bytes: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.parallel_threads: 1
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression_opts.enabled: false
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.max_dict_buffer_bytes: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.use_zstd_dict_trainer: true
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :            Options.compression_opts.window_bits: -14
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                  Options.compression_opts.level: 32767
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :               Options.compression_opts.strategy: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.max_dict_bytes: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.zstd_max_train_bytes: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.use_zstd_dict_trainer: true
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.parallel_threads: 1
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                  Options.compression_opts.enabled: false
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.max_dict_buffer_bytes: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :      Options.level0_file_num_compaction_trigger: 4
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :          Options.level0_slowdown_writes_trigger: 20
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :              Options.level0_stop_writes_trigger: 36
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                   Options.target_file_size_base: 67108864
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :             Options.target_file_size_multiplier: 1
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                Options.max_bytes_for_level_base: 268435456
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Options.level_compaction_dynamic_level_bytes: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :          Options.max_bytes_for_level_multiplier: 10.000000
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[0]: 1
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[1]: 1
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[2]: 1
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[3]: 1
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[4]: 1
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[5]: 1
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[6]: 1
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :       Options.max_sequential_skip_in_iterations: 8
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                    Options.max_compaction_bytes: 1677721600
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                        Options.arena_block_size: 1048576
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :   Options.soft_pending_compaction_bytes_limit: 68719476736
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :   Options.hard_pending_compaction_bytes_limit: 274877906944
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                Options.disable_auto_compactions: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                        Options.compaction_style: kCompactionStyleLevel
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                          Options.compaction_pri: kMinOverlappingRatio
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.size_ratio: 1
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.min_merge_width: 2
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.max_merge_width: 4294967295
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.max_size_amplification_percent: 200
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.compression_size_percent: -1
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.stop_style: kCompactionStopStyleTotalSize
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_fifo.max_table_files_size: 1073741824
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_fifo.allow_compaction: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                   Options.table_properties_collectors: 
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                   Options.inplace_update_support: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                 Options.inplace_update_num_locks: 10000
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :               Options.memtable_prefix_bloom_size_ratio: 0.000000
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :               Options.memtable_whole_key_filtering: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :   Options.memtable_huge_page_size: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                           Options.bloom_locality: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                    Options.max_successive_merges: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                Options.optimize_filters_for_hits: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                Options.paranoid_file_checks: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                Options.force_consistency_checks: 1
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                Options.report_bg_io_stats: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                               Options.ttl: 2592000
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :          Options.periodic_compaction_seconds: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :  Options.preclude_last_level_data_seconds: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                       Options.enable_blob_files: false
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                           Options.min_blob_size: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                          Options.blob_file_size: 268435456
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                   Options.blob_compression_type: NoCompression
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :          Options.enable_blob_garbage_collection: false
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :      Options.blob_garbage_collection_age_cutoff: 0.250000
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Options.blob_garbage_collection_force_threshold: 1.000000
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :          Options.blob_compaction_readahead_size: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                Options.blob_file_starting_level: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Options.experimental_mempurge_threshold: 0.000000
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : [/db_impl/db_impl.cc:2861] Created column family [hudi_pending_log_compaction__user_hive_warehouse_stock_ticks_mor] (ID 3)
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : [/column_family.cc:618] --------------- Options for column family [hudi_bootstrap_basefile__user_hive_warehouse_stock_ticks_mor]:

24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :               Options.comparator: leveldb.BytewiseComparator
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :           Options.merge_operator: None
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :        Options.compaction_filter: None
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :        Options.compaction_filter_factory: None
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :  Options.sst_partitioner_factory: None
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :         Options.memtable_factory: SkipListFactory
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :            Options.table_factory: BlockBasedTable
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :            table_factory options:   flush_block_policy_factory: FlushBlockBySizePolicyFactory (0x7feb641649a0)
  cache_index_and_filter_blocks: 0
  cache_index_and_filter_blocks_with_high_priority: 1
  pin_l0_filter_and_index_blocks_in_cache: 0
  pin_top_level_index_and_filter: 1
  index_type: 0
  data_block_index_type: 0
  index_shortening: 1
  data_block_hash_table_util_ratio: 0.750000
  checksum: 1
  no_block_cache: 0
  block_cache: 0x7feb641649f0
  block_cache_name: LRUCache
  block_cache_options:
    capacity : 8388608
    num_shard_bits : 4
    strict_capacity_limit : 0
    memory_allocator : None
    high_pri_pool_ratio: 0.000
  block_cache_compressed: (nil)
  persistent_cache: (nil)
  block_size: 4096
  block_size_deviation: 10
  block_restart_interval: 16
  index_block_restart_interval: 1
  metadata_block_size: 4096
  partition_filters: 0
  use_delta_encoding: 1
  filter_policy: nullptr
  whole_key_filtering: 1
  verify_compression: 0
  read_amp_bytes_per_bit: 0
  format_version: 5
  enable_index_compression: 1
  block_align: 0
  max_auto_readahead_size: 262144
  prepopulate_block_cache: 0
  initial_auto_readahead_size: 8192

24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :        Options.write_buffer_size: 67108864
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :  Options.max_write_buffer_number: 2
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :          Options.compression: Snappy
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression: Disabled
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :       Options.prefix_extractor: nullptr
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :   Options.memtable_insert_with_hint_prefix_extractor: nullptr
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :             Options.num_levels: 7
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :        Options.min_write_buffer_number_to_merge: 1
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :     Options.max_write_buffer_number_to_maintain: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :     Options.max_write_buffer_size_to_maintain: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :            Options.bottommost_compression_opts.window_bits: -14
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression_opts.level: 32767
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :               Options.bottommost_compression_opts.strategy: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.max_dict_bytes: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.zstd_max_train_bytes: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.parallel_threads: 1
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression_opts.enabled: false
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.max_dict_buffer_bytes: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.use_zstd_dict_trainer: true
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :            Options.compression_opts.window_bits: -14
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                  Options.compression_opts.level: 32767
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :               Options.compression_opts.strategy: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.max_dict_bytes: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.zstd_max_train_bytes: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.use_zstd_dict_trainer: true
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.parallel_threads: 1
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                  Options.compression_opts.enabled: false
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.max_dict_buffer_bytes: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :      Options.level0_file_num_compaction_trigger: 4
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :          Options.level0_slowdown_writes_trigger: 20
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :              Options.level0_stop_writes_trigger: 36
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                   Options.target_file_size_base: 67108864
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :             Options.target_file_size_multiplier: 1
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                Options.max_bytes_for_level_base: 268435456
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Options.level_compaction_dynamic_level_bytes: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :          Options.max_bytes_for_level_multiplier: 10.000000
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[0]: 1
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[1]: 1
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[2]: 1
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[3]: 1
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[4]: 1
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[5]: 1
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[6]: 1
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :       Options.max_sequential_skip_in_iterations: 8
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                    Options.max_compaction_bytes: 1677721600
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                        Options.arena_block_size: 1048576
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :   Options.soft_pending_compaction_bytes_limit: 68719476736
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :   Options.hard_pending_compaction_bytes_limit: 274877906944
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                Options.disable_auto_compactions: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                        Options.compaction_style: kCompactionStyleLevel
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                          Options.compaction_pri: kMinOverlappingRatio
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.size_ratio: 1
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.min_merge_width: 2
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.max_merge_width: 4294967295
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.max_size_amplification_percent: 200
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.compression_size_percent: -1
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.stop_style: kCompactionStopStyleTotalSize
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_fifo.max_table_files_size: 1073741824
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_fifo.allow_compaction: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                   Options.table_properties_collectors: 
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                   Options.inplace_update_support: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                 Options.inplace_update_num_locks: 10000
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :               Options.memtable_prefix_bloom_size_ratio: 0.000000
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :               Options.memtable_whole_key_filtering: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :   Options.memtable_huge_page_size: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                           Options.bloom_locality: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                    Options.max_successive_merges: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                Options.optimize_filters_for_hits: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                Options.paranoid_file_checks: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                Options.force_consistency_checks: 1
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                Options.report_bg_io_stats: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                               Options.ttl: 2592000
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :          Options.periodic_compaction_seconds: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :  Options.preclude_last_level_data_seconds: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                       Options.enable_blob_files: false
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                           Options.min_blob_size: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                          Options.blob_file_size: 268435456
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                   Options.blob_compression_type: NoCompression
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :          Options.enable_blob_garbage_collection: false
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :      Options.blob_garbage_collection_age_cutoff: 0.250000
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Options.blob_garbage_collection_force_threshold: 1.000000
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :          Options.blob_compaction_readahead_size: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                Options.blob_file_starting_level: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Options.experimental_mempurge_threshold: 0.000000
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : [/db_impl/db_impl.cc:2861] Created column family [hudi_bootstrap_basefile__user_hive_warehouse_stock_ticks_mor] (ID 4)
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : [/column_family.cc:618] --------------- Options for column family [hudi_partitions__user_hive_warehouse_stock_ticks_mor]:

24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :               Options.comparator: leveldb.BytewiseComparator
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :           Options.merge_operator: None
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :        Options.compaction_filter: None
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :        Options.compaction_filter_factory: None
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :  Options.sst_partitioner_factory: None
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :         Options.memtable_factory: SkipListFactory
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :            Options.table_factory: BlockBasedTable
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :            table_factory options:   flush_block_policy_factory: FlushBlockBySizePolicyFactory (0x7feb6416dd30)
  cache_index_and_filter_blocks: 0
  cache_index_and_filter_blocks_with_high_priority: 1
  pin_l0_filter_and_index_blocks_in_cache: 0
  pin_top_level_index_and_filter: 1
  index_type: 0
  data_block_index_type: 0
  index_shortening: 1
  data_block_hash_table_util_ratio: 0.750000
  checksum: 1
  no_block_cache: 0
  block_cache: 0x7feb6416dd80
  block_cache_name: LRUCache
  block_cache_options:
    capacity : 8388608
    num_shard_bits : 4
    strict_capacity_limit : 0
    memory_allocator : None
    high_pri_pool_ratio: 0.000
  block_cache_compressed: (nil)
  persistent_cache: (nil)
  block_size: 4096
  block_size_deviation: 10
  block_restart_interval: 16
  index_block_restart_interval: 1
  metadata_block_size: 4096
  partition_filters: 0
  use_delta_encoding: 1
  filter_policy: nullptr
  whole_key_filtering: 1
  verify_compression: 0
  read_amp_bytes_per_bit: 0
  format_version: 5
  enable_index_compression: 1
  block_align: 0
  max_auto_readahead_size: 262144
  prepopulate_block_cache: 0
  initial_auto_readahead_size: 8192

24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :        Options.write_buffer_size: 67108864
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :  Options.max_write_buffer_number: 2
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :          Options.compression: Snappy
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression: Disabled
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :       Options.prefix_extractor: nullptr
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :   Options.memtable_insert_with_hint_prefix_extractor: nullptr
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :             Options.num_levels: 7
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :        Options.min_write_buffer_number_to_merge: 1
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :     Options.max_write_buffer_number_to_maintain: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :     Options.max_write_buffer_size_to_maintain: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :            Options.bottommost_compression_opts.window_bits: -14
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression_opts.level: 32767
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :               Options.bottommost_compression_opts.strategy: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.max_dict_bytes: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.zstd_max_train_bytes: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.parallel_threads: 1
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression_opts.enabled: false
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.max_dict_buffer_bytes: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.use_zstd_dict_trainer: true
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :            Options.compression_opts.window_bits: -14
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                  Options.compression_opts.level: 32767
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :               Options.compression_opts.strategy: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.max_dict_bytes: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.zstd_max_train_bytes: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.use_zstd_dict_trainer: true
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.parallel_threads: 1
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                  Options.compression_opts.enabled: false
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.max_dict_buffer_bytes: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :      Options.level0_file_num_compaction_trigger: 4
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :          Options.level0_slowdown_writes_trigger: 20
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :              Options.level0_stop_writes_trigger: 36
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                   Options.target_file_size_base: 67108864
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :             Options.target_file_size_multiplier: 1
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                Options.max_bytes_for_level_base: 268435456
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Options.level_compaction_dynamic_level_bytes: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :          Options.max_bytes_for_level_multiplier: 10.000000
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[0]: 1
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[1]: 1
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[2]: 1
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[3]: 1
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[4]: 1
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[5]: 1
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[6]: 1
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :       Options.max_sequential_skip_in_iterations: 8
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                    Options.max_compaction_bytes: 1677721600
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                        Options.arena_block_size: 1048576
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :   Options.soft_pending_compaction_bytes_limit: 68719476736
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :   Options.hard_pending_compaction_bytes_limit: 274877906944
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                Options.disable_auto_compactions: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                        Options.compaction_style: kCompactionStyleLevel
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                          Options.compaction_pri: kMinOverlappingRatio
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.size_ratio: 1
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.min_merge_width: 2
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.max_merge_width: 4294967295
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.max_size_amplification_percent: 200
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.compression_size_percent: -1
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.stop_style: kCompactionStopStyleTotalSize
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_fifo.max_table_files_size: 1073741824
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_fifo.allow_compaction: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                   Options.table_properties_collectors: 
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                   Options.inplace_update_support: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                 Options.inplace_update_num_locks: 10000
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :               Options.memtable_prefix_bloom_size_ratio: 0.000000
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :               Options.memtable_whole_key_filtering: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :   Options.memtable_huge_page_size: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                           Options.bloom_locality: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                    Options.max_successive_merges: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                Options.optimize_filters_for_hits: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                Options.paranoid_file_checks: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                Options.force_consistency_checks: 1
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                Options.report_bg_io_stats: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                               Options.ttl: 2592000
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :          Options.periodic_compaction_seconds: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :  Options.preclude_last_level_data_seconds: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                       Options.enable_blob_files: false
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                           Options.min_blob_size: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                          Options.blob_file_size: 268435456
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                   Options.blob_compression_type: NoCompression
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :          Options.enable_blob_garbage_collection: false
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :      Options.blob_garbage_collection_age_cutoff: 0.250000
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Options.blob_garbage_collection_force_threshold: 1.000000
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :          Options.blob_compaction_readahead_size: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                Options.blob_file_starting_level: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Options.experimental_mempurge_threshold: 0.000000
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : [/db_impl/db_impl.cc:2861] Created column family [hudi_partitions__user_hive_warehouse_stock_ticks_mor] (ID 5)
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : [/column_family.cc:618] --------------- Options for column family [hudi_replaced_fg_user_hive_warehouse_stock_ticks_mor]:

24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :               Options.comparator: leveldb.BytewiseComparator
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :           Options.merge_operator: None
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :        Options.compaction_filter: None
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :        Options.compaction_filter_factory: None
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :  Options.sst_partitioner_factory: None
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :         Options.memtable_factory: SkipListFactory
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :            Options.table_factory: BlockBasedTable
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :            table_factory options:   flush_block_policy_factory: FlushBlockBySizePolicyFactory (0x7feb6417fdc0)
  cache_index_and_filter_blocks: 0
  cache_index_and_filter_blocks_with_high_priority: 1
  pin_l0_filter_and_index_blocks_in_cache: 0
  pin_top_level_index_and_filter: 1
  index_type: 0
  data_block_index_type: 0
  index_shortening: 1
  data_block_hash_table_util_ratio: 0.750000
  checksum: 1
  no_block_cache: 0
  block_cache: 0x7feb6417fe10
  block_cache_name: LRUCache
  block_cache_options:
    capacity : 8388608
    num_shard_bits : 4
    strict_capacity_limit : 0
    memory_allocator : None
    high_pri_pool_ratio: 0.000
  block_cache_compressed: (nil)
  persistent_cache: (nil)
  block_size: 4096
  block_size_deviation: 10
  block_restart_interval: 16
  index_block_restart_interval: 1
  metadata_block_size: 4096
  partition_filters: 0
  use_delta_encoding: 1
  filter_policy: nullptr
  whole_key_filtering: 1
  verify_compression: 0
  read_amp_bytes_per_bit: 0
  format_version: 5
  enable_index_compression: 1
  block_align: 0
  max_auto_readahead_size: 262144
  prepopulate_block_cache: 0
  initial_auto_readahead_size: 8192

24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :        Options.write_buffer_size: 67108864
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :  Options.max_write_buffer_number: 2
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :          Options.compression: Snappy
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression: Disabled
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :       Options.prefix_extractor: nullptr
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :   Options.memtable_insert_with_hint_prefix_extractor: nullptr
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :             Options.num_levels: 7
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :        Options.min_write_buffer_number_to_merge: 1
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :     Options.max_write_buffer_number_to_maintain: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :     Options.max_write_buffer_size_to_maintain: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :            Options.bottommost_compression_opts.window_bits: -14
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression_opts.level: 32767
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :               Options.bottommost_compression_opts.strategy: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.max_dict_bytes: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.zstd_max_train_bytes: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.parallel_threads: 1
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression_opts.enabled: false
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.max_dict_buffer_bytes: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.use_zstd_dict_trainer: true
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :            Options.compression_opts.window_bits: -14
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                  Options.compression_opts.level: 32767
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :               Options.compression_opts.strategy: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.max_dict_bytes: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.zstd_max_train_bytes: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.use_zstd_dict_trainer: true
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.parallel_threads: 1
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                  Options.compression_opts.enabled: false
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.max_dict_buffer_bytes: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :      Options.level0_file_num_compaction_trigger: 4
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :          Options.level0_slowdown_writes_trigger: 20
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :              Options.level0_stop_writes_trigger: 36
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                   Options.target_file_size_base: 67108864
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :             Options.target_file_size_multiplier: 1
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                Options.max_bytes_for_level_base: 268435456
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Options.level_compaction_dynamic_level_bytes: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :          Options.max_bytes_for_level_multiplier: 10.000000
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[0]: 1
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[1]: 1
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[2]: 1
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[3]: 1
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[4]: 1
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[5]: 1
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[6]: 1
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :       Options.max_sequential_skip_in_iterations: 8
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                    Options.max_compaction_bytes: 1677721600
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                        Options.arena_block_size: 1048576
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :   Options.soft_pending_compaction_bytes_limit: 68719476736
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :   Options.hard_pending_compaction_bytes_limit: 274877906944
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                Options.disable_auto_compactions: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                        Options.compaction_style: kCompactionStyleLevel
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                          Options.compaction_pri: kMinOverlappingRatio
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.size_ratio: 1
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.min_merge_width: 2
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.max_merge_width: 4294967295
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.max_size_amplification_percent: 200
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.compression_size_percent: -1
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.stop_style: kCompactionStopStyleTotalSize
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_fifo.max_table_files_size: 1073741824
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_fifo.allow_compaction: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                   Options.table_properties_collectors: 
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                   Options.inplace_update_support: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                 Options.inplace_update_num_locks: 10000
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :               Options.memtable_prefix_bloom_size_ratio: 0.000000
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :               Options.memtable_whole_key_filtering: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :   Options.memtable_huge_page_size: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                           Options.bloom_locality: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                    Options.max_successive_merges: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                Options.optimize_filters_for_hits: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                Options.paranoid_file_checks: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                Options.force_consistency_checks: 1
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                Options.report_bg_io_stats: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                               Options.ttl: 2592000
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :          Options.periodic_compaction_seconds: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :  Options.preclude_last_level_data_seconds: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                       Options.enable_blob_files: false
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                           Options.min_blob_size: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                          Options.blob_file_size: 268435456
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                   Options.blob_compression_type: NoCompression
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :          Options.enable_blob_garbage_collection: false
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :      Options.blob_garbage_collection_age_cutoff: 0.250000
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Options.blob_garbage_collection_force_threshold: 1.000000
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :          Options.blob_compaction_readahead_size: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                Options.blob_file_starting_level: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Options.experimental_mempurge_threshold: 0.000000
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : [/db_impl/db_impl.cc:2861] Created column family [hudi_replaced_fg_user_hive_warehouse_stock_ticks_mor] (ID 6)
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : [/column_family.cc:618] --------------- Options for column family [hudi_pending_clustering_fg_user_hive_warehouse_stock_ticks_mor]:

24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :               Options.comparator: leveldb.BytewiseComparator
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :           Options.merge_operator: None
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :        Options.compaction_filter: None
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :        Options.compaction_filter_factory: None
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :  Options.sst_partitioner_factory: None
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :         Options.memtable_factory: SkipListFactory
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :            Options.table_factory: BlockBasedTable
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :            table_factory options:   flush_block_policy_factory: FlushBlockBySizePolicyFactory (0x7feb64199100)
  cache_index_and_filter_blocks: 0
  cache_index_and_filter_blocks_with_high_priority: 1
  pin_l0_filter_and_index_blocks_in_cache: 0
  pin_top_level_index_and_filter: 1
  index_type: 0
  data_block_index_type: 0
  index_shortening: 1
  data_block_hash_table_util_ratio: 0.750000
  checksum: 1
  no_block_cache: 0
  block_cache: 0x7feb64199150
  block_cache_name: LRUCache
  block_cache_options:
    capacity : 8388608
    num_shard_bits : 4
    strict_capacity_limit : 0
    memory_allocator : None
    high_pri_pool_ratio: 0.000
  block_cache_compressed: (nil)
  persistent_cache: (nil)
  block_size: 4096
  block_size_deviation: 10
  block_restart_interval: 16
  index_block_restart_interval: 1
  metadata_block_size: 4096
  partition_filters: 0
  use_delta_encoding: 1
  filter_policy: nullptr
  whole_key_filtering: 1
  verify_compression: 0
  read_amp_bytes_per_bit: 0
  format_version: 5
  enable_index_compression: 1
  block_align: 0
  max_auto_readahead_size: 262144
  prepopulate_block_cache: 0
  initial_auto_readahead_size: 8192

24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :        Options.write_buffer_size: 67108864
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :  Options.max_write_buffer_number: 2
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :          Options.compression: Snappy
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression: Disabled
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :       Options.prefix_extractor: nullptr
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :   Options.memtable_insert_with_hint_prefix_extractor: nullptr
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :             Options.num_levels: 7
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :        Options.min_write_buffer_number_to_merge: 1
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :     Options.max_write_buffer_number_to_maintain: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :     Options.max_write_buffer_size_to_maintain: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :            Options.bottommost_compression_opts.window_bits: -14
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression_opts.level: 32767
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :               Options.bottommost_compression_opts.strategy: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.max_dict_bytes: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.zstd_max_train_bytes: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.parallel_threads: 1
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression_opts.enabled: false
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.max_dict_buffer_bytes: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.use_zstd_dict_trainer: true
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :            Options.compression_opts.window_bits: -14
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                  Options.compression_opts.level: 32767
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :               Options.compression_opts.strategy: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.max_dict_bytes: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.zstd_max_train_bytes: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.use_zstd_dict_trainer: true
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.parallel_threads: 1
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                  Options.compression_opts.enabled: false
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.max_dict_buffer_bytes: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :      Options.level0_file_num_compaction_trigger: 4
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :          Options.level0_slowdown_writes_trigger: 20
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :              Options.level0_stop_writes_trigger: 36
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                   Options.target_file_size_base: 67108864
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :             Options.target_file_size_multiplier: 1
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                Options.max_bytes_for_level_base: 268435456
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Options.level_compaction_dynamic_level_bytes: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :          Options.max_bytes_for_level_multiplier: 10.000000
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[0]: 1
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[1]: 1
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[2]: 1
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[3]: 1
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[4]: 1
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[5]: 1
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[6]: 1
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :       Options.max_sequential_skip_in_iterations: 8
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                    Options.max_compaction_bytes: 1677721600
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                        Options.arena_block_size: 1048576
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :   Options.soft_pending_compaction_bytes_limit: 68719476736
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :   Options.hard_pending_compaction_bytes_limit: 274877906944
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                Options.disable_auto_compactions: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                        Options.compaction_style: kCompactionStyleLevel
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                          Options.compaction_pri: kMinOverlappingRatio
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.size_ratio: 1
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.min_merge_width: 2
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.max_merge_width: 4294967295
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.max_size_amplification_percent: 200
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.compression_size_percent: -1
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.stop_style: kCompactionStopStyleTotalSize
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_fifo.max_table_files_size: 1073741824
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_fifo.allow_compaction: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                   Options.table_properties_collectors: 
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                   Options.inplace_update_support: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                 Options.inplace_update_num_locks: 10000
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :               Options.memtable_prefix_bloom_size_ratio: 0.000000
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :               Options.memtable_whole_key_filtering: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :   Options.memtable_huge_page_size: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                           Options.bloom_locality: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                    Options.max_successive_merges: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                Options.optimize_filters_for_hits: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                Options.paranoid_file_checks: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                Options.force_consistency_checks: 1
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                Options.report_bg_io_stats: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                               Options.ttl: 2592000
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :          Options.periodic_compaction_seconds: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :  Options.preclude_last_level_data_seconds: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                       Options.enable_blob_files: false
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                           Options.min_blob_size: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                          Options.blob_file_size: 268435456
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                   Options.blob_compression_type: NoCompression
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :          Options.enable_blob_garbage_collection: false
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :      Options.blob_garbage_collection_age_cutoff: 0.250000
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Options.blob_garbage_collection_force_threshold: 1.000000
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :          Options.blob_compaction_readahead_size: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB :                Options.blob_file_starting_level: 0
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : Options.experimental_mempurge_threshold: 0.000000
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : [/db_impl/db_impl.cc:2861] Created column family [hudi_pending_clustering_fg_user_hive_warehouse_stock_ticks_mor] (ID 7)
24/01/05 02:43:53 INFO view.RocksDbBasedFileSystemView: Resetting replacedFileGroups to ROCKSDB based file-system view at /tmp/hoodie_timeline_rocksdb, Total file-groups=0
24/01/05 02:43:53 INFO collection.RocksDBDAO: Prefix DELETE (query=part=) on hudi_replaced_fg_user_hive_warehouse_stock_ticks_mor
24/01/05 02:43:53 INFO view.RocksDbBasedFileSystemView: Resetting replacedFileGroups to ROCKSDB based file-system view complete
24/01/05 02:43:53 INFO view.AbstractTableFileSystemView: Took 2 ms to read  0 instants, 0 replaced file groups
24/01/05 02:43:53 INFO view.RocksDbBasedFileSystemView: Initializing pending compaction operations. Count=0
24/01/05 02:43:53 INFO view.RocksDbBasedFileSystemView: Initializing pending Log compaction operations. Count=0
24/01/05 02:43:53 INFO view.RocksDbBasedFileSystemView: Initializing external data file mapping. Count=0
24/01/05 02:43:53 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:43:53 INFO view.RocksDbBasedFileSystemView: Resetting file groups in pending clustering to ROCKSDB based file-system view at /tmp/hoodie_timeline_rocksdb, Total file-groups=0
24/01/05 02:43:53 INFO collection.RocksDBDAO: Prefix DELETE (query=part=) on hudi_pending_clustering_fg_user_hive_warehouse_stock_ticks_mor
24/01/05 02:43:53 INFO view.RocksDbBasedFileSystemView: Resetting replacedFileGroups to ROCKSDB based file-system view complete
24/01/05 02:43:53 INFO view.RocksDbBasedFileSystemView: Created ROCKSDB based file-system view at /tmp/hoodie_timeline_rocksdb
24/01/05 02:43:53 INFO collection.RocksDBDAO: Prefix Search for (query=) on hudi_pending_compaction__user_hive_warehouse_stock_ticks_mor. Total Time Taken (msec)=0. Serialization Time taken(micro)=0, num entries=0
24/01/05 02:43:53 INFO view.RemoteHoodieTableFileSystemView: Sending request : (http://adhoc-1:37525/v1/hoodie/view/logcompactions/pending/?basepath=%2Fuser%2Fhive%2Fwarehouse%2Fstock_ticks_mor&lastinstantts=20240105024343265&timelinehash=7a0b5e1482c059272c847fc6668f12da532a637ce11f6f599d2728792005a6a1)
24/01/05 02:43:53 INFO collection.RocksDBDAO: Prefix Search for (query=) on hudi_pending_log_compaction__user_hive_warehouse_stock_ticks_mor. Total Time Taken (msec)=0. Serialization Time taken(micro)=0, num entries=0
24/01/05 02:43:53 INFO clean.CleanPlanner: No earliest commit to retain. No need to scan partitions !!
24/01/05 02:43:53 INFO clean.CleanPlanActionExecutor: Nothing to clean here. It is already clean
24/01/05 02:43:53 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105024343265__deltacommit__COMPLETED__20240105024352675]}
24/01/05 02:43:53 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:43:53 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:43:53 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:43:53 INFO table.HoodieTableMetaClient: Loading Active commit timeline for /user/hive/warehouse/stock_ticks_mor
24/01/05 02:43:53 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105024343265__deltacommit__COMPLETED__20240105024352675]}
24/01/05 02:43:53 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:43:53 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:43:53 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:43:53 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:43:53 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:43:53 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:43:53 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105024343265__deltacommit__COMPLETED__20240105024352164]}
24/01/05 02:43:53 INFO view.AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
24/01/05 02:43:53 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:43:53 INFO view.FileSystemViewManager: Creating View Manager with storage type :REMOTE_FIRST
24/01/05 02:43:53 INFO view.FileSystemViewManager: Creating remote first table view
24/01/05 02:43:53 INFO client.BaseHoodieWriteClient: Start to archive synchronously.
24/01/05 02:43:53 INFO transaction.TransactionManager: Transaction starting for Optional.empty with latest completed transaction instant Optional.empty
24/01/05 02:43:53 INFO lock.LockManager: LockProvider org.apache.hudi.client.transaction.lock.InProcessLockProvider
24/01/05 02:43:53 INFO lock.InProcessLockProvider: Base Path /user/hive/warehouse/stock_ticks_mor, Lock Instance java.util.concurrent.locks.ReentrantReadWriteLock@592a1882[Write locks = 0, Read locks = 0], Thread main, In-process lock state ACQUIRING
24/01/05 02:43:53 INFO lock.InProcessLockProvider: Base Path /user/hive/warehouse/stock_ticks_mor, Lock Instance java.util.concurrent.locks.ReentrantReadWriteLock@592a1882[Write locks = 1, Read locks = 0], Thread main, In-process lock state ACQUIRED
24/01/05 02:43:53 INFO transaction.TransactionManager: Transaction started for Optional.empty with latest completed transaction instant Optional.empty
24/01/05 02:43:53 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105024343265__deltacommit__COMPLETED__20240105024352675]}
24/01/05 02:43:53 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:43:53 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:43:53 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:43:53 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:43:53 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:43:53 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:43:53 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105024343265__deltacommit__COMPLETED__20240105024352164]}
24/01/05 02:43:53 INFO view.AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
24/01/05 02:43:53 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:43:53 INFO client.HoodieTimelineArchiver: Not archiving as there is no compaction yet on the metadata table
24/01/05 02:43:53 INFO client.HoodieTimelineArchiver: No Instants to archive
24/01/05 02:43:53 INFO transaction.TransactionManager: Transaction ending with transaction owner Optional.empty
24/01/05 02:43:53 INFO lock.InProcessLockProvider: Base Path /user/hive/warehouse/stock_ticks_mor, Lock Instance java.util.concurrent.locks.ReentrantReadWriteLock@592a1882[Write locks = 1, Read locks = 0], Thread main, In-process lock state RELEASING
24/01/05 02:43:53 INFO lock.InProcessLockProvider: Base Path /user/hive/warehouse/stock_ticks_mor, Lock Instance java.util.concurrent.locks.ReentrantReadWriteLock@592a1882[Write locks = 0, Read locks = 0], Thread main, In-process lock state RELEASED
24/01/05 02:43:53 INFO lock.InProcessLockProvider: Base Path /user/hive/warehouse/stock_ticks_mor, Lock Instance java.util.concurrent.locks.ReentrantReadWriteLock@592a1882[Write locks = 0, Read locks = 0], Thread main, In-process lock state ALREADY_RELEASED
24/01/05 02:43:53 INFO lock.LockManager: Released connection created for acquiring lock
24/01/05 02:43:53 INFO transaction.TransactionManager: Transaction ended with transaction owner Optional.empty
24/01/05 02:43:53 INFO view.FileSystemViewManager: Creating remote view for basePath /user/hive/warehouse/stock_ticks_mor. Server=adhoc-1:37525, Timeout=300
24/01/05 02:43:53 INFO view.FileSystemViewManager: Creating InMemory based view for basePath /user/hive/warehouse/stock_ticks_mor
24/01/05 02:43:53 INFO view.AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
24/01/05 02:43:53 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:43:53 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105024343265__deltacommit__COMPLETED__20240105024352675]}
24/01/05 02:43:53 INFO view.RemoteHoodieTableFileSystemView: Sending request : (http://adhoc-1:37525/v1/hoodie/view/refresh/?basepath=%2Fuser%2Fhive%2Fwarehouse%2Fstock_ticks_mor&lastinstantts=20240105024343265&timelinehash=7a0b5e1482c059272c847fc6668f12da532a637ce11f6f599d2728792005a6a1)
24/01/05 02:43:53 INFO view.RocksDbBasedFileSystemView: Closing Rocksdb !!
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : [/db_impl/db_impl.cc:478] Shutdown: canceling all background work
24/01/05 02:43:53 INFO collection.RocksDBDAO: From Rocks DB : [/db_impl/db_impl.cc:677] Shutdown complete
24/01/05 02:43:53 INFO view.RocksDbBasedFileSystemView: Closed Rocksdb !!
24/01/05 02:43:53 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105024343265__deltacommit__COMPLETED__20240105024352675]}
24/01/05 02:43:53 INFO view.AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
24/01/05 02:43:53 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:43:53 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105024343265__deltacommit__COMPLETED__20240105024352675]}
24/01/05 02:43:53 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105024343265__deltacommit__COMPLETED__20240105024352164]}
24/01/05 02:43:53 INFO view.AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
24/01/05 02:43:53 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:43:53 INFO streamer.StreamSync: Commit 20240105024343265 successful!
24/01/05 02:43:53 INFO transaction.TransactionManager: Transaction manager closed
24/01/05 02:43:53 INFO transaction.TransactionManager: Transaction manager closed
24/01/05 02:43:53 INFO streamer.StreamSync: Shutting down embedded timeline server
24/01/05 02:43:53 INFO embedded.EmbeddedTimelineService: Closing Timeline server
24/01/05 02:43:53 INFO service.TimelineService: Closing Timeline Service
24/01/05 02:43:53 INFO javalin.Javalin: Stopping Javalin ...
24/01/05 02:43:53 INFO javalin.Javalin: Javalin has stopped
24/01/05 02:43:53 INFO service.TimelineService: Closed Timeline Service
24/01/05 02:43:53 INFO embedded.EmbeddedTimelineService: Closed Timeline server
24/01/05 02:43:53 INFO ingestion.HoodieIngestionService: Ingestion service (run-once mode) has been shut down.
24/01/05 02:43:53 INFO server.AbstractConnector: Stopped Spark@61019f59{HTTP/1.1,[http/1.1]}{0.0.0.0:8090}
24/01/05 02:43:53 INFO ui.SparkUI: Stopped Spark web UI at http://adhoc-1:8090
24/01/05 02:43:53 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
24/01/05 02:43:53 INFO memory.MemoryStore: MemoryStore cleared
24/01/05 02:43:53 INFO storage.BlockManager: BlockManager stopped
24/01/05 02:43:53 INFO storage.BlockManagerMaster: BlockManagerMaster stopped
24/01/05 02:43:53 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
24/01/05 02:43:53 INFO spark.SparkContext: Successfully stopped SparkContext
24/01/05 02:43:53 INFO util.ShutdownHookManager: Shutdown hook called
24/01/05 02:43:53 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-ed3eef16-baa7-4b27-86ce-84191a56e225
24/01/05 02:43:53 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-f001e7e0-823a-4ddc-a6da-c40036711c21
+ /var/hoodie/ws/hudi-sync/hudi-hive-sync/run_sync_tool.sh --jdbc-url jdbc:hive2://hiveserver:10000 --user hive --pass hive --partitioned-by dt --base-path /user/hive/warehouse/stock_ticks_mor --database default --table stock_ticks_mor --partition-value-extractor org.apache.hudi.hive.SlashEncodedDayPartitionValueExtractor
Running Command : java -cp /opt/hive/lib/hive-metastore-2.3.3.jar::/opt/hive/lib/hive-service-2.3.3.jar::/opt/hive/lib/hive-exec-2.3.3.jar::/opt/hive/lib/hive-jdbc-2.3.3.jar:/opt/hive/lib/hive-jdbc-handler-2.3.3.jar::/opt/hive/lib/jackson-annotations-2.6.0.jar:/opt/hive/lib/jackson-core-2.6.5.jar:/opt/hive/lib/jackson-databind-2.6.5.jar:/opt/hive/lib/jackson-dataformat-smile-2.4.6.jar:/opt/hive/lib/jackson-datatype-guava-2.4.6.jar:/opt/hive/lib/jackson-datatype-joda-2.4.6.jar:/opt/hive/lib/jackson-jaxrs-1.9.13.jar:/opt/hive/lib/jackson-jaxrs-base-2.4.6.jar:/opt/hive/lib/jackson-jaxrs-json-provider-2.4.6.jar:/opt/hive/lib/jackson-jaxrs-smile-provider-2.4.6.jar:/opt/hive/lib/jackson-module-jaxb-annotations-2.4.6.jar:/opt/hive/lib/jackson-xc-1.9.13.jar::/opt/hadoop-2.8.4/share/hadoop/common/*:/opt/hadoop-2.8.4/share/hadoop/mapreduce/*:/opt/hadoop-2.8.4/share/hadoop/hdfs/*:/opt/hadoop-2.8.4/share/hadoop/common/lib/*:/opt/hadoop-2.8.4/share/hadoop/hdfs/lib/*:/etc/hadoop:/var/hoodie/ws/hudi-sync/hudi-hive-sync/../../packaging/hudi-hive-sync-bundle/target/hudi-hive-sync-bundle-0.14.1-rc2.jar org.apache.hudi.hive.HiveSyncTool --jdbc-url jdbc:hive2://hiveserver:10000 --user hive --pass hive --partitioned-by dt --base-path /user/hive/warehouse/stock_ticks_mor --database default --table stock_ticks_mor --partition-value-extractor org.apache.hudi.hive.SlashEncodedDayPartitionValueExtractor
2024-01-05 02:43:54,577 INFO  [main] conf.HiveConf (HiveConf.java:findConfigFile(181)) - Found configuration file file:/opt/hadoop-2.8.4/etc/hadoop/hive-site.xml
2024-01-05 02:43:55,050 WARN  [main] util.NativeCodeLoader (NativeCodeLoader.java:<clinit>(62)) - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2024-01-05 02:43:55,133 INFO  [main] table.HoodieTableMetaClient (HoodieTableMetaClient.java:<init>(133)) - Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
2024-01-05 02:43:55,595 INFO  [main] table.HoodieTableConfig (HoodieTableConfig.java:<init>(276)) - Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
2024-01-05 02:43:55,659 INFO  [main] table.HoodieTableMetaClient (HoodieTableMetaClient.java:<init>(152)) - Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
2024-01-05 02:43:55,659 INFO  [main] table.HoodieTableMetaClient (HoodieTableMetaClient.java:<init>(155)) - Loading Active commit timeline for /user/hive/warehouse/stock_ticks_mor
2024-01-05 02:43:55,685 INFO  [main] timeline.HoodieActiveTimeline (HoodieActiveTimeline.java:<init>(172)) - Loaded instants upto : Option{val=[20240105024343265__deltacommit__COMPLETED__20240105024352675]}
2024-01-05 02:43:55,948 INFO  [main] hive.metastore (HiveMetaStoreClient.java:open(407)) - Trying to connect to metastore with URI thrift://hivemetastore:9083
2024-01-05 02:43:55,964 INFO  [main] hive.metastore (HiveMetaStoreClient.java:open(481)) - Opened a connection to metastore, current connections: 1
2024-01-05 02:43:55,981 INFO  [main] hive.metastore (HiveMetaStoreClient.java:open(534)) - Connected to metastore.
2024-01-05 02:43:56,137 INFO  [main] jdbc.Utils (Utils.java:parseURL(325)) - Supplied authorities: hiveserver:10000
2024-01-05 02:43:56,137 INFO  [main] jdbc.Utils (Utils.java:parseURL(444)) - Resolved authority: hiveserver:10000
2024-01-05 02:43:56,594 INFO  [main] ddl.QueryBasedDDLExecutor (JDBCExecutor.java:createHiveConnection(105)) - Successfully established Hive connection to  jdbc:hive2://hiveserver:10000
2024-01-05 02:43:56,595 INFO  [main] hive.HiveSyncTool (HiveSyncTool.java:syncHoodieTable(162)) - Syncing target hoodie table with hive table(default.stock_ticks_mor). Hive metastore URL from HiveConf:thrift://hivemetastore:9083). Hive metastore URL from HiveSyncConfig:null, basePath :/user/hive/warehouse/stock_ticks_mor
2024-01-05 02:43:56,595 INFO  [main] hive.HiveSyncTool (HiveSyncTool.java:syncHoodieTable(224)) - Trying to sync hoodie table stock_ticks_mor_ro with base path /user/hive/warehouse/stock_ticks_mor of type MERGE_ON_READ
2024-01-05 02:43:56,836 INFO  [main] table.TableSchemaResolver (TableSchemaResolver.java:readSchemaFromParquetBaseFile(329)) - Reading schema from /user/hive/warehouse/stock_ticks_mor/2018/08/31/6ad87b7b-db22-4d47-afc3-cb2b288006ba-0_0-23-24_20240105024343265.parquet
SLF4J: Failed to load class "org.slf4j.impl.StaticLoggerBinder".
SLF4J: Defaulting to no-operation (NOP) logger implementation
SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.
2024-01-05 02:43:57,072 INFO  [main] hive.HiveSyncTool (HiveSyncTool.java:syncFirstTime(321)) - Sync table stock_ticks_mor_ro for the first time.
2024-01-05 02:43:57,083 INFO  [main] ddl.QueryBasedDDLExecutor (QueryBasedDDLExecutor.java:createTable(91)) - Creating table with CREATE EXTERNAL TABLE IF NOT EXISTS `default`.`stock_ticks_mor_ro`( `_hoodie_commit_time` string, `_hoodie_commit_seqno` string, `_hoodie_record_key` string, `_hoodie_partition_path` string, `_hoodie_file_name` string, `volume` bigint, `ts` string, `symbol` string, `year` int, `month` string, `high` double, `low` double, `key` string, `date` string, `close` double, `open` double, `day` string) PARTITIONED BY (`dt` string) ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe' WITH SERDEPROPERTIES ('hoodie.query.as.ro.table'='true','path'='/user/hive/warehouse/stock_ticks_mor') STORED AS INPUTFORMAT 'org.apache.hudi.hadoop.HoodieParquetInputFormat' OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat' LOCATION '/user/hive/warehouse/stock_ticks_mor' TBLPROPERTIES('spark.sql.sources.schema.partCol.0'='dt','spark.sql.sources.schema.numParts'='1','spark.sql.sources.schema.numPartCols'='1','spark.sql.sources.provider'='hudi','spark.sql.sources.schema.part.0'='{"type":"struct","fields":[{"name":"_hoodie_commit_time","type":"string","nullable":true,"metadata":{}},{"name":"_hoodie_commit_seqno","type":"string","nullable":true,"metadata":{}},{"name":"_hoodie_record_key","type":"string","nullable":true,"metadata":{}},{"name":"_hoodie_partition_path","type":"string","nullable":true,"metadata":{}},{"name":"_hoodie_file_name","type":"string","nullable":true,"metadata":{}},{"name":"volume","type":"long","nullable":false,"metadata":{}},{"name":"ts","type":"string","nullable":false,"metadata":{}},{"name":"symbol","type":"string","nullable":false,"metadata":{}},{"name":"year","type":"integer","nullable":false,"metadata":{}},{"name":"month","type":"string","nullable":false,"metadata":{}},{"name":"high","type":"double","nullable":false,"metadata":{}},{"name":"low","type":"double","nullable":false,"metadata":{}},{"name":"key","type":"string","nullable":false,"metadata":{}},{"name":"date","type":"string","nullable":false,"metadata":{}},{"name":"close","type":"double","nullable":false,"metadata":{}},{"name":"open","type":"double","nullable":false,"metadata":{}},{"name":"day","type":"string","nullable":false,"metadata":{}},{"name":"dt","type":"string","nullable":false,"metadata":{}}]}')
2024-01-05 02:43:57,085 INFO  [main] ddl.QueryBasedDDLExecutor (JDBCExecutor.java:runSQL(67)) - Executing SQL CREATE EXTERNAL TABLE IF NOT EXISTS `default`.`stock_ticks_mor_ro`( `_hoodie_commit_time` string, `_hoodie_commit_seqno` string, `_hoodie_record_key` string, `_hoodie_partition_path` string, `_hoodie_file_name` string, `volume` bigint, `ts` string, `symbol` string, `year` int, `month` string, `high` double, `low` double, `key` string, `date` string, `close` double, `open` double, `day` string) PARTITIONED BY (`dt` string) ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe' WITH SERDEPROPERTIES ('hoodie.query.as.ro.table'='true','path'='/user/hive/warehouse/stock_ticks_mor') STORED AS INPUTFORMAT 'org.apache.hudi.hadoop.HoodieParquetInputFormat' OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat' LOCATION '/user/hive/warehouse/stock_ticks_mor' TBLPROPERTIES('spark.sql.sources.schema.partCol.0'='dt','spark.sql.sources.schema.numParts'='1','spark.sql.sources.schema.numPartCols'='1','spark.sql.sources.provider'='hudi','spark.sql.sources.schema.part.0'='{"type":"struct","fields":[{"name":"_hoodie_commit_time","type":"string","nullable":true,"metadata":{}},{"name":"_hoodie_commit_seqno","type":"string","nullable":true,"metadata":{}},{"name":"_hoodie_record_key","type":"string","nullable":true,"metadata":{}},{"name":"_hoodie_partition_path","type":"string","nullable":true,"metadata":{}},{"name":"_hoodie_file_name","type":"string","nullable":true,"metadata":{}},{"name":"volume","type":"long","nullable":false,"metadata":{}},{"name":"ts","type":"string","nullable":false,"metadata":{}},{"name":"symbol","type":"string","nullable":false,"metadata":{}},{"name":"year","type":"integer","nullable":false,"metadata":{}},{"name":"month","type":"string","nullable":false,"metadata":{}},{"name":"high","type":"double","nullable":false,"metadata":{}},{"name":"low","type":"double","nullable":false,"metadata":{}},{"name":"key","type":"string","nullable":false,"metadata":{}},{"name":"date","type":"string","nullable":false,"metadata":{}},{"name":"close","type":"double","nullable":false,"metadata":{}},{"name":"open","type":"double","nullable":false,"metadata":{}},{"name":"day","type":"string","nullable":false,"metadata":{}},{"name":"dt","type":"string","nullable":false,"metadata":{}}]}')
2024-01-05 02:43:58,138 INFO  [main] hive.HiveSyncTool (HiveSyncTool.java:syncHoodieTable(265)) - Last commit time synced was found to be null, last commit completion time is found to be null
2024-01-05 02:43:58,138 INFO  [main] hive.HiveSyncTool (HiveSyncTool.java:syncHoodieTable(279)) - Sync all partitions given the last commit time synced is empty or before the start of the active timeline. Listing all partitions in /user/hive/warehouse/stock_ticks_mor, file system: DFS[DFSClient[clientName=DFSClient_NONMAPREDUCE_-1699841838_1, ugi=root (auth:SIMPLE)]]
2024-01-05 02:43:58,190 INFO  [main] table.HoodieTableConfig (HoodieTableConfig.java:<init>(276)) - Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
2024-01-05 02:43:58,218 INFO  [main] hive.HiveSyncTool (HiveSyncTool.java:syncPartitions(459)) - New Partitions [2018/08/31]
2024-01-05 02:43:58,218 INFO  [main] ddl.QueryBasedDDLExecutor (QueryBasedDDLExecutor.java:addPartitionsToTable(122)) - Adding partitions 1 to table stock_ticks_mor_ro
2024-01-05 02:43:58,219 INFO  [main] ddl.QueryBasedDDLExecutor (JDBCExecutor.java:runSQL(67)) - Executing SQL ALTER TABLE `default`.`stock_ticks_mor_ro` ADD IF NOT EXISTS   PARTITION (`dt`='2018-08-31') LOCATION '/user/hive/warehouse/stock_ticks_mor/2018/08/31' 
2024-01-05 02:43:58,477 INFO  [main] hive.HiveSyncTool (HiveSyncTool.java:syncHoodieTable(298)) - Sync complete for stock_ticks_mor_ro
2024-01-05 02:43:58,477 INFO  [main] hive.HiveSyncTool (HiveSyncTool.java:syncHoodieTable(224)) - Trying to sync hoodie table stock_ticks_mor_rt with base path /user/hive/warehouse/stock_ticks_mor of type MERGE_ON_READ
2024-01-05 02:43:58,491 INFO  [main] table.TableSchemaResolver (TableSchemaResolver.java:readSchemaFromParquetBaseFile(329)) - Reading schema from /user/hive/warehouse/stock_ticks_mor/2018/08/31/6ad87b7b-db22-4d47-afc3-cb2b288006ba-0_0-23-24_20240105024343265.parquet
2024-01-05 02:43:58,506 INFO  [main] hive.HiveSyncTool (HiveSyncTool.java:syncFirstTime(321)) - Sync table stock_ticks_mor_rt for the first time.
2024-01-05 02:43:58,506 INFO  [main] ddl.QueryBasedDDLExecutor (QueryBasedDDLExecutor.java:createTable(91)) - Creating table with CREATE EXTERNAL TABLE IF NOT EXISTS `default`.`stock_ticks_mor_rt`( `_hoodie_commit_time` string, `_hoodie_commit_seqno` string, `_hoodie_record_key` string, `_hoodie_partition_path` string, `_hoodie_file_name` string, `volume` bigint, `ts` string, `symbol` string, `year` int, `month` string, `high` double, `low` double, `key` string, `date` string, `close` double, `open` double, `day` string) PARTITIONED BY (`dt` string) ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe' WITH SERDEPROPERTIES ('hoodie.query.as.ro.table'='false','path'='/user/hive/warehouse/stock_ticks_mor') STORED AS INPUTFORMAT 'org.apache.hudi.hadoop.realtime.HoodieParquetRealtimeInputFormat' OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat' LOCATION '/user/hive/warehouse/stock_ticks_mor' TBLPROPERTIES('spark.sql.sources.schema.partCol.0'='dt','spark.sql.sources.schema.numParts'='1','spark.sql.sources.schema.numPartCols'='1','spark.sql.sources.provider'='hudi','spark.sql.sources.schema.part.0'='{"type":"struct","fields":[{"name":"_hoodie_commit_time","type":"string","nullable":true,"metadata":{}},{"name":"_hoodie_commit_seqno","type":"string","nullable":true,"metadata":{}},{"name":"_hoodie_record_key","type":"string","nullable":true,"metadata":{}},{"name":"_hoodie_partition_path","type":"string","nullable":true,"metadata":{}},{"name":"_hoodie_file_name","type":"string","nullable":true,"metadata":{}},{"name":"volume","type":"long","nullable":false,"metadata":{}},{"name":"ts","type":"string","nullable":false,"metadata":{}},{"name":"symbol","type":"string","nullable":false,"metadata":{}},{"name":"year","type":"integer","nullable":false,"metadata":{}},{"name":"month","type":"string","nullable":false,"metadata":{}},{"name":"high","type":"double","nullable":false,"metadata":{}},{"name":"low","type":"double","nullable":false,"metadata":{}},{"name":"key","type":"string","nullable":false,"metadata":{}},{"name":"date","type":"string","nullable":false,"metadata":{}},{"name":"close","type":"double","nullable":false,"metadata":{}},{"name":"open","type":"double","nullable":false,"metadata":{}},{"name":"day","type":"string","nullable":false,"metadata":{}},{"name":"dt","type":"string","nullable":false,"metadata":{}}]}')
2024-01-05 02:43:58,506 INFO  [main] ddl.QueryBasedDDLExecutor (JDBCExecutor.java:runSQL(67)) - Executing SQL CREATE EXTERNAL TABLE IF NOT EXISTS `default`.`stock_ticks_mor_rt`( `_hoodie_commit_time` string, `_hoodie_commit_seqno` string, `_hoodie_record_key` string, `_hoodie_partition_path` string, `_hoodie_file_name` string, `volume` bigint, `ts` string, `symbol` string, `year` int, `month` string, `high` double, `low` double, `key` string, `date` string, `close` double, `open` double, `day` string) PARTITIONED BY (`dt` string) ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe' WITH SERDEPROPERTIES ('hoodie.query.as.ro.table'='false','path'='/user/hive/warehouse/stock_ticks_mor') STORED AS INPUTFORMAT 'org.apache.hudi.hadoop.realtime.HoodieParquetRealtimeInputFormat' OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat' LOCATION '/user/hive/warehouse/stock_ticks_mor' TBLPROPERTIES('spark.sql.sources.schema.partCol.0'='dt','spark.sql.sources.schema.numParts'='1','spark.sql.sources.schema.numPartCols'='1','spark.sql.sources.provider'='hudi','spark.sql.sources.schema.part.0'='{"type":"struct","fields":[{"name":"_hoodie_commit_time","type":"string","nullable":true,"metadata":{}},{"name":"_hoodie_commit_seqno","type":"string","nullable":true,"metadata":{}},{"name":"_hoodie_record_key","type":"string","nullable":true,"metadata":{}},{"name":"_hoodie_partition_path","type":"string","nullable":true,"metadata":{}},{"name":"_hoodie_file_name","type":"string","nullable":true,"metadata":{}},{"name":"volume","type":"long","nullable":false,"metadata":{}},{"name":"ts","type":"string","nullable":false,"metadata":{}},{"name":"symbol","type":"string","nullable":false,"metadata":{}},{"name":"year","type":"integer","nullable":false,"metadata":{}},{"name":"month","type":"string","nullable":false,"metadata":{}},{"name":"high","type":"double","nullable":false,"metadata":{}},{"name":"low","type":"double","nullable":false,"metadata":{}},{"name":"key","type":"string","nullable":false,"metadata":{}},{"name":"date","type":"string","nullable":false,"metadata":{}},{"name":"close","type":"double","nullable":false,"metadata":{}},{"name":"open","type":"double","nullable":false,"metadata":{}},{"name":"day","type":"string","nullable":false,"metadata":{}},{"name":"dt","type":"string","nullable":false,"metadata":{}}]}')
2024-01-05 02:43:58,562 INFO  [main] hive.HiveSyncTool (HiveSyncTool.java:syncHoodieTable(265)) - Last commit time synced was found to be null, last commit completion time is found to be null
2024-01-05 02:43:58,563 INFO  [main] hive.HiveSyncTool (HiveSyncTool.java:syncHoodieTable(279)) - Sync all partitions given the last commit time synced is empty or before the start of the active timeline. Listing all partitions in /user/hive/warehouse/stock_ticks_mor, file system: DFS[DFSClient[clientName=DFSClient_NONMAPREDUCE_-1699841838_1, ugi=root (auth:SIMPLE)]]
2024-01-05 02:43:58,582 INFO  [main] table.HoodieTableConfig (HoodieTableConfig.java:<init>(276)) - Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
2024-01-05 02:43:58,596 INFO  [main] hive.HiveSyncTool (HiveSyncTool.java:syncPartitions(459)) - New Partitions [2018/08/31]
2024-01-05 02:43:58,596 INFO  [main] ddl.QueryBasedDDLExecutor (QueryBasedDDLExecutor.java:addPartitionsToTable(122)) - Adding partitions 1 to table stock_ticks_mor_rt
2024-01-05 02:43:58,597 INFO  [main] ddl.QueryBasedDDLExecutor (JDBCExecutor.java:runSQL(67)) - Executing SQL ALTER TABLE `default`.`stock_ticks_mor_rt` ADD IF NOT EXISTS   PARTITION (`dt`='2018-08-31') LOCATION '/user/hive/warehouse/stock_ticks_mor/2018/08/31' 
2024-01-05 02:43:58,722 INFO  [main] hive.HiveSyncTool (HiveSyncTool.java:syncHoodieTable(298)) - Sync complete for stock_ticks_mor_rt
2024-01-05 02:43:58,753 INFO  [main] hive.metastore (HiveMetaStoreClient.java:close(564)) - Closed a connection to metastore, current connections: 0
+ cat /home/alex/github/alexttx/hudi/docker/demo/data/batch_2.json
+ head -n100
+ kcat -b kafkabroker -t stock_ticks -P
+ docker exec -i adhoc-2 /bin/bash -x
+ spark-submit --class org.apache.hudi.utilities.streamer.HoodieStreamer /var/hoodie/ws/docker/hoodie/hadoop/hive_base/target/hoodie-utilities.jar --table-type MERGE_ON_READ --source-class org.apache.hudi.utilities.sources.JsonKafkaSource --source-ordering-field ts --target-base-path /user/hive/warehouse/stock_ticks_mor --target-table stock_ticks_mor --props /var/demo/config/kafka-source.properties --schemaprovider-class org.apache.hudi.utilities.schema.FilebasedSchemaProvider --disable-compaction
24/01/05 02:44:00 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
24/01/05 02:44:00 WARN streamer.SchedulerConfGenerator: Job Scheduling Configs will not be in effect as spark.scheduler.mode is not set to FAIR at instantiation time. Continuing without scheduling configs
24/01/05 02:44:00 INFO spark.SparkContext: Running Spark version 2.4.4
24/01/05 02:44:00 INFO spark.SparkContext: Submitted application: streamer-stock_ticks_mor
24/01/05 02:44:00 INFO spark.SecurityManager: Changing view acls to: root
24/01/05 02:44:00 INFO spark.SecurityManager: Changing modify acls to: root
24/01/05 02:44:00 INFO spark.SecurityManager: Changing view acls groups to: 
24/01/05 02:44:00 INFO spark.SecurityManager: Changing modify acls groups to: 
24/01/05 02:44:00 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
24/01/05 02:44:00 INFO Configuration.deprecation: mapred.output.compression.codec is deprecated. Instead, use mapreduce.output.fileoutputformat.compress.codec
24/01/05 02:44:00 INFO Configuration.deprecation: mapred.output.compress is deprecated. Instead, use mapreduce.output.fileoutputformat.compress
24/01/05 02:44:00 INFO Configuration.deprecation: mapred.output.compression.type is deprecated. Instead, use mapreduce.output.fileoutputformat.compress.type
24/01/05 02:44:00 INFO util.Utils: Successfully started service 'sparkDriver' on port 33613.
24/01/05 02:44:00 INFO spark.SparkEnv: Registering MapOutputTracker
24/01/05 02:44:00 INFO spark.SparkEnv: Registering BlockManagerMaster
24/01/05 02:44:00 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
24/01/05 02:44:00 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
24/01/05 02:44:00 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-991ac7fb-d206-4072-bc86-ff4c9f8d9ea7
24/01/05 02:44:00 INFO memory.MemoryStore: MemoryStore started with capacity 366.3 MB
24/01/05 02:44:00 INFO spark.SparkEnv: Registering OutputCommitCoordinator
24/01/05 02:44:00 INFO util.log: Logging initialized @1626ms
24/01/05 02:44:00 INFO server.Server: jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
24/01/05 02:44:00 INFO server.Server: Started @1688ms
24/01/05 02:44:00 INFO server.AbstractConnector: Started ServerConnector@61019f59{HTTP/1.1,[http/1.1]}{0.0.0.0:8090}
24/01/05 02:44:00 INFO util.Utils: Successfully started service 'SparkUI' on port 8090.
24/01/05 02:44:00 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3e1162e7{/jobs,null,AVAILABLE,@Spark}
24/01/05 02:44:00 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@17f460bb{/jobs/json,null,AVAILABLE,@Spark}
24/01/05 02:44:00 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@64a1923a{/jobs/job,null,AVAILABLE,@Spark}
24/01/05 02:44:00 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@18ca3c62{/jobs/job/json,null,AVAILABLE,@Spark}
24/01/05 02:44:00 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2c0f7678{/stages,null,AVAILABLE,@Spark}
24/01/05 02:44:00 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@44d70181{/stages/json,null,AVAILABLE,@Spark}
24/01/05 02:44:00 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6aa648b9{/stages/stage,null,AVAILABLE,@Spark}
24/01/05 02:44:00 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@88a8218{/stages/stage/json,null,AVAILABLE,@Spark}
24/01/05 02:44:00 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@50b1f030{/stages/pool,null,AVAILABLE,@Spark}
24/01/05 02:44:00 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4163f1cd{/stages/pool/json,null,AVAILABLE,@Spark}
24/01/05 02:44:00 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5fa05212{/storage,null,AVAILABLE,@Spark}
24/01/05 02:44:00 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3e681bc{/storage/json,null,AVAILABLE,@Spark}
24/01/05 02:44:00 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5c09d180{/storage/rdd,null,AVAILABLE,@Spark}
24/01/05 02:44:00 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@23aae55{/storage/rdd/json,null,AVAILABLE,@Spark}
24/01/05 02:44:00 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5f574cc2{/environment,null,AVAILABLE,@Spark}
24/01/05 02:44:00 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@680bddf5{/environment/json,null,AVAILABLE,@Spark}
24/01/05 02:44:00 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7a9c84a5{/executors,null,AVAILABLE,@Spark}
24/01/05 02:44:00 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2d83c5a5{/executors/json,null,AVAILABLE,@Spark}
24/01/05 02:44:00 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@48d7ad8b{/executors/threadDump,null,AVAILABLE,@Spark}
24/01/05 02:44:00 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7e053511{/executors/threadDump/json,null,AVAILABLE,@Spark}
24/01/05 02:44:00 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@60222fd8{/static,null,AVAILABLE,@Spark}
24/01/05 02:44:00 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@37ff4054{/,null,AVAILABLE,@Spark}
24/01/05 02:44:00 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@894858{/api,null,AVAILABLE,@Spark}
24/01/05 02:44:00 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@74cf8b28{/jobs/job/kill,null,AVAILABLE,@Spark}
24/01/05 02:44:00 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@36c54a56{/stages/stage/kill,null,AVAILABLE,@Spark}
24/01/05 02:44:00 INFO ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://adhoc-2:8090
24/01/05 02:44:00 INFO spark.SparkContext: Added JAR file:/var/hoodie/ws/docker/hoodie/hadoop/hive_base/target/hoodie-utilities.jar at spark://adhoc-2:33613/jars/hoodie-utilities.jar with timestamp 1704422640915
24/01/05 02:44:00 INFO executor.Executor: Starting executor ID driver on host localhost
24/01/05 02:44:01 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46387.
24/01/05 02:44:01 INFO netty.NettyBlockTransferService: Server created on adhoc-2:46387
24/01/05 02:44:01 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
24/01/05 02:44:01 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, adhoc-2, 46387, None)
24/01/05 02:44:01 INFO storage.BlockManagerMasterEndpoint: Registering block manager adhoc-2:46387 with 366.3 MB RAM, BlockManagerId(driver, adhoc-2, 46387, None)
24/01/05 02:44:01 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, adhoc-2, 46387, None)
24/01/05 02:44:01 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, adhoc-2, 46387, None)
24/01/05 02:44:01 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@62e6a3ec{/metrics/json,null,AVAILABLE,@Spark}
24/01/05 02:44:01 WARN config.DFSPropertiesConfiguration: Cannot find HUDI_CONF_DIR, please set it as the dir of hudi-defaults.conf
24/01/05 02:44:01 WARN config.DFSPropertiesConfiguration: Properties file file:/etc/hudi/conf/hudi-defaults.conf not found. Ignoring to load props file
24/01/05 02:44:02 INFO internal.SharedState: loading hive config file: file:/opt/hadoop-2.8.4/etc/hadoop/hive-site.xml
24/01/05 02:44:02 INFO internal.SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/opt/spark-warehouse').
24/01/05 02:44:02 INFO internal.SharedState: Warehouse path is 'file:/opt/spark-warehouse'.
24/01/05 02:44:02 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@423c5404{/SQL,null,AVAILABLE,@Spark}
24/01/05 02:44:02 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5853ca50{/SQL/json,null,AVAILABLE,@Spark}
24/01/05 02:44:02 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3c79088e{/SQL/execution,null,AVAILABLE,@Spark}
24/01/05 02:44:02 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4a37191a{/SQL/execution/json,null,AVAILABLE,@Spark}
24/01/05 02:44:02 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@24eb65e3{/static/sql,null,AVAILABLE,@Spark}
24/01/05 02:44:02 INFO state.StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
24/01/05 02:44:02 WARN sql.SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
24/01/05 02:44:02 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:44:02 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:44:02 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:44:02 INFO factory.HoodieSparkKeyGeneratorFactory: The value of hoodie.datasource.write.keygenerator.type is empty; inferred to be SIMPLE
24/01/05 02:44:02 INFO streamer.HoodieStreamer: Creating Hudi Streamer with configs:
auto.offset.reset: earliest
bootstrap.servers: kafkabroker:9092
hoodie.auto.adjust.lock.configs: true
hoodie.bulkinsert.shuffle.parallelism: 2
hoodie.compact.inline: false
hoodie.datasource.write.partitionpath.field: date
hoodie.datasource.write.reconcile.schema: false
hoodie.datasource.write.recordkey.field: key
hoodie.delete.shuffle.parallelism: 2
hoodie.embed.timeline.server: true
hoodie.filesystem.view.type: EMBEDDED_KV_STORE
hoodie.insert.shuffle.parallelism: 2
hoodie.streamer.schemaprovider.source.schema.file: /var/demo/config/schema.avsc
hoodie.streamer.schemaprovider.target.schema.file: /var/demo/config/schema.avsc
hoodie.streamer.source.kafka.topic: stock_ticks
hoodie.table.type: MERGE_ON_READ
hoodie.upsert.shuffle.parallelism: 2

24/01/05 02:44:02 INFO fs.FSUtils: Resolving file /var/demo/config/schema.avscto be a remote file.
24/01/05 02:44:02 INFO factory.HoodieSparkKeyGeneratorFactory: The value of hoodie.datasource.write.keygenerator.type is empty; inferred to be SIMPLE
24/01/05 02:44:02 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:44:02 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:44:02 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:44:02 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105024343265__deltacommit__COMPLETED__20240105024352675]}
24/01/05 02:44:02 INFO config.HoodieWriteConfig: Automatically set hoodie.write.lock.provider=org.apache.hudi.client.transaction.lock.InProcessLockProvider since user has not set the lock provider for single writer with async table services
24/01/05 02:44:02 INFO config.HoodieWriteConfig: Automatically set hoodie.write.lock.provider=org.apache.hudi.client.transaction.lock.InProcessLockProvider since user has not set the lock provider for single writer with async table services
24/01/05 02:44:02 INFO ingestion.HoodieIngestionService: Ingestion service starts running in run-once mode
24/01/05 02:44:02 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:44:02 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:44:02 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:44:02 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105024343265__deltacommit__COMPLETED__20240105024352675]}
24/01/05 02:44:02 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:44:02 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:44:02 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:44:02 INFO streamer.StreamSync: Checkpoint to resume from : Option{val=stock_ticks,0:1866}
24/01/05 02:44:02 INFO consumer.ConsumerConfig: ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [kafkabroker:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

24/01/05 02:44:02 INFO utils.AppInfoParser: Kafka version : 2.0.0
24/01/05 02:44:02 INFO utils.AppInfoParser: Kafka commitId : 3402a8361b734732
24/01/05 02:44:02 INFO clients.Metadata: Cluster ID: XFgecpgeR0meVK58zP5FxA
24/01/05 02:44:03 INFO helpers.KafkaOffsetGen: SourceLimit not configured, set numEvents to default value : 5000000
24/01/05 02:44:03 INFO helpers.KafkaOffsetGen: getNextOffsetRanges set config hoodie.streamer.source.kafka.minPartitions to 0
24/01/05 02:44:03 INFO sources.KafkaSource: About to read 100 from Kafka for topic :stock_ticks
24/01/05 02:44:03 WARN kafka010.KafkaUtils: overriding enable.auto.commit to false for executor
24/01/05 02:44:03 WARN kafka010.KafkaUtils: overriding auto.offset.reset to none for executor
24/01/05 02:44:03 ERROR kafka010.KafkaUtils: group.id is null, you should probably set it
24/01/05 02:44:03 WARN kafka010.KafkaUtils: overriding executor group.id to spark-executor-null
24/01/05 02:44:03 WARN kafka010.KafkaUtils: overriding receive.buffer.bytes to 65536 see KAFKA-3135
24/01/05 02:44:03 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105024343265__deltacommit__COMPLETED__20240105024352675]}
24/01/05 02:44:03 INFO factory.HoodieSparkKeyGeneratorFactory: The value of hoodie.datasource.write.keygenerator.type is empty; inferred to be SIMPLE
24/01/05 02:44:03 INFO streamer.StreamSync: Setting up new Hoodie Write Client
24/01/05 02:44:03 INFO config.HoodieWriteConfig: Automatically set hoodie.write.lock.provider=org.apache.hudi.client.transaction.lock.InProcessLockProvider since user has not set the lock provider for single writer with async table services
24/01/05 02:44:03 INFO embedded.EmbeddedTimelineService: Overriding hostIp to (adhoc-2) found in spark-conf. It was null
24/01/05 02:44:03 INFO view.FileSystemViewManager: Creating View Manager with storage type :EMBEDDED_KV_STORE
24/01/05 02:44:03 INFO view.FileSystemViewManager: Creating embedded rocks-db based Table View
24/01/05 02:44:03 INFO util.log: Logging initialized @4218ms to org.apache.hudi.org.eclipse.jetty.util.log.Slf4jLog
24/01/05 02:44:03 INFO javalin.Javalin: 
       __                      __ _            __ __
      / /____ _ _   __ ____ _ / /(_)____      / // /
 __  / // __ `/| | / // __ `// // // __ \    / // /_
/ /_/ // /_/ / | |/ // /_/ // // // / / /   /__  __/
\____/ \__,_/  |___/ \__,_//_//_//_/ /_/      /_/

          https://javalin.io/documentation

24/01/05 02:44:03 INFO javalin.Javalin: Starting Javalin ...
24/01/05 02:44:03 INFO javalin.Javalin: You are running Javalin 4.6.7 (released October 24, 2022. Your Javalin version is 437 days old. Consider checking for a newer version.).
24/01/05 02:44:03 INFO server.Server: jetty-9.4.48.v20220622; built: 2022-06-21T20:42:25.880Z; git: 6b67c5719d1f4371b33655ff2d047d24e171e49a; jvm 1.8.0_212-b04
24/01/05 02:44:03 INFO server.Server: Started @4655ms
24/01/05 02:44:03 INFO javalin.Javalin: Listening on http://localhost:42425/
24/01/05 02:44:03 INFO javalin.Javalin: Javalin started in 179ms \o/
24/01/05 02:44:03 INFO service.TimelineService: Starting Timeline server on port :42425
24/01/05 02:44:03 INFO embedded.EmbeddedTimelineService: Started embedded timeline server at adhoc-2:42425
24/01/05 02:44:03 INFO client.BaseHoodieClient: Timeline Server already running. Not restarting the service
24/01/05 02:44:03 INFO client.BaseHoodieClient: Timeline Server already running. Not restarting the service
24/01/05 02:44:03 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:44:03 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:44:03 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:44:03 INFO table.HoodieTableMetaClient: Loading Active commit timeline for /user/hive/warehouse/stock_ticks_mor
24/01/05 02:44:03 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105024343265__deltacommit__COMPLETED__20240105024352675]}
24/01/05 02:44:03 INFO util.CleanerUtils: Cleaned failed attempts if any
24/01/05 02:44:03 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:44:03 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:44:03 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:44:03 INFO table.HoodieTableMetaClient: Loading Active commit timeline for /user/hive/warehouse/stock_ticks_mor
24/01/05 02:44:03 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105024343265__deltacommit__COMPLETED__20240105024352675]}
24/01/05 02:44:03 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:44:03 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:44:03 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:44:03 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:44:03 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:44:03 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:44:03 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105024343265__deltacommit__COMPLETED__20240105024352164]}
24/01/05 02:44:03 INFO view.AbstractTableFileSystemView: Took 1 ms to read  0 instants, 0 replaced file groups
24/01/05 02:44:03 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:44:03 INFO view.FileSystemViewManager: Creating View Manager with storage type :REMOTE_FIRST
24/01/05 02:44:03 INFO view.FileSystemViewManager: Creating remote first table view
24/01/05 02:44:03 INFO client.BaseHoodieWriteClient: Generate a new instant time: 20240105024402665 action: deltacommit
24/01/05 02:44:03 INFO timeline.HoodieActiveTimeline: Creating a new instant [==>20240105024402665__deltacommit__REQUESTED]
24/01/05 02:44:03 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:44:03 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:44:03 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:44:03 INFO table.HoodieTableMetaClient: Loading Active commit timeline for /user/hive/warehouse/stock_ticks_mor
24/01/05 02:44:03 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[==>20240105024402665__deltacommit__REQUESTED__20240105024403928]}
24/01/05 02:44:03 INFO transaction.TransactionManager: Transaction starting for Option{val=[==>20240105024402665__deltacommit__INFLIGHT]} with latest completed transaction instant Optional.empty
24/01/05 02:44:03 INFO lock.LockManager: LockProvider org.apache.hudi.client.transaction.lock.InProcessLockProvider
24/01/05 02:44:03 INFO lock.InProcessLockProvider: Base Path /user/hive/warehouse/stock_ticks_mor, Lock Instance java.util.concurrent.locks.ReentrantReadWriteLock@7254838[Write locks = 0, Read locks = 0], Thread main, In-process lock state ACQUIRING
24/01/05 02:44:03 INFO lock.InProcessLockProvider: Base Path /user/hive/warehouse/stock_ticks_mor, Lock Instance java.util.concurrent.locks.ReentrantReadWriteLock@7254838[Write locks = 1, Read locks = 0], Thread main, In-process lock state ACQUIRED
24/01/05 02:44:03 INFO transaction.TransactionManager: Transaction started for Option{val=[==>20240105024402665__deltacommit__INFLIGHT]} with latest completed transaction instant Optional.empty
24/01/05 02:44:03 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:44:03 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:44:03 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:44:04 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:44:04 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:44:04 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:44:04 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105024343265__deltacommit__COMPLETED__20240105024352164]}
24/01/05 02:44:04 INFO metadata.HoodieBackedTableMetadataWriter: Async metadata indexing disabled and following partitions already initialized: [files]
24/01/05 02:44:04 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:44:04 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:44:04 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:44:04 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:44:04 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:44:04 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:44:04 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105024343265__deltacommit__COMPLETED__20240105024352164]}
24/01/05 02:44:04 INFO view.AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
24/01/05 02:44:04 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:44:04 INFO client.BaseHoodieClient: Embedded Timeline Server is disabled. Not starting timeline service
24/01/05 02:44:04 INFO client.BaseHoodieClient: Embedded Timeline Server is disabled. Not starting timeline service
24/01/05 02:44:04 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:44:04 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:44:04 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:44:04 INFO table.HoodieTableMetaClient: Loading Active commit timeline for /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:44:04 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105024343265__deltacommit__COMPLETED__20240105024352164]}
24/01/05 02:44:04 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:44:04 INFO view.FileSystemViewManager: Creating View Manager with storage type :MEMORY
24/01/05 02:44:04 INFO view.FileSystemViewManager: Creating in-memory based Table View
24/01/05 02:44:04 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:44:04 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:44:04 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:44:04 INFO table.HoodieTableMetaClient: Loading Active commit timeline for /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:44:04 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105024343265__deltacommit__COMPLETED__20240105024352164]}
24/01/05 02:44:04 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:44:04 INFO view.FileSystemViewManager: Creating View Manager with storage type :MEMORY
24/01/05 02:44:04 INFO view.FileSystemViewManager: Creating in-memory based Table View
24/01/05 02:44:04 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105024343265__deltacommit__COMPLETED__20240105024352164]}
24/01/05 02:44:04 INFO metadata.HoodieBackedTableMetadataWriter: Latest deltacommit time found is 20240105024343265, running clean operations.
24/01/05 02:44:04 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105024343265__deltacommit__COMPLETED__20240105024352164]}
24/01/05 02:44:04 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:44:04 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:44:04 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:44:04 INFO table.HoodieTableMetaClient: Loading Active commit timeline for /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:44:04 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105024343265__deltacommit__COMPLETED__20240105024352164]}
24/01/05 02:44:04 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:44:04 INFO view.FileSystemViewManager: Creating View Manager with storage type :MEMORY
24/01/05 02:44:04 INFO view.FileSystemViewManager: Creating in-memory based Table View
24/01/05 02:44:04 INFO client.BaseHoodieWriteClient: Cleaner started
24/01/05 02:44:04 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:44:04 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:44:04 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:44:04 INFO table.HoodieTableMetaClient: Loading Active commit timeline for /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:44:04 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105024343265__deltacommit__COMPLETED__20240105024352164]}
24/01/05 02:44:04 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:44:04 INFO view.FileSystemViewManager: Creating View Manager with storage type :MEMORY
24/01/05 02:44:04 INFO view.FileSystemViewManager: Creating in-memory based Table View
24/01/05 02:44:04 INFO client.BaseHoodieWriteClient: Scheduling cleaning at instant time :20240105024343265002
24/01/05 02:44:04 INFO view.FileSystemViewManager: Creating InMemory based view for basePath /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:44:04 INFO view.AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
24/01/05 02:44:04 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:44:04 INFO clean.CleanPlanner: No earliest commit to retain. No need to scan partitions !!
24/01/05 02:44:04 INFO clean.CleanPlanActionExecutor: Nothing to clean here. It is already clean
24/01/05 02:44:04 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105024343265__deltacommit__COMPLETED__20240105024352164]}
24/01/05 02:44:04 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:44:04 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:44:04 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:44:04 INFO table.HoodieTableMetaClient: Loading Active commit timeline for /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:44:04 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105024343265__deltacommit__COMPLETED__20240105024352164]}
24/01/05 02:44:04 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:44:04 INFO view.FileSystemViewManager: Creating View Manager with storage type :MEMORY
24/01/05 02:44:04 INFO view.FileSystemViewManager: Creating in-memory based Table View
24/01/05 02:44:04 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[==>20240105024402665__deltacommit__REQUESTED__20240105024403928]}
24/01/05 02:44:04 INFO client.BaseHoodieWriteClient: Scheduling table service COMPACT
24/01/05 02:44:04 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:44:04 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:44:04 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:44:04 INFO table.HoodieTableMetaClient: Loading Active commit timeline for /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:44:04 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105024343265__deltacommit__COMPLETED__20240105024352164]}
24/01/05 02:44:04 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:44:04 INFO view.FileSystemViewManager: Creating View Manager with storage type :MEMORY
24/01/05 02:44:04 INFO view.FileSystemViewManager: Creating in-memory based Table View
24/01/05 02:44:04 INFO client.BaseHoodieWriteClient: Scheduling compaction at instant time :20240105024343265001
24/01/05 02:44:04 INFO compact.ScheduleCompactionActionExecutor: Checking if compaction needs to be run on /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:44:04 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:44:04 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:44:04 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:44:04 INFO table.HoodieTableMetaClient: Loading Active commit timeline for /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:44:04 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105024343265__deltacommit__COMPLETED__20240105024352164]}
24/01/05 02:44:04 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:44:04 INFO view.FileSystemViewManager: Creating View Manager with storage type :MEMORY
24/01/05 02:44:04 INFO view.FileSystemViewManager: Creating in-memory based Table View
24/01/05 02:44:04 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105024343265__deltacommit__COMPLETED__20240105024352164]}
24/01/05 02:44:04 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:44:04 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:44:04 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:44:04 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[==>20240105024402665__deltacommit__REQUESTED__20240105024403928]}
24/01/05 02:44:04 INFO client.HoodieTimelineArchiver: No Instants to archive
24/01/05 02:44:04 INFO metadata.HoodieBackedTableMetadataWriter: All the table services operations on MDT completed successfully
24/01/05 02:44:04 INFO transaction.TransactionManager: Transaction ending with transaction owner Option{val=[==>20240105024402665__deltacommit__INFLIGHT]}
24/01/05 02:44:04 INFO lock.InProcessLockProvider: Base Path /user/hive/warehouse/stock_ticks_mor, Lock Instance java.util.concurrent.locks.ReentrantReadWriteLock@7254838[Write locks = 1, Read locks = 0], Thread main, In-process lock state RELEASING
24/01/05 02:44:04 INFO lock.InProcessLockProvider: Base Path /user/hive/warehouse/stock_ticks_mor, Lock Instance java.util.concurrent.locks.ReentrantReadWriteLock@7254838[Write locks = 0, Read locks = 0], Thread main, In-process lock state RELEASED
24/01/05 02:44:04 INFO lock.InProcessLockProvider: Base Path /user/hive/warehouse/stock_ticks_mor, Lock Instance java.util.concurrent.locks.ReentrantReadWriteLock@7254838[Write locks = 0, Read locks = 0], Thread main, In-process lock state ALREADY_RELEASED
24/01/05 02:44:04 INFO lock.LockManager: Released connection created for acquiring lock
24/01/05 02:44:04 INFO transaction.TransactionManager: Transaction ended with transaction owner Option{val=[==>20240105024402665__deltacommit__INFLIGHT]}
24/01/05 02:44:04 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:44:04 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:44:04 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:44:04 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:44:04 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:44:04 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:44:04 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105024343265__deltacommit__COMPLETED__20240105024352164]}
24/01/05 02:44:04 INFO view.AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
24/01/05 02:44:04 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:44:04 INFO view.FileSystemViewManager: Creating View Manager with storage type :REMOTE_FIRST
24/01/05 02:44:04 INFO view.FileSystemViewManager: Creating remote first table view
24/01/05 02:44:04 INFO async.AsyncCleanerService: The HoodieWriteClient is not configured to auto & async clean. Async clean service will not start.
24/01/05 02:44:04 INFO async.AsyncArchiveService: The HoodieWriteClient is not configured to auto & async archive. Async archive service will not start.
24/01/05 02:44:04 INFO spark.SparkContext: Starting job: collect at HoodieJavaRDD.java:177
24/01/05 02:44:04 INFO scheduler.DAGScheduler: Registering RDD 7 (mapToPair at HoodieJavaRDD.java:149)
24/01/05 02:44:04 INFO scheduler.DAGScheduler: Registering RDD 13 (distinct at HoodieJavaRDD.java:157)
24/01/05 02:44:04 INFO scheduler.DAGScheduler: Got job 0 (collect at HoodieJavaRDD.java:177) with 2 output partitions
24/01/05 02:44:04 INFO scheduler.DAGScheduler: Final stage: ResultStage 2 (collect at HoodieJavaRDD.java:177)
24/01/05 02:44:04 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 1)
24/01/05 02:44:04 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 1)
24/01/05 02:44:04 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[7] at mapToPair at HoodieJavaRDD.java:149), which has no missing parents
24/01/05 02:44:04 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 29.5 KB, free 366.3 MB)
24/01/05 02:44:04 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 14.0 KB, free 366.3 MB)
24/01/05 02:44:04 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on adhoc-2:46387 (size: 14.0 KB, free: 366.3 MB)
24/01/05 02:44:04 INFO spark.SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1161
24/01/05 02:44:04 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[7] at mapToPair at HoodieJavaRDD.java:149) (first 15 tasks are for partitions Vector(0))
24/01/05 02:44:04 INFO scheduler.TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
24/01/05 02:44:04 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7739 bytes)
24/01/05 02:44:04 INFO executor.Executor: Running task 0.0 in stage 0.0 (TID 0)
24/01/05 02:44:04 INFO executor.Executor: Fetching spark://adhoc-2:33613/jars/hoodie-utilities.jar with timestamp 1704422640915
24/01/05 02:44:04 INFO client.TransportClientFactory: Successfully created connection to adhoc-2/172.31.0.18:33613 after 25 ms (0 ms spent in bootstraps)
24/01/05 02:44:04 INFO util.Utils: Fetching spark://adhoc-2:33613/jars/hoodie-utilities.jar to /tmp/spark-0499c23b-2793-43c2-89ab-9ec8ad2d7d05/userFiles-4b8d577c-f51e-4c6d-a17d-98592f6420dc/fetchFileTemp6572576279366874802.tmp
24/01/05 02:44:05 INFO executor.Executor: Adding file:/tmp/spark-0499c23b-2793-43c2-89ab-9ec8ad2d7d05/userFiles-4b8d577c-f51e-4c6d-a17d-98592f6420dc/hoodie-utilities.jar to class loader
24/01/05 02:44:05 INFO kafka010.KafkaRDD: Computing topic stock_ticks, partition 0 offsets 1866 -> 1966
24/01/05 02:44:05 INFO kafka010.KafkaDataConsumer: Initializing cache 16 64 0.75
24/01/05 02:44:05 INFO consumer.ConsumerConfig: ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [kafkabroker:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-executor-null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

24/01/05 02:44:05 INFO utils.AppInfoParser: Kafka version : 2.0.0
24/01/05 02:44:05 INFO utils.AppInfoParser: Kafka commitId : 3402a8361b734732
24/01/05 02:44:05 INFO factory.HoodieSparkKeyGeneratorFactory: The value of hoodie.datasource.write.keygenerator.type is empty; inferred to be SIMPLE
24/01/05 02:44:05 INFO kafka010.InternalKafkaConsumer: Initial fetch for spark-executor-null stock_ticks-0 1866
24/01/05 02:44:05 INFO clients.Metadata: Cluster ID: XFgecpgeR0meVK58zP5FxA
24/01/05 02:44:05 INFO executor.Executor: Finished task 0.0 in stage 0.0 (TID 0). 951 bytes result sent to driver
24/01/05 02:44:05 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 639 ms on localhost (executor driver) (1/1)
24/01/05 02:44:05 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
24/01/05 02:44:05 INFO scheduler.DAGScheduler: ShuffleMapStage 0 (mapToPair at HoodieJavaRDD.java:149) finished in 0.747 s
24/01/05 02:44:05 INFO scheduler.DAGScheduler: looking for newly runnable stages
24/01/05 02:44:05 INFO scheduler.DAGScheduler: running: Set()
24/01/05 02:44:05 INFO scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 1, ResultStage 2)
24/01/05 02:44:05 INFO scheduler.DAGScheduler: failed: Set()
24/01/05 02:44:05 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 1 (MapPartitionsRDD[13] at distinct at HoodieJavaRDD.java:157), which has no missing parents
24/01/05 02:44:05 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 24.3 KB, free 366.2 MB)
24/01/05 02:44:05 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 11.6 KB, free 366.2 MB)
24/01/05 02:44:05 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on adhoc-2:46387 (size: 11.6 KB, free: 366.3 MB)
24/01/05 02:44:05 INFO spark.SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1161
24/01/05 02:44:05 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[13] at distinct at HoodieJavaRDD.java:157) (first 15 tasks are for partitions Vector(0, 1))
24/01/05 02:44:05 INFO scheduler.TaskSchedulerImpl: Adding task set 1.0 with 2 tasks
24/01/05 02:44:05 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 7651 bytes)
24/01/05 02:44:05 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 1.0 (TID 2, localhost, executor driver, partition 1, ANY, 7651 bytes)
24/01/05 02:44:05 INFO executor.Executor: Running task 0.0 in stage 1.0 (TID 1)
24/01/05 02:44:05 INFO executor.Executor: Running task 1.0 in stage 1.0 (TID 2)
24/01/05 02:44:05 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
24/01/05 02:44:05 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
24/01/05 02:44:05 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms
24/01/05 02:44:05 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms
24/01/05 02:44:05 INFO memory.MemoryStore: Block rdd_9_0 stored as values in memory (estimated size 0.0 B, free 366.2 MB)
24/01/05 02:44:05 INFO storage.BlockManagerInfo: Added rdd_9_0 in memory on adhoc-2:46387 (size: 0.0 B, free: 366.3 MB)
24/01/05 02:44:05 INFO memory.MemoryStore: Block rdd_9_1 stored as values in memory (estimated size 634.0 B, free 366.2 MB)
24/01/05 02:44:05 INFO storage.BlockManagerInfo: Added rdd_9_1 in memory on adhoc-2:46387 (size: 634.0 B, free: 366.3 MB)
24/01/05 02:44:05 INFO storage.BlockManagerInfo: Removed broadcast_0_piece0 on adhoc-2:46387 in memory (size: 14.0 KB, free: 366.3 MB)
24/01/05 02:44:05 INFO executor.Executor: Finished task 0.0 in stage 1.0 (TID 1). 1209 bytes result sent to driver
24/01/05 02:44:05 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 111 ms on localhost (executor driver) (1/2)
24/01/05 02:44:05 INFO executor.Executor: Finished task 1.0 in stage 1.0 (TID 2). 1295 bytes result sent to driver
24/01/05 02:44:05 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 1.0 (TID 2) in 117 ms on localhost (executor driver) (2/2)
24/01/05 02:44:05 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
24/01/05 02:44:05 INFO scheduler.DAGScheduler: ShuffleMapStage 1 (distinct at HoodieJavaRDD.java:157) finished in 0.137 s
24/01/05 02:44:05 INFO scheduler.DAGScheduler: looking for newly runnable stages
24/01/05 02:44:05 INFO scheduler.DAGScheduler: running: Set()
24/01/05 02:44:05 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 2)
24/01/05 02:44:05 INFO scheduler.DAGScheduler: failed: Set()
24/01/05 02:44:05 INFO scheduler.DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[15] at distinct at HoodieJavaRDD.java:157), which has no missing parents
24/01/05 02:44:05 INFO memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 4.1 KB, free 366.3 MB)
24/01/05 02:44:05 INFO memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.4 KB, free 366.3 MB)
24/01/05 02:44:05 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on adhoc-2:46387 (size: 2.4 KB, free: 366.3 MB)
24/01/05 02:44:05 INFO spark.SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1161
24/01/05 02:44:05 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ResultStage 2 (MapPartitionsRDD[15] at distinct at HoodieJavaRDD.java:157) (first 15 tasks are for partitions Vector(0, 1))
24/01/05 02:44:05 INFO scheduler.TaskSchedulerImpl: Adding task set 2.0 with 2 tasks
24/01/05 02:44:05 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 2.0 (TID 3, localhost, executor driver, partition 0, PROCESS_LOCAL, 7662 bytes)
24/01/05 02:44:05 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 2.0 (TID 4, localhost, executor driver, partition 1, ANY, 7662 bytes)
24/01/05 02:44:05 INFO executor.Executor: Running task 0.0 in stage 2.0 (TID 3)
24/01/05 02:44:05 INFO executor.Executor: Running task 1.0 in stage 2.0 (TID 4)
24/01/05 02:44:05 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
24/01/05 02:44:05 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
24/01/05 02:44:05 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
24/01/05 02:44:05 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
24/01/05 02:44:05 INFO executor.Executor: Finished task 1.0 in stage 2.0 (TID 4). 1110 bytes result sent to driver
24/01/05 02:44:05 INFO executor.Executor: Finished task 0.0 in stage 2.0 (TID 3). 1098 bytes result sent to driver
24/01/05 02:44:05 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 2.0 (TID 3) in 24 ms on localhost (executor driver) (1/2)
24/01/05 02:44:05 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 2.0 (TID 4) in 32 ms on localhost (executor driver) (2/2)
24/01/05 02:44:05 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
24/01/05 02:44:05 INFO scheduler.DAGScheduler: ResultStage 2 (collect at HoodieJavaRDD.java:177) finished in 0.046 s
24/01/05 02:44:05 INFO scheduler.DAGScheduler: Job 0 finished: collect at HoodieJavaRDD.java:177, took 1.162172 s
24/01/05 02:44:05 INFO spark.SparkContext: Starting job: collect at HoodieSparkEngineContext.java:150
24/01/05 02:44:05 INFO scheduler.DAGScheduler: Got job 1 (collect at HoodieSparkEngineContext.java:150) with 1 output partitions
24/01/05 02:44:05 INFO scheduler.DAGScheduler: Final stage: ResultStage 3 (collect at HoodieSparkEngineContext.java:150)
24/01/05 02:44:05 INFO scheduler.DAGScheduler: Parents of final stage: List()
24/01/05 02:44:05 INFO scheduler.DAGScheduler: Missing parents: List()
24/01/05 02:44:05 INFO scheduler.DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[17] at flatMap at HoodieSparkEngineContext.java:150), which has no missing parents
24/01/05 02:44:05 INFO memory.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 339.1 KB, free 365.9 MB)
24/01/05 02:44:05 INFO memory.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 121.3 KB, free 365.8 MB)
24/01/05 02:44:05 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on adhoc-2:46387 (size: 121.3 KB, free: 366.2 MB)
24/01/05 02:44:05 INFO spark.SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1161
24/01/05 02:44:05 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[17] at flatMap at HoodieSparkEngineContext.java:150) (first 15 tasks are for partitions Vector(0))
24/01/05 02:44:05 INFO scheduler.TaskSchedulerImpl: Adding task set 3.0 with 1 tasks
24/01/05 02:44:05 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 3.0 (TID 5, localhost, executor driver, partition 0, PROCESS_LOCAL, 7735 bytes)
24/01/05 02:44:05 INFO executor.Executor: Running task 0.0 in stage 3.0 (TID 5)
24/01/05 02:44:05 INFO view.FileSystemViewManager: Creating View Manager with storage type :REMOTE_FIRST
24/01/05 02:44:05 INFO view.FileSystemViewManager: Creating remote first table view
24/01/05 02:44:05 INFO view.FileSystemViewManager: Creating remote view for basePath /user/hive/warehouse/stock_ticks_mor. Server=adhoc-2:42425, Timeout=300
24/01/05 02:44:05 INFO view.FileSystemViewManager: Creating InMemory based view for basePath /user/hive/warehouse/stock_ticks_mor
24/01/05 02:44:05 INFO view.AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
24/01/05 02:44:05 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:44:05 INFO view.RemoteHoodieTableFileSystemView: Sending request : (http://adhoc-2:42425/v1/hoodie/view/datafiles/beforeoron/latest/?partition=2018%2F08%2F31&maxinstant=20240105024343265&basepath=%2Fuser%2Fhive%2Fwarehouse%2Fstock_ticks_mor&lastinstantts=20240105024343265&timelinehash=7a0b5e1482c059272c847fc6668f12da532a637ce11f6f599d2728792005a6a1)
24/01/05 02:44:06 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:44:06 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:44:06 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:44:06 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[==>20240105024402665__deltacommit__REQUESTED__20240105024403928]}
24/01/05 02:44:06 INFO collection.RocksDBDAO: DELETING RocksDB persisted at /tmp/hoodie_timeline_rocksdb/_user_hive_warehouse_stock_ticks_mor/fd1434f1-9002-4c34-b56c-d74cb6c4a14c
24/01/05 02:44:06 INFO collection.RocksDBDAO: No column family found. Loading default
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : RocksDB version: 7.5.3

24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Compile date 2022-09-02 09:50:20
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : DB SUMMARY

24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : DB Session ID:  BDLA281754O6OA5HJ7UA

24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : SST files in /tmp/hoodie_timeline_rocksdb/_user_hive_warehouse_stock_ticks_mor/fd1434f1-9002-4c34-b56c-d74cb6c4a14c dir, Total Num: 0, files: 

24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Write Ahead Log file in /tmp/hoodie_timeline_rocksdb/_user_hive_warehouse_stock_ticks_mor/fd1434f1-9002-4c34-b56c-d74cb6c4a14c: 

24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                         Options.error_if_exists: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                       Options.create_if_missing: 1
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                         Options.paranoid_checks: 1
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :             Options.flush_verify_memtable_count: 1
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                               Options.track_and_verify_wals_in_manifest: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :        Options.verify_sst_unique_id_in_manifest: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                                     Options.env: 0x7f2e039e45e0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                                      Options.fs: PosixFileSystem
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                                Options.info_log: 0x7f2d24081fb8
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                Options.max_file_opening_threads: 16
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                              Options.statistics: 0x7f2d24081ef0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                               Options.use_fsync: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                       Options.max_log_file_size: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                  Options.max_manifest_file_size: 1073741824
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                   Options.log_file_time_to_roll: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                       Options.keep_log_file_num: 1000
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                    Options.recycle_log_file_num: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                         Options.allow_fallocate: 1
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                        Options.allow_mmap_reads: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                       Options.allow_mmap_writes: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                        Options.use_direct_reads: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                        Options.use_direct_io_for_flush_and_compaction: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :          Options.create_missing_column_families: 1
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                              Options.db_log_dir: 
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                                 Options.wal_dir: 
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                Options.table_cache_numshardbits: 6
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                         Options.WAL_ttl_seconds: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                       Options.WAL_size_limit_MB: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                        Options.max_write_batch_group_size_bytes: 1048576
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :             Options.manifest_preallocation_size: 4194304
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                     Options.is_fd_close_on_exec: 1
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                   Options.advise_random_on_open: 1
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                    Options.db_write_buffer_size: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                    Options.write_buffer_manager: 0x7f2d240fa660
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :         Options.access_hint_on_compaction_start: 1
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :           Options.random_access_max_buffer_size: 1048576
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                      Options.use_adaptive_mutex: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                            Options.rate_limiter: (nil)
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :     Options.sst_file_manager.rate_bytes_per_sec: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                       Options.wal_recovery_mode: 2
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                  Options.enable_thread_tracking: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                  Options.enable_pipelined_write: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                  Options.unordered_write: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :         Options.allow_concurrent_memtable_write: 1
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :      Options.enable_write_thread_adaptive_yield: 1
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :             Options.write_thread_max_yield_usec: 100
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :            Options.write_thread_slow_yield_usec: 3
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                               Options.row_cache: None
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                              Options.wal_filter: None
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :             Options.avoid_flush_during_recovery: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :             Options.allow_ingest_behind: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :             Options.two_write_queues: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :             Options.manual_wal_flush: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :             Options.wal_compression: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :             Options.atomic_flush: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :             Options.avoid_unnecessary_blocking_io: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                 Options.persist_stats_to_disk: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                 Options.write_dbid_to_manifest: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                 Options.log_readahead_size: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                 Options.file_checksum_gen_factory: Unknown
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                 Options.best_efforts_recovery: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                Options.max_bgerror_resume_count: 2147483647
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :            Options.bgerror_resume_retry_interval: 1000000
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :             Options.allow_data_in_errors: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :             Options.db_host_id: __hostname__
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :             Options.enforce_single_del_contracts: true
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :             Options.max_background_jobs: 2
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :             Options.max_background_compactions: -1
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :             Options.max_subcompactions: 1
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :             Options.avoid_flush_during_shutdown: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :           Options.writable_file_max_buffer_size: 1048576
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :             Options.delayed_write_rate : 16777216
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :             Options.max_total_wal_size: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :             Options.delete_obsolete_files_period_micros: 21600000000
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                   Options.stats_dump_period_sec: 300
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                 Options.stats_persist_period_sec: 600
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                 Options.stats_history_buffer_size: 1048576
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                          Options.max_open_files: -1
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                          Options.bytes_per_sync: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                      Options.wal_bytes_per_sync: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                   Options.strict_bytes_per_sync: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :       Options.compaction_readahead_size: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                  Options.max_background_flushes: -1
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Compression algorithms supported:
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : 	kZSTDNotFinalCompression supported: 1
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : 	kZSTD supported: 1
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : 	kXpressCompression supported: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : 	kLZ4HCCompression supported: 1
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : 	kLZ4Compression supported: 1
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : 	kBZip2Compression supported: 1
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : 	kZlibCompression supported: 1
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : 	kSnappyCompression supported: 1
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Fast CRC32 supported: Not supported on x86
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : DMutex implementation: pthread_mutex_t
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : [/db_impl/db_impl_open.cc:313] Creating manifest 1 

24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : [/version_set.cc:4994] Recovering from manifest file: /tmp/hoodie_timeline_rocksdb/_user_hive_warehouse_stock_ticks_mor/fd1434f1-9002-4c34-b56c-d74cb6c4a14c/MANIFEST-000001

24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : [/column_family.cc:618] --------------- Options for column family [default]:

24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :               Options.comparator: leveldb.BytewiseComparator
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :           Options.merge_operator: None
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :        Options.compaction_filter: None
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :        Options.compaction_filter_factory: None
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :  Options.sst_partitioner_factory: None
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :         Options.memtable_factory: SkipListFactory
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :            Options.table_factory: BlockBasedTable
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :            table_factory options:   flush_block_policy_factory: FlushBlockBySizePolicyFactory (0x7f2d240f8590)
  cache_index_and_filter_blocks: 0
  cache_index_and_filter_blocks_with_high_priority: 1
  pin_l0_filter_and_index_blocks_in_cache: 0
  pin_top_level_index_and_filter: 1
  index_type: 0
  data_block_index_type: 0
  index_shortening: 1
  data_block_hash_table_util_ratio: 0.750000
  checksum: 1
  no_block_cache: 0
  block_cache: 0x7f2d240f85e0
  block_cache_name: LRUCache
  block_cache_options:
    capacity : 8388608
    num_shard_bits : 4
    strict_capacity_limit : 0
    memory_allocator : None
    high_pri_pool_ratio: 0.000
  block_cache_compressed: (nil)
  persistent_cache: (nil)
  block_size: 4096
  block_size_deviation: 10
  block_restart_interval: 16
  index_block_restart_interval: 1
  metadata_block_size: 4096
  partition_filters: 0
  use_delta_encoding: 1
  filter_policy: nullptr
  whole_key_filtering: 1
  verify_compression: 0
  read_amp_bytes_per_bit: 0
  format_version: 5
  enable_index_compression: 1
  block_align: 0
  max_auto_readahead_size: 262144
  prepopulate_block_cache: 0
  initial_auto_readahead_size: 8192

24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :        Options.write_buffer_size: 67108864
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :  Options.max_write_buffer_number: 2
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :          Options.compression: Snappy
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression: Disabled
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :       Options.prefix_extractor: nullptr
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :   Options.memtable_insert_with_hint_prefix_extractor: nullptr
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :             Options.num_levels: 7
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :        Options.min_write_buffer_number_to_merge: 1
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :     Options.max_write_buffer_number_to_maintain: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :     Options.max_write_buffer_size_to_maintain: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :            Options.bottommost_compression_opts.window_bits: -14
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression_opts.level: 32767
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :               Options.bottommost_compression_opts.strategy: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.max_dict_bytes: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.zstd_max_train_bytes: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.parallel_threads: 1
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression_opts.enabled: false
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.max_dict_buffer_bytes: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.use_zstd_dict_trainer: true
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :            Options.compression_opts.window_bits: -14
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                  Options.compression_opts.level: 32767
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :               Options.compression_opts.strategy: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.max_dict_bytes: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.zstd_max_train_bytes: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.use_zstd_dict_trainer: true
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.parallel_threads: 1
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                  Options.compression_opts.enabled: false
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.max_dict_buffer_bytes: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :      Options.level0_file_num_compaction_trigger: 4
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :          Options.level0_slowdown_writes_trigger: 20
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :              Options.level0_stop_writes_trigger: 36
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                   Options.target_file_size_base: 67108864
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :             Options.target_file_size_multiplier: 1
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                Options.max_bytes_for_level_base: 268435456
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Options.level_compaction_dynamic_level_bytes: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :          Options.max_bytes_for_level_multiplier: 10.000000
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[0]: 1
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[1]: 1
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[2]: 1
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[3]: 1
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[4]: 1
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[5]: 1
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[6]: 1
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :       Options.max_sequential_skip_in_iterations: 8
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                    Options.max_compaction_bytes: 1677721600
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                        Options.arena_block_size: 1048576
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :   Options.soft_pending_compaction_bytes_limit: 68719476736
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :   Options.hard_pending_compaction_bytes_limit: 274877906944
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                Options.disable_auto_compactions: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                        Options.compaction_style: kCompactionStyleLevel
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                          Options.compaction_pri: kMinOverlappingRatio
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.size_ratio: 1
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.min_merge_width: 2
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.max_merge_width: 4294967295
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.max_size_amplification_percent: 200
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.compression_size_percent: -1
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.stop_style: kCompactionStopStyleTotalSize
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_fifo.max_table_files_size: 1073741824
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_fifo.allow_compaction: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                   Options.table_properties_collectors: 
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                   Options.inplace_update_support: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                 Options.inplace_update_num_locks: 10000
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :               Options.memtable_prefix_bloom_size_ratio: 0.000000
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :               Options.memtable_whole_key_filtering: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :   Options.memtable_huge_page_size: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                           Options.bloom_locality: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                    Options.max_successive_merges: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                Options.optimize_filters_for_hits: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                Options.paranoid_file_checks: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                Options.force_consistency_checks: 1
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                Options.report_bg_io_stats: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                               Options.ttl: 2592000
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :          Options.periodic_compaction_seconds: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :  Options.preclude_last_level_data_seconds: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                       Options.enable_blob_files: false
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                           Options.min_blob_size: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                          Options.blob_file_size: 268435456
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                   Options.blob_compression_type: NoCompression
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :          Options.enable_blob_garbage_collection: false
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :      Options.blob_garbage_collection_age_cutoff: 0.250000
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Options.blob_garbage_collection_force_threshold: 1.000000
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :          Options.blob_compaction_readahead_size: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                Options.blob_file_starting_level: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Options.experimental_mempurge_threshold: 0.000000
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : [/version_set.cc:5042] Recovered from manifest file:/tmp/hoodie_timeline_rocksdb/_user_hive_warehouse_stock_ticks_mor/fd1434f1-9002-4c34-b56c-d74cb6c4a14c/MANIFEST-000001 succeeded,manifest_file_number is 1, next_file_number is 3, last_sequence is 0, log_number is 0,prev_log_number is 0,max_column_family is 0,min_log_number_to_keep is 0

24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : [/version_set.cc:5051] Column family [default] (ID 0), log number is 0

24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : [/db_impl/db_impl_open.cc:532] DB ID: 16d76b13-5d9f-45b7-9e62-eb6c27b9c33e

24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : [/version_set.cc:4513] Creating manifest 5

24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : [/db_impl/db_impl_open.cc:2024] SstFileManager instance 0x7f2d240faa50
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : DB pointer 0x7f2d240fb480
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : [/db_impl/db_impl.cc:1070] ------- DUMPING STATS -------
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : [/db_impl/db_impl.cc:1071] 
** DB Stats **
Uptime(secs): 0.0 total, 0.0 interval
Cumulative writes: 0 writes, 0 keys, 0 commit groups, 0.0 writes per commit group, ingest: 0.00 GB, 0.00 MB/s
Cumulative WAL: 0 writes, 0 syncs, 0.00 writes per sync, written: 0.00 GB, 0.00 MB/s
Cumulative stall: 00:00:0.000 H:M:S, 0.0 percent
Interval writes: 0 writes, 0 keys, 0 commit groups, 0.0 writes per commit group, ingest: 0.00 MB, 0.00 MB/s
Interval WAL: 0 writes, 0 syncs, 0.00 writes per sync, written: 0.00 GB, 0.00 MB/s
Interval stall: 00:00:0.000 H:M:S, 0.0 percent

** Compaction Stats [default] **
Level    Files   Size     Score Read(GB)  Rn(GB) Rnp1(GB) Write(GB) Wnew(GB) Moved(GB) W-Amp Rd(MB/s) Wr(MB/s) Comp(sec) CompMergeCPU(sec) Comp(cnt) Avg(sec) KeyIn KeyDrop Rblob(GB) Wblob(GB)
------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
 Sum      0/0    0.00 KB   0.0      0.0     0.0      0.0       0.0      0.0       0.0   0.0      0.0      0.0      0.00              0.00         0    0.000       0      0       0.0       0.0
 Int      0/0    0.00 KB   0.0      0.0     0.0      0.0       0.0      0.0       0.0   0.0      0.0      0.0      0.00              0.00         0    0.000       0      0       0.0       0.0

** Compaction Stats [default] **
Priority    Files   Size     Score Read(GB)  Rn(GB) Rnp1(GB) Write(GB) Wnew(GB) Moved(GB) W-Amp Rd(MB/s) Wr(MB/s) Comp(sec) CompMergeCPU(sec) Comp(cnt) Avg(sec) KeyIn KeyDrop Rblob(GB) Wblob(GB)
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Blob file count: 0, total size: 0.0 GB, garbage size: 0.0 GB, space amp: 0.0

Uptime(secs): 0.0 total, 0.0 interval
Flush(GB): cumulative 0.000, interval 0.000
AddFile(GB): cumulative 0.000, interval 0.000
AddFile(Total Files): cumulative 0, interval 0
AddFile(L0 Files): cumulative 0, interval 0
AddFile(Keys): cumulative 0, interval 0
Cumulative compaction: 0.00 GB write, 0.00 MB/s write, 0.00 GB read, 0.00 MB/s read, 0.0 seconds
Interval compaction: 0.00 GB write, 0.00 MB/s write, 0.00 GB read, 0.00 MB/s read, 0.0 seconds
Stalls(count): 0 level0_slowdown, 0 level0_slowdown_with_compaction, 0 level0_numfiles, 0 level0_numfiles_with_compaction, 0 stop for pending_compaction_bytes, 0 slowdown for pending_compaction_bytes, 0 memtable_compaction, 0 memtable_slowdown, interval 0 total count
Block cache LRUCache@0x7f2d240f85e0#600 capacity: 8.00 MB collections: 1 last_copies: 0 last_secs: 6.7e-05 secs_since: 0
Block cache entry stats(count,size,portion): Misc(1,0.00 KB,0%)

** File Read Latency Histogram By Level [default] **

24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : [/db_impl/db_impl.cc:762] STATISTICS:
 rocksdb.block.cache.miss COUNT : 0
rocksdb.block.cache.hit COUNT : 0
rocksdb.block.cache.add COUNT : 0
rocksdb.block.cache.add.failures COUNT : 0
rocksdb.block.cache.index.miss COUNT : 0
rocksdb.block.cache.index.hit COUNT : 0
rocksdb.block.cache.index.add COUNT : 0
rocksdb.block.cache.index.bytes.insert COUNT : 0
rocksdb.block.cache.index.bytes.evict COUNT : 0
rocksdb.block.cache.filter.miss COUNT : 0
rocksdb.block.cache.filter.hit COUNT : 0
rocksdb.block.cache.filter.add COUNT : 0
rocksdb.block.cache.filter.bytes.insert COUNT : 0
rocksdb.block.cache.filter.bytes.evict COUNT : 0
rocksdb.block.cache.data.miss COUNT : 0
rocksdb.block.cache.data.hit COUNT : 0
rocksdb.block.cache.data.add COUNT : 0
rocksdb.block.cache.data.bytes.insert COUNT : 0
rocksdb.block.cache.bytes.read COUNT : 0
rocksdb.block.cache.bytes.write COUNT : 0
rocksdb.bloom.filter.useful COUNT : 0
rocksdb.bloom.filter.full.positive COUNT : 0
rocksdb.bloom.filter.full.true.positive COUNT : 0
rocksdb.bloom.filter.micros COUNT : 0
rocksdb.persistent.cache.hit COUNT : 0
rocksdb.persistent.cache.miss COUNT : 0
rocksdb.sim.block.cache.hit COUNT : 0
rocksdb.sim.block.cache.miss COUNT : 0
rocksdb.memtable.hit COUNT : 0
rocksdb.memtable.miss COUNT : 0
rocksdb.l0.hit COUNT : 0
rocksdb.l1.hit COUNT : 0
rocksdb.l2andup.hit COUNT : 0
rocksdb.compaction.key.drop.new COUNT : 0
rocksdb.compaction.key.drop.obsolete COUNT : 0
rocksdb.compaction.key.drop.range_del COUNT : 0
rocksdb.compaction.key.drop.user COUNT : 0
rocksdb.compaction.range_del.drop.obsolete COUNT : 0
rocksdb.compaction.optimized.del.drop.obsolete COUNT : 0
rocksdb.compaction.cancelled COUNT : 0
rocksdb.number.keys.written COUNT : 0
rocksdb.number.keys.read COUNT : 0
rocksdb.number.keys.updated COUNT : 0
rocksdb.bytes.written COUNT : 0
rocksdb.bytes.read COUNT : 0
rocksdb.number.db.seek COUNT : 0
rocksdb.number.db.next COUNT : 0
rocksdb.number.db.prev COUNT : 0
rocksdb.number.db.seek.found COUNT : 0
rocksdb.number.db.next.found COUNT : 0
rocksdb.number.db.prev.found COUNT : 0
rocksdb.db.iter.bytes.read COUNT : 0
rocksdb.no.file.closes COUNT : 0
rocksdb.no.file.opens COUNT : 0
rocksdb.no.file.errors COUNT : 0
rocksdb.l0.slowdown.micros COUNT : 0
rocksdb.memtable.compaction.micros COUNT : 0
rocksdb.l0.num.files.stall.micros COUNT : 0
rocksdb.stall.micros COUNT : 0
rocksdb.db.mutex.wait.micros COUNT : 0
rocksdb.rate.limit.delay.millis COUNT : 0
rocksdb.num.iterators COUNT : 0
rocksdb.number.multiget.get COUNT : 0
rocksdb.number.multiget.keys.read COUNT : 0
rocksdb.number.multiget.bytes.read COUNT : 0
rocksdb.number.deletes.filtered COUNT : 0
rocksdb.number.merge.failures COUNT : 0
rocksdb.bloom.filter.prefix.checked COUNT : 0
rocksdb.bloom.filter.prefix.useful COUNT : 0
rocksdb.number.reseeks.iteration COUNT : 0
rocksdb.getupdatessince.calls COUNT : 0
rocksdb.block.cachecompressed.miss COUNT : 0
rocksdb.block.cachecompressed.hit COUNT : 0
rocksdb.block.cachecompressed.add COUNT : 0
rocksdb.block.cachecompressed.add.failures COUNT : 0
rocksdb.wal.synced COUNT : 0
rocksdb.wal.bytes COUNT : 0
rocksdb.write.self COUNT : 0
rocksdb.write.other COUNT : 0
rocksdb.write.timeout COUNT : 0
rocksdb.write.wal COUNT : 0
rocksdb.compact.read.bytes COUNT : 0
rocksdb.compact.write.bytes COUNT : 0
rocksdb.flush.write.bytes COUNT : 0
rocksdb.compact.read.marked.bytes COUNT : 0
rocksdb.compact.read.periodic.bytes COUNT : 0
rocksdb.compact.read.ttl.bytes COUNT : 0
rocksdb.compact.write.marked.bytes COUNT : 0
rocksdb.compact.write.periodic.bytes COUNT : 0
rocksdb.compact.write.ttl.bytes COUNT : 0
rocksdb.number.direct.load.table.properties COUNT : 0
rocksdb.number.superversion_acquires COUNT : 0
rocksdb.number.superversion_releases COUNT : 0
rocksdb.number.superversion_cleanups COUNT : 0
rocksdb.number.block.compressed COUNT : 0
rocksdb.number.block.decompressed COUNT : 0
rocksdb.number.block.not_compressed COUNT : 0
rocksdb.merge.operation.time.nanos COUNT : 0
rocksdb.filter.operation.time.nanos COUNT : 0
rocksdb.row.cache.hit COUNT : 0
rocksdb.row.cache.miss COUNT : 0
rocksdb.read.amp.estimate.useful.bytes COUNT : 0
rocksdb.read.amp.total.read.bytes COUNT : 0
rocksdb.number.rate_limiter.drains COUNT : 0
rocksdb.number.iter.skip COUNT : 0
rocksdb.blobdb.num.put COUNT : 0
rocksdb.blobdb.num.write COUNT : 0
rocksdb.blobdb.num.get COUNT : 0
rocksdb.blobdb.num.multiget COUNT : 0
rocksdb.blobdb.num.seek COUNT : 0
rocksdb.blobdb.num.next COUNT : 0
rocksdb.blobdb.num.prev COUNT : 0
rocksdb.blobdb.num.keys.written COUNT : 0
rocksdb.blobdb.num.keys.read COUNT : 0
rocksdb.blobdb.bytes.written COUNT : 0
rocksdb.blobdb.bytes.read COUNT : 0
rocksdb.blobdb.write.inlined COUNT : 0
rocksdb.blobdb.write.inlined.ttl COUNT : 0
rocksdb.blobdb.write.blob COUNT : 0
rocksdb.blobdb.write.blob.ttl COUNT : 0
rocksdb.blobdb.blob.file.bytes.written COUNT : 0
rocksdb.blobdb.blob.file.bytes.read COUNT : 0
rocksdb.blobdb.blob.file.synced COUNT : 0
rocksdb.blobdb.blob.index.expired.count COUNT : 0
rocksdb.blobdb.blob.index.expired.size COUNT : 0
rocksdb.blobdb.blob.index.evicted.count COUNT : 0
rocksdb.blobdb.blob.index.evicted.size COUNT : 0
rocksdb.blobdb.gc.num.files COUNT : 0
rocksdb.blobdb.gc.num.new.files COUNT : 0
rocksdb.blobdb.gc.failures COUNT : 0
rocksdb.blobdb.gc.num.keys.overwritten COUNT : 0
rocksdb.blobdb.gc.num.keys.expired COUNT : 0
rocksdb.blobdb.gc.num.keys.relocated COUNT : 0
rocksdb.blobdb.gc.bytes.overwritten COUNT : 0
rocksdb.blobdb.gc.bytes.expired COUNT : 0
rocksdb.blobdb.gc.bytes.relocated COUNT : 0
rocksdb.blobdb.fifo.num.files.evicted COUNT : 0
rocksdb.blobdb.fifo.num.keys.evicted COUNT : 0
rocksdb.blobdb.fifo.bytes.evicted COUNT : 0
rocksdb.txn.overhead.mutex.prepare COUNT : 0
rocksdb.txn.overhead.mutex.old.commit.map COUNT : 0
rocksdb.txn.overhead.duplicate.key COUNT : 0
rocksdb.txn.overhead.mutex.snapshot COUNT : 0
rocksdb.txn.get.tryagain COUNT : 0
rocksdb.number.multiget.keys.found COUNT : 0
rocksdb.num.iterator.created COUNT : 0
rocksdb.num.iterator.deleted COUNT : 0
rocksdb.block.cache.compression.dict.miss COUNT : 0
rocksdb.block.cache.compression.dict.hit COUNT : 0
rocksdb.block.cache.compression.dict.add COUNT : 0
rocksdb.block.cache.compression.dict.bytes.insert COUNT : 0
rocksdb.block.cache.compression.dict.bytes.evict COUNT : 0
rocksdb.block.cache.add.redundant COUNT : 0
rocksdb.block.cache.index.add.redundant COUNT : 0
rocksdb.block.cache.filter.add.redundant COUNT : 0
rocksdb.block.cache.data.add.redundant COUNT : 0
rocksdb.block.cache.compression.dict.add.redundant COUNT : 0
rocksdb.files.marked.trash COUNT : 0
rocksdb.files.deleted.immediately COUNT : 0
rocksdb.error.handler.bg.errro.count COUNT : 0
rocksdb.error.handler.bg.io.errro.count COUNT : 0
rocksdb.error.handler.bg.retryable.io.errro.count COUNT : 0
rocksdb.error.handler.autoresume.count COUNT : 0
rocksdb.error.handler.autoresume.retry.total.count COUNT : 0
rocksdb.error.handler.autoresume.success.count COUNT : 0
rocksdb.memtable.payload.bytes.at.flush COUNT : 0
rocksdb.memtable.garbage.bytes.at.flush COUNT : 0
rocksdb.secondary.cache.hits COUNT : 0
rocksdb.verify_checksum.read.bytes COUNT : 0
rocksdb.backup.read.bytes COUNT : 0
rocksdb.backup.write.bytes COUNT : 0
rocksdb.remote.compact.read.bytes COUNT : 0
rocksdb.remote.compact.write.bytes COUNT : 0
rocksdb.hot.file.read.bytes COUNT : 0
rocksdb.warm.file.read.bytes COUNT : 0
rocksdb.cold.file.read.bytes COUNT : 0
rocksdb.hot.file.read.count COUNT : 0
rocksdb.warm.file.read.count COUNT : 0
rocksdb.cold.file.read.count COUNT : 0
rocksdb.last.level.read.bytes COUNT : 0
rocksdb.last.level.read.count COUNT : 0
rocksdb.non.last.level.read.bytes COUNT : 0
rocksdb.non.last.level.read.count COUNT : 0
rocksdb.block.checksum.compute.count COUNT : 0
rocksdb.multiget.coroutine.count COUNT : 0
rocksdb.blobdb.cache.miss COUNT : 0
rocksdb.blobdb.cache.hit COUNT : 0
rocksdb.blobdb.cache.add COUNT : 0
rocksdb.blobdb.cache.add.failures COUNT : 0
rocksdb.blobdb.cache.bytes.read COUNT : 0
rocksdb.blobdb.cache.bytes.write COUNT : 0
rocksdb.db.get.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.db.write.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.compaction.times.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.compaction.times.cpu_micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.subcompaction.setup.times.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.table.sync.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.compaction.outfile.sync.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.wal.file.sync.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.manifest.file.sync.micros P50 : 870.000000 P95 : 910.000000 P99 : 910.000000 P100 : 910.000000 COUNT : 2 SUM : 1689
rocksdb.table.open.io.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.db.multiget.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.read.block.compaction.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.read.block.get.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.write.raw.block.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.l0.slowdown.count P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.memtable.compaction.count P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.num.files.stall.count P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.hard.rate.limit.delay.count P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.soft.rate.limit.delay.count P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.numfiles.in.singlecompaction P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.db.seek.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.db.write.stall P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.sst.read.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.num.subcompactions.scheduled P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.bytes.per.read P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.bytes.per.write P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.bytes.per.multiget P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.bytes.compressed P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.bytes.decompressed P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.compression.times.nanos P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.decompression.times.nanos P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.read.num.merge_operands P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.blobdb.key.size P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.blobdb.value.size P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.blobdb.write.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.blobdb.get.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.blobdb.multiget.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.blobdb.seek.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.blobdb.next.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.blobdb.prev.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.blobdb.blob.file.write.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.blobdb.blob.file.read.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.blobdb.blob.file.sync.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.blobdb.gc.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.blobdb.compression.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.blobdb.decompression.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.db.flush.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.sst.batch.size P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.num.index.and.filter.blocks.read.per.level P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.num.data.blocks.read.per.level P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.num.sst.read.per.level P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.error.handler.autoresume.retry.count P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.async.read.bytes P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.poll.wait.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.prefetched.bytes.discarded P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.multiget.io.batch.size P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0
rocksdb.num.level.read.per.multiget P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0

24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : [/column_family.cc:618] --------------- Options for column family [hudi_view__user_hive_warehouse_stock_ticks_mor]:

24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :               Options.comparator: leveldb.BytewiseComparator
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :           Options.merge_operator: None
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :        Options.compaction_filter: None
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :        Options.compaction_filter_factory: None
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :  Options.sst_partitioner_factory: None
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :         Options.memtable_factory: SkipListFactory
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :            Options.table_factory: BlockBasedTable
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :            table_factory options:   flush_block_policy_factory: FlushBlockBySizePolicyFactory (0x7f2d241228a0)
  cache_index_and_filter_blocks: 0
  cache_index_and_filter_blocks_with_high_priority: 1
  pin_l0_filter_and_index_blocks_in_cache: 0
  pin_top_level_index_and_filter: 1
  index_type: 0
  data_block_index_type: 0
  index_shortening: 1
  data_block_hash_table_util_ratio: 0.750000
  checksum: 1
  no_block_cache: 0
  block_cache: 0x7f2d24125df0
  block_cache_name: LRUCache
  block_cache_options:
    capacity : 8388608
    num_shard_bits : 4
    strict_capacity_limit : 0
    memory_allocator : None
    high_pri_pool_ratio: 0.000
  block_cache_compressed: (nil)
  persistent_cache: (nil)
  block_size: 4096
  block_size_deviation: 10
  block_restart_interval: 16
  index_block_restart_interval: 1
  metadata_block_size: 4096
  partition_filters: 0
  use_delta_encoding: 1
  filter_policy: nullptr
  whole_key_filtering: 1
  verify_compression: 0
  read_amp_bytes_per_bit: 0
  format_version: 5
  enable_index_compression: 1
  block_align: 0
  max_auto_readahead_size: 262144
  prepopulate_block_cache: 0
  initial_auto_readahead_size: 8192

24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :        Options.write_buffer_size: 67108864
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :  Options.max_write_buffer_number: 2
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :          Options.compression: Snappy
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression: Disabled
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :       Options.prefix_extractor: nullptr
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :   Options.memtable_insert_with_hint_prefix_extractor: nullptr
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :             Options.num_levels: 7
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :        Options.min_write_buffer_number_to_merge: 1
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :     Options.max_write_buffer_number_to_maintain: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :     Options.max_write_buffer_size_to_maintain: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :            Options.bottommost_compression_opts.window_bits: -14
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression_opts.level: 32767
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :               Options.bottommost_compression_opts.strategy: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.max_dict_bytes: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.zstd_max_train_bytes: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.parallel_threads: 1
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression_opts.enabled: false
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.max_dict_buffer_bytes: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.use_zstd_dict_trainer: true
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :            Options.compression_opts.window_bits: -14
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                  Options.compression_opts.level: 32767
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :               Options.compression_opts.strategy: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.max_dict_bytes: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.zstd_max_train_bytes: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.use_zstd_dict_trainer: true
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.parallel_threads: 1
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                  Options.compression_opts.enabled: false
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.max_dict_buffer_bytes: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :      Options.level0_file_num_compaction_trigger: 4
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :          Options.level0_slowdown_writes_trigger: 20
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :              Options.level0_stop_writes_trigger: 36
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                   Options.target_file_size_base: 67108864
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :             Options.target_file_size_multiplier: 1
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                Options.max_bytes_for_level_base: 268435456
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Options.level_compaction_dynamic_level_bytes: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :          Options.max_bytes_for_level_multiplier: 10.000000
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[0]: 1
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[1]: 1
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[2]: 1
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[3]: 1
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[4]: 1
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[5]: 1
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[6]: 1
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :       Options.max_sequential_skip_in_iterations: 8
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                    Options.max_compaction_bytes: 1677721600
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                        Options.arena_block_size: 1048576
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :   Options.soft_pending_compaction_bytes_limit: 68719476736
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :   Options.hard_pending_compaction_bytes_limit: 274877906944
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                Options.disable_auto_compactions: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                        Options.compaction_style: kCompactionStyleLevel
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                          Options.compaction_pri: kMinOverlappingRatio
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.size_ratio: 1
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.min_merge_width: 2
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.max_merge_width: 4294967295
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.max_size_amplification_percent: 200
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.compression_size_percent: -1
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.stop_style: kCompactionStopStyleTotalSize
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_fifo.max_table_files_size: 1073741824
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_fifo.allow_compaction: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                   Options.table_properties_collectors: 
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                   Options.inplace_update_support: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                 Options.inplace_update_num_locks: 10000
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :               Options.memtable_prefix_bloom_size_ratio: 0.000000
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :               Options.memtable_whole_key_filtering: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :   Options.memtable_huge_page_size: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                           Options.bloom_locality: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                    Options.max_successive_merges: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                Options.optimize_filters_for_hits: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                Options.paranoid_file_checks: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                Options.force_consistency_checks: 1
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                Options.report_bg_io_stats: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                               Options.ttl: 2592000
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :          Options.periodic_compaction_seconds: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :  Options.preclude_last_level_data_seconds: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                       Options.enable_blob_files: false
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                           Options.min_blob_size: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                          Options.blob_file_size: 268435456
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                   Options.blob_compression_type: NoCompression
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :          Options.enable_blob_garbage_collection: false
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :      Options.blob_garbage_collection_age_cutoff: 0.250000
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Options.blob_garbage_collection_force_threshold: 1.000000
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :          Options.blob_compaction_readahead_size: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                Options.blob_file_starting_level: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Options.experimental_mempurge_threshold: 0.000000
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : [/db_impl/db_impl.cc:2861] Created column family [hudi_view__user_hive_warehouse_stock_ticks_mor] (ID 1)
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : [/column_family.cc:618] --------------- Options for column family [hudi_pending_compaction__user_hive_warehouse_stock_ticks_mor]:

24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :               Options.comparator: leveldb.BytewiseComparator
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :           Options.merge_operator: None
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :        Options.compaction_filter: None
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :        Options.compaction_filter_factory: None
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :  Options.sst_partitioner_factory: None
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :         Options.memtable_factory: SkipListFactory
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :            Options.table_factory: BlockBasedTable
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :            table_factory options:   flush_block_policy_factory: FlushBlockBySizePolicyFactory (0x7f2d241e4de0)
  cache_index_and_filter_blocks: 0
  cache_index_and_filter_blocks_with_high_priority: 1
  pin_l0_filter_and_index_blocks_in_cache: 0
  pin_top_level_index_and_filter: 1
  index_type: 0
  data_block_index_type: 0
  index_shortening: 1
  data_block_hash_table_util_ratio: 0.750000
  checksum: 1
  no_block_cache: 0
  block_cache: 0x7f2d240324b0
  block_cache_name: LRUCache
  block_cache_options:
    capacity : 8388608
    num_shard_bits : 4
    strict_capacity_limit : 0
    memory_allocator : None
    high_pri_pool_ratio: 0.000
  block_cache_compressed: (nil)
  persistent_cache: (nil)
  block_size: 4096
  block_size_deviation: 10
  block_restart_interval: 16
  index_block_restart_interval: 1
  metadata_block_size: 4096
  partition_filters: 0
  use_delta_encoding: 1
  filter_policy: nullptr
  whole_key_filtering: 1
  verify_compression: 0
  read_amp_bytes_per_bit: 0
  format_version: 5
  enable_index_compression: 1
  block_align: 0
  max_auto_readahead_size: 262144
  prepopulate_block_cache: 0
  initial_auto_readahead_size: 8192

24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :        Options.write_buffer_size: 67108864
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :  Options.max_write_buffer_number: 2
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :          Options.compression: Snappy
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression: Disabled
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :       Options.prefix_extractor: nullptr
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :   Options.memtable_insert_with_hint_prefix_extractor: nullptr
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :             Options.num_levels: 7
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :        Options.min_write_buffer_number_to_merge: 1
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :     Options.max_write_buffer_number_to_maintain: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :     Options.max_write_buffer_size_to_maintain: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :            Options.bottommost_compression_opts.window_bits: -14
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression_opts.level: 32767
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :               Options.bottommost_compression_opts.strategy: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.max_dict_bytes: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.zstd_max_train_bytes: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.parallel_threads: 1
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression_opts.enabled: false
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.max_dict_buffer_bytes: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.use_zstd_dict_trainer: true
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :            Options.compression_opts.window_bits: -14
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                  Options.compression_opts.level: 32767
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :               Options.compression_opts.strategy: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.max_dict_bytes: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.zstd_max_train_bytes: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.use_zstd_dict_trainer: true
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.parallel_threads: 1
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                  Options.compression_opts.enabled: false
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.max_dict_buffer_bytes: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :      Options.level0_file_num_compaction_trigger: 4
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :          Options.level0_slowdown_writes_trigger: 20
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :              Options.level0_stop_writes_trigger: 36
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                   Options.target_file_size_base: 67108864
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :             Options.target_file_size_multiplier: 1
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                Options.max_bytes_for_level_base: 268435456
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Options.level_compaction_dynamic_level_bytes: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :          Options.max_bytes_for_level_multiplier: 10.000000
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[0]: 1
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[1]: 1
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[2]: 1
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[3]: 1
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[4]: 1
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[5]: 1
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[6]: 1
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :       Options.max_sequential_skip_in_iterations: 8
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                    Options.max_compaction_bytes: 1677721600
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                        Options.arena_block_size: 1048576
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :   Options.soft_pending_compaction_bytes_limit: 68719476736
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :   Options.hard_pending_compaction_bytes_limit: 274877906944
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                Options.disable_auto_compactions: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                        Options.compaction_style: kCompactionStyleLevel
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                          Options.compaction_pri: kMinOverlappingRatio
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.size_ratio: 1
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.min_merge_width: 2
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.max_merge_width: 4294967295
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.max_size_amplification_percent: 200
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.compression_size_percent: -1
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.stop_style: kCompactionStopStyleTotalSize
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_fifo.max_table_files_size: 1073741824
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_fifo.allow_compaction: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                   Options.table_properties_collectors: 
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                   Options.inplace_update_support: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                 Options.inplace_update_num_locks: 10000
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :               Options.memtable_prefix_bloom_size_ratio: 0.000000
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :               Options.memtable_whole_key_filtering: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :   Options.memtable_huge_page_size: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                           Options.bloom_locality: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                    Options.max_successive_merges: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                Options.optimize_filters_for_hits: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                Options.paranoid_file_checks: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                Options.force_consistency_checks: 1
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                Options.report_bg_io_stats: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                               Options.ttl: 2592000
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :          Options.periodic_compaction_seconds: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :  Options.preclude_last_level_data_seconds: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                       Options.enable_blob_files: false
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                           Options.min_blob_size: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                          Options.blob_file_size: 268435456
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                   Options.blob_compression_type: NoCompression
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :          Options.enable_blob_garbage_collection: false
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :      Options.blob_garbage_collection_age_cutoff: 0.250000
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Options.blob_garbage_collection_force_threshold: 1.000000
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :          Options.blob_compaction_readahead_size: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                Options.blob_file_starting_level: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Options.experimental_mempurge_threshold: 0.000000
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : [/db_impl/db_impl.cc:2861] Created column family [hudi_pending_compaction__user_hive_warehouse_stock_ticks_mor] (ID 2)
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : [/column_family.cc:618] --------------- Options for column family [hudi_pending_log_compaction__user_hive_warehouse_stock_ticks_mor]:

24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :               Options.comparator: leveldb.BytewiseComparator
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :           Options.merge_operator: None
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :        Options.compaction_filter: None
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :        Options.compaction_filter_factory: None
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :  Options.sst_partitioner_factory: None
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :         Options.memtable_factory: SkipListFactory
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :            Options.table_factory: BlockBasedTable
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :            table_factory options:   flush_block_policy_factory: FlushBlockBySizePolicyFactory (0x7f2d241149f0)
  cache_index_and_filter_blocks: 0
  cache_index_and_filter_blocks_with_high_priority: 1
  pin_l0_filter_and_index_blocks_in_cache: 0
  pin_top_level_index_and_filter: 1
  index_type: 0
  data_block_index_type: 0
  index_shortening: 1
  data_block_hash_table_util_ratio: 0.750000
  checksum: 1
  no_block_cache: 0
  block_cache: 0x7f2d24114a40
  block_cache_name: LRUCache
  block_cache_options:
    capacity : 8388608
    num_shard_bits : 4
    strict_capacity_limit : 0
    memory_allocator : None
    high_pri_pool_ratio: 0.000
  block_cache_compressed: (nil)
  persistent_cache: (nil)
  block_size: 4096
  block_size_deviation: 10
  block_restart_interval: 16
  index_block_restart_interval: 1
  metadata_block_size: 4096
  partition_filters: 0
  use_delta_encoding: 1
  filter_policy: nullptr
  whole_key_filtering: 1
  verify_compression: 0
  read_amp_bytes_per_bit: 0
  format_version: 5
  enable_index_compression: 1
  block_align: 0
  max_auto_readahead_size: 262144
  prepopulate_block_cache: 0
  initial_auto_readahead_size: 8192

24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :        Options.write_buffer_size: 67108864
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :  Options.max_write_buffer_number: 2
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :          Options.compression: Snappy
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression: Disabled
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :       Options.prefix_extractor: nullptr
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :   Options.memtable_insert_with_hint_prefix_extractor: nullptr
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :             Options.num_levels: 7
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :        Options.min_write_buffer_number_to_merge: 1
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :     Options.max_write_buffer_number_to_maintain: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :     Options.max_write_buffer_size_to_maintain: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :            Options.bottommost_compression_opts.window_bits: -14
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression_opts.level: 32767
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :               Options.bottommost_compression_opts.strategy: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.max_dict_bytes: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.zstd_max_train_bytes: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.parallel_threads: 1
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression_opts.enabled: false
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.max_dict_buffer_bytes: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.use_zstd_dict_trainer: true
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :            Options.compression_opts.window_bits: -14
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                  Options.compression_opts.level: 32767
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :               Options.compression_opts.strategy: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.max_dict_bytes: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.zstd_max_train_bytes: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.use_zstd_dict_trainer: true
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.parallel_threads: 1
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                  Options.compression_opts.enabled: false
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.max_dict_buffer_bytes: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :      Options.level0_file_num_compaction_trigger: 4
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :          Options.level0_slowdown_writes_trigger: 20
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :              Options.level0_stop_writes_trigger: 36
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                   Options.target_file_size_base: 67108864
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :             Options.target_file_size_multiplier: 1
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                Options.max_bytes_for_level_base: 268435456
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Options.level_compaction_dynamic_level_bytes: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :          Options.max_bytes_for_level_multiplier: 10.000000
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[0]: 1
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[1]: 1
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[2]: 1
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[3]: 1
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[4]: 1
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[5]: 1
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[6]: 1
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :       Options.max_sequential_skip_in_iterations: 8
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                    Options.max_compaction_bytes: 1677721600
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                        Options.arena_block_size: 1048576
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :   Options.soft_pending_compaction_bytes_limit: 68719476736
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :   Options.hard_pending_compaction_bytes_limit: 274877906944
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                Options.disable_auto_compactions: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                        Options.compaction_style: kCompactionStyleLevel
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                          Options.compaction_pri: kMinOverlappingRatio
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.size_ratio: 1
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.min_merge_width: 2
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.max_merge_width: 4294967295
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.max_size_amplification_percent: 200
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.compression_size_percent: -1
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.stop_style: kCompactionStopStyleTotalSize
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_fifo.max_table_files_size: 1073741824
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_fifo.allow_compaction: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                   Options.table_properties_collectors: 
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                   Options.inplace_update_support: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                 Options.inplace_update_num_locks: 10000
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :               Options.memtable_prefix_bloom_size_ratio: 0.000000
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :               Options.memtable_whole_key_filtering: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :   Options.memtable_huge_page_size: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                           Options.bloom_locality: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                    Options.max_successive_merges: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                Options.optimize_filters_for_hits: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                Options.paranoid_file_checks: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                Options.force_consistency_checks: 1
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                Options.report_bg_io_stats: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                               Options.ttl: 2592000
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :          Options.periodic_compaction_seconds: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :  Options.preclude_last_level_data_seconds: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                       Options.enable_blob_files: false
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                           Options.min_blob_size: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                          Options.blob_file_size: 268435456
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                   Options.blob_compression_type: NoCompression
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :          Options.enable_blob_garbage_collection: false
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :      Options.blob_garbage_collection_age_cutoff: 0.250000
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Options.blob_garbage_collection_force_threshold: 1.000000
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :          Options.blob_compaction_readahead_size: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                Options.blob_file_starting_level: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Options.experimental_mempurge_threshold: 0.000000
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : [/db_impl/db_impl.cc:2861] Created column family [hudi_pending_log_compaction__user_hive_warehouse_stock_ticks_mor] (ID 3)
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : [/column_family.cc:618] --------------- Options for column family [hudi_bootstrap_basefile__user_hive_warehouse_stock_ticks_mor]:

24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :               Options.comparator: leveldb.BytewiseComparator
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :           Options.merge_operator: None
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :        Options.compaction_filter: None
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :        Options.compaction_filter_factory: None
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :  Options.sst_partitioner_factory: None
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :         Options.memtable_factory: SkipListFactory
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :            Options.table_factory: BlockBasedTable
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :            table_factory options:   flush_block_policy_factory: FlushBlockBySizePolicyFactory (0x7f2d24159210)
  cache_index_and_filter_blocks: 0
  cache_index_and_filter_blocks_with_high_priority: 1
  pin_l0_filter_and_index_blocks_in_cache: 0
  pin_top_level_index_and_filter: 1
  index_type: 0
  data_block_index_type: 0
  index_shortening: 1
  data_block_hash_table_util_ratio: 0.750000
  checksum: 1
  no_block_cache: 0
  block_cache: 0x7f2d24159260
  block_cache_name: LRUCache
  block_cache_options:
    capacity : 8388608
    num_shard_bits : 4
    strict_capacity_limit : 0
    memory_allocator : None
    high_pri_pool_ratio: 0.000
  block_cache_compressed: (nil)
  persistent_cache: (nil)
  block_size: 4096
  block_size_deviation: 10
  block_restart_interval: 16
  index_block_restart_interval: 1
  metadata_block_size: 4096
  partition_filters: 0
  use_delta_encoding: 1
  filter_policy: nullptr
  whole_key_filtering: 1
  verify_compression: 0
  read_amp_bytes_per_bit: 0
  format_version: 5
  enable_index_compression: 1
  block_align: 0
  max_auto_readahead_size: 262144
  prepopulate_block_cache: 0
  initial_auto_readahead_size: 8192

24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :        Options.write_buffer_size: 67108864
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :  Options.max_write_buffer_number: 2
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :          Options.compression: Snappy
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression: Disabled
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :       Options.prefix_extractor: nullptr
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :   Options.memtable_insert_with_hint_prefix_extractor: nullptr
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :             Options.num_levels: 7
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :        Options.min_write_buffer_number_to_merge: 1
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :     Options.max_write_buffer_number_to_maintain: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :     Options.max_write_buffer_size_to_maintain: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :            Options.bottommost_compression_opts.window_bits: -14
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression_opts.level: 32767
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :               Options.bottommost_compression_opts.strategy: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.max_dict_bytes: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.zstd_max_train_bytes: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.parallel_threads: 1
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression_opts.enabled: false
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.max_dict_buffer_bytes: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.use_zstd_dict_trainer: true
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :            Options.compression_opts.window_bits: -14
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                  Options.compression_opts.level: 32767
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :               Options.compression_opts.strategy: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.max_dict_bytes: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.zstd_max_train_bytes: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.use_zstd_dict_trainer: true
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.parallel_threads: 1
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                  Options.compression_opts.enabled: false
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.max_dict_buffer_bytes: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :      Options.level0_file_num_compaction_trigger: 4
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :          Options.level0_slowdown_writes_trigger: 20
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :              Options.level0_stop_writes_trigger: 36
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                   Options.target_file_size_base: 67108864
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :             Options.target_file_size_multiplier: 1
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                Options.max_bytes_for_level_base: 268435456
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Options.level_compaction_dynamic_level_bytes: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :          Options.max_bytes_for_level_multiplier: 10.000000
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[0]: 1
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[1]: 1
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[2]: 1
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[3]: 1
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[4]: 1
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[5]: 1
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[6]: 1
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :       Options.max_sequential_skip_in_iterations: 8
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                    Options.max_compaction_bytes: 1677721600
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                        Options.arena_block_size: 1048576
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :   Options.soft_pending_compaction_bytes_limit: 68719476736
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :   Options.hard_pending_compaction_bytes_limit: 274877906944
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                Options.disable_auto_compactions: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                        Options.compaction_style: kCompactionStyleLevel
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                          Options.compaction_pri: kMinOverlappingRatio
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.size_ratio: 1
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.min_merge_width: 2
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.max_merge_width: 4294967295
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.max_size_amplification_percent: 200
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.compression_size_percent: -1
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.stop_style: kCompactionStopStyleTotalSize
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_fifo.max_table_files_size: 1073741824
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_fifo.allow_compaction: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                   Options.table_properties_collectors: 
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                   Options.inplace_update_support: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                 Options.inplace_update_num_locks: 10000
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :               Options.memtable_prefix_bloom_size_ratio: 0.000000
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :               Options.memtable_whole_key_filtering: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :   Options.memtable_huge_page_size: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                           Options.bloom_locality: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                    Options.max_successive_merges: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                Options.optimize_filters_for_hits: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                Options.paranoid_file_checks: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                Options.force_consistency_checks: 1
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                Options.report_bg_io_stats: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                               Options.ttl: 2592000
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :          Options.periodic_compaction_seconds: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :  Options.preclude_last_level_data_seconds: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                       Options.enable_blob_files: false
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                           Options.min_blob_size: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                          Options.blob_file_size: 268435456
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                   Options.blob_compression_type: NoCompression
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :          Options.enable_blob_garbage_collection: false
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :      Options.blob_garbage_collection_age_cutoff: 0.250000
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Options.blob_garbage_collection_force_threshold: 1.000000
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :          Options.blob_compaction_readahead_size: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                Options.blob_file_starting_level: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Options.experimental_mempurge_threshold: 0.000000
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : [/db_impl/db_impl.cc:2861] Created column family [hudi_bootstrap_basefile__user_hive_warehouse_stock_ticks_mor] (ID 4)
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : [/column_family.cc:618] --------------- Options for column family [hudi_partitions__user_hive_warehouse_stock_ticks_mor]:

24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :               Options.comparator: leveldb.BytewiseComparator
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :           Options.merge_operator: None
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :        Options.compaction_filter: None
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :        Options.compaction_filter_factory: None
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :  Options.sst_partitioner_factory: None
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :         Options.memtable_factory: SkipListFactory
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :            Options.table_factory: BlockBasedTable
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :            table_factory options:   flush_block_policy_factory: FlushBlockBySizePolicyFactory (0x7f2d241638a0)
  cache_index_and_filter_blocks: 0
  cache_index_and_filter_blocks_with_high_priority: 1
  pin_l0_filter_and_index_blocks_in_cache: 0
  pin_top_level_index_and_filter: 1
  index_type: 0
  data_block_index_type: 0
  index_shortening: 1
  data_block_hash_table_util_ratio: 0.750000
  checksum: 1
  no_block_cache: 0
  block_cache: 0x7f2d241638f0
  block_cache_name: LRUCache
  block_cache_options:
    capacity : 8388608
    num_shard_bits : 4
    strict_capacity_limit : 0
    memory_allocator : None
    high_pri_pool_ratio: 0.000
  block_cache_compressed: (nil)
  persistent_cache: (nil)
  block_size: 4096
  block_size_deviation: 10
  block_restart_interval: 16
  index_block_restart_interval: 1
  metadata_block_size: 4096
  partition_filters: 0
  use_delta_encoding: 1
  filter_policy: nullptr
  whole_key_filtering: 1
  verify_compression: 0
  read_amp_bytes_per_bit: 0
  format_version: 5
  enable_index_compression: 1
  block_align: 0
  max_auto_readahead_size: 262144
  prepopulate_block_cache: 0
  initial_auto_readahead_size: 8192

24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :        Options.write_buffer_size: 67108864
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :  Options.max_write_buffer_number: 2
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :          Options.compression: Snappy
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression: Disabled
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :       Options.prefix_extractor: nullptr
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :   Options.memtable_insert_with_hint_prefix_extractor: nullptr
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :             Options.num_levels: 7
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :        Options.min_write_buffer_number_to_merge: 1
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :     Options.max_write_buffer_number_to_maintain: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :     Options.max_write_buffer_size_to_maintain: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :            Options.bottommost_compression_opts.window_bits: -14
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression_opts.level: 32767
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :               Options.bottommost_compression_opts.strategy: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.max_dict_bytes: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.zstd_max_train_bytes: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.parallel_threads: 1
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression_opts.enabled: false
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.max_dict_buffer_bytes: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.use_zstd_dict_trainer: true
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :            Options.compression_opts.window_bits: -14
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                  Options.compression_opts.level: 32767
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :               Options.compression_opts.strategy: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.max_dict_bytes: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.zstd_max_train_bytes: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.use_zstd_dict_trainer: true
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.parallel_threads: 1
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                  Options.compression_opts.enabled: false
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.max_dict_buffer_bytes: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :      Options.level0_file_num_compaction_trigger: 4
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :          Options.level0_slowdown_writes_trigger: 20
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :              Options.level0_stop_writes_trigger: 36
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                   Options.target_file_size_base: 67108864
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :             Options.target_file_size_multiplier: 1
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                Options.max_bytes_for_level_base: 268435456
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Options.level_compaction_dynamic_level_bytes: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :          Options.max_bytes_for_level_multiplier: 10.000000
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[0]: 1
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[1]: 1
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[2]: 1
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[3]: 1
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[4]: 1
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[5]: 1
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[6]: 1
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :       Options.max_sequential_skip_in_iterations: 8
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                    Options.max_compaction_bytes: 1677721600
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                        Options.arena_block_size: 1048576
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :   Options.soft_pending_compaction_bytes_limit: 68719476736
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :   Options.hard_pending_compaction_bytes_limit: 274877906944
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                Options.disable_auto_compactions: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                        Options.compaction_style: kCompactionStyleLevel
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                          Options.compaction_pri: kMinOverlappingRatio
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.size_ratio: 1
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.min_merge_width: 2
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.max_merge_width: 4294967295
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.max_size_amplification_percent: 200
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.compression_size_percent: -1
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.stop_style: kCompactionStopStyleTotalSize
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_fifo.max_table_files_size: 1073741824
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_fifo.allow_compaction: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                   Options.table_properties_collectors: 
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                   Options.inplace_update_support: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                 Options.inplace_update_num_locks: 10000
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :               Options.memtable_prefix_bloom_size_ratio: 0.000000
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :               Options.memtable_whole_key_filtering: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :   Options.memtable_huge_page_size: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                           Options.bloom_locality: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                    Options.max_successive_merges: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                Options.optimize_filters_for_hits: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                Options.paranoid_file_checks: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                Options.force_consistency_checks: 1
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                Options.report_bg_io_stats: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                               Options.ttl: 2592000
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :          Options.periodic_compaction_seconds: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :  Options.preclude_last_level_data_seconds: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                       Options.enable_blob_files: false
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                           Options.min_blob_size: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                          Options.blob_file_size: 268435456
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                   Options.blob_compression_type: NoCompression
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :          Options.enable_blob_garbage_collection: false
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :      Options.blob_garbage_collection_age_cutoff: 0.250000
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Options.blob_garbage_collection_force_threshold: 1.000000
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :          Options.blob_compaction_readahead_size: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                Options.blob_file_starting_level: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Options.experimental_mempurge_threshold: 0.000000
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : [/db_impl/db_impl.cc:2861] Created column family [hudi_partitions__user_hive_warehouse_stock_ticks_mor] (ID 5)
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : [/column_family.cc:618] --------------- Options for column family [hudi_replaced_fg_user_hive_warehouse_stock_ticks_mor]:

24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :               Options.comparator: leveldb.BytewiseComparator
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :           Options.merge_operator: None
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :        Options.compaction_filter: None
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :        Options.compaction_filter_factory: None
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :  Options.sst_partitioner_factory: None
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :         Options.memtable_factory: SkipListFactory
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :            Options.table_factory: BlockBasedTable
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :            table_factory options:   flush_block_policy_factory: FlushBlockBySizePolicyFactory (0x7f2d24174580)
  cache_index_and_filter_blocks: 0
  cache_index_and_filter_blocks_with_high_priority: 1
  pin_l0_filter_and_index_blocks_in_cache: 0
  pin_top_level_index_and_filter: 1
  index_type: 0
  data_block_index_type: 0
  index_shortening: 1
  data_block_hash_table_util_ratio: 0.750000
  checksum: 1
  no_block_cache: 0
  block_cache: 0x7f2d241745d0
  block_cache_name: LRUCache
  block_cache_options:
    capacity : 8388608
    num_shard_bits : 4
    strict_capacity_limit : 0
    memory_allocator : None
    high_pri_pool_ratio: 0.000
  block_cache_compressed: (nil)
  persistent_cache: (nil)
  block_size: 4096
  block_size_deviation: 10
  block_restart_interval: 16
  index_block_restart_interval: 1
  metadata_block_size: 4096
  partition_filters: 0
  use_delta_encoding: 1
  filter_policy: nullptr
  whole_key_filtering: 1
  verify_compression: 0
  read_amp_bytes_per_bit: 0
  format_version: 5
  enable_index_compression: 1
  block_align: 0
  max_auto_readahead_size: 262144
  prepopulate_block_cache: 0
  initial_auto_readahead_size: 8192

24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :        Options.write_buffer_size: 67108864
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :  Options.max_write_buffer_number: 2
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :          Options.compression: Snappy
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression: Disabled
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :       Options.prefix_extractor: nullptr
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :   Options.memtable_insert_with_hint_prefix_extractor: nullptr
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :             Options.num_levels: 7
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :        Options.min_write_buffer_number_to_merge: 1
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :     Options.max_write_buffer_number_to_maintain: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :     Options.max_write_buffer_size_to_maintain: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :            Options.bottommost_compression_opts.window_bits: -14
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression_opts.level: 32767
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :               Options.bottommost_compression_opts.strategy: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.max_dict_bytes: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.zstd_max_train_bytes: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.parallel_threads: 1
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression_opts.enabled: false
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.max_dict_buffer_bytes: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.use_zstd_dict_trainer: true
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :            Options.compression_opts.window_bits: -14
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                  Options.compression_opts.level: 32767
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :               Options.compression_opts.strategy: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.max_dict_bytes: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.zstd_max_train_bytes: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.use_zstd_dict_trainer: true
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.parallel_threads: 1
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                  Options.compression_opts.enabled: false
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.max_dict_buffer_bytes: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :      Options.level0_file_num_compaction_trigger: 4
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :          Options.level0_slowdown_writes_trigger: 20
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :              Options.level0_stop_writes_trigger: 36
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                   Options.target_file_size_base: 67108864
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :             Options.target_file_size_multiplier: 1
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                Options.max_bytes_for_level_base: 268435456
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Options.level_compaction_dynamic_level_bytes: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :          Options.max_bytes_for_level_multiplier: 10.000000
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[0]: 1
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[1]: 1
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[2]: 1
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[3]: 1
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[4]: 1
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[5]: 1
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[6]: 1
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :       Options.max_sequential_skip_in_iterations: 8
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                    Options.max_compaction_bytes: 1677721600
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                        Options.arena_block_size: 1048576
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :   Options.soft_pending_compaction_bytes_limit: 68719476736
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :   Options.hard_pending_compaction_bytes_limit: 274877906944
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                Options.disable_auto_compactions: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                        Options.compaction_style: kCompactionStyleLevel
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                          Options.compaction_pri: kMinOverlappingRatio
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.size_ratio: 1
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.min_merge_width: 2
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.max_merge_width: 4294967295
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.max_size_amplification_percent: 200
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.compression_size_percent: -1
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.stop_style: kCompactionStopStyleTotalSize
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_fifo.max_table_files_size: 1073741824
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_fifo.allow_compaction: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                   Options.table_properties_collectors: 
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                   Options.inplace_update_support: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                 Options.inplace_update_num_locks: 10000
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :               Options.memtable_prefix_bloom_size_ratio: 0.000000
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :               Options.memtable_whole_key_filtering: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :   Options.memtable_huge_page_size: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                           Options.bloom_locality: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                    Options.max_successive_merges: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                Options.optimize_filters_for_hits: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                Options.paranoid_file_checks: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                Options.force_consistency_checks: 1
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                Options.report_bg_io_stats: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                               Options.ttl: 2592000
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :          Options.periodic_compaction_seconds: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :  Options.preclude_last_level_data_seconds: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                       Options.enable_blob_files: false
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                           Options.min_blob_size: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                          Options.blob_file_size: 268435456
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                   Options.blob_compression_type: NoCompression
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :          Options.enable_blob_garbage_collection: false
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :      Options.blob_garbage_collection_age_cutoff: 0.250000
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Options.blob_garbage_collection_force_threshold: 1.000000
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :          Options.blob_compaction_readahead_size: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                Options.blob_file_starting_level: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Options.experimental_mempurge_threshold: 0.000000
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : [/db_impl/db_impl.cc:2861] Created column family [hudi_replaced_fg_user_hive_warehouse_stock_ticks_mor] (ID 6)
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : [/column_family.cc:618] --------------- Options for column family [hudi_pending_clustering_fg_user_hive_warehouse_stock_ticks_mor]:

24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :               Options.comparator: leveldb.BytewiseComparator
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :           Options.merge_operator: None
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :        Options.compaction_filter: None
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :        Options.compaction_filter_factory: None
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :  Options.sst_partitioner_factory: None
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :         Options.memtable_factory: SkipListFactory
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :            Options.table_factory: BlockBasedTable
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :            table_factory options:   flush_block_policy_factory: FlushBlockBySizePolicyFactory (0x7f2d2418d9e0)
  cache_index_and_filter_blocks: 0
  cache_index_and_filter_blocks_with_high_priority: 1
  pin_l0_filter_and_index_blocks_in_cache: 0
  pin_top_level_index_and_filter: 1
  index_type: 0
  data_block_index_type: 0
  index_shortening: 1
  data_block_hash_table_util_ratio: 0.750000
  checksum: 1
  no_block_cache: 0
  block_cache: 0x7f2d2418da30
  block_cache_name: LRUCache
  block_cache_options:
    capacity : 8388608
    num_shard_bits : 4
    strict_capacity_limit : 0
    memory_allocator : None
    high_pri_pool_ratio: 0.000
  block_cache_compressed: (nil)
  persistent_cache: (nil)
  block_size: 4096
  block_size_deviation: 10
  block_restart_interval: 16
  index_block_restart_interval: 1
  metadata_block_size: 4096
  partition_filters: 0
  use_delta_encoding: 1
  filter_policy: nullptr
  whole_key_filtering: 1
  verify_compression: 0
  read_amp_bytes_per_bit: 0
  format_version: 5
  enable_index_compression: 1
  block_align: 0
  max_auto_readahead_size: 262144
  prepopulate_block_cache: 0
  initial_auto_readahead_size: 8192

24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :        Options.write_buffer_size: 67108864
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :  Options.max_write_buffer_number: 2
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :          Options.compression: Snappy
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression: Disabled
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :       Options.prefix_extractor: nullptr
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :   Options.memtable_insert_with_hint_prefix_extractor: nullptr
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :             Options.num_levels: 7
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :        Options.min_write_buffer_number_to_merge: 1
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :     Options.max_write_buffer_number_to_maintain: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :     Options.max_write_buffer_size_to_maintain: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :            Options.bottommost_compression_opts.window_bits: -14
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression_opts.level: 32767
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :               Options.bottommost_compression_opts.strategy: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.max_dict_bytes: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.zstd_max_train_bytes: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.parallel_threads: 1
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression_opts.enabled: false
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.max_dict_buffer_bytes: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.use_zstd_dict_trainer: true
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :            Options.compression_opts.window_bits: -14
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                  Options.compression_opts.level: 32767
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :               Options.compression_opts.strategy: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.max_dict_bytes: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.zstd_max_train_bytes: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.use_zstd_dict_trainer: true
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.parallel_threads: 1
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                  Options.compression_opts.enabled: false
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.max_dict_buffer_bytes: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :      Options.level0_file_num_compaction_trigger: 4
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :          Options.level0_slowdown_writes_trigger: 20
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :              Options.level0_stop_writes_trigger: 36
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                   Options.target_file_size_base: 67108864
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :             Options.target_file_size_multiplier: 1
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                Options.max_bytes_for_level_base: 268435456
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Options.level_compaction_dynamic_level_bytes: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :          Options.max_bytes_for_level_multiplier: 10.000000
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[0]: 1
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[1]: 1
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[2]: 1
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[3]: 1
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[4]: 1
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[5]: 1
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[6]: 1
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :       Options.max_sequential_skip_in_iterations: 8
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                    Options.max_compaction_bytes: 1677721600
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                        Options.arena_block_size: 1048576
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :   Options.soft_pending_compaction_bytes_limit: 68719476736
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :   Options.hard_pending_compaction_bytes_limit: 274877906944
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                Options.disable_auto_compactions: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                        Options.compaction_style: kCompactionStyleLevel
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                          Options.compaction_pri: kMinOverlappingRatio
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.size_ratio: 1
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.min_merge_width: 2
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.max_merge_width: 4294967295
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.max_size_amplification_percent: 200
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.compression_size_percent: -1
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.stop_style: kCompactionStopStyleTotalSize
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_fifo.max_table_files_size: 1073741824
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_fifo.allow_compaction: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                   Options.table_properties_collectors: 
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                   Options.inplace_update_support: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                 Options.inplace_update_num_locks: 10000
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :               Options.memtable_prefix_bloom_size_ratio: 0.000000
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :               Options.memtable_whole_key_filtering: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :   Options.memtable_huge_page_size: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                           Options.bloom_locality: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                    Options.max_successive_merges: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                Options.optimize_filters_for_hits: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                Options.paranoid_file_checks: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                Options.force_consistency_checks: 1
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                Options.report_bg_io_stats: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                               Options.ttl: 2592000
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :          Options.periodic_compaction_seconds: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :  Options.preclude_last_level_data_seconds: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                       Options.enable_blob_files: false
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                           Options.min_blob_size: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                          Options.blob_file_size: 268435456
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                   Options.blob_compression_type: NoCompression
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :          Options.enable_blob_garbage_collection: false
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :      Options.blob_garbage_collection_age_cutoff: 0.250000
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Options.blob_garbage_collection_force_threshold: 1.000000
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :          Options.blob_compaction_readahead_size: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB :                Options.blob_file_starting_level: 0
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : Options.experimental_mempurge_threshold: 0.000000
24/01/05 02:44:06 INFO collection.RocksDBDAO: From Rocks DB : [/db_impl/db_impl.cc:2861] Created column family [hudi_pending_clustering_fg_user_hive_warehouse_stock_ticks_mor] (ID 7)
24/01/05 02:44:06 INFO view.RocksDbBasedFileSystemView: Resetting replacedFileGroups to ROCKSDB based file-system view at /tmp/hoodie_timeline_rocksdb, Total file-groups=0
24/01/05 02:44:06 INFO collection.RocksDBDAO: Prefix DELETE (query=part=) on hudi_replaced_fg_user_hive_warehouse_stock_ticks_mor
24/01/05 02:44:06 INFO view.RocksDbBasedFileSystemView: Resetting replacedFileGroups to ROCKSDB based file-system view complete
24/01/05 02:44:06 INFO view.AbstractTableFileSystemView: Took 2 ms to read  0 instants, 0 replaced file groups
24/01/05 02:44:06 INFO view.RocksDbBasedFileSystemView: Initializing pending compaction operations. Count=0
24/01/05 02:44:06 INFO view.RocksDbBasedFileSystemView: Initializing pending Log compaction operations. Count=0
24/01/05 02:44:06 INFO view.RocksDbBasedFileSystemView: Initializing external data file mapping. Count=0
24/01/05 02:44:06 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:44:06 INFO view.RocksDbBasedFileSystemView: Resetting file groups in pending clustering to ROCKSDB based file-system view at /tmp/hoodie_timeline_rocksdb, Total file-groups=0
24/01/05 02:44:06 INFO collection.RocksDBDAO: Prefix DELETE (query=part=) on hudi_pending_clustering_fg_user_hive_warehouse_stock_ticks_mor
24/01/05 02:44:06 INFO view.RocksDbBasedFileSystemView: Resetting replacedFileGroups to ROCKSDB based file-system view complete
24/01/05 02:44:06 INFO view.RocksDbBasedFileSystemView: Created ROCKSDB based file-system view at /tmp/hoodie_timeline_rocksdb
24/01/05 02:44:06 INFO view.AbstractTableFileSystemView: Building file system view for partition (2018/08/31)
24/01/05 02:44:06 INFO view.RocksDbBasedFileSystemView: Resetting and adding new partition (2018/08/31) to ROCKSDB based file-system view at /tmp/hoodie_timeline_rocksdb, Total file-groups=1
24/01/05 02:44:06 INFO collection.RocksDBDAO: Prefix DELETE (query=type=slice,part=2018/08/31,id=) on hudi_view__user_hive_warehouse_stock_ticks_mor
24/01/05 02:44:06 INFO collection.RocksDBDAO: Prefix DELETE (query=type=df,part=2018/08/31,id=) on hudi_view__user_hive_warehouse_stock_ticks_mor
24/01/05 02:44:06 INFO view.RocksDbBasedFileSystemView: Finished adding new partition (2018/08/31) to ROCKSDB based file-system view at /tmp/hoodie_timeline_rocksdb, Total file-groups=1
24/01/05 02:44:06 INFO collection.RocksDBDAO: Prefix Search for (query=type=slice,part=2018/08/31,id=) on hudi_view__user_hive_warehouse_stock_ticks_mor. Total Time Taken (msec)=5. Serialization Time taken(micro)=5259, num entries=1
24/01/05 02:44:06 INFO executor.Executor: Finished task 0.0 in stage 3.0 (TID 5). 1064 bytes result sent to driver
24/01/05 02:44:06 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 3.0 (TID 5) in 808 ms on localhost (executor driver) (1/1)
24/01/05 02:44:06 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
24/01/05 02:44:06 INFO scheduler.DAGScheduler: ResultStage 3 (collect at HoodieSparkEngineContext.java:150) finished in 0.843 s
24/01/05 02:44:06 INFO scheduler.DAGScheduler: Job 1 finished: collect at HoodieSparkEngineContext.java:150, took 0.845988 s
24/01/05 02:44:06 INFO rdd.MapPartitionsRDD: Removing RDD 9 from persistence list
24/01/05 02:44:06 INFO storage.BlockManager: Removing RDD 9
24/01/05 02:44:06 INFO view.AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
24/01/05 02:44:06 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:44:06 INFO spark.SparkContext: Starting job: countByKey at HoodieJavaPairRDD.java:105
24/01/05 02:44:06 INFO scheduler.DAGScheduler: Registering RDD 20 (mapToPair at HoodieJavaRDD.java:149)
24/01/05 02:44:06 INFO scheduler.DAGScheduler: Registering RDD 10 (mapToPair at HoodieJavaRDD.java:149)
24/01/05 02:44:06 INFO scheduler.DAGScheduler: Registering RDD 28 (countByKey at HoodieJavaPairRDD.java:105)
24/01/05 02:44:06 INFO scheduler.DAGScheduler: Got job 2 (countByKey at HoodieJavaPairRDD.java:105) with 2 output partitions
24/01/05 02:44:06 INFO scheduler.DAGScheduler: Final stage: ResultStage 8 (countByKey at HoodieJavaPairRDD.java:105)
24/01/05 02:44:06 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 7)
24/01/05 02:44:06 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 7)
24/01/05 02:44:06 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 5 (MapPartitionsRDD[20] at mapToPair at HoodieJavaRDD.java:149), which has no missing parents
24/01/05 02:44:06 INFO memory.MemoryStore: Block broadcast_4 stored as values in memory (estimated size 341.0 KB, free 365.5 MB)
24/01/05 02:44:06 INFO memory.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 122.2 KB, free 365.4 MB)
24/01/05 02:44:06 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on adhoc-2:46387 (size: 122.2 KB, free: 366.0 MB)
24/01/05 02:44:06 INFO spark.SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1161
24/01/05 02:44:06 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 5 (MapPartitionsRDD[20] at mapToPair at HoodieJavaRDD.java:149) (first 15 tasks are for partitions Vector(0))
24/01/05 02:44:06 INFO scheduler.TaskSchedulerImpl: Adding task set 5.0 with 1 tasks
24/01/05 02:44:06 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 6 (MapPartitionsRDD[10] at mapToPair at HoodieJavaRDD.java:149), which has no missing parents
24/01/05 02:44:06 INFO memory.MemoryStore: Block broadcast_5 stored as values in memory (estimated size 23.7 KB, free 365.3 MB)
24/01/05 02:44:06 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 5.0 (TID 6, localhost, executor driver, partition 0, PROCESS_LOCAL, 8108 bytes)
24/01/05 02:44:06 INFO executor.Executor: Running task 0.0 in stage 5.0 (TID 6)
24/01/05 02:44:06 INFO memory.MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 11.4 KB, free 365.3 MB)
24/01/05 02:44:06 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on adhoc-2:46387 (size: 11.4 KB, free: 366.0 MB)
24/01/05 02:44:06 INFO spark.SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1161
24/01/05 02:44:06 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[10] at mapToPair at HoodieJavaRDD.java:149) (first 15 tasks are for partitions Vector(0, 1))
24/01/05 02:44:06 INFO scheduler.TaskSchedulerImpl: Adding task set 6.0 with 2 tasks
24/01/05 02:44:06 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 6.0 (TID 7, localhost, executor driver, partition 0, PROCESS_LOCAL, 7651 bytes)
24/01/05 02:44:06 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 6.0 (TID 8, localhost, executor driver, partition 1, ANY, 7651 bytes)
24/01/05 02:44:06 INFO executor.Executor: Running task 0.0 in stage 6.0 (TID 7)
24/01/05 02:44:06 INFO executor.Executor: Running task 1.0 in stage 6.0 (TID 8)
24/01/05 02:44:06 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
24/01/05 02:44:06 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
24/01/05 02:44:06 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
24/01/05 02:44:06 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
24/01/05 02:44:06 INFO executor.Executor: Finished task 0.0 in stage 6.0 (TID 7). 1123 bytes result sent to driver
24/01/05 02:44:06 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 6.0 (TID 7) in 19 ms on localhost (executor driver) (1/2)
24/01/05 02:44:06 INFO executor.Executor: Finished task 1.0 in stage 6.0 (TID 8). 1252 bytes result sent to driver
24/01/05 02:44:06 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 6.0 (TID 8) in 23 ms on localhost (executor driver) (2/2)
24/01/05 02:44:06 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool 
24/01/05 02:44:06 INFO scheduler.DAGScheduler: ShuffleMapStage 6 (mapToPair at HoodieJavaRDD.java:149) finished in 0.033 s
24/01/05 02:44:06 INFO scheduler.DAGScheduler: looking for newly runnable stages
24/01/05 02:44:06 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 5)
24/01/05 02:44:06 INFO scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 7, ResultStage 8)
24/01/05 02:44:06 INFO scheduler.DAGScheduler: failed: Set()
24/01/05 02:44:06 INFO hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 102 records.
24/01/05 02:44:06 INFO hadoop.InternalParquetRecordReader: at row 0. reading next block
24/01/05 02:44:06 INFO compress.CodecPool: Got brand-new decompressor [.gz]
24/01/05 02:44:06 INFO hadoop.InternalParquetRecordReader: block read in memory in 36 ms. row count = 102
24/01/05 02:44:07 INFO spark.ContextCleaner: Cleaned accumulator 35
24/01/05 02:44:07 INFO spark.ContextCleaner: Cleaned accumulator 43
24/01/05 02:44:07 INFO spark.ContextCleaner: Cleaned accumulator 63
24/01/05 02:44:07 INFO spark.ContextCleaner: Cleaned accumulator 3
24/01/05 02:44:07 INFO spark.ContextCleaner: Cleaned accumulator 32
24/01/05 02:44:07 INFO spark.ContextCleaner: Cleaned accumulator 89
24/01/05 02:44:07 INFO spark.ContextCleaner: Cleaned accumulator 96
24/01/05 02:44:07 INFO spark.ContextCleaner: Cleaned accumulator 97
24/01/05 02:44:07 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on adhoc-2:46387 in memory (size: 11.6 KB, free: 366.0 MB)
24/01/05 02:44:07 INFO spark.ContextCleaner: Cleaned accumulator 68
24/01/05 02:44:07 INFO spark.ContextCleaner: Cleaned accumulator 83
24/01/05 02:44:07 INFO spark.ContextCleaner: Cleaned accumulator 36
24/01/05 02:44:07 INFO spark.ContextCleaner: Cleaned accumulator 20
24/01/05 02:44:07 INFO spark.ContextCleaner: Cleaned accumulator 21
24/01/05 02:44:07 INFO spark.ContextCleaner: Cleaned accumulator 49
24/01/05 02:44:07 INFO spark.ContextCleaner: Cleaned accumulator 76
24/01/05 02:44:07 INFO spark.ContextCleaner: Cleaned accumulator 37
24/01/05 02:44:07 INFO spark.ContextCleaner: Cleaned accumulator 28
24/01/05 02:44:07 INFO spark.ContextCleaner: Cleaned accumulator 58
24/01/05 02:44:07 INFO spark.ContextCleaner: Cleaned accumulator 13
24/01/05 02:44:07 INFO spark.ContextCleaner: Cleaned accumulator 8
24/01/05 02:44:07 INFO spark.ContextCleaner: Cleaned accumulator 44
24/01/05 02:44:07 INFO spark.ContextCleaner: Cleaned accumulator 25
24/01/05 02:44:07 INFO spark.ContextCleaner: Cleaned shuffle 0
24/01/05 02:44:07 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on adhoc-2:46387 in memory (size: 11.4 KB, free: 366.1 MB)
24/01/05 02:44:07 INFO spark.ContextCleaner: Cleaned accumulator 77
24/01/05 02:44:07 INFO spark.ContextCleaner: Cleaned accumulator 0
24/01/05 02:44:07 INFO spark.ContextCleaner: Cleaned accumulator 9
24/01/05 02:44:07 INFO spark.ContextCleaner: Cleaned accumulator 65
24/01/05 02:44:07 INFO spark.ContextCleaner: Cleaned accumulator 91
24/01/05 02:44:07 INFO spark.ContextCleaner: Cleaned accumulator 92
24/01/05 02:44:07 INFO spark.ContextCleaner: Cleaned accumulator 79
24/01/05 02:44:07 INFO spark.ContextCleaner: Cleaned accumulator 52
24/01/05 02:44:07 INFO spark.ContextCleaner: Cleaned accumulator 30
24/01/05 02:44:07 INFO spark.ContextCleaner: Cleaned accumulator 16
24/01/05 02:44:07 INFO spark.ContextCleaner: Cleaned accumulator 7
24/01/05 02:44:07 INFO spark.ContextCleaner: Cleaned accumulator 6
24/01/05 02:44:07 INFO spark.ContextCleaner: Cleaned accumulator 24
24/01/05 02:44:07 INFO spark.ContextCleaner: Cleaned accumulator 54
24/01/05 02:44:07 INFO storage.BlockManagerInfo: Removed broadcast_2_piece0 on adhoc-2:46387 in memory (size: 2.4 KB, free: 366.1 MB)
24/01/05 02:44:07 INFO spark.ContextCleaner: Cleaned accumulator 61
24/01/05 02:44:07 INFO spark.ContextCleaner: Cleaned accumulator 82
24/01/05 02:44:07 INFO spark.ContextCleaner: Cleaned accumulator 99
24/01/05 02:44:07 INFO spark.ContextCleaner: Cleaned accumulator 19
24/01/05 02:44:07 INFO spark.ContextCleaner: Cleaned accumulator 22
24/01/05 02:44:07 INFO spark.ContextCleaner: Cleaned accumulator 23
24/01/05 02:44:07 INFO spark.ContextCleaner: Cleaned accumulator 27
24/01/05 02:44:07 INFO spark.ContextCleaner: Cleaned accumulator 51
24/01/05 02:44:07 INFO spark.ContextCleaner: Cleaned accumulator 2
24/01/05 02:44:07 INFO spark.ContextCleaner: Cleaned accumulator 29
24/01/05 02:44:07 INFO spark.ContextCleaner: Cleaned accumulator 60
24/01/05 02:44:07 INFO spark.ContextCleaner: Cleaned accumulator 33
24/01/05 02:44:07 INFO spark.ContextCleaner: Cleaned accumulator 34
24/01/05 02:44:07 INFO spark.ContextCleaner: Cleaned accumulator 90
24/01/05 02:44:07 INFO spark.ContextCleaner: Cleaned accumulator 38
24/01/05 02:44:07 INFO spark.ContextCleaner: Cleaned accumulator 47
24/01/05 02:44:07 INFO spark.ContextCleaner: Cleaned accumulator 46
24/01/05 02:44:07 INFO spark.ContextCleaner: Cleaned accumulator 69
24/01/05 02:44:07 INFO spark.ContextCleaner: Cleaned accumulator 70
24/01/05 02:44:07 INFO spark.ContextCleaner: Cleaned accumulator 78
24/01/05 02:44:07 INFO spark.ContextCleaner: Cleaned accumulator 17
24/01/05 02:44:07 INFO spark.ContextCleaner: Cleaned accumulator 80
24/01/05 02:44:07 INFO spark.ContextCleaner: Cleaned accumulator 94
24/01/05 02:44:07 INFO spark.ContextCleaner: Cleaned accumulator 72
24/01/05 02:44:07 INFO spark.ContextCleaner: Cleaned accumulator 41
24/01/05 02:44:07 INFO spark.ContextCleaner: Cleaned accumulator 4
24/01/05 02:44:07 INFO spark.ContextCleaner: Cleaned accumulator 10
24/01/05 02:44:07 INFO spark.ContextCleaner: Cleaned accumulator 81
24/01/05 02:44:07 INFO spark.ContextCleaner: Cleaned accumulator 62
24/01/05 02:44:07 INFO spark.ContextCleaner: Cleaned accumulator 88
24/01/05 02:44:07 INFO spark.ContextCleaner: Cleaned accumulator 39
24/01/05 02:44:07 INFO spark.ContextCleaner: Cleaned accumulator 15
24/01/05 02:44:07 INFO spark.ContextCleaner: Cleaned accumulator 57
24/01/05 02:44:07 INFO spark.ContextCleaner: Cleaned accumulator 73
24/01/05 02:44:07 INFO spark.ContextCleaner: Cleaned accumulator 84
24/01/05 02:44:07 INFO spark.ContextCleaner: Cleaned accumulator 14
24/01/05 02:44:07 INFO spark.ContextCleaner: Cleaned accumulator 64
24/01/05 02:44:07 INFO spark.ContextCleaner: Cleaned accumulator 86
24/01/05 02:44:07 INFO spark.ContextCleaner: Cleaned accumulator 31
24/01/05 02:44:07 INFO spark.ContextCleaner: Cleaned accumulator 42
24/01/05 02:44:07 INFO spark.ContextCleaner: Cleaned accumulator 75
24/01/05 02:44:07 INFO spark.ContextCleaner: Cleaned accumulator 5
24/01/05 02:44:07 INFO spark.ContextCleaner: Cleaned accumulator 71
24/01/05 02:44:07 INFO spark.ContextCleaner: Cleaned accumulator 40
24/01/05 02:44:07 INFO spark.ContextCleaner: Cleaned accumulator 1
24/01/05 02:44:07 INFO spark.ContextCleaner: Cleaned accumulator 45
24/01/05 02:44:07 INFO spark.ContextCleaner: Cleaned accumulator 67
24/01/05 02:44:07 INFO spark.ContextCleaner: Cleaned accumulator 85
24/01/05 02:44:07 INFO spark.ContextCleaner: Cleaned accumulator 11
24/01/05 02:44:07 INFO spark.ContextCleaner: Cleaned accumulator 18
24/01/05 02:44:07 INFO spark.ContextCleaner: Cleaned accumulator 98
24/01/05 02:44:07 INFO spark.ContextCleaner: Cleaned accumulator 66
24/01/05 02:44:07 INFO spark.ContextCleaner: Cleaned accumulator 95
24/01/05 02:44:07 INFO spark.ContextCleaner: Cleaned accumulator 48
24/01/05 02:44:07 INFO spark.ContextCleaner: Cleaned accumulator 55
24/01/05 02:44:07 INFO spark.ContextCleaner: Cleaned accumulator 87
24/01/05 02:44:07 INFO spark.ContextCleaner: Cleaned accumulator 12
24/01/05 02:44:07 INFO spark.ContextCleaner: Cleaned accumulator 26
24/01/05 02:44:07 INFO spark.ContextCleaner: Cleaned accumulator 56
24/01/05 02:44:07 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on adhoc-2:46387 in memory (size: 121.3 KB, free: 366.2 MB)
24/01/05 02:44:07 INFO spark.ContextCleaner: Cleaned accumulator 53
24/01/05 02:44:07 INFO spark.ContextCleaner: Cleaned accumulator 59
24/01/05 02:44:07 INFO spark.ContextCleaner: Cleaned accumulator 93
24/01/05 02:44:07 INFO spark.ContextCleaner: Cleaned accumulator 74
24/01/05 02:44:07 INFO spark.ContextCleaner: Cleaned accumulator 50
24/01/05 02:44:07 INFO executor.Executor: Finished task 0.0 in stage 5.0 (TID 6). 865 bytes result sent to driver
24/01/05 02:44:07 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 5.0 (TID 6) in 530 ms on localhost (executor driver) (1/1)
24/01/05 02:44:07 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool 
24/01/05 02:44:07 INFO scheduler.DAGScheduler: ShuffleMapStage 5 (mapToPair at HoodieJavaRDD.java:149) finished in 0.562 s
24/01/05 02:44:07 INFO scheduler.DAGScheduler: looking for newly runnable stages
24/01/05 02:44:07 INFO scheduler.DAGScheduler: running: Set()
24/01/05 02:44:07 INFO scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 7, ResultStage 8)
24/01/05 02:44:07 INFO scheduler.DAGScheduler: failed: Set()
24/01/05 02:44:07 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 7 (MapPartitionsRDD[28] at countByKey at HoodieJavaPairRDD.java:105), which has no missing parents
24/01/05 02:44:07 INFO memory.MemoryStore: Block broadcast_6 stored as values in memory (estimated size 7.7 KB, free 365.8 MB)
24/01/05 02:44:07 INFO memory.MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 4.0 KB, free 365.8 MB)
24/01/05 02:44:07 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on adhoc-2:46387 (size: 4.0 KB, free: 366.2 MB)
24/01/05 02:44:07 INFO spark.SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1161
24/01/05 02:44:07 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 7 (MapPartitionsRDD[28] at countByKey at HoodieJavaPairRDD.java:105) (first 15 tasks are for partitions Vector(0, 1))
24/01/05 02:44:07 INFO scheduler.TaskSchedulerImpl: Adding task set 7.0 with 2 tasks
24/01/05 02:44:07 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 7.0 (TID 9, localhost, executor driver, partition 0, PROCESS_LOCAL, 7714 bytes)
24/01/05 02:44:07 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 7.0 (TID 10, localhost, executor driver, partition 1, PROCESS_LOCAL, 7714 bytes)
24/01/05 02:44:07 INFO executor.Executor: Running task 0.0 in stage 7.0 (TID 9)
24/01/05 02:44:07 INFO executor.Executor: Running task 1.0 in stage 7.0 (TID 10)
24/01/05 02:44:07 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
24/01/05 02:44:07 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
24/01/05 02:44:07 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
24/01/05 02:44:07 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
24/01/05 02:44:07 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
24/01/05 02:44:07 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
24/01/05 02:44:07 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
24/01/05 02:44:07 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
24/01/05 02:44:07 INFO memory.MemoryStore: Block rdd_26_0 stored as values in memory (estimated size 0.0 B, free 365.8 MB)
24/01/05 02:44:07 INFO storage.BlockManagerInfo: Added rdd_26_0 in memory on adhoc-2:46387 (size: 0.0 B, free: 366.2 MB)
24/01/05 02:44:07 INFO executor.Executor: Finished task 0.0 in stage 7.0 (TID 9). 1166 bytes result sent to driver
24/01/05 02:44:07 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 7.0 (TID 9) in 48 ms on localhost (executor driver) (1/2)
24/01/05 02:44:07 INFO memory.MemoryStore: Block rdd_26_1 stored as values in memory (estimated size 802.0 B, free 365.8 MB)
24/01/05 02:44:07 INFO storage.BlockManagerInfo: Added rdd_26_1 in memory on adhoc-2:46387 (size: 802.0 B, free: 366.2 MB)
24/01/05 02:44:07 INFO executor.Executor: Finished task 1.0 in stage 7.0 (TID 10). 1252 bytes result sent to driver
24/01/05 02:44:07 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 7.0 (TID 10) in 60 ms on localhost (executor driver) (2/2)
24/01/05 02:44:07 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool 
24/01/05 02:44:07 INFO scheduler.DAGScheduler: ShuffleMapStage 7 (countByKey at HoodieJavaPairRDD.java:105) finished in 0.072 s
24/01/05 02:44:07 INFO scheduler.DAGScheduler: looking for newly runnable stages
24/01/05 02:44:07 INFO scheduler.DAGScheduler: running: Set()
24/01/05 02:44:07 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 8)
24/01/05 02:44:07 INFO scheduler.DAGScheduler: failed: Set()
24/01/05 02:44:07 INFO scheduler.DAGScheduler: Submitting ResultStage 8 (ShuffledRDD[29] at countByKey at HoodieJavaPairRDD.java:105), which has no missing parents
24/01/05 02:44:07 INFO memory.MemoryStore: Block broadcast_7 stored as values in memory (estimated size 3.6 KB, free 365.8 MB)
24/01/05 02:44:07 INFO memory.MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 2.1 KB, free 365.8 MB)
24/01/05 02:44:07 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on adhoc-2:46387 (size: 2.1 KB, free: 366.2 MB)
24/01/05 02:44:07 INFO spark.SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1161
24/01/05 02:44:07 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ResultStage 8 (ShuffledRDD[29] at countByKey at HoodieJavaPairRDD.java:105) (first 15 tasks are for partitions Vector(0, 1))
24/01/05 02:44:07 INFO scheduler.TaskSchedulerImpl: Adding task set 8.0 with 2 tasks
24/01/05 02:44:07 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 8.0 (TID 11, localhost, executor driver, partition 0, PROCESS_LOCAL, 7662 bytes)
24/01/05 02:44:07 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 8.0 (TID 12, localhost, executor driver, partition 1, ANY, 7662 bytes)
24/01/05 02:44:07 INFO executor.Executor: Running task 0.0 in stage 8.0 (TID 11)
24/01/05 02:44:07 INFO executor.Executor: Running task 1.0 in stage 8.0 (TID 12)
24/01/05 02:44:07 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
24/01/05 02:44:07 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
24/01/05 02:44:07 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
24/01/05 02:44:07 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
24/01/05 02:44:07 INFO executor.Executor: Finished task 0.0 in stage 8.0 (TID 11). 1098 bytes result sent to driver
24/01/05 02:44:07 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 8.0 (TID 11) in 12 ms on localhost (executor driver) (1/2)
24/01/05 02:44:07 INFO executor.Executor: Finished task 1.0 in stage 8.0 (TID 12). 1232 bytes result sent to driver
24/01/05 02:44:07 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 8.0 (TID 12) in 16 ms on localhost (executor driver) (2/2)
24/01/05 02:44:07 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool 
24/01/05 02:44:07 INFO scheduler.DAGScheduler: ResultStage 8 (countByKey at HoodieJavaPairRDD.java:105) finished in 0.025 s
24/01/05 02:44:07 INFO scheduler.DAGScheduler: Job 2 finished: countByKey at HoodieJavaPairRDD.java:105, took 0.667395 s
24/01/05 02:44:07 INFO commit.UpsertPartitioner: AvgRecordSize => 1024
24/01/05 02:44:07 INFO collection.RocksDBDAO: From Rocks DB : [/db_impl/db_impl.cc:896] ------- PERSISTING STATS -------
24/01/05 02:44:07 INFO collection.RocksDBDAO: From Rocks DB : [/db_impl/db_impl.cc:966] [Pre-GC] In-memory stats history size: 48 bytes, slice count: 0
24/01/05 02:44:07 INFO collection.RocksDBDAO: From Rocks DB : [/db_impl/db_impl.cc:975] [Post-GC] In-memory stats history size: 48 bytes, slice count: 0
24/01/05 02:44:07 INFO spark.SparkContext: Starting job: collectAsMap at UpsertPartitioner.java:282
24/01/05 02:44:07 INFO scheduler.DAGScheduler: Got job 3 (collectAsMap at UpsertPartitioner.java:282) with 1 output partitions
24/01/05 02:44:07 INFO scheduler.DAGScheduler: Final stage: ResultStage 9 (collectAsMap at UpsertPartitioner.java:282)
24/01/05 02:44:07 INFO scheduler.DAGScheduler: Parents of final stage: List()
24/01/05 02:44:07 INFO scheduler.DAGScheduler: Missing parents: List()
24/01/05 02:44:07 INFO scheduler.DAGScheduler: Submitting ResultStage 9 (MapPartitionsRDD[31] at mapToPair at UpsertPartitioner.java:281), which has no missing parents
24/01/05 02:44:07 INFO memory.MemoryStore: Block broadcast_8 stored as values in memory (estimated size 340.5 KB, free 365.5 MB)
24/01/05 02:44:07 INFO memory.MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 121.8 KB, free 365.4 MB)
24/01/05 02:44:07 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on adhoc-2:46387 (size: 121.8 KB, free: 366.1 MB)
24/01/05 02:44:07 INFO spark.SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1161
24/01/05 02:44:07 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[31] at mapToPair at UpsertPartitioner.java:281) (first 15 tasks are for partitions Vector(0))
24/01/05 02:44:07 INFO scheduler.TaskSchedulerImpl: Adding task set 9.0 with 1 tasks
24/01/05 02:44:07 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 9.0 (TID 13, localhost, executor driver, partition 0, PROCESS_LOCAL, 7735 bytes)
24/01/05 02:44:07 INFO executor.Executor: Running task 0.0 in stage 9.0 (TID 13)
24/01/05 02:44:07 INFO view.FileSystemViewManager: Creating View Manager with storage type :REMOTE_FIRST
24/01/05 02:44:07 INFO view.FileSystemViewManager: Creating remote first table view
24/01/05 02:44:07 INFO view.FileSystemViewManager: Creating remote view for basePath /user/hive/warehouse/stock_ticks_mor. Server=adhoc-2:42425, Timeout=300
24/01/05 02:44:07 INFO view.FileSystemViewManager: Creating InMemory based view for basePath /user/hive/warehouse/stock_ticks_mor
24/01/05 02:44:07 INFO view.AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
24/01/05 02:44:07 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:44:07 INFO view.RemoteHoodieTableFileSystemView: Sending request : (http://adhoc-2:42425/v1/hoodie/view/slices/beforeoron/latest/?partition=2018%2F08%2F31&maxinstant=20240105024343265&includependingcompaction=false&basepath=%2Fuser%2Fhive%2Fwarehouse%2Fstock_ticks_mor&lastinstantts=20240105024343265&timelinehash=7a0b5e1482c059272c847fc6668f12da532a637ce11f6f599d2728792005a6a1)
24/01/05 02:44:07 INFO collection.RocksDBDAO: Prefix Search for (query=type=slice,part=2018/08/31,id=) on hudi_view__user_hive_warehouse_stock_ticks_mor. Total Time Taken (msec)=4. Serialization Time taken(micro)=4189, num entries=1
24/01/05 02:44:07 INFO executor.Executor: Finished task 0.0 in stage 9.0 (TID 13). 813 bytes result sent to driver
24/01/05 02:44:07 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 9.0 (TID 13) in 59 ms on localhost (executor driver) (1/1)
24/01/05 02:44:07 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool 
24/01/05 02:44:07 INFO scheduler.DAGScheduler: ResultStage 9 (collectAsMap at UpsertPartitioner.java:282) finished in 0.090 s
24/01/05 02:44:07 INFO scheduler.DAGScheduler: Job 3 finished: collectAsMap at UpsertPartitioner.java:282, took 0.092150 s
24/01/05 02:44:07 INFO view.AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
24/01/05 02:44:07 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:44:07 INFO commit.UpsertPartitioner: Total Buckets: 1
24/01/05 02:44:07 INFO timeline.HoodieActiveTimeline: Checking for file exists ?/user/hive/warehouse/stock_ticks_mor/.hoodie/20240105024402665.deltacommit.requested
24/01/05 02:44:07 INFO timeline.HoodieActiveTimeline: Create new file for toInstant ?/user/hive/warehouse/stock_ticks_mor/.hoodie/20240105024402665.deltacommit.inflight
24/01/05 02:44:07 INFO commit.BaseSparkCommitActionExecutor: no validators configured.
24/01/05 02:44:07 INFO commit.BaseCommitActionExecutor: Auto commit disabled for 20240105024402665
24/01/05 02:44:07 INFO spark.SparkContext: Starting job: sum at StreamSync.java:783
24/01/05 02:44:07 INFO scheduler.DAGScheduler: Registering RDD 32 (mapToPair at HoodieJavaRDD.java:149)
24/01/05 02:44:07 INFO scheduler.DAGScheduler: Got job 4 (sum at StreamSync.java:783) with 1 output partitions
24/01/05 02:44:07 INFO scheduler.DAGScheduler: Final stage: ResultStage 14 (sum at StreamSync.java:783)
24/01/05 02:44:07 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 13)
24/01/05 02:44:07 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 13)
24/01/05 02:44:07 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 13 (MapPartitionsRDD[32] at mapToPair at HoodieJavaRDD.java:149), which has no missing parents
24/01/05 02:44:07 INFO memory.MemoryStore: Block broadcast_9 stored as values in memory (estimated size 345.1 KB, free 365.0 MB)
24/01/05 02:44:07 INFO memory.MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 123.4 KB, free 364.9 MB)
24/01/05 02:44:07 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on adhoc-2:46387 (size: 123.4 KB, free: 365.9 MB)
24/01/05 02:44:07 INFO spark.SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1161
24/01/05 02:44:07 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 13 (MapPartitionsRDD[32] at mapToPair at HoodieJavaRDD.java:149) (first 15 tasks are for partitions Vector(0, 1))
24/01/05 02:44:07 INFO scheduler.TaskSchedulerImpl: Adding task set 13.0 with 2 tasks
24/01/05 02:44:07 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 13.0 (TID 14, localhost, executor driver, partition 0, PROCESS_LOCAL, 7714 bytes)
24/01/05 02:44:07 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 13.0 (TID 15, localhost, executor driver, partition 1, PROCESS_LOCAL, 7714 bytes)
24/01/05 02:44:07 INFO executor.Executor: Running task 1.0 in stage 13.0 (TID 15)
24/01/05 02:44:07 INFO executor.Executor: Running task 0.0 in stage 13.0 (TID 14)
24/01/05 02:44:07 INFO storage.BlockManager: Found block rdd_26_0 locally
24/01/05 02:44:07 INFO storage.BlockManager: Found block rdd_26_1 locally
24/01/05 02:44:07 INFO executor.Executor: Finished task 0.0 in stage 13.0 (TID 14). 735 bytes result sent to driver
24/01/05 02:44:07 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 13.0 (TID 14) in 32 ms on localhost (executor driver) (1/2)
24/01/05 02:44:07 INFO executor.Executor: Finished task 1.0 in stage 13.0 (TID 15). 907 bytes result sent to driver
24/01/05 02:44:07 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 13.0 (TID 15) in 38 ms on localhost (executor driver) (2/2)
24/01/05 02:44:07 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 13.0, whose tasks have all completed, from pool 
24/01/05 02:44:07 INFO scheduler.DAGScheduler: ShuffleMapStage 13 (mapToPair at HoodieJavaRDD.java:149) finished in 0.073 s
24/01/05 02:44:07 INFO scheduler.DAGScheduler: looking for newly runnable stages
24/01/05 02:44:07 INFO scheduler.DAGScheduler: running: Set()
24/01/05 02:44:07 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 14)
24/01/05 02:44:07 INFO scheduler.DAGScheduler: failed: Set()
24/01/05 02:44:07 INFO scheduler.DAGScheduler: Submitting ResultStage 14 (MapPartitionsRDD[37] at mapToDouble at StreamSync.java:783), which has no missing parents
24/01/05 02:44:07 INFO memory.MemoryStore: Block broadcast_10 stored as values in memory (estimated size 352.1 KB, free 364.6 MB)
24/01/05 02:44:07 INFO memory.MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 127.3 KB, free 364.5 MB)
24/01/05 02:44:07 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on adhoc-2:46387 (size: 127.3 KB, free: 365.8 MB)
24/01/05 02:44:07 INFO spark.SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1161
24/01/05 02:44:07 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 14 (MapPartitionsRDD[37] at mapToDouble at StreamSync.java:783) (first 15 tasks are for partitions Vector(0))
24/01/05 02:44:07 INFO scheduler.TaskSchedulerImpl: Adding task set 14.0 with 1 tasks
24/01/05 02:44:07 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 14.0 (TID 16, localhost, executor driver, partition 0, ANY, 7662 bytes)
24/01/05 02:44:07 INFO executor.Executor: Running task 0.0 in stage 14.0 (TID 16)
24/01/05 02:44:07 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
24/01/05 02:44:07 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
24/01/05 02:44:07 INFO deltacommit.BaseSparkDeltaCommitActionExecutor: Merging updates for commit 20240105024402665 for file 6ad87b7b-db22-4d47-afc3-cb2b288006ba-0
24/01/05 02:44:07 INFO deltacommit.BaseSparkDeltaCommitActionExecutor: Small file corrections for updates for commit 20240105024402665 for file 6ad87b7b-db22-4d47-afc3-cb2b288006ba-0
24/01/05 02:44:07 INFO view.FileSystemViewManager: Creating View Manager with storage type :REMOTE_FIRST
24/01/05 02:44:07 INFO view.FileSystemViewManager: Creating remote first table view
24/01/05 02:44:07 INFO view.FileSystemViewManager: Creating remote view for basePath /user/hive/warehouse/stock_ticks_mor. Server=adhoc-2:42425, Timeout=300
24/01/05 02:44:07 INFO view.FileSystemViewManager: Creating InMemory based view for basePath /user/hive/warehouse/stock_ticks_mor
24/01/05 02:44:07 INFO view.AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
24/01/05 02:44:07 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:44:07 INFO view.RemoteHoodieTableFileSystemView: Sending request : (http://adhoc-2:42425/v1/hoodie/view/datafile/latest/partition?partition=2018%2F08%2F31&basepath=%2Fuser%2Fhive%2Fwarehouse%2Fstock_ticks_mor&fileid=6ad87b7b-db22-4d47-afc3-cb2b288006ba-0&lastinstantts=20240105024343265&timelinehash=7a0b5e1482c059272c847fc6668f12da532a637ce11f6f599d2728792005a6a1)
24/01/05 02:44:07 INFO collection.RocksDBDAO: Prefix Search for (query=type=df,part=2018/08/31,id=6ad87b7b-db22-4d47-afc3-cb2b288006ba-0,instant=) on hudi_view__user_hive_warehouse_stock_ticks_mor. Total Time Taken (msec)=2. Serialization Time taken(micro)=2347, num entries=1
24/01/05 02:44:07 INFO io.HoodieMergeHandleFactory: Create update handle for fileId 6ad87b7b-db22-4d47-afc3-cb2b288006ba-0 and partition path 2018/08/31 at commit 20240105024402665
24/01/05 02:44:07 INFO view.RemoteHoodieTableFileSystemView: Sending request : (http://adhoc-2:42425/v1/hoodie/view/datafile/latest/partition?partition=2018%2F08%2F31&basepath=%2Fuser%2Fhive%2Fwarehouse%2Fstock_ticks_mor&fileid=6ad87b7b-db22-4d47-afc3-cb2b288006ba-0&lastinstantts=20240105024343265&timelinehash=7a0b5e1482c059272c847fc6668f12da532a637ce11f6f599d2728792005a6a1)
24/01/05 02:44:07 INFO collection.RocksDBDAO: Prefix Search for (query=type=df,part=2018/08/31,id=6ad87b7b-db22-4d47-afc3-cb2b288006ba-0,instant=) on hudi_view__user_hive_warehouse_stock_ticks_mor. Total Time Taken (msec)=2. Serialization Time taken(micro)=2377, num entries=1
24/01/05 02:44:07 INFO io.HoodieMergeHandle: MaxMemoryPerPartitionMerge => 1073741824
# WARNING: Unable to attach Serviceability Agent. Unable to attach even with module exceptions: [org.apache.hudi.org.openjdk.jol.vm.sa.SASupportException: Sense failed., org.apache.hudi.org.openjdk.jol.vm.sa.SASupportException: Sense failed., org.apache.hudi.org.openjdk.jol.vm.sa.SASupportException: Sense failed.]
24/01/05 02:44:08 INFO io.HoodieMergeHandle: Number of entries in MemoryBasedMap => 4, Total size in bytes of MemoryBasedMap => 3840, Number of entries in BitCaskDiskMap => 0, Size of file spilled to disk => 0
24/01/05 02:44:08 INFO io.HoodieMergeHandle: partitionPath:2018/08/31, fileId to be merged:6ad87b7b-db22-4d47-afc3-cb2b288006ba-0
24/01/05 02:44:08 INFO io.HoodieMergeHandle: Merging new data into oldPath /user/hive/warehouse/stock_ticks_mor/2018/08/31/6ad87b7b-db22-4d47-afc3-cb2b288006ba-0_0-23-24_20240105024343265.parquet, as newPath /user/hive/warehouse/stock_ticks_mor/2018/08/31/6ad87b7b-db22-4d47-afc3-cb2b288006ba-0_0-14-16_20240105024402665.parquet
24/01/05 02:44:08 WARN marker.WriteMarkersFactory: Timeline-server-based markers are not supported for HDFS: base path /user/hive/warehouse/stock_ticks_mor.  Falling back to direct markers.
24/01/05 02:44:08 INFO marker.DirectWriteMarkers: Creating Marker Path=/user/hive/warehouse/stock_ticks_mor/.hoodie/.temp/20240105024402665/2018/08/31/6ad87b7b-db22-4d47-afc3-cb2b288006ba-0_0-14-16_20240105024402665.parquet.marker.MERGE
24/01/05 02:44:08 INFO marker.DirectWriteMarkers: [direct] Created marker file /user/hive/warehouse/stock_ticks_mor/.hoodie/.temp/20240105024402665/2018/08/31/6ad87b7b-db22-4d47-afc3-cb2b288006ba-0_0-14-16_20240105024402665.parquet.marker.MERGE in 12 ms
24/01/05 02:44:08 INFO compress.CodecPool: Got brand-new compressor [.gz]
24/01/05 02:44:08 INFO queue.SimpleExecutor: Starting consumer, consuming records from the records iterator directly
24/01/05 02:44:08 INFO hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 102 records.
24/01/05 02:44:08 INFO hadoop.InternalParquetRecordReader: at row 0. reading next block
24/01/05 02:44:08 INFO compress.CodecPool: Got brand-new decompressor [.gz]
24/01/05 02:44:08 INFO hadoop.InternalParquetRecordReader: block read in memory in 3 ms. row count = 102
24/01/05 02:44:08 INFO hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 19417
24/01/05 02:44:08 INFO spark.ContextCleaner: Cleaned accumulator 104
24/01/05 02:44:08 INFO spark.ContextCleaner: Cleaned accumulator 118
24/01/05 02:44:08 INFO spark.ContextCleaner: Cleaned accumulator 220
24/01/05 02:44:08 INFO spark.ContextCleaner: Cleaned accumulator 145
24/01/05 02:44:08 INFO spark.ContextCleaner: Cleaned accumulator 164
24/01/05 02:44:08 INFO spark.ContextCleaner: Cleaned accumulator 188
24/01/05 02:44:08 INFO spark.ContextCleaner: Cleaned accumulator 165
24/01/05 02:44:08 INFO spark.ContextCleaner: Cleaned accumulator 146
24/01/05 02:44:08 INFO spark.ContextCleaner: Cleaned accumulator 123
24/01/05 02:44:08 INFO spark.ContextCleaner: Cleaned accumulator 140
24/01/05 02:44:08 INFO spark.ContextCleaner: Cleaned accumulator 113
24/01/05 02:44:08 INFO spark.ContextCleaner: Cleaned accumulator 201
24/01/05 02:44:08 INFO spark.ContextCleaner: Cleaned accumulator 194
24/01/05 02:44:08 INFO spark.ContextCleaner: Cleaned accumulator 107
24/01/05 02:44:08 INFO spark.ContextCleaner: Cleaned accumulator 124
24/01/05 02:44:08 INFO spark.ContextCleaner: Cleaned accumulator 121
24/01/05 02:44:08 INFO spark.ContextCleaner: Cleaned accumulator 183
24/01/05 02:44:08 INFO spark.ContextCleaner: Cleaned accumulator 200
24/01/05 02:44:08 INFO spark.ContextCleaner: Cleaned accumulator 193
24/01/05 02:44:08 INFO spark.ContextCleaner: Cleaned accumulator 190
24/01/05 02:44:08 INFO spark.ContextCleaner: Cleaned accumulator 212
24/01/05 02:44:08 INFO spark.ContextCleaner: Cleaned accumulator 221
24/01/05 02:44:08 INFO spark.ContextCleaner: Cleaned accumulator 214
24/01/05 02:44:08 INFO storage.BlockManagerInfo: Removed broadcast_7_piece0 on adhoc-2:46387 in memory (size: 2.1 KB, free: 365.8 MB)
24/01/05 02:44:08 INFO spark.ContextCleaner: Cleaned accumulator 108
24/01/05 02:44:08 INFO spark.ContextCleaner: Cleaned accumulator 171
24/01/05 02:44:08 INFO spark.ContextCleaner: Cleaned accumulator 129
24/01/05 02:44:08 INFO spark.ContextCleaner: Cleaned accumulator 223
24/01/05 02:44:08 INFO spark.ContextCleaner: Cleaned accumulator 158
24/01/05 02:44:08 INFO spark.ContextCleaner: Cleaned accumulator 161
24/01/05 02:44:08 INFO spark.ContextCleaner: Cleaned accumulator 116
24/01/05 02:44:08 INFO spark.ContextCleaner: Cleaned accumulator 172
24/01/05 02:44:08 INFO spark.ContextCleaner: Cleaned accumulator 152
24/01/05 02:44:08 INFO spark.ContextCleaner: Cleaned accumulator 187
24/01/05 02:44:08 INFO spark.ContextCleaner: Cleaned accumulator 120
24/01/05 02:44:08 INFO spark.ContextCleaner: Cleaned accumulator 156
24/01/05 02:44:08 INFO spark.ContextCleaner: Cleaned accumulator 126
24/01/05 02:44:08 INFO spark.ContextCleaner: Cleaned accumulator 192
24/01/05 02:44:08 INFO spark.ContextCleaner: Cleaned accumulator 157
24/01/05 02:44:08 INFO spark.ContextCleaner: Cleaned accumulator 189
24/01/05 02:44:08 INFO spark.ContextCleaner: Cleaned accumulator 122
24/01/05 02:44:08 INFO spark.ContextCleaner: Cleaned accumulator 134
24/01/05 02:44:08 INFO spark.ContextCleaner: Cleaned accumulator 173
24/01/05 02:44:08 INFO spark.ContextCleaner: Cleaned accumulator 105
24/01/05 02:44:08 INFO spark.ContextCleaner: Cleaned accumulator 139
24/01/05 02:44:08 INFO spark.ContextCleaner: Cleaned accumulator 209
24/01/05 02:44:08 INFO spark.ContextCleaner: Cleaned accumulator 218
24/01/05 02:44:08 INFO spark.ContextCleaner: Cleaned accumulator 153
24/01/05 02:44:08 INFO spark.ContextCleaner: Cleaned accumulator 186
24/01/05 02:44:08 INFO spark.ContextCleaner: Cleaned accumulator 167
24/01/05 02:44:08 INFO spark.ContextCleaner: Cleaned accumulator 176
24/01/05 02:44:08 INFO spark.ContextCleaner: Cleaned accumulator 199
24/01/05 02:44:08 INFO spark.ContextCleaner: Cleaned accumulator 154
24/01/05 02:44:08 INFO spark.ContextCleaner: Cleaned accumulator 166
24/01/05 02:44:08 INFO spark.ContextCleaner: Cleaned accumulator 170
24/01/05 02:44:08 INFO io.HoodieMergeHandle: MergeHandle for partitionPath 2018/08/31 fileID 6ad87b7b-db22-4d47-afc3-cb2b288006ba-0, took 923 ms.
24/01/05 02:44:08 INFO storage.BlockManagerInfo: Removed broadcast_8_piece0 on adhoc-2:46387 in memory (size: 121.8 KB, free: 365.9 MB)
24/01/05 02:44:08 INFO spark.ContextCleaner: Cleaned accumulator 130
24/01/05 02:44:08 INFO spark.ContextCleaner: Cleaned accumulator 196
24/01/05 02:44:08 INFO memory.MemoryStore: Block rdd_36_0 stored as values in memory (estimated size 387.0 B, free 365.0 MB)
24/01/05 02:44:08 INFO storage.BlockManagerInfo: Removed broadcast_9_piece0 on adhoc-2:46387 in memory (size: 123.4 KB, free: 366.1 MB)
24/01/05 02:44:08 INFO storage.BlockManagerInfo: Added rdd_36_0 in memory on adhoc-2:46387 (size: 387.0 B, free: 366.1 MB)
24/01/05 02:44:08 INFO spark.ContextCleaner: Cleaned accumulator 147
24/01/05 02:44:08 INFO spark.ContextCleaner: Cleaned accumulator 106
24/01/05 02:44:08 INFO spark.ContextCleaner: Cleaned accumulator 109
24/01/05 02:44:08 INFO spark.ContextCleaner: Cleaned accumulator 175
24/01/05 02:44:08 INFO spark.ContextCleaner: Cleaned accumulator 207
24/01/05 02:44:08 INFO spark.ContextCleaner: Cleaned accumulator 159
24/01/05 02:44:08 INFO spark.ContextCleaner: Cleaned accumulator 111
24/01/05 02:44:08 INFO spark.ContextCleaner: Cleaned accumulator 222
24/01/05 02:44:08 INFO spark.ContextCleaner: Cleaned accumulator 163
24/01/05 02:44:08 INFO spark.ContextCleaner: Cleaned accumulator 125
24/01/05 02:44:08 INFO spark.ContextCleaner: Cleaned accumulator 160
24/01/05 02:44:08 INFO spark.ContextCleaner: Cleaned accumulator 191
24/01/05 02:44:08 INFO spark.ContextCleaner: Cleaned shuffle 4
24/01/05 02:44:08 INFO spark.ContextCleaner: Cleaned accumulator 150
24/01/05 02:44:08 INFO spark.ContextCleaner: Cleaned accumulator 100
24/01/05 02:44:08 INFO spark.ContextCleaner: Cleaned accumulator 114
24/01/05 02:44:08 INFO spark.ContextCleaner: Cleaned accumulator 144
24/01/05 02:44:08 INFO spark.ContextCleaner: Cleaned accumulator 143
24/01/05 02:44:08 INFO spark.ContextCleaner: Cleaned accumulator 208
24/01/05 02:44:08 INFO spark.ContextCleaner: Cleaned accumulator 142
24/01/05 02:44:08 INFO spark.ContextCleaner: Cleaned accumulator 205
24/01/05 02:44:08 INFO spark.ContextCleaner: Cleaned accumulator 178
24/01/05 02:44:08 INFO spark.ContextCleaner: Cleaned accumulator 213
24/01/05 02:44:08 INFO spark.ContextCleaner: Cleaned accumulator 217
24/01/05 02:44:08 INFO spark.ContextCleaner: Cleaned accumulator 137
24/01/05 02:44:08 INFO spark.ContextCleaner: Cleaned accumulator 224
24/01/05 02:44:08 INFO spark.ContextCleaner: Cleaned accumulator 182
24/01/05 02:44:08 INFO spark.ContextCleaner: Cleaned accumulator 197
24/01/05 02:44:08 INFO spark.ContextCleaner: Cleaned accumulator 117
24/01/05 02:44:08 INFO spark.ContextCleaner: Cleaned accumulator 155
24/01/05 02:44:08 INFO spark.ContextCleaner: Cleaned accumulator 162
24/01/05 02:44:08 INFO spark.ContextCleaner: Cleaned accumulator 185
24/01/05 02:44:08 INFO spark.ContextCleaner: Cleaned accumulator 202
24/01/05 02:44:08 INFO spark.ContextCleaner: Cleaned accumulator 198
24/01/05 02:44:08 INFO spark.ContextCleaner: Cleaned accumulator 179
24/01/05 02:44:08 INFO spark.ContextCleaner: Cleaned accumulator 148
24/01/05 02:44:08 INFO storage.BlockManagerInfo: Removed broadcast_6_piece0 on adhoc-2:46387 in memory (size: 4.0 KB, free: 366.1 MB)
24/01/05 02:44:08 INFO spark.ContextCleaner: Cleaned accumulator 128
24/01/05 02:44:08 INFO spark.ContextCleaner: Cleaned accumulator 115
24/01/05 02:44:08 INFO spark.ContextCleaner: Cleaned accumulator 132
24/01/05 02:44:08 INFO spark.ContextCleaner: Cleaned accumulator 206
24/01/05 02:44:08 INFO spark.ContextCleaner: Cleaned accumulator 215
24/01/05 02:44:08 INFO spark.ContextCleaner: Cleaned accumulator 204
24/01/05 02:44:08 INFO spark.ContextCleaner: Cleaned accumulator 133
24/01/05 02:44:08 INFO spark.ContextCleaner: Cleaned accumulator 138
24/01/05 02:44:08 INFO spark.ContextCleaner: Cleaned accumulator 174
24/01/05 02:44:08 INFO spark.ContextCleaner: Cleaned accumulator 136
24/01/05 02:44:08 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on adhoc-2:46387 in memory (size: 122.2 KB, free: 366.2 MB)
24/01/05 02:44:08 INFO spark.ContextCleaner: Cleaned accumulator 112
24/01/05 02:44:08 INFO executor.Executor: Finished task 0.0 in stage 14.0 (TID 16). 1017 bytes result sent to driver
24/01/05 02:44:08 INFO spark.ContextCleaner: Cleaned accumulator 184
24/01/05 02:44:08 INFO spark.ContextCleaner: Cleaned accumulator 210
24/01/05 02:44:08 INFO spark.ContextCleaner: Cleaned accumulator 149
24/01/05 02:44:08 INFO spark.ContextCleaner: Cleaned accumulator 181
24/01/05 02:44:08 INFO spark.ContextCleaner: Cleaned accumulator 102
24/01/05 02:44:08 INFO spark.ContextCleaner: Cleaned accumulator 151
24/01/05 02:44:08 INFO spark.ContextCleaner: Cleaned accumulator 103
24/01/05 02:44:08 INFO spark.ContextCleaner: Cleaned accumulator 127
24/01/05 02:44:08 INFO spark.ContextCleaner: Cleaned accumulator 119
24/01/05 02:44:08 INFO spark.ContextCleaner: Cleaned accumulator 211
24/01/05 02:44:08 INFO spark.ContextCleaner: Cleaned accumulator 110
24/01/05 02:44:08 INFO spark.ContextCleaner: Cleaned accumulator 219
24/01/05 02:44:08 INFO spark.ContextCleaner: Cleaned accumulator 135
24/01/05 02:44:08 INFO spark.ContextCleaner: Cleaned accumulator 101
24/01/05 02:44:08 INFO spark.ContextCleaner: Cleaned accumulator 169
24/01/05 02:44:08 INFO spark.ContextCleaner: Cleaned accumulator 168
24/01/05 02:44:08 INFO spark.ContextCleaner: Cleaned accumulator 195
24/01/05 02:44:08 INFO spark.ContextCleaner: Cleaned accumulator 180
24/01/05 02:44:08 INFO spark.ContextCleaner: Cleaned accumulator 216
24/01/05 02:44:08 INFO spark.ContextCleaner: Cleaned accumulator 177
24/01/05 02:44:08 INFO spark.ContextCleaner: Cleaned accumulator 203
24/01/05 02:44:08 INFO spark.ContextCleaner: Cleaned accumulator 131
24/01/05 02:44:08 INFO spark.ContextCleaner: Cleaned accumulator 141
24/01/05 02:44:08 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 14.0 (TID 16) in 991 ms on localhost (executor driver) (1/1)
24/01/05 02:44:08 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 14.0, whose tasks have all completed, from pool 
24/01/05 02:44:08 INFO scheduler.DAGScheduler: ResultStage 14 (sum at StreamSync.java:783) finished in 1.022 s
24/01/05 02:44:08 INFO scheduler.DAGScheduler: Job 4 finished: sum at StreamSync.java:783, took 1.103629 s
24/01/05 02:44:08 INFO spark.SparkContext: Starting job: sum at StreamSync.java:784
24/01/05 02:44:08 INFO scheduler.DAGScheduler: Got job 5 (sum at StreamSync.java:784) with 1 output partitions
24/01/05 02:44:08 INFO scheduler.DAGScheduler: Final stage: ResultStage 19 (sum at StreamSync.java:784)
24/01/05 02:44:08 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 18)
24/01/05 02:44:08 INFO scheduler.DAGScheduler: Missing parents: List()
24/01/05 02:44:08 INFO scheduler.DAGScheduler: Submitting ResultStage 19 (MapPartitionsRDD[39] at mapToDouble at StreamSync.java:784), which has no missing parents
24/01/05 02:44:08 INFO memory.MemoryStore: Block broadcast_11 stored as values in memory (estimated size 352.1 KB, free 365.5 MB)
24/01/05 02:44:08 INFO memory.MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 127.3 KB, free 365.4 MB)
24/01/05 02:44:08 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on adhoc-2:46387 (size: 127.3 KB, free: 366.1 MB)
24/01/05 02:44:08 INFO spark.SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1161
24/01/05 02:44:08 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 19 (MapPartitionsRDD[39] at mapToDouble at StreamSync.java:784) (first 15 tasks are for partitions Vector(0))
24/01/05 02:44:08 INFO scheduler.TaskSchedulerImpl: Adding task set 19.0 with 1 tasks
24/01/05 02:44:08 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 19.0 (TID 17, localhost, executor driver, partition 0, PROCESS_LOCAL, 7662 bytes)
24/01/05 02:44:08 INFO executor.Executor: Running task 0.0 in stage 19.0 (TID 17)
24/01/05 02:44:08 INFO storage.BlockManager: Found block rdd_36_0 locally
24/01/05 02:44:08 INFO executor.Executor: Finished task 0.0 in stage 19.0 (TID 17). 759 bytes result sent to driver
24/01/05 02:44:08 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 19.0 (TID 17) in 29 ms on localhost (executor driver) (1/1)
24/01/05 02:44:08 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 19.0, whose tasks have all completed, from pool 
24/01/05 02:44:08 INFO scheduler.DAGScheduler: ResultStage 19 (sum at StreamSync.java:784) finished in 0.060 s
24/01/05 02:44:08 INFO scheduler.DAGScheduler: Job 5 finished: sum at StreamSync.java:784, took 0.062655 s
24/01/05 02:44:08 INFO streamer.StreamSync: instantTime=20240105024402665, totalRecords=4, totalErrorRecords=0, totalSuccessfulRecords=4
24/01/05 02:44:08 INFO spark.SparkContext: Starting job: collect at SparkRDDWriteClient.java:103
24/01/05 02:44:08 INFO scheduler.DAGScheduler: Got job 6 (collect at SparkRDDWriteClient.java:103) with 1 output partitions
24/01/05 02:44:08 INFO scheduler.DAGScheduler: Final stage: ResultStage 24 (collect at SparkRDDWriteClient.java:103)
24/01/05 02:44:08 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 23)
24/01/05 02:44:08 INFO scheduler.DAGScheduler: Missing parents: List()
24/01/05 02:44:08 INFO scheduler.DAGScheduler: Submitting ResultStage 24 (MapPartitionsRDD[41] at map at SparkRDDWriteClient.java:103), which has no missing parents
24/01/05 02:44:08 INFO memory.MemoryStore: Block broadcast_12 stored as values in memory (estimated size 352.3 KB, free 365.0 MB)
24/01/05 02:44:08 INFO memory.MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 127.4 KB, free 364.9 MB)
24/01/05 02:44:08 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on adhoc-2:46387 (size: 127.4 KB, free: 365.9 MB)
24/01/05 02:44:08 INFO spark.SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1161
24/01/05 02:44:08 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 24 (MapPartitionsRDD[41] at map at SparkRDDWriteClient.java:103) (first 15 tasks are for partitions Vector(0))
24/01/05 02:44:08 INFO scheduler.TaskSchedulerImpl: Adding task set 24.0 with 1 tasks
24/01/05 02:44:08 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 24.0 (TID 18, localhost, executor driver, partition 0, PROCESS_LOCAL, 7662 bytes)
24/01/05 02:44:08 INFO executor.Executor: Running task 0.0 in stage 24.0 (TID 18)
24/01/05 02:44:08 INFO storage.BlockManager: Found block rdd_36_0 locally
24/01/05 02:44:08 INFO executor.Executor: Finished task 0.0 in stage 24.0 (TID 18). 1045 bytes result sent to driver
24/01/05 02:44:08 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 24.0 (TID 18) in 19 ms on localhost (executor driver) (1/1)
24/01/05 02:44:08 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 24.0, whose tasks have all completed, from pool 
24/01/05 02:44:08 INFO scheduler.DAGScheduler: ResultStage 24 (collect at SparkRDDWriteClient.java:103) finished in 0.047 s
24/01/05 02:44:08 INFO scheduler.DAGScheduler: Job 6 finished: collect at SparkRDDWriteClient.java:103, took 0.049781 s
24/01/05 02:44:08 INFO client.BaseHoodieWriteClient: Committing 20240105024402665 action deltacommit
24/01/05 02:44:08 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:44:08 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:44:08 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:44:08 INFO table.HoodieTableMetaClient: Loading Active commit timeline for /user/hive/warehouse/stock_ticks_mor
24/01/05 02:44:08 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[==>20240105024402665__deltacommit__INFLIGHT__20240105024407573]}
24/01/05 02:44:08 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:44:08 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:44:08 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:44:08 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:44:08 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:44:08 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:44:08 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105024343265__deltacommit__COMPLETED__20240105024352164]}
24/01/05 02:44:08 INFO view.AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
24/01/05 02:44:08 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:44:08 INFO view.FileSystemViewManager: Creating View Manager with storage type :REMOTE_FIRST
24/01/05 02:44:08 INFO view.FileSystemViewManager: Creating remote first table view
24/01/05 02:44:08 INFO util.CommitUtils: Creating  metadata for UPSERT numWriteStats:1 numReplaceFileIds:0
24/01/05 02:44:08 INFO transaction.TransactionManager: Transaction starting for Option{val=[==>20240105024402665__deltacommit__INFLIGHT]} with latest completed transaction instant Option{val=[20240105024343265__deltacommit__COMPLETED__20240105024352675]}
24/01/05 02:44:08 INFO lock.LockManager: LockProvider org.apache.hudi.client.transaction.lock.InProcessLockProvider
24/01/05 02:44:08 INFO lock.InProcessLockProvider: Base Path /user/hive/warehouse/stock_ticks_mor, Lock Instance java.util.concurrent.locks.ReentrantReadWriteLock@7254838[Write locks = 0, Read locks = 0], Thread main, In-process lock state ACQUIRING
24/01/05 02:44:08 INFO lock.InProcessLockProvider: Base Path /user/hive/warehouse/stock_ticks_mor, Lock Instance java.util.concurrent.locks.ReentrantReadWriteLock@7254838[Write locks = 1, Read locks = 0], Thread main, In-process lock state ACQUIRED
24/01/05 02:44:08 INFO transaction.TransactionManager: Transaction started for Option{val=[==>20240105024402665__deltacommit__INFLIGHT]} with latest completed transaction instant Option{val=[20240105024343265__deltacommit__COMPLETED__20240105024352675]}
24/01/05 02:44:08 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:44:08 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:44:09 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:44:09 INFO table.HoodieTableMetaClient: Loading Active commit timeline for /user/hive/warehouse/stock_ticks_mor
24/01/05 02:44:09 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[==>20240105024402665__deltacommit__INFLIGHT__20240105024407573]}
24/01/05 02:44:09 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:44:09 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:44:09 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:44:09 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:44:09 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:44:09 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:44:09 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105024343265__deltacommit__COMPLETED__20240105024352164]}
24/01/05 02:44:09 INFO view.AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
24/01/05 02:44:09 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:44:09 INFO view.FileSystemViewManager: Creating View Manager with storage type :REMOTE_FIRST
24/01/05 02:44:09 INFO view.FileSystemViewManager: Creating remote first table view
24/01/05 02:44:09 INFO client.BaseHoodieWriteClient: Committing 20240105024402665 action deltacommit
24/01/05 02:44:09 WARN marker.WriteMarkersFactory: Timeline-server-based markers are not supported for HDFS: base path /user/hive/warehouse/stock_ticks_mor.  Falling back to direct markers.
24/01/05 02:44:09 INFO spark.SparkContext: Starting job: collect at HoodieSparkEngineContext.java:150
24/01/05 02:44:09 INFO scheduler.DAGScheduler: Got job 7 (collect at HoodieSparkEngineContext.java:150) with 1 output partitions
24/01/05 02:44:09 INFO scheduler.DAGScheduler: Final stage: ResultStage 25 (collect at HoodieSparkEngineContext.java:150)
24/01/05 02:44:09 INFO scheduler.DAGScheduler: Parents of final stage: List()
24/01/05 02:44:09 INFO scheduler.DAGScheduler: Missing parents: List()
24/01/05 02:44:09 INFO scheduler.DAGScheduler: Submitting ResultStage 25 (MapPartitionsRDD[43] at flatMap at HoodieSparkEngineContext.java:150), which has no missing parents
24/01/05 02:44:09 INFO memory.MemoryStore: Block broadcast_13 stored as values in memory (estimated size 72.5 KB, free 364.8 MB)
24/01/05 02:44:09 INFO memory.MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 26.7 KB, free 364.8 MB)
24/01/05 02:44:09 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on adhoc-2:46387 (size: 26.7 KB, free: 365.9 MB)
24/01/05 02:44:09 INFO spark.SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1161
24/01/05 02:44:09 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 25 (MapPartitionsRDD[43] at flatMap at HoodieSparkEngineContext.java:150) (first 15 tasks are for partitions Vector(0))
24/01/05 02:44:09 INFO scheduler.TaskSchedulerImpl: Adding task set 25.0 with 1 tasks
24/01/05 02:44:09 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 25.0 (TID 19, localhost, executor driver, partition 0, PROCESS_LOCAL, 7820 bytes)
24/01/05 02:44:09 INFO executor.Executor: Running task 0.0 in stage 25.0 (TID 19)
24/01/05 02:44:09 INFO executor.Executor: Finished task 0.0 in stage 25.0 (TID 19). 755 bytes result sent to driver
24/01/05 02:44:09 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 25.0 (TID 19) in 16 ms on localhost (executor driver) (1/1)
24/01/05 02:44:09 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 25.0, whose tasks have all completed, from pool 
24/01/05 02:44:09 INFO scheduler.DAGScheduler: ResultStage 25 (collect at HoodieSparkEngineContext.java:150) finished in 0.029 s
24/01/05 02:44:09 INFO scheduler.DAGScheduler: Job 7 finished: collect at HoodieSparkEngineContext.java:150, took 0.030090 s
24/01/05 02:44:09 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:44:09 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:44:09 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:44:09 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:44:09 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:44:09 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:44:09 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105024343265__deltacommit__COMPLETED__20240105024352164]}
24/01/05 02:44:09 INFO metadata.HoodieBackedTableMetadataWriter: Async metadata indexing disabled and following partitions already initialized: [files]
24/01/05 02:44:09 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:44:09 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:44:09 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:44:09 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:44:09 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:44:09 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:44:09 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105024343265__deltacommit__COMPLETED__20240105024352164]}
24/01/05 02:44:09 INFO view.AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
24/01/05 02:44:09 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:44:09 INFO metadata.HoodieTableMetadataUtil: Updating at 20240105024402665 from Commit/UPSERT. #partitions_updated=2, #files_added=1
24/01/05 02:44:09 INFO view.AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
24/01/05 02:44:09 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:44:09 INFO metadata.HoodieTableMetadataUtil: Loading latest file slices for metadata table partition files
24/01/05 02:44:09 INFO view.AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
24/01/05 02:44:09 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:44:09 INFO view.AbstractTableFileSystemView: Building file system view for partition (files)
24/01/05 02:44:09 INFO client.BaseHoodieClient: Embedded Timeline Server is disabled. Not starting timeline service
24/01/05 02:44:09 INFO client.BaseHoodieClient: Embedded Timeline Server is disabled. Not starting timeline service
24/01/05 02:44:09 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:44:09 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:44:09 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:44:09 INFO table.HoodieTableMetaClient: Loading Active commit timeline for /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:44:09 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105024343265__deltacommit__COMPLETED__20240105024352164]}
24/01/05 02:44:09 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:44:09 INFO view.FileSystemViewManager: Creating View Manager with storage type :MEMORY
24/01/05 02:44:09 INFO view.FileSystemViewManager: Creating in-memory based Table View
24/01/05 02:44:09 INFO metadata.HoodieBackedTableMetadataWriter: New commit at 20240105024402665 being applied to MDT.
24/01/05 02:44:09 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:44:09 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:44:09 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:44:09 INFO table.HoodieTableMetaClient: Loading Active commit timeline for /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:44:09 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105024343265__deltacommit__COMPLETED__20240105024352164]}
24/01/05 02:44:09 INFO util.CleanerUtils: Cleaned failed attempts if any
24/01/05 02:44:09 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:44:09 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:44:09 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:44:09 INFO table.HoodieTableMetaClient: Loading Active commit timeline for /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:44:09 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105024343265__deltacommit__COMPLETED__20240105024352164]}
24/01/05 02:44:09 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:44:09 INFO view.FileSystemViewManager: Creating View Manager with storage type :MEMORY
24/01/05 02:44:09 INFO view.FileSystemViewManager: Creating in-memory based Table View
24/01/05 02:44:09 INFO client.BaseHoodieWriteClient: Generate a new instant time: 20240105024402665 action: deltacommit
24/01/05 02:44:09 INFO timeline.HoodieActiveTimeline: Creating a new instant [==>20240105024402665__deltacommit__REQUESTED]
24/01/05 02:44:09 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:44:09 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:44:09 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:44:09 INFO table.HoodieTableMetaClient: Loading Active commit timeline for /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:44:09 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[==>20240105024402665__deltacommit__REQUESTED__20240105024409202]}
24/01/05 02:44:09 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:44:09 INFO view.FileSystemViewManager: Creating View Manager with storage type :MEMORY
24/01/05 02:44:09 INFO view.FileSystemViewManager: Creating in-memory based Table View
24/01/05 02:44:09 INFO async.AsyncCleanerService: The HoodieWriteClient is not configured to auto & async clean. Async clean service will not start.
24/01/05 02:44:09 INFO async.AsyncArchiveService: The HoodieWriteClient is not configured to auto & async archive. Async archive service will not start.
24/01/05 02:44:09 INFO view.AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
24/01/05 02:44:09 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:44:09 INFO spark.SparkContext: Starting job: countByKey at HoodieJavaPairRDD.java:105
24/01/05 02:44:09 INFO scheduler.DAGScheduler: Registering RDD 52 (countByKey at HoodieJavaPairRDD.java:105)
24/01/05 02:44:09 INFO scheduler.DAGScheduler: Got job 8 (countByKey at HoodieJavaPairRDD.java:105) with 1 output partitions
24/01/05 02:44:09 INFO scheduler.DAGScheduler: Final stage: ResultStage 27 (countByKey at HoodieJavaPairRDD.java:105)
24/01/05 02:44:09 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 26)
24/01/05 02:44:09 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 26)
24/01/05 02:44:09 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 26 (MapPartitionsRDD[52] at countByKey at HoodieJavaPairRDD.java:105), which has no missing parents
24/01/05 02:44:09 INFO memory.MemoryStore: Block broadcast_14 stored as values in memory (estimated size 8.6 KB, free 364.8 MB)
24/01/05 02:44:09 INFO memory.MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 4.7 KB, free 364.8 MB)
24/01/05 02:44:09 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on adhoc-2:46387 (size: 4.7 KB, free: 365.9 MB)
24/01/05 02:44:09 INFO spark.SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1161
24/01/05 02:44:09 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 26 (MapPartitionsRDD[52] at countByKey at HoodieJavaPairRDD.java:105) (first 15 tasks are for partitions Vector(0))
24/01/05 02:44:09 INFO scheduler.TaskSchedulerImpl: Adding task set 26.0 with 1 tasks
24/01/05 02:44:09 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 26.0 (TID 20, localhost, executor driver, partition 0, PROCESS_LOCAL, 8086 bytes)
24/01/05 02:44:09 INFO executor.Executor: Running task 0.0 in stage 26.0 (TID 20)
24/01/05 02:44:09 INFO memory.MemoryStore: Block rdd_50_0 stored as values in memory (estimated size 398.0 B, free 364.8 MB)
24/01/05 02:44:09 INFO storage.BlockManagerInfo: Added rdd_50_0 in memory on adhoc-2:46387 (size: 398.0 B, free: 365.9 MB)
24/01/05 02:44:09 INFO executor.Executor: Finished task 0.0 in stage 26.0 (TID 20). 950 bytes result sent to driver
24/01/05 02:44:09 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 26.0 (TID 20) in 21 ms on localhost (executor driver) (1/1)
24/01/05 02:44:09 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 26.0, whose tasks have all completed, from pool 
24/01/05 02:44:09 INFO scheduler.DAGScheduler: ShuffleMapStage 26 (countByKey at HoodieJavaPairRDD.java:105) finished in 0.036 s
24/01/05 02:44:09 INFO scheduler.DAGScheduler: looking for newly runnable stages
24/01/05 02:44:09 INFO scheduler.DAGScheduler: running: Set()
24/01/05 02:44:09 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 27)
24/01/05 02:44:09 INFO scheduler.DAGScheduler: failed: Set()
24/01/05 02:44:09 INFO scheduler.DAGScheduler: Submitting ResultStage 27 (ShuffledRDD[53] at countByKey at HoodieJavaPairRDD.java:105), which has no missing parents
24/01/05 02:44:09 INFO memory.MemoryStore: Block broadcast_15 stored as values in memory (estimated size 3.6 KB, free 364.8 MB)
24/01/05 02:44:09 INFO memory.MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 2.1 KB, free 364.8 MB)
24/01/05 02:44:09 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on adhoc-2:46387 (size: 2.1 KB, free: 365.9 MB)
24/01/05 02:44:09 INFO spark.SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1161
24/01/05 02:44:09 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 27 (ShuffledRDD[53] at countByKey at HoodieJavaPairRDD.java:105) (first 15 tasks are for partitions Vector(0))
24/01/05 02:44:09 INFO scheduler.TaskSchedulerImpl: Adding task set 27.0 with 1 tasks
24/01/05 02:44:09 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 27.0 (TID 21, localhost, executor driver, partition 0, ANY, 7662 bytes)
24/01/05 02:44:09 INFO executor.Executor: Running task 0.0 in stage 27.0 (TID 21)
24/01/05 02:44:09 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
24/01/05 02:44:09 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
24/01/05 02:44:09 INFO executor.Executor: Finished task 0.0 in stage 27.0 (TID 21). 1222 bytes result sent to driver
24/01/05 02:44:09 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 27.0 (TID 21) in 13 ms on localhost (executor driver) (1/1)
24/01/05 02:44:09 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 27.0, whose tasks have all completed, from pool 
24/01/05 02:44:09 INFO scheduler.DAGScheduler: ResultStage 27 (countByKey at HoodieJavaPairRDD.java:105) finished in 0.020 s
24/01/05 02:44:09 INFO scheduler.DAGScheduler: Job 8 finished: countByKey at HoodieJavaPairRDD.java:105, took 0.061404 s
24/01/05 02:44:09 INFO commit.UpsertPartitioner: AvgRecordSize => 1024
24/01/05 02:44:09 INFO spark.SparkContext: Starting job: collectAsMap at UpsertPartitioner.java:282
24/01/05 02:44:09 INFO scheduler.DAGScheduler: Got job 9 (collectAsMap at UpsertPartitioner.java:282) with 1 output partitions
24/01/05 02:44:09 INFO scheduler.DAGScheduler: Final stage: ResultStage 28 (collectAsMap at UpsertPartitioner.java:282)
24/01/05 02:44:09 INFO scheduler.DAGScheduler: Parents of final stage: List()
24/01/05 02:44:09 INFO scheduler.DAGScheduler: Missing parents: List()
24/01/05 02:44:09 INFO scheduler.DAGScheduler: Submitting ResultStage 28 (MapPartitionsRDD[55] at mapToPair at UpsertPartitioner.java:281), which has no missing parents
24/01/05 02:44:09 INFO memory.MemoryStore: Block broadcast_16 stored as values in memory (estimated size 268.9 KB, free 364.5 MB)
24/01/05 02:44:09 INFO memory.MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 95.8 KB, free 364.4 MB)
24/01/05 02:44:09 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on adhoc-2:46387 (size: 95.8 KB, free: 365.8 MB)
24/01/05 02:44:09 INFO spark.SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1161
24/01/05 02:44:09 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 28 (MapPartitionsRDD[55] at mapToPair at UpsertPartitioner.java:281) (first 15 tasks are for partitions Vector(0))
24/01/05 02:44:09 INFO scheduler.TaskSchedulerImpl: Adding task set 28.0 with 1 tasks
24/01/05 02:44:09 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 28.0 (TID 22, localhost, executor driver, partition 0, PROCESS_LOCAL, 7730 bytes)
24/01/05 02:44:09 INFO executor.Executor: Running task 0.0 in stage 28.0 (TID 22)
24/01/05 02:44:09 INFO view.FileSystemViewManager: Creating View Manager with storage type :MEMORY
24/01/05 02:44:09 INFO view.FileSystemViewManager: Creating in-memory based Table View
24/01/05 02:44:09 INFO view.FileSystemViewManager: Creating InMemory based view for basePath /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:44:09 INFO view.AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
24/01/05 02:44:09 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:44:09 INFO view.AbstractTableFileSystemView: Building file system view for partition (files)
24/01/05 02:44:09 INFO executor.Executor: Finished task 0.0 in stage 28.0 (TID 22). 700 bytes result sent to driver
24/01/05 02:44:09 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 28.0 (TID 22) in 26 ms on localhost (executor driver) (1/1)
24/01/05 02:44:09 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 28.0, whose tasks have all completed, from pool 
24/01/05 02:44:09 INFO scheduler.DAGScheduler: ResultStage 28 (collectAsMap at UpsertPartitioner.java:282) finished in 0.048 s
24/01/05 02:44:09 INFO scheduler.DAGScheduler: Job 9 finished: collectAsMap at UpsertPartitioner.java:282, took 0.050003 s
24/01/05 02:44:09 INFO view.AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
24/01/05 02:44:09 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:44:09 INFO commit.UpsertPartitioner: Total Buckets: 1
24/01/05 02:44:09 INFO timeline.HoodieActiveTimeline: Checking for file exists ?/user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/20240105024402665.deltacommit.requested
24/01/05 02:44:09 INFO timeline.HoodieActiveTimeline: Create new file for toInstant ?/user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/20240105024402665.deltacommit.inflight
24/01/05 02:44:09 INFO commit.BaseSparkCommitActionExecutor: no validators configured.
24/01/05 02:44:09 INFO commit.BaseCommitActionExecutor: Auto commit enabled: Committing 20240105024402665
24/01/05 02:44:09 INFO spark.SparkContext: Starting job: collect at HoodieJavaRDD.java:177
24/01/05 02:44:09 INFO scheduler.DAGScheduler: Registering RDD 56 (mapToPair at HoodieJavaRDD.java:149)
24/01/05 02:44:09 INFO scheduler.DAGScheduler: Got job 10 (collect at HoodieJavaRDD.java:177) with 1 output partitions
24/01/05 02:44:09 INFO scheduler.DAGScheduler: Final stage: ResultStage 30 (collect at HoodieJavaRDD.java:177)
24/01/05 02:44:09 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 29)
24/01/05 02:44:09 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 29)
24/01/05 02:44:09 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 29 (MapPartitionsRDD[56] at mapToPair at HoodieJavaRDD.java:149), which has no missing parents
24/01/05 02:44:09 INFO memory.MemoryStore: Block broadcast_17 stored as values in memory (estimated size 273.7 KB, free 364.2 MB)
24/01/05 02:44:09 INFO memory.MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 95.5 KB, free 364.1 MB)
24/01/05 02:44:09 INFO storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on adhoc-2:46387 (size: 95.5 KB, free: 365.7 MB)
24/01/05 02:44:09 INFO spark.SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1161
24/01/05 02:44:09 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 29 (MapPartitionsRDD[56] at mapToPair at HoodieJavaRDD.java:149) (first 15 tasks are for partitions Vector(0))
24/01/05 02:44:09 INFO scheduler.TaskSchedulerImpl: Adding task set 29.0 with 1 tasks
24/01/05 02:44:09 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 29.0 (TID 23, localhost, executor driver, partition 0, PROCESS_LOCAL, 8086 bytes)
24/01/05 02:44:09 INFO executor.Executor: Running task 0.0 in stage 29.0 (TID 23)
24/01/05 02:44:09 INFO storage.BlockManager: Found block rdd_50_0 locally
24/01/05 02:44:09 INFO executor.Executor: Finished task 0.0 in stage 29.0 (TID 23). 907 bytes result sent to driver
24/01/05 02:44:09 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 29.0 (TID 23) in 23 ms on localhost (executor driver) (1/1)
24/01/05 02:44:09 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 29.0, whose tasks have all completed, from pool 
24/01/05 02:44:09 INFO scheduler.DAGScheduler: ShuffleMapStage 29 (mapToPair at HoodieJavaRDD.java:149) finished in 0.048 s
24/01/05 02:44:09 INFO scheduler.DAGScheduler: looking for newly runnable stages
24/01/05 02:44:09 INFO scheduler.DAGScheduler: running: Set()
24/01/05 02:44:09 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 30)
24/01/05 02:44:09 INFO scheduler.DAGScheduler: failed: Set()
24/01/05 02:44:09 INFO scheduler.DAGScheduler: Submitting ResultStage 30 (MapPartitionsRDD[61] at map at HoodieJavaRDD.java:125), which has no missing parents
24/01/05 02:44:09 INFO memory.MemoryStore: Block broadcast_18 stored as values in memory (estimated size 348.5 KB, free 363.7 MB)
24/01/05 02:44:09 INFO memory.MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 126.9 KB, free 363.6 MB)
24/01/05 02:44:09 INFO storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on adhoc-2:46387 (size: 126.9 KB, free: 365.6 MB)
24/01/05 02:44:09 INFO spark.SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1161
24/01/05 02:44:09 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 30 (MapPartitionsRDD[61] at map at HoodieJavaRDD.java:125) (first 15 tasks are for partitions Vector(0))
24/01/05 02:44:09 INFO scheduler.TaskSchedulerImpl: Adding task set 30.0 with 1 tasks
24/01/05 02:44:09 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 30.0 (TID 24, localhost, executor driver, partition 0, ANY, 7662 bytes)
24/01/05 02:44:09 INFO executor.Executor: Running task 0.0 in stage 30.0 (TID 24)
24/01/05 02:44:09 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
24/01/05 02:44:09 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
24/01/05 02:44:09 INFO deltacommit.BaseSparkDeltaCommitActionExecutor: Merging updates for commit 20240105024402665 for file files-0000-0
24/01/05 02:44:09 INFO view.FileSystemViewManager: Creating View Manager with storage type :MEMORY
24/01/05 02:44:09 INFO view.FileSystemViewManager: Creating in-memory based Table View
24/01/05 02:44:09 INFO view.FileSystemViewManager: Creating InMemory based view for basePath /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:44:09 INFO view.AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
24/01/05 02:44:09 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:44:09 INFO view.AbstractTableFileSystemView: Building file system view for partition (files)
24/01/05 02:44:09 INFO marker.DirectWriteMarkers: Creating Marker Path=/user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/.temp/20240105024402665/files/files-0000-0_0-30-24_00000000000000010.hfile.marker.APPEND
24/01/05 02:44:09 INFO marker.DirectWriteMarkers: [direct] Created marker file /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/.temp/20240105024402665/files/files-0000-0_0-30-24_00000000000000010.hfile.marker.APPEND in 5 ms
24/01/05 02:44:09 INFO log.HoodieLogFormat$WriterBuilder: Building HoodieLogFormat Writer
24/01/05 02:44:09 INFO log.HoodieLogFormat$WriterBuilder: HoodieLogFile on path /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/files/.files-0000-0_00000000000000010.log.1_0-0-0
24/01/05 02:44:09 INFO storage.BlockManagerInfo: Removed broadcast_17_piece0 on adhoc-2:46387 in memory (size: 95.5 KB, free: 365.7 MB)
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 419
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 255
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 229
24/01/05 02:44:09 INFO storage.BlockManagerInfo: Removed broadcast_12_piece0 on adhoc-2:46387 in memory (size: 127.4 KB, free: 365.8 MB)
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 329
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 407
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 366
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 391
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 365
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 289
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 232
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 317
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 257
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 283
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 422
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 254
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 245
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 230
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 226
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 246
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 371
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 302
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 324
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned shuffle 6
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 388
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 258
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 360
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 374
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 293
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 348
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 259
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 252
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 400
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 389
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 241
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 403
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 416
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 240
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 326
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 273
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 285
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 344
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 305
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 358
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 227
24/01/05 02:44:09 INFO storage.BlockManagerInfo: Removed broadcast_14_piece0 on adhoc-2:46387 in memory (size: 4.7 KB, free: 365.8 MB)
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 243
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 394
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 277
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 281
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 248
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 267
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 345
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 233
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 330
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 249
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 296
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 409
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 415
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 385
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 307
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 271
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 304
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 397
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 319
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 339
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 354
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 399
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 310
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 318
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 382
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 402
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 284
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 352
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 235
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 338
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 381
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 404
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 380
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 342
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 363
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 392
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 424
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 260
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 383
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 313
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 393
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 372
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 412
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 262
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 253
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 362
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 256
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 244
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 335
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 325
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 386
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 300
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 322
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 292
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 413
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 286
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 323
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 378
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 349
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 268
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 408
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 234
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 379
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 346
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 251
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 299
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 370
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 301
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 401
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 228
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 290
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 367
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 375
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 387
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 368
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 266
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 314
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 331
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 294
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 414
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 239
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 308
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 278
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 356
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 275
24/01/05 02:44:09 INFO storage.BlockManagerInfo: Removed broadcast_10_piece0 on adhoc-2:46387 in memory (size: 127.3 KB, free: 365.9 MB)
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 288
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 405
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 343
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 274
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 316
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 320
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 334
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 369
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 280
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 312
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 351
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 353
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 297
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 373
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 242
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 406
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 261
24/01/05 02:44:09 INFO storage.BlockManagerInfo: Removed broadcast_16_piece0 on adhoc-2:46387 in memory (size: 95.8 KB, free: 366.0 MB)
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 311
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 328
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 420
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 264
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 306
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 236
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 269
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 303
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 357
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 396
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 364
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 376
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 276
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 332
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 410
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 327
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 359
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 395
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 350
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 231
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 398
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 347
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 337
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 282
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 377
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 265
24/01/05 02:44:09 INFO storage.BlockManagerInfo: Removed broadcast_15_piece0 on adhoc-2:46387 in memory (size: 2.1 KB, free: 366.0 MB)
24/01/05 02:44:09 INFO storage.BlockManagerInfo: Removed broadcast_13_piece0 on adhoc-2:46387 in memory (size: 26.7 KB, free: 366.1 MB)
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 247
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 315
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 361
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 238
24/01/05 02:44:09 INFO storage.BlockManagerInfo: Removed broadcast_11_piece0 on adhoc-2:46387 in memory (size: 127.3 KB, free: 366.2 MB)
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 390
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 423
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 341
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 421
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 340
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 295
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 384
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 411
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 309
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 263
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 418
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 270
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 225
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 417
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 237
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 287
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 298
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 336
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 291
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 321
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 355
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 272
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 279
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 250
24/01/05 02:44:09 INFO spark.ContextCleaner: Cleaned accumulator 333
24/01/05 02:44:09 INFO log.HoodieLogFormatWriter: HoodieLogFile{pathStr='/user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/files/.files-0000-0_00000000000000010.log.1_0-0-0', fileLen=13458} exists. Appending to existing file
24/01/05 02:44:09 INFO impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
24/01/05 02:44:09 INFO impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
24/01/05 02:44:09 INFO impl.MetricsSystemImpl: HBase metrics system started
24/01/05 02:44:09 INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.hudi.org.apache.hadoop.hbase.metrics.impl.MetricRegistriesImpl
24/01/05 02:44:09 INFO compress.CodecPool: Got brand-new compressor [.gz]
24/01/05 02:44:09 INFO compress.CodecPool: Got brand-new compressor [.gz]
24/01/05 02:44:10 INFO io.HoodieAppendHandle: AppendHandle for partitionPath files filePath files/.files-0000-0_00000000000000010.log.1_0-0-0, took 597 ms.
24/01/05 02:44:10 INFO memory.MemoryStore: Block rdd_60_0 stored as values in memory (estimated size 438.0 B, free 365.8 MB)
24/01/05 02:44:10 INFO storage.BlockManagerInfo: Added rdd_60_0 in memory on adhoc-2:46387 (size: 438.0 B, free: 366.2 MB)
24/01/05 02:44:10 INFO executor.Executor: Finished task 0.0 in stage 30.0 (TID 24). 1529 bytes result sent to driver
24/01/05 02:44:10 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 30.0 (TID 24) in 671 ms on localhost (executor driver) (1/1)
24/01/05 02:44:10 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 30.0, whose tasks have all completed, from pool 
24/01/05 02:44:10 INFO scheduler.DAGScheduler: ResultStage 30 (collect at HoodieJavaRDD.java:177) finished in 0.701 s
24/01/05 02:44:10 INFO scheduler.DAGScheduler: Job 10 finished: collect at HoodieJavaRDD.java:177, took 0.752304 s
24/01/05 02:44:10 INFO util.CommitUtils: Creating  metadata for UPSERT_PREPPED numWriteStats:1 numReplaceFileIds:0
24/01/05 02:44:10 INFO commit.BaseSparkCommitActionExecutor: Committing 20240105024402665, action Type deltacommit, operation Type UPSERT_PREPPED
24/01/05 02:44:10 INFO spark.SparkContext: Starting job: collect at HoodieSparkEngineContext.java:150
24/01/05 02:44:10 INFO scheduler.DAGScheduler: Got job 11 (collect at HoodieSparkEngineContext.java:150) with 1 output partitions
24/01/05 02:44:10 INFO scheduler.DAGScheduler: Final stage: ResultStage 31 (collect at HoodieSparkEngineContext.java:150)
24/01/05 02:44:10 INFO scheduler.DAGScheduler: Parents of final stage: List()
24/01/05 02:44:10 INFO scheduler.DAGScheduler: Missing parents: List()
24/01/05 02:44:10 INFO scheduler.DAGScheduler: Submitting ResultStage 31 (MapPartitionsRDD[63] at flatMap at HoodieSparkEngineContext.java:150), which has no missing parents
24/01/05 02:44:10 INFO memory.MemoryStore: Block broadcast_19 stored as values in memory (estimated size 72.5 KB, free 365.8 MB)
24/01/05 02:44:10 INFO memory.MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 26.7 KB, free 365.7 MB)
24/01/05 02:44:10 INFO storage.BlockManagerInfo: Added broadcast_19_piece0 in memory on adhoc-2:46387 (size: 26.7 KB, free: 366.1 MB)
24/01/05 02:44:10 INFO spark.SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1161
24/01/05 02:44:10 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 31 (MapPartitionsRDD[63] at flatMap at HoodieSparkEngineContext.java:150) (first 15 tasks are for partitions Vector(0))
24/01/05 02:44:10 INFO scheduler.TaskSchedulerImpl: Adding task set 31.0 with 1 tasks
24/01/05 02:44:10 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 31.0 (TID 25, localhost, executor driver, partition 0, PROCESS_LOCAL, 7838 bytes)
24/01/05 02:44:10 INFO executor.Executor: Running task 0.0 in stage 31.0 (TID 25)
24/01/05 02:44:10 INFO executor.Executor: Finished task 0.0 in stage 31.0 (TID 25). 668 bytes result sent to driver
24/01/05 02:44:10 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 31.0 (TID 25) in 12 ms on localhost (executor driver) (1/1)
24/01/05 02:44:10 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 31.0, whose tasks have all completed, from pool 
24/01/05 02:44:10 INFO scheduler.DAGScheduler: ResultStage 31 (collect at HoodieSparkEngineContext.java:150) finished in 0.028 s
24/01/05 02:44:10 INFO scheduler.DAGScheduler: Job 11 finished: collect at HoodieSparkEngineContext.java:150, took 0.030034 s
24/01/05 02:44:10 INFO timeline.HoodieActiveTimeline: Marking instant complete [==>20240105024402665__deltacommit__INFLIGHT]
24/01/05 02:44:10 INFO timeline.HoodieActiveTimeline: Checking for file exists ?/user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/20240105024402665.deltacommit.inflight
24/01/05 02:44:10 INFO timeline.HoodieActiveTimeline: Create new file for toInstant ?/user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/20240105024402665.deltacommit
24/01/05 02:44:10 INFO timeline.HoodieActiveTimeline: Completed [==>20240105024402665__deltacommit__INFLIGHT]
24/01/05 02:44:10 INFO commit.BaseSparkCommitActionExecutor: Committed 20240105024402665
24/01/05 02:44:10 INFO spark.SparkContext: Starting job: collectAsMap at HoodieSparkEngineContext.java:164
24/01/05 02:44:10 INFO scheduler.DAGScheduler: Got job 12 (collectAsMap at HoodieSparkEngineContext.java:164) with 1 output partitions
24/01/05 02:44:10 INFO scheduler.DAGScheduler: Final stage: ResultStage 32 (collectAsMap at HoodieSparkEngineContext.java:164)
24/01/05 02:44:10 INFO scheduler.DAGScheduler: Parents of final stage: List()
24/01/05 02:44:10 INFO scheduler.DAGScheduler: Missing parents: List()
24/01/05 02:44:10 INFO scheduler.DAGScheduler: Submitting ResultStage 32 (MapPartitionsRDD[65] at mapToPair at HoodieSparkEngineContext.java:161), which has no missing parents
24/01/05 02:44:10 INFO memory.MemoryStore: Block broadcast_20 stored as values in memory (estimated size 72.8 KB, free 365.7 MB)
24/01/05 02:44:10 INFO memory.MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 26.8 KB, free 365.6 MB)
24/01/05 02:44:10 INFO storage.BlockManagerInfo: Added broadcast_20_piece0 in memory on adhoc-2:46387 (size: 26.8 KB, free: 366.1 MB)
24/01/05 02:44:10 INFO spark.SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:1161
24/01/05 02:44:10 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 32 (MapPartitionsRDD[65] at mapToPair at HoodieSparkEngineContext.java:161) (first 15 tasks are for partitions Vector(0))
24/01/05 02:44:10 INFO scheduler.TaskSchedulerImpl: Adding task set 32.0 with 1 tasks
24/01/05 02:44:10 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 32.0 (TID 26, localhost, executor driver, partition 0, PROCESS_LOCAL, 7838 bytes)
24/01/05 02:44:10 INFO executor.Executor: Running task 0.0 in stage 32.0 (TID 26)
24/01/05 02:44:10 INFO executor.Executor: Finished task 0.0 in stage 32.0 (TID 26). 787 bytes result sent to driver
24/01/05 02:44:10 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 32.0 (TID 26) in 15 ms on localhost (executor driver) (1/1)
24/01/05 02:44:10 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 32.0, whose tasks have all completed, from pool 
24/01/05 02:44:10 INFO scheduler.DAGScheduler: ResultStage 32 (collectAsMap at HoodieSparkEngineContext.java:164) finished in 0.026 s
24/01/05 02:44:10 INFO scheduler.DAGScheduler: Job 12 finished: collectAsMap at HoodieSparkEngineContext.java:164, took 0.027939 s
24/01/05 02:44:10 INFO fs.FSUtils: Removed directory at /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/.temp/20240105024402665
24/01/05 02:44:10 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:44:10 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:44:10 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:44:10 INFO table.HoodieTableMetaClient: Loading Active commit timeline for /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:44:10 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105024402665__deltacommit__COMPLETED__20240105024410330]}
24/01/05 02:44:10 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:44:10 INFO view.FileSystemViewManager: Creating View Manager with storage type :MEMORY
24/01/05 02:44:10 INFO view.FileSystemViewManager: Creating in-memory based Table View
24/01/05 02:44:10 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105024402665__deltacommit__COMPLETED__20240105024410330]}
24/01/05 02:44:10 INFO timeline.HoodieActiveTimeline: Marking instant complete [==>20240105024402665__deltacommit__INFLIGHT]
24/01/05 02:44:10 INFO timeline.HoodieActiveTimeline: Checking for file exists ?/user/hive/warehouse/stock_ticks_mor/.hoodie/20240105024402665.deltacommit.inflight
24/01/05 02:44:10 INFO timeline.HoodieActiveTimeline: Create new file for toInstant ?/user/hive/warehouse/stock_ticks_mor/.hoodie/20240105024402665.deltacommit
24/01/05 02:44:10 INFO timeline.HoodieActiveTimeline: Completed [==>20240105024402665__deltacommit__INFLIGHT]
24/01/05 02:44:10 WARN marker.WriteMarkersFactory: Timeline-server-based markers are not supported for HDFS: base path /user/hive/warehouse/stock_ticks_mor.  Falling back to direct markers.
24/01/05 02:44:10 INFO spark.SparkContext: Starting job: collectAsMap at HoodieSparkEngineContext.java:164
24/01/05 02:44:10 INFO scheduler.DAGScheduler: Got job 13 (collectAsMap at HoodieSparkEngineContext.java:164) with 1 output partitions
24/01/05 02:44:10 INFO scheduler.DAGScheduler: Final stage: ResultStage 33 (collectAsMap at HoodieSparkEngineContext.java:164)
24/01/05 02:44:10 INFO scheduler.DAGScheduler: Parents of final stage: List()
24/01/05 02:44:10 INFO scheduler.DAGScheduler: Missing parents: List()
24/01/05 02:44:10 INFO scheduler.DAGScheduler: Submitting ResultStage 33 (MapPartitionsRDD[67] at mapToPair at HoodieSparkEngineContext.java:161), which has no missing parents
24/01/05 02:44:10 INFO memory.MemoryStore: Block broadcast_21 stored as values in memory (estimated size 72.8 KB, free 365.6 MB)
24/01/05 02:44:10 INFO memory.MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 26.8 KB, free 365.5 MB)
24/01/05 02:44:10 INFO storage.BlockManagerInfo: Added broadcast_21_piece0 in memory on adhoc-2:46387 (size: 26.8 KB, free: 366.1 MB)
24/01/05 02:44:10 INFO spark.SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:1161
24/01/05 02:44:10 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 33 (MapPartitionsRDD[67] at mapToPair at HoodieSparkEngineContext.java:161) (first 15 tasks are for partitions Vector(0))
24/01/05 02:44:10 INFO scheduler.TaskSchedulerImpl: Adding task set 33.0 with 1 tasks
24/01/05 02:44:10 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 33.0 (TID 27, localhost, executor driver, partition 0, PROCESS_LOCAL, 7820 bytes)
24/01/05 02:44:10 INFO executor.Executor: Running task 0.0 in stage 33.0 (TID 27)
24/01/05 02:44:10 INFO executor.Executor: Finished task 0.0 in stage 33.0 (TID 27). 769 bytes result sent to driver
24/01/05 02:44:10 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 33.0 (TID 27) in 10 ms on localhost (executor driver) (1/1)
24/01/05 02:44:10 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 33.0, whose tasks have all completed, from pool 
24/01/05 02:44:10 INFO scheduler.DAGScheduler: ResultStage 33 (collectAsMap at HoodieSparkEngineContext.java:164) finished in 0.022 s
24/01/05 02:44:10 INFO scheduler.DAGScheduler: Job 13 finished: collectAsMap at HoodieSparkEngineContext.java:164, took 0.023557 s
24/01/05 02:44:10 INFO fs.FSUtils: Removed directory at /user/hive/warehouse/stock_ticks_mor/.hoodie/.temp/20240105024402665
24/01/05 02:44:10 INFO client.BaseHoodieWriteClient: Committed 20240105024402665
24/01/05 02:44:10 INFO rdd.MapPartitionsRDD: Removing RDD 26 from persistence list
24/01/05 02:44:10 INFO storage.BlockManager: Removing RDD 26
24/01/05 02:44:10 INFO rdd.MapPartitionsRDD: Removing RDD 36 from persistence list
24/01/05 02:44:10 INFO storage.BlockManager: Removing RDD 36
24/01/05 02:44:10 INFO rdd.UnionRDD: Removing RDD 50 from persistence list
24/01/05 02:44:10 INFO storage.BlockManager: Removing RDD 50
24/01/05 02:44:10 INFO rdd.MapPartitionsRDD: Removing RDD 60 from persistence list
24/01/05 02:44:10 INFO storage.BlockManager: Removing RDD 60
24/01/05 02:44:10 INFO transaction.TransactionManager: Transaction ending with transaction owner Option{val=[==>20240105024402665__deltacommit__INFLIGHT]}
24/01/05 02:44:10 INFO lock.InProcessLockProvider: Base Path /user/hive/warehouse/stock_ticks_mor, Lock Instance java.util.concurrent.locks.ReentrantReadWriteLock@7254838[Write locks = 1, Read locks = 0], Thread main, In-process lock state RELEASING
24/01/05 02:44:10 INFO lock.InProcessLockProvider: Base Path /user/hive/warehouse/stock_ticks_mor, Lock Instance java.util.concurrent.locks.ReentrantReadWriteLock@7254838[Write locks = 0, Read locks = 0], Thread main, In-process lock state RELEASED
24/01/05 02:44:10 INFO lock.InProcessLockProvider: Base Path /user/hive/warehouse/stock_ticks_mor, Lock Instance java.util.concurrent.locks.ReentrantReadWriteLock@7254838[Write locks = 0, Read locks = 0], Thread main, In-process lock state ALREADY_RELEASED
24/01/05 02:44:10 INFO lock.LockManager: Released connection created for acquiring lock
24/01/05 02:44:10 INFO transaction.TransactionManager: Transaction ended with transaction owner Option{val=[==>20240105024402665__deltacommit__INFLIGHT]}
24/01/05 02:44:10 INFO client.BaseHoodieWriteClient: Start to clean synchronously.
24/01/05 02:44:10 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:44:10 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:44:10 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:44:10 INFO table.HoodieTableMetaClient: Loading Active commit timeline for /user/hive/warehouse/stock_ticks_mor
24/01/05 02:44:10 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105024402665__deltacommit__COMPLETED__20240105024410413]}
24/01/05 02:44:10 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:44:10 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:44:10 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:44:10 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:44:10 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:44:10 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:44:10 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105024402665__deltacommit__COMPLETED__20240105024410330]}
24/01/05 02:44:10 INFO view.AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
24/01/05 02:44:10 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:44:10 INFO view.FileSystemViewManager: Creating View Manager with storage type :REMOTE_FIRST
24/01/05 02:44:10 INFO view.FileSystemViewManager: Creating remote first table view
24/01/05 02:44:10 INFO client.BaseHoodieWriteClient: Cleaner started
24/01/05 02:44:10 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:44:10 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:44:10 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:44:10 INFO table.HoodieTableMetaClient: Loading Active commit timeline for /user/hive/warehouse/stock_ticks_mor
24/01/05 02:44:10 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105024402665__deltacommit__COMPLETED__20240105024410413]}
24/01/05 02:44:10 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:44:10 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:44:10 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:44:10 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:44:10 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:44:10 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:44:10 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105024402665__deltacommit__COMPLETED__20240105024410330]}
24/01/05 02:44:10 INFO view.AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
24/01/05 02:44:10 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:44:10 INFO view.FileSystemViewManager: Creating View Manager with storage type :REMOTE_FIRST
24/01/05 02:44:10 INFO view.FileSystemViewManager: Creating remote first table view
24/01/05 02:44:10 INFO client.BaseHoodieWriteClient: Scheduling cleaning at instant time :20240105024410477
24/01/05 02:44:10 INFO view.FileSystemViewManager: Creating remote view for basePath /user/hive/warehouse/stock_ticks_mor. Server=adhoc-2:42425, Timeout=300
24/01/05 02:44:10 INFO view.FileSystemViewManager: Creating InMemory based view for basePath /user/hive/warehouse/stock_ticks_mor
24/01/05 02:44:10 INFO view.AbstractTableFileSystemView: Took 1 ms to read  0 instants, 0 replaced file groups
24/01/05 02:44:10 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:44:10 INFO view.RemoteHoodieTableFileSystemView: Sending request : (http://adhoc-2:42425/v1/hoodie/view/compactions/pending/?basepath=%2Fuser%2Fhive%2Fwarehouse%2Fstock_ticks_mor&lastinstantts=20240105024402665&timelinehash=a2d25c4c0dfac66e565adad6b2d12f51946731409571f20219d2d4ee6dc1a8e0)
24/01/05 02:44:10 INFO service.RequestHandler: Syncing view as client passed last known instant 20240105024402665 as last known instant but server has the following last instant on timeline :Option{val=[20240105024343265__deltacommit__COMPLETED__20240105024352675]}
24/01/05 02:44:10 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105024402665__deltacommit__COMPLETED__20240105024410413]}
24/01/05 02:44:10 INFO view.RocksDbBasedFileSystemView: Deleting all rocksdb data associated with table filesystem view
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : [/db_impl/db_impl.cc:478] Shutdown: canceling all background work
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : [/db_impl/db_impl.cc:677] Shutdown complete
24/01/05 02:44:10 INFO collection.RocksDBDAO: DELETING RocksDB persisted at /tmp/hoodie_timeline_rocksdb/_user_hive_warehouse_stock_ticks_mor/9147b15e-4165-4822-ad0e-ed2c5823dfb6
24/01/05 02:44:10 INFO collection.RocksDBDAO: No column family found. Loading default
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : RocksDB version: 7.5.3

24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Compile date 2022-09-02 09:50:20
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : DB SUMMARY

24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : DB Session ID:  BDLA281754O6OA5HJ7UB

24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : SST files in /tmp/hoodie_timeline_rocksdb/_user_hive_warehouse_stock_ticks_mor/9147b15e-4165-4822-ad0e-ed2c5823dfb6 dir, Total Num: 0, files: 

24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Write Ahead Log file in /tmp/hoodie_timeline_rocksdb/_user_hive_warehouse_stock_ticks_mor/9147b15e-4165-4822-ad0e-ed2c5823dfb6: 

24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                         Options.error_if_exists: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                       Options.create_if_missing: 1
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                         Options.paranoid_checks: 1
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :             Options.flush_verify_memtable_count: 1
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                               Options.track_and_verify_wals_in_manifest: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :        Options.verify_sst_unique_id_in_manifest: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                                     Options.env: 0x7f2e039e45e0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                                      Options.fs: PosixFileSystem
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                                Options.info_log: 0x7f2d28005b98
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                Options.max_file_opening_threads: 16
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                              Options.statistics: 0x7f2d280073a0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                               Options.use_fsync: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                       Options.max_log_file_size: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                  Options.max_manifest_file_size: 1073741824
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                   Options.log_file_time_to_roll: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                       Options.keep_log_file_num: 1000
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                    Options.recycle_log_file_num: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                         Options.allow_fallocate: 1
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                        Options.allow_mmap_reads: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                       Options.allow_mmap_writes: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                        Options.use_direct_reads: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                        Options.use_direct_io_for_flush_and_compaction: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :          Options.create_missing_column_families: 1
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                              Options.db_log_dir: 
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                                 Options.wal_dir: 
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                Options.table_cache_numshardbits: 6
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                         Options.WAL_ttl_seconds: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                       Options.WAL_size_limit_MB: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                        Options.max_write_batch_group_size_bytes: 1048576
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :             Options.manifest_preallocation_size: 4194304
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                     Options.is_fd_close_on_exec: 1
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                   Options.advise_random_on_open: 1
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                    Options.db_write_buffer_size: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                    Options.write_buffer_manager: 0x7f2d2807e970
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :         Options.access_hint_on_compaction_start: 1
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :           Options.random_access_max_buffer_size: 1048576
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                      Options.use_adaptive_mutex: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                            Options.rate_limiter: (nil)
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :     Options.sst_file_manager.rate_bytes_per_sec: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                       Options.wal_recovery_mode: 2
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                  Options.enable_thread_tracking: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                  Options.enable_pipelined_write: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                  Options.unordered_write: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :         Options.allow_concurrent_memtable_write: 1
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :      Options.enable_write_thread_adaptive_yield: 1
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :             Options.write_thread_max_yield_usec: 100
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :            Options.write_thread_slow_yield_usec: 3
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                               Options.row_cache: None
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                              Options.wal_filter: None
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :             Options.avoid_flush_during_recovery: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :             Options.allow_ingest_behind: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :             Options.two_write_queues: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :             Options.manual_wal_flush: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :             Options.wal_compression: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :             Options.atomic_flush: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :             Options.avoid_unnecessary_blocking_io: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                 Options.persist_stats_to_disk: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                 Options.write_dbid_to_manifest: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                 Options.log_readahead_size: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                 Options.file_checksum_gen_factory: Unknown
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                 Options.best_efforts_recovery: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                Options.max_bgerror_resume_count: 2147483647
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :            Options.bgerror_resume_retry_interval: 1000000
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :             Options.allow_data_in_errors: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :             Options.db_host_id: __hostname__
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :             Options.enforce_single_del_contracts: true
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :             Options.max_background_jobs: 2
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :             Options.max_background_compactions: -1
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :             Options.max_subcompactions: 1
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :             Options.avoid_flush_during_shutdown: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :           Options.writable_file_max_buffer_size: 1048576
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :             Options.delayed_write_rate : 16777216
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :             Options.max_total_wal_size: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :             Options.delete_obsolete_files_period_micros: 21600000000
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                   Options.stats_dump_period_sec: 300
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                 Options.stats_persist_period_sec: 600
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                 Options.stats_history_buffer_size: 1048576
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                          Options.max_open_files: -1
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                          Options.bytes_per_sync: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                      Options.wal_bytes_per_sync: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                   Options.strict_bytes_per_sync: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :       Options.compaction_readahead_size: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                  Options.max_background_flushes: -1
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Compression algorithms supported:
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : 	kZSTDNotFinalCompression supported: 1
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : 	kZSTD supported: 1
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : 	kXpressCompression supported: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : 	kLZ4HCCompression supported: 1
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : 	kLZ4Compression supported: 1
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : 	kBZip2Compression supported: 1
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : 	kZlibCompression supported: 1
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : 	kSnappyCompression supported: 1
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Fast CRC32 supported: Not supported on x86
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : DMutex implementation: pthread_mutex_t
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : [/db_impl/db_impl_open.cc:313] Creating manifest 1 

24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : [/version_set.cc:4994] Recovering from manifest file: /tmp/hoodie_timeline_rocksdb/_user_hive_warehouse_stock_ticks_mor/9147b15e-4165-4822-ad0e-ed2c5823dfb6/MANIFEST-000001

24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : [/column_family.cc:618] --------------- Options for column family [default]:

24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :               Options.comparator: leveldb.BytewiseComparator
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :           Options.merge_operator: None
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :        Options.compaction_filter: None
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :        Options.compaction_filter_factory: None
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :  Options.sst_partitioner_factory: None
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :         Options.memtable_factory: SkipListFactory
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :            Options.table_factory: BlockBasedTable
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :            table_factory options:   flush_block_policy_factory: FlushBlockBySizePolicyFactory (0x7f2d2807ab30)
  cache_index_and_filter_blocks: 0
  cache_index_and_filter_blocks_with_high_priority: 1
  pin_l0_filter_and_index_blocks_in_cache: 0
  pin_top_level_index_and_filter: 1
  index_type: 0
  data_block_index_type: 0
  index_shortening: 1
  data_block_hash_table_util_ratio: 0.750000
  checksum: 1
  no_block_cache: 0
  block_cache: 0x7f2d2807a9f0
  block_cache_name: LRUCache
  block_cache_options:
    capacity : 8388608
    num_shard_bits : 4
    strict_capacity_limit : 0
    memory_allocator : None
    high_pri_pool_ratio: 0.000
  block_cache_compressed: (nil)
  persistent_cache: (nil)
  block_size: 4096
  block_size_deviation: 10
  block_restart_interval: 16
  index_block_restart_interval: 1
  metadata_block_size: 4096
  partition_filters: 0
  use_delta_encoding: 1
  filter_policy: nullptr
  whole_key_filtering: 1
  verify_compression: 0
  read_amp_bytes_per_bit: 0
  format_version: 5
  enable_index_compression: 1
  block_align: 0
  max_auto_readahead_size: 262144
  prepopulate_block_cache: 0
  initial_auto_readahead_size: 8192

24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :        Options.write_buffer_size: 67108864
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :  Options.max_write_buffer_number: 2
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :          Options.compression: Snappy
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression: Disabled
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :       Options.prefix_extractor: nullptr
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :   Options.memtable_insert_with_hint_prefix_extractor: nullptr
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :             Options.num_levels: 7
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :        Options.min_write_buffer_number_to_merge: 1
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :     Options.max_write_buffer_number_to_maintain: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :     Options.max_write_buffer_size_to_maintain: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :            Options.bottommost_compression_opts.window_bits: -14
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression_opts.level: 32767
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :               Options.bottommost_compression_opts.strategy: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.max_dict_bytes: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.zstd_max_train_bytes: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.parallel_threads: 1
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression_opts.enabled: false
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.max_dict_buffer_bytes: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.use_zstd_dict_trainer: true
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :            Options.compression_opts.window_bits: -14
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                  Options.compression_opts.level: 32767
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :               Options.compression_opts.strategy: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.max_dict_bytes: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.zstd_max_train_bytes: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.use_zstd_dict_trainer: true
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.parallel_threads: 1
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                  Options.compression_opts.enabled: false
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.max_dict_buffer_bytes: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :      Options.level0_file_num_compaction_trigger: 4
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :          Options.level0_slowdown_writes_trigger: 20
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :              Options.level0_stop_writes_trigger: 36
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                   Options.target_file_size_base: 67108864
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :             Options.target_file_size_multiplier: 1
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                Options.max_bytes_for_level_base: 268435456
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Options.level_compaction_dynamic_level_bytes: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :          Options.max_bytes_for_level_multiplier: 10.000000
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[0]: 1
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[1]: 1
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[2]: 1
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[3]: 1
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[4]: 1
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[5]: 1
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[6]: 1
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :       Options.max_sequential_skip_in_iterations: 8
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                    Options.max_compaction_bytes: 1677721600
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                        Options.arena_block_size: 1048576
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :   Options.soft_pending_compaction_bytes_limit: 68719476736
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :   Options.hard_pending_compaction_bytes_limit: 274877906944
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                Options.disable_auto_compactions: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                        Options.compaction_style: kCompactionStyleLevel
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                          Options.compaction_pri: kMinOverlappingRatio
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.size_ratio: 1
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.min_merge_width: 2
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.max_merge_width: 4294967295
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.max_size_amplification_percent: 200
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.compression_size_percent: -1
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.stop_style: kCompactionStopStyleTotalSize
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_fifo.max_table_files_size: 1073741824
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_fifo.allow_compaction: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                   Options.table_properties_collectors: 
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                   Options.inplace_update_support: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                 Options.inplace_update_num_locks: 10000
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :               Options.memtable_prefix_bloom_size_ratio: 0.000000
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :               Options.memtable_whole_key_filtering: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :   Options.memtable_huge_page_size: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                           Options.bloom_locality: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                    Options.max_successive_merges: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                Options.optimize_filters_for_hits: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                Options.paranoid_file_checks: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                Options.force_consistency_checks: 1
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                Options.report_bg_io_stats: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                               Options.ttl: 2592000
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :          Options.periodic_compaction_seconds: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :  Options.preclude_last_level_data_seconds: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                       Options.enable_blob_files: false
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                           Options.min_blob_size: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                          Options.blob_file_size: 268435456
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                   Options.blob_compression_type: NoCompression
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :          Options.enable_blob_garbage_collection: false
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :      Options.blob_garbage_collection_age_cutoff: 0.250000
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Options.blob_garbage_collection_force_threshold: 1.000000
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :          Options.blob_compaction_readahead_size: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                Options.blob_file_starting_level: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Options.experimental_mempurge_threshold: 0.000000
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : [/version_set.cc:5042] Recovered from manifest file:/tmp/hoodie_timeline_rocksdb/_user_hive_warehouse_stock_ticks_mor/9147b15e-4165-4822-ad0e-ed2c5823dfb6/MANIFEST-000001 succeeded,manifest_file_number is 1, next_file_number is 3, last_sequence is 0, log_number is 0,prev_log_number is 0,max_column_family is 0,min_log_number_to_keep is 0

24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : [/version_set.cc:5051] Column family [default] (ID 0), log number is 0

24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : [/db_impl/db_impl_open.cc:532] DB ID: 756b3386-90b3-47c2-8ddb-3735eab75c87

24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : [/version_set.cc:4513] Creating manifest 5

24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : [/db_impl/db_impl_open.cc:2024] SstFileManager instance 0x7f2d2807ea10
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : DB pointer 0x7f2d2807d180
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : [/column_family.cc:618] --------------- Options for column family [hudi_view__user_hive_warehouse_stock_ticks_mor]:

24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :               Options.comparator: leveldb.BytewiseComparator
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :           Options.merge_operator: None
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :        Options.compaction_filter: None
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :        Options.compaction_filter_factory: None
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :  Options.sst_partitioner_factory: None
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :         Options.memtable_factory: SkipListFactory
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :            Options.table_factory: BlockBasedTable
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :            table_factory options:   flush_block_policy_factory: FlushBlockBySizePolicyFactory (0x7f2d2809d380)
  cache_index_and_filter_blocks: 0
  cache_index_and_filter_blocks_with_high_priority: 1
  pin_l0_filter_and_index_blocks_in_cache: 0
  pin_top_level_index_and_filter: 1
  index_type: 0
  data_block_index_type: 0
  index_shortening: 1
  data_block_hash_table_util_ratio: 0.750000
  checksum: 1
  no_block_cache: 0
  block_cache: 0x7f2d280a0b00
  block_cache_name: LRUCache
  block_cache_options:
    capacity : 8388608
    num_shard_bits : 4
    strict_capacity_limit : 0
    memory_allocator : None
    high_pri_pool_ratio: 0.000
  block_cache_compressed: (nil)
  persistent_cache: (nil)
  block_size: 4096
  block_size_deviation: 10
  block_restart_interval: 16
  index_block_restart_interval: 1
  metadata_block_size: 4096
  partition_filters: 0
  use_delta_encoding: 1
  filter_policy: nullptr
  whole_key_filtering: 1
  verify_compression: 0
  read_amp_bytes_per_bit: 0
  format_version: 5
  enable_index_compression: 1
  block_align: 0
  max_auto_readahead_size: 262144
  prepopulate_block_cache: 0
  initial_auto_readahead_size: 8192

24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :        Options.write_buffer_size: 67108864
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :  Options.max_write_buffer_number: 2
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :          Options.compression: Snappy
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression: Disabled
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :       Options.prefix_extractor: nullptr
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :   Options.memtable_insert_with_hint_prefix_extractor: nullptr
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :             Options.num_levels: 7
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :        Options.min_write_buffer_number_to_merge: 1
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :     Options.max_write_buffer_number_to_maintain: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :     Options.max_write_buffer_size_to_maintain: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :            Options.bottommost_compression_opts.window_bits: -14
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression_opts.level: 32767
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :               Options.bottommost_compression_opts.strategy: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.max_dict_bytes: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.zstd_max_train_bytes: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.parallel_threads: 1
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression_opts.enabled: false
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.max_dict_buffer_bytes: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.use_zstd_dict_trainer: true
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :            Options.compression_opts.window_bits: -14
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                  Options.compression_opts.level: 32767
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :               Options.compression_opts.strategy: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.max_dict_bytes: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.zstd_max_train_bytes: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.use_zstd_dict_trainer: true
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.parallel_threads: 1
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                  Options.compression_opts.enabled: false
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.max_dict_buffer_bytes: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :      Options.level0_file_num_compaction_trigger: 4
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :          Options.level0_slowdown_writes_trigger: 20
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :              Options.level0_stop_writes_trigger: 36
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                   Options.target_file_size_base: 67108864
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :             Options.target_file_size_multiplier: 1
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                Options.max_bytes_for_level_base: 268435456
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Options.level_compaction_dynamic_level_bytes: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :          Options.max_bytes_for_level_multiplier: 10.000000
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[0]: 1
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[1]: 1
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[2]: 1
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[3]: 1
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[4]: 1
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[5]: 1
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[6]: 1
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :       Options.max_sequential_skip_in_iterations: 8
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                    Options.max_compaction_bytes: 1677721600
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                        Options.arena_block_size: 1048576
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :   Options.soft_pending_compaction_bytes_limit: 68719476736
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :   Options.hard_pending_compaction_bytes_limit: 274877906944
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                Options.disable_auto_compactions: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                        Options.compaction_style: kCompactionStyleLevel
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                          Options.compaction_pri: kMinOverlappingRatio
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.size_ratio: 1
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.min_merge_width: 2
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.max_merge_width: 4294967295
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.max_size_amplification_percent: 200
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.compression_size_percent: -1
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.stop_style: kCompactionStopStyleTotalSize
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_fifo.max_table_files_size: 1073741824
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_fifo.allow_compaction: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                   Options.table_properties_collectors: 
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                   Options.inplace_update_support: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                 Options.inplace_update_num_locks: 10000
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :               Options.memtable_prefix_bloom_size_ratio: 0.000000
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :               Options.memtable_whole_key_filtering: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :   Options.memtable_huge_page_size: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                           Options.bloom_locality: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                    Options.max_successive_merges: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                Options.optimize_filters_for_hits: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                Options.paranoid_file_checks: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                Options.force_consistency_checks: 1
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                Options.report_bg_io_stats: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                               Options.ttl: 2592000
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :          Options.periodic_compaction_seconds: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :  Options.preclude_last_level_data_seconds: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                       Options.enable_blob_files: false
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                           Options.min_blob_size: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                          Options.blob_file_size: 268435456
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                   Options.blob_compression_type: NoCompression
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :          Options.enable_blob_garbage_collection: false
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :      Options.blob_garbage_collection_age_cutoff: 0.250000
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Options.blob_garbage_collection_force_threshold: 1.000000
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :          Options.blob_compaction_readahead_size: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                Options.blob_file_starting_level: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Options.experimental_mempurge_threshold: 0.000000
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : [/db_impl/db_impl.cc:2861] Created column family [hudi_view__user_hive_warehouse_stock_ticks_mor] (ID 1)
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : [/column_family.cc:618] --------------- Options for column family [hudi_pending_compaction__user_hive_warehouse_stock_ticks_mor]:

24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :               Options.comparator: leveldb.BytewiseComparator
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :           Options.merge_operator: None
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :        Options.compaction_filter: None
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :        Options.compaction_filter_factory: None
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :  Options.sst_partitioner_factory: None
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :         Options.memtable_factory: SkipListFactory
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :            Options.table_factory: BlockBasedTable
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :            table_factory options:   flush_block_policy_factory: FlushBlockBySizePolicyFactory (0x7f2d280a2430)
  cache_index_and_filter_blocks: 0
  cache_index_and_filter_blocks_with_high_priority: 1
  pin_l0_filter_and_index_blocks_in_cache: 0
  pin_top_level_index_and_filter: 1
  index_type: 0
  data_block_index_type: 0
  index_shortening: 1
  data_block_hash_table_util_ratio: 0.750000
  checksum: 1
  no_block_cache: 0
  block_cache: 0x7f2d28090f80
  block_cache_name: LRUCache
  block_cache_options:
    capacity : 8388608
    num_shard_bits : 4
    strict_capacity_limit : 0
    memory_allocator : None
    high_pri_pool_ratio: 0.000
  block_cache_compressed: (nil)
  persistent_cache: (nil)
  block_size: 4096
  block_size_deviation: 10
  block_restart_interval: 16
  index_block_restart_interval: 1
  metadata_block_size: 4096
  partition_filters: 0
  use_delta_encoding: 1
  filter_policy: nullptr
  whole_key_filtering: 1
  verify_compression: 0
  read_amp_bytes_per_bit: 0
  format_version: 5
  enable_index_compression: 1
  block_align: 0
  max_auto_readahead_size: 262144
  prepopulate_block_cache: 0
  initial_auto_readahead_size: 8192

24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :        Options.write_buffer_size: 67108864
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :  Options.max_write_buffer_number: 2
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :          Options.compression: Snappy
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression: Disabled
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :       Options.prefix_extractor: nullptr
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :   Options.memtable_insert_with_hint_prefix_extractor: nullptr
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :             Options.num_levels: 7
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :        Options.min_write_buffer_number_to_merge: 1
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :     Options.max_write_buffer_number_to_maintain: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :     Options.max_write_buffer_size_to_maintain: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :            Options.bottommost_compression_opts.window_bits: -14
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression_opts.level: 32767
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :               Options.bottommost_compression_opts.strategy: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.max_dict_bytes: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.zstd_max_train_bytes: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.parallel_threads: 1
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression_opts.enabled: false
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.max_dict_buffer_bytes: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.use_zstd_dict_trainer: true
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :            Options.compression_opts.window_bits: -14
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                  Options.compression_opts.level: 32767
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :               Options.compression_opts.strategy: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.max_dict_bytes: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.zstd_max_train_bytes: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.use_zstd_dict_trainer: true
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.parallel_threads: 1
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                  Options.compression_opts.enabled: false
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.max_dict_buffer_bytes: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :      Options.level0_file_num_compaction_trigger: 4
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :          Options.level0_slowdown_writes_trigger: 20
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :              Options.level0_stop_writes_trigger: 36
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                   Options.target_file_size_base: 67108864
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :             Options.target_file_size_multiplier: 1
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                Options.max_bytes_for_level_base: 268435456
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Options.level_compaction_dynamic_level_bytes: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :          Options.max_bytes_for_level_multiplier: 10.000000
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[0]: 1
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[1]: 1
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[2]: 1
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[3]: 1
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[4]: 1
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[5]: 1
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[6]: 1
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :       Options.max_sequential_skip_in_iterations: 8
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                    Options.max_compaction_bytes: 1677721600
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                        Options.arena_block_size: 1048576
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :   Options.soft_pending_compaction_bytes_limit: 68719476736
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :   Options.hard_pending_compaction_bytes_limit: 274877906944
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                Options.disable_auto_compactions: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                        Options.compaction_style: kCompactionStyleLevel
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                          Options.compaction_pri: kMinOverlappingRatio
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.size_ratio: 1
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.min_merge_width: 2
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.max_merge_width: 4294967295
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.max_size_amplification_percent: 200
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.compression_size_percent: -1
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.stop_style: kCompactionStopStyleTotalSize
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_fifo.max_table_files_size: 1073741824
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_fifo.allow_compaction: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                   Options.table_properties_collectors: 
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                   Options.inplace_update_support: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                 Options.inplace_update_num_locks: 10000
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :               Options.memtable_prefix_bloom_size_ratio: 0.000000
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :               Options.memtable_whole_key_filtering: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :   Options.memtable_huge_page_size: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                           Options.bloom_locality: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                    Options.max_successive_merges: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                Options.optimize_filters_for_hits: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                Options.paranoid_file_checks: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                Options.force_consistency_checks: 1
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                Options.report_bg_io_stats: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                               Options.ttl: 2592000
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :          Options.periodic_compaction_seconds: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :  Options.preclude_last_level_data_seconds: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                       Options.enable_blob_files: false
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                           Options.min_blob_size: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                          Options.blob_file_size: 268435456
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                   Options.blob_compression_type: NoCompression
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :          Options.enable_blob_garbage_collection: false
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :      Options.blob_garbage_collection_age_cutoff: 0.250000
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Options.blob_garbage_collection_force_threshold: 1.000000
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :          Options.blob_compaction_readahead_size: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                Options.blob_file_starting_level: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Options.experimental_mempurge_threshold: 0.000000
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : [/db_impl/db_impl.cc:2861] Created column family [hudi_pending_compaction__user_hive_warehouse_stock_ticks_mor] (ID 2)
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : [/column_family.cc:618] --------------- Options for column family [hudi_pending_log_compaction__user_hive_warehouse_stock_ticks_mor]:

24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :               Options.comparator: leveldb.BytewiseComparator
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :           Options.merge_operator: None
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :        Options.compaction_filter: None
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :        Options.compaction_filter_factory: None
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :  Options.sst_partitioner_factory: None
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :         Options.memtable_factory: SkipListFactory
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :            Options.table_factory: BlockBasedTable
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :            table_factory options:   flush_block_policy_factory: FlushBlockBySizePolicyFactory (0x7f2d280c9c60)
  cache_index_and_filter_blocks: 0
  cache_index_and_filter_blocks_with_high_priority: 1
  pin_l0_filter_and_index_blocks_in_cache: 0
  pin_top_level_index_and_filter: 1
  index_type: 0
  data_block_index_type: 0
  index_shortening: 1
  data_block_hash_table_util_ratio: 0.750000
  checksum: 1
  no_block_cache: 0
  block_cache: 0x7f2d280c9a80
  block_cache_name: LRUCache
  block_cache_options:
    capacity : 8388608
    num_shard_bits : 4
    strict_capacity_limit : 0
    memory_allocator : None
    high_pri_pool_ratio: 0.000
  block_cache_compressed: (nil)
  persistent_cache: (nil)
  block_size: 4096
  block_size_deviation: 10
  block_restart_interval: 16
  index_block_restart_interval: 1
  metadata_block_size: 4096
  partition_filters: 0
  use_delta_encoding: 1
  filter_policy: nullptr
  whole_key_filtering: 1
  verify_compression: 0
  read_amp_bytes_per_bit: 0
  format_version: 5
  enable_index_compression: 1
  block_align: 0
  max_auto_readahead_size: 262144
  prepopulate_block_cache: 0
  initial_auto_readahead_size: 8192

24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :        Options.write_buffer_size: 67108864
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :  Options.max_write_buffer_number: 2
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :          Options.compression: Snappy
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression: Disabled
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :       Options.prefix_extractor: nullptr
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :   Options.memtable_insert_with_hint_prefix_extractor: nullptr
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :             Options.num_levels: 7
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :        Options.min_write_buffer_number_to_merge: 1
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :     Options.max_write_buffer_number_to_maintain: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :     Options.max_write_buffer_size_to_maintain: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :            Options.bottommost_compression_opts.window_bits: -14
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression_opts.level: 32767
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :               Options.bottommost_compression_opts.strategy: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.max_dict_bytes: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.zstd_max_train_bytes: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.parallel_threads: 1
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression_opts.enabled: false
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.max_dict_buffer_bytes: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.use_zstd_dict_trainer: true
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :            Options.compression_opts.window_bits: -14
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                  Options.compression_opts.level: 32767
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :               Options.compression_opts.strategy: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.max_dict_bytes: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.zstd_max_train_bytes: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.use_zstd_dict_trainer: true
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.parallel_threads: 1
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                  Options.compression_opts.enabled: false
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.max_dict_buffer_bytes: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :      Options.level0_file_num_compaction_trigger: 4
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :          Options.level0_slowdown_writes_trigger: 20
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :              Options.level0_stop_writes_trigger: 36
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                   Options.target_file_size_base: 67108864
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :             Options.target_file_size_multiplier: 1
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                Options.max_bytes_for_level_base: 268435456
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Options.level_compaction_dynamic_level_bytes: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :          Options.max_bytes_for_level_multiplier: 10.000000
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[0]: 1
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[1]: 1
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[2]: 1
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[3]: 1
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[4]: 1
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[5]: 1
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[6]: 1
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :       Options.max_sequential_skip_in_iterations: 8
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                    Options.max_compaction_bytes: 1677721600
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                        Options.arena_block_size: 1048576
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :   Options.soft_pending_compaction_bytes_limit: 68719476736
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :   Options.hard_pending_compaction_bytes_limit: 274877906944
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                Options.disable_auto_compactions: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                        Options.compaction_style: kCompactionStyleLevel
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                          Options.compaction_pri: kMinOverlappingRatio
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.size_ratio: 1
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.min_merge_width: 2
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.max_merge_width: 4294967295
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.max_size_amplification_percent: 200
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.compression_size_percent: -1
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.stop_style: kCompactionStopStyleTotalSize
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_fifo.max_table_files_size: 1073741824
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_fifo.allow_compaction: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                   Options.table_properties_collectors: 
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                   Options.inplace_update_support: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                 Options.inplace_update_num_locks: 10000
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :               Options.memtable_prefix_bloom_size_ratio: 0.000000
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :               Options.memtable_whole_key_filtering: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :   Options.memtable_huge_page_size: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                           Options.bloom_locality: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                    Options.max_successive_merges: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                Options.optimize_filters_for_hits: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                Options.paranoid_file_checks: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                Options.force_consistency_checks: 1
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                Options.report_bg_io_stats: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                               Options.ttl: 2592000
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :          Options.periodic_compaction_seconds: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :  Options.preclude_last_level_data_seconds: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                       Options.enable_blob_files: false
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                           Options.min_blob_size: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                          Options.blob_file_size: 268435456
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                   Options.blob_compression_type: NoCompression
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :          Options.enable_blob_garbage_collection: false
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :      Options.blob_garbage_collection_age_cutoff: 0.250000
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Options.blob_garbage_collection_force_threshold: 1.000000
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :          Options.blob_compaction_readahead_size: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                Options.blob_file_starting_level: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Options.experimental_mempurge_threshold: 0.000000
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : [/db_impl/db_impl.cc:2861] Created column family [hudi_pending_log_compaction__user_hive_warehouse_stock_ticks_mor] (ID 3)
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : [/column_family.cc:618] --------------- Options for column family [hudi_bootstrap_basefile__user_hive_warehouse_stock_ticks_mor]:

24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :               Options.comparator: leveldb.BytewiseComparator
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :           Options.merge_operator: None
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :        Options.compaction_filter: None
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :        Options.compaction_filter_factory: None
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :  Options.sst_partitioner_factory: None
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :         Options.memtable_factory: SkipListFactory
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :            Options.table_factory: BlockBasedTable
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :            table_factory options:   flush_block_policy_factory: FlushBlockBySizePolicyFactory (0x7f2d280e2920)
  cache_index_and_filter_blocks: 0
  cache_index_and_filter_blocks_with_high_priority: 1
  pin_l0_filter_and_index_blocks_in_cache: 0
  pin_top_level_index_and_filter: 1
  index_type: 0
  data_block_index_type: 0
  index_shortening: 1
  data_block_hash_table_util_ratio: 0.750000
  checksum: 1
  no_block_cache: 0
  block_cache: 0x7f2d280d7500
  block_cache_name: LRUCache
  block_cache_options:
    capacity : 8388608
    num_shard_bits : 4
    strict_capacity_limit : 0
    memory_allocator : None
    high_pri_pool_ratio: 0.000
  block_cache_compressed: (nil)
  persistent_cache: (nil)
  block_size: 4096
  block_size_deviation: 10
  block_restart_interval: 16
  index_block_restart_interval: 1
  metadata_block_size: 4096
  partition_filters: 0
  use_delta_encoding: 1
  filter_policy: nullptr
  whole_key_filtering: 1
  verify_compression: 0
  read_amp_bytes_per_bit: 0
  format_version: 5
  enable_index_compression: 1
  block_align: 0
  max_auto_readahead_size: 262144
  prepopulate_block_cache: 0
  initial_auto_readahead_size: 8192

24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :        Options.write_buffer_size: 67108864
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :  Options.max_write_buffer_number: 2
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :          Options.compression: Snappy
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression: Disabled
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :       Options.prefix_extractor: nullptr
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :   Options.memtable_insert_with_hint_prefix_extractor: nullptr
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :             Options.num_levels: 7
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :        Options.min_write_buffer_number_to_merge: 1
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :     Options.max_write_buffer_number_to_maintain: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :     Options.max_write_buffer_size_to_maintain: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :            Options.bottommost_compression_opts.window_bits: -14
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression_opts.level: 32767
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :               Options.bottommost_compression_opts.strategy: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.max_dict_bytes: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.zstd_max_train_bytes: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.parallel_threads: 1
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression_opts.enabled: false
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.max_dict_buffer_bytes: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.use_zstd_dict_trainer: true
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :            Options.compression_opts.window_bits: -14
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                  Options.compression_opts.level: 32767
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :               Options.compression_opts.strategy: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.max_dict_bytes: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.zstd_max_train_bytes: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.use_zstd_dict_trainer: true
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.parallel_threads: 1
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                  Options.compression_opts.enabled: false
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.max_dict_buffer_bytes: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :      Options.level0_file_num_compaction_trigger: 4
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :          Options.level0_slowdown_writes_trigger: 20
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :              Options.level0_stop_writes_trigger: 36
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                   Options.target_file_size_base: 67108864
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :             Options.target_file_size_multiplier: 1
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                Options.max_bytes_for_level_base: 268435456
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Options.level_compaction_dynamic_level_bytes: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :          Options.max_bytes_for_level_multiplier: 10.000000
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[0]: 1
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[1]: 1
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[2]: 1
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[3]: 1
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[4]: 1
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[5]: 1
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[6]: 1
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :       Options.max_sequential_skip_in_iterations: 8
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                    Options.max_compaction_bytes: 1677721600
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                        Options.arena_block_size: 1048576
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :   Options.soft_pending_compaction_bytes_limit: 68719476736
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :   Options.hard_pending_compaction_bytes_limit: 274877906944
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                Options.disable_auto_compactions: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                        Options.compaction_style: kCompactionStyleLevel
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                          Options.compaction_pri: kMinOverlappingRatio
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.size_ratio: 1
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.min_merge_width: 2
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.max_merge_width: 4294967295
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.max_size_amplification_percent: 200
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.compression_size_percent: -1
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.stop_style: kCompactionStopStyleTotalSize
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_fifo.max_table_files_size: 1073741824
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_fifo.allow_compaction: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                   Options.table_properties_collectors: 
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                   Options.inplace_update_support: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                 Options.inplace_update_num_locks: 10000
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :               Options.memtable_prefix_bloom_size_ratio: 0.000000
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :               Options.memtable_whole_key_filtering: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :   Options.memtable_huge_page_size: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                           Options.bloom_locality: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                    Options.max_successive_merges: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                Options.optimize_filters_for_hits: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                Options.paranoid_file_checks: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                Options.force_consistency_checks: 1
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                Options.report_bg_io_stats: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                               Options.ttl: 2592000
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :          Options.periodic_compaction_seconds: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :  Options.preclude_last_level_data_seconds: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                       Options.enable_blob_files: false
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                           Options.min_blob_size: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                          Options.blob_file_size: 268435456
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                   Options.blob_compression_type: NoCompression
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :          Options.enable_blob_garbage_collection: false
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :      Options.blob_garbage_collection_age_cutoff: 0.250000
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Options.blob_garbage_collection_force_threshold: 1.000000
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :          Options.blob_compaction_readahead_size: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                Options.blob_file_starting_level: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Options.experimental_mempurge_threshold: 0.000000
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : [/db_impl/db_impl.cc:2861] Created column family [hudi_bootstrap_basefile__user_hive_warehouse_stock_ticks_mor] (ID 4)
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : [/column_family.cc:618] --------------- Options for column family [hudi_partitions__user_hive_warehouse_stock_ticks_mor]:

24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :               Options.comparator: leveldb.BytewiseComparator
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :           Options.merge_operator: None
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :        Options.compaction_filter: None
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :        Options.compaction_filter_factory: None
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :  Options.sst_partitioner_factory: None
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :         Options.memtable_factory: SkipListFactory
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :            Options.table_factory: BlockBasedTable
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :            table_factory options:   flush_block_policy_factory: FlushBlockBySizePolicyFactory (0x7f2d280e5050)
  cache_index_and_filter_blocks: 0
  cache_index_and_filter_blocks_with_high_priority: 1
  pin_l0_filter_and_index_blocks_in_cache: 0
  pin_top_level_index_and_filter: 1
  index_type: 0
  data_block_index_type: 0
  index_shortening: 1
  data_block_hash_table_util_ratio: 0.750000
  checksum: 1
  no_block_cache: 0
  block_cache: 0x7f2d280e4ee0
  block_cache_name: LRUCache
  block_cache_options:
    capacity : 8388608
    num_shard_bits : 4
    strict_capacity_limit : 0
    memory_allocator : None
    high_pri_pool_ratio: 0.000
  block_cache_compressed: (nil)
  persistent_cache: (nil)
  block_size: 4096
  block_size_deviation: 10
  block_restart_interval: 16
  index_block_restart_interval: 1
  metadata_block_size: 4096
  partition_filters: 0
  use_delta_encoding: 1
  filter_policy: nullptr
  whole_key_filtering: 1
  verify_compression: 0
  read_amp_bytes_per_bit: 0
  format_version: 5
  enable_index_compression: 1
  block_align: 0
  max_auto_readahead_size: 262144
  prepopulate_block_cache: 0
  initial_auto_readahead_size: 8192

24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :        Options.write_buffer_size: 67108864
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :  Options.max_write_buffer_number: 2
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :          Options.compression: Snappy
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression: Disabled
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :       Options.prefix_extractor: nullptr
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :   Options.memtable_insert_with_hint_prefix_extractor: nullptr
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :             Options.num_levels: 7
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :        Options.min_write_buffer_number_to_merge: 1
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :     Options.max_write_buffer_number_to_maintain: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :     Options.max_write_buffer_size_to_maintain: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :            Options.bottommost_compression_opts.window_bits: -14
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression_opts.level: 32767
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :               Options.bottommost_compression_opts.strategy: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.max_dict_bytes: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.zstd_max_train_bytes: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.parallel_threads: 1
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression_opts.enabled: false
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.max_dict_buffer_bytes: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.use_zstd_dict_trainer: true
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :            Options.compression_opts.window_bits: -14
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                  Options.compression_opts.level: 32767
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :               Options.compression_opts.strategy: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.max_dict_bytes: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.zstd_max_train_bytes: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.use_zstd_dict_trainer: true
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.parallel_threads: 1
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                  Options.compression_opts.enabled: false
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.max_dict_buffer_bytes: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :      Options.level0_file_num_compaction_trigger: 4
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :          Options.level0_slowdown_writes_trigger: 20
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :              Options.level0_stop_writes_trigger: 36
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                   Options.target_file_size_base: 67108864
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :             Options.target_file_size_multiplier: 1
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                Options.max_bytes_for_level_base: 268435456
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Options.level_compaction_dynamic_level_bytes: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :          Options.max_bytes_for_level_multiplier: 10.000000
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[0]: 1
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[1]: 1
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[2]: 1
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[3]: 1
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[4]: 1
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[5]: 1
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[6]: 1
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :       Options.max_sequential_skip_in_iterations: 8
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                    Options.max_compaction_bytes: 1677721600
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                        Options.arena_block_size: 1048576
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :   Options.soft_pending_compaction_bytes_limit: 68719476736
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :   Options.hard_pending_compaction_bytes_limit: 274877906944
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                Options.disable_auto_compactions: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                        Options.compaction_style: kCompactionStyleLevel
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                          Options.compaction_pri: kMinOverlappingRatio
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.size_ratio: 1
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.min_merge_width: 2
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.max_merge_width: 4294967295
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.max_size_amplification_percent: 200
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.compression_size_percent: -1
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.stop_style: kCompactionStopStyleTotalSize
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_fifo.max_table_files_size: 1073741824
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_fifo.allow_compaction: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                   Options.table_properties_collectors: 
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                   Options.inplace_update_support: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                 Options.inplace_update_num_locks: 10000
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :               Options.memtable_prefix_bloom_size_ratio: 0.000000
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :               Options.memtable_whole_key_filtering: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :   Options.memtable_huge_page_size: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                           Options.bloom_locality: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                    Options.max_successive_merges: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                Options.optimize_filters_for_hits: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                Options.paranoid_file_checks: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                Options.force_consistency_checks: 1
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                Options.report_bg_io_stats: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                               Options.ttl: 2592000
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :          Options.periodic_compaction_seconds: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :  Options.preclude_last_level_data_seconds: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                       Options.enable_blob_files: false
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                           Options.min_blob_size: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                          Options.blob_file_size: 268435456
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                   Options.blob_compression_type: NoCompression
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :          Options.enable_blob_garbage_collection: false
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :      Options.blob_garbage_collection_age_cutoff: 0.250000
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Options.blob_garbage_collection_force_threshold: 1.000000
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :          Options.blob_compaction_readahead_size: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                Options.blob_file_starting_level: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Options.experimental_mempurge_threshold: 0.000000
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : [/db_impl/db_impl.cc:2861] Created column family [hudi_partitions__user_hive_warehouse_stock_ticks_mor] (ID 5)
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : [/column_family.cc:618] --------------- Options for column family [hudi_replaced_fg_user_hive_warehouse_stock_ticks_mor]:

24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :               Options.comparator: leveldb.BytewiseComparator
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :           Options.merge_operator: None
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :        Options.compaction_filter: None
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :        Options.compaction_filter_factory: None
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :  Options.sst_partitioner_factory: None
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :         Options.memtable_factory: SkipListFactory
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :            Options.table_factory: BlockBasedTable
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :            table_factory options:   flush_block_policy_factory: FlushBlockBySizePolicyFactory (0x7f2d280fdc60)
  cache_index_and_filter_blocks: 0
  cache_index_and_filter_blocks_with_high_priority: 1
  pin_l0_filter_and_index_blocks_in_cache: 0
  pin_top_level_index_and_filter: 1
  index_type: 0
  data_block_index_type: 0
  index_shortening: 1
  data_block_hash_table_util_ratio: 0.750000
  checksum: 1
  no_block_cache: 0
  block_cache: 0x7f2d280f2920
  block_cache_name: LRUCache
  block_cache_options:
    capacity : 8388608
    num_shard_bits : 4
    strict_capacity_limit : 0
    memory_allocator : None
    high_pri_pool_ratio: 0.000
  block_cache_compressed: (nil)
  persistent_cache: (nil)
  block_size: 4096
  block_size_deviation: 10
  block_restart_interval: 16
  index_block_restart_interval: 1
  metadata_block_size: 4096
  partition_filters: 0
  use_delta_encoding: 1
  filter_policy: nullptr
  whole_key_filtering: 1
  verify_compression: 0
  read_amp_bytes_per_bit: 0
  format_version: 5
  enable_index_compression: 1
  block_align: 0
  max_auto_readahead_size: 262144
  prepopulate_block_cache: 0
  initial_auto_readahead_size: 8192

24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :        Options.write_buffer_size: 67108864
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :  Options.max_write_buffer_number: 2
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :          Options.compression: Snappy
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression: Disabled
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :       Options.prefix_extractor: nullptr
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :   Options.memtable_insert_with_hint_prefix_extractor: nullptr
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :             Options.num_levels: 7
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :        Options.min_write_buffer_number_to_merge: 1
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :     Options.max_write_buffer_number_to_maintain: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :     Options.max_write_buffer_size_to_maintain: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :            Options.bottommost_compression_opts.window_bits: -14
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression_opts.level: 32767
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :               Options.bottommost_compression_opts.strategy: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.max_dict_bytes: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.zstd_max_train_bytes: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.parallel_threads: 1
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression_opts.enabled: false
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.max_dict_buffer_bytes: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.use_zstd_dict_trainer: true
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :            Options.compression_opts.window_bits: -14
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                  Options.compression_opts.level: 32767
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :               Options.compression_opts.strategy: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.max_dict_bytes: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.zstd_max_train_bytes: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.use_zstd_dict_trainer: true
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.parallel_threads: 1
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                  Options.compression_opts.enabled: false
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.max_dict_buffer_bytes: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :      Options.level0_file_num_compaction_trigger: 4
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :          Options.level0_slowdown_writes_trigger: 20
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :              Options.level0_stop_writes_trigger: 36
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                   Options.target_file_size_base: 67108864
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :             Options.target_file_size_multiplier: 1
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                Options.max_bytes_for_level_base: 268435456
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Options.level_compaction_dynamic_level_bytes: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :          Options.max_bytes_for_level_multiplier: 10.000000
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[0]: 1
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[1]: 1
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[2]: 1
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[3]: 1
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[4]: 1
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[5]: 1
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[6]: 1
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :       Options.max_sequential_skip_in_iterations: 8
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                    Options.max_compaction_bytes: 1677721600
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                        Options.arena_block_size: 1048576
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :   Options.soft_pending_compaction_bytes_limit: 68719476736
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :   Options.hard_pending_compaction_bytes_limit: 274877906944
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                Options.disable_auto_compactions: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                        Options.compaction_style: kCompactionStyleLevel
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                          Options.compaction_pri: kMinOverlappingRatio
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.size_ratio: 1
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.min_merge_width: 2
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.max_merge_width: 4294967295
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.max_size_amplification_percent: 200
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.compression_size_percent: -1
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.stop_style: kCompactionStopStyleTotalSize
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_fifo.max_table_files_size: 1073741824
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_fifo.allow_compaction: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                   Options.table_properties_collectors: 
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                   Options.inplace_update_support: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                 Options.inplace_update_num_locks: 10000
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :               Options.memtable_prefix_bloom_size_ratio: 0.000000
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :               Options.memtable_whole_key_filtering: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :   Options.memtable_huge_page_size: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                           Options.bloom_locality: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                    Options.max_successive_merges: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                Options.optimize_filters_for_hits: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                Options.paranoid_file_checks: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                Options.force_consistency_checks: 1
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                Options.report_bg_io_stats: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                               Options.ttl: 2592000
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :          Options.periodic_compaction_seconds: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :  Options.preclude_last_level_data_seconds: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                       Options.enable_blob_files: false
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                           Options.min_blob_size: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                          Options.blob_file_size: 268435456
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                   Options.blob_compression_type: NoCompression
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :          Options.enable_blob_garbage_collection: false
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :      Options.blob_garbage_collection_age_cutoff: 0.250000
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Options.blob_garbage_collection_force_threshold: 1.000000
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :          Options.blob_compaction_readahead_size: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                Options.blob_file_starting_level: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Options.experimental_mempurge_threshold: 0.000000
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : [/db_impl/db_impl.cc:2861] Created column family [hudi_replaced_fg_user_hive_warehouse_stock_ticks_mor] (ID 6)
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : [/column_family.cc:618] --------------- Options for column family [hudi_pending_clustering_fg_user_hive_warehouse_stock_ticks_mor]:

24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :               Options.comparator: leveldb.BytewiseComparator
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :           Options.merge_operator: None
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :        Options.compaction_filter: None
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :        Options.compaction_filter_factory: None
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :  Options.sst_partitioner_factory: None
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :         Options.memtable_factory: SkipListFactory
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :            Options.table_factory: BlockBasedTable
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :            table_factory options:   flush_block_policy_factory: FlushBlockBySizePolicyFactory (0x7f2d2810b720)
  cache_index_and_filter_blocks: 0
  cache_index_and_filter_blocks_with_high_priority: 1
  pin_l0_filter_and_index_blocks_in_cache: 0
  pin_top_level_index_and_filter: 1
  index_type: 0
  data_block_index_type: 0
  index_shortening: 1
  data_block_hash_table_util_ratio: 0.750000
  checksum: 1
  no_block_cache: 0
  block_cache: 0x7f2d28100240
  block_cache_name: LRUCache
  block_cache_options:
    capacity : 8388608
    num_shard_bits : 4
    strict_capacity_limit : 0
    memory_allocator : None
    high_pri_pool_ratio: 0.000
  block_cache_compressed: (nil)
  persistent_cache: (nil)
  block_size: 4096
  block_size_deviation: 10
  block_restart_interval: 16
  index_block_restart_interval: 1
  metadata_block_size: 4096
  partition_filters: 0
  use_delta_encoding: 1
  filter_policy: nullptr
  whole_key_filtering: 1
  verify_compression: 0
  read_amp_bytes_per_bit: 0
  format_version: 5
  enable_index_compression: 1
  block_align: 0
  max_auto_readahead_size: 262144
  prepopulate_block_cache: 0
  initial_auto_readahead_size: 8192

24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :        Options.write_buffer_size: 67108864
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :  Options.max_write_buffer_number: 2
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :          Options.compression: Snappy
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression: Disabled
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :       Options.prefix_extractor: nullptr
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :   Options.memtable_insert_with_hint_prefix_extractor: nullptr
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :             Options.num_levels: 7
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :        Options.min_write_buffer_number_to_merge: 1
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :     Options.max_write_buffer_number_to_maintain: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :     Options.max_write_buffer_size_to_maintain: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :            Options.bottommost_compression_opts.window_bits: -14
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression_opts.level: 32767
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :               Options.bottommost_compression_opts.strategy: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.max_dict_bytes: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.zstd_max_train_bytes: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.parallel_threads: 1
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                  Options.bottommost_compression_opts.enabled: false
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.max_dict_buffer_bytes: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :         Options.bottommost_compression_opts.use_zstd_dict_trainer: true
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :            Options.compression_opts.window_bits: -14
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                  Options.compression_opts.level: 32767
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :               Options.compression_opts.strategy: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.max_dict_bytes: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.zstd_max_train_bytes: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.use_zstd_dict_trainer: true
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.parallel_threads: 1
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                  Options.compression_opts.enabled: false
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :         Options.compression_opts.max_dict_buffer_bytes: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :      Options.level0_file_num_compaction_trigger: 4
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :          Options.level0_slowdown_writes_trigger: 20
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :              Options.level0_stop_writes_trigger: 36
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                   Options.target_file_size_base: 67108864
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :             Options.target_file_size_multiplier: 1
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                Options.max_bytes_for_level_base: 268435456
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Options.level_compaction_dynamic_level_bytes: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :          Options.max_bytes_for_level_multiplier: 10.000000
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[0]: 1
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[1]: 1
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[2]: 1
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[3]: 1
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[4]: 1
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[5]: 1
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Options.max_bytes_for_level_multiplier_addtl[6]: 1
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :       Options.max_sequential_skip_in_iterations: 8
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                    Options.max_compaction_bytes: 1677721600
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                        Options.arena_block_size: 1048576
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :   Options.soft_pending_compaction_bytes_limit: 68719476736
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :   Options.hard_pending_compaction_bytes_limit: 274877906944
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                Options.disable_auto_compactions: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                        Options.compaction_style: kCompactionStyleLevel
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                          Options.compaction_pri: kMinOverlappingRatio
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.size_ratio: 1
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.min_merge_width: 2
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.max_merge_width: 4294967295
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.max_size_amplification_percent: 200
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.compression_size_percent: -1
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_universal.stop_style: kCompactionStopStyleTotalSize
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_fifo.max_table_files_size: 1073741824
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Options.compaction_options_fifo.allow_compaction: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                   Options.table_properties_collectors: 
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                   Options.inplace_update_support: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                 Options.inplace_update_num_locks: 10000
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :               Options.memtable_prefix_bloom_size_ratio: 0.000000
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :               Options.memtable_whole_key_filtering: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :   Options.memtable_huge_page_size: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                           Options.bloom_locality: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                    Options.max_successive_merges: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                Options.optimize_filters_for_hits: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                Options.paranoid_file_checks: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                Options.force_consistency_checks: 1
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                Options.report_bg_io_stats: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                               Options.ttl: 2592000
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :          Options.periodic_compaction_seconds: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :  Options.preclude_last_level_data_seconds: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                       Options.enable_blob_files: false
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                           Options.min_blob_size: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                          Options.blob_file_size: 268435456
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                   Options.blob_compression_type: NoCompression
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :          Options.enable_blob_garbage_collection: false
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :      Options.blob_garbage_collection_age_cutoff: 0.250000
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Options.blob_garbage_collection_force_threshold: 1.000000
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :          Options.blob_compaction_readahead_size: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB :                Options.blob_file_starting_level: 0
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : Options.experimental_mempurge_threshold: 0.000000
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : [/db_impl/db_impl.cc:2861] Created column family [hudi_pending_clustering_fg_user_hive_warehouse_stock_ticks_mor] (ID 7)
24/01/05 02:44:10 INFO view.RocksDbBasedFileSystemView: Resetting replacedFileGroups to ROCKSDB based file-system view at /tmp/hoodie_timeline_rocksdb, Total file-groups=0
24/01/05 02:44:10 INFO collection.RocksDBDAO: Prefix DELETE (query=part=) on hudi_replaced_fg_user_hive_warehouse_stock_ticks_mor
24/01/05 02:44:10 INFO view.RocksDbBasedFileSystemView: Resetting replacedFileGroups to ROCKSDB based file-system view complete
24/01/05 02:44:10 INFO view.AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
24/01/05 02:44:10 INFO view.RocksDbBasedFileSystemView: Initializing pending compaction operations. Count=0
24/01/05 02:44:10 INFO view.RocksDbBasedFileSystemView: Initializing pending Log compaction operations. Count=0
24/01/05 02:44:10 INFO view.RocksDbBasedFileSystemView: Initializing external data file mapping. Count=0
24/01/05 02:44:10 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:44:10 INFO view.RocksDbBasedFileSystemView: Resetting file groups in pending clustering to ROCKSDB based file-system view at /tmp/hoodie_timeline_rocksdb, Total file-groups=0
24/01/05 02:44:10 INFO collection.RocksDBDAO: Prefix DELETE (query=part=) on hudi_pending_clustering_fg_user_hive_warehouse_stock_ticks_mor
24/01/05 02:44:10 INFO view.RocksDbBasedFileSystemView: Resetting replacedFileGroups to ROCKSDB based file-system view complete
24/01/05 02:44:10 INFO view.RocksDbBasedFileSystemView: Created ROCKSDB based file-system view at /tmp/hoodie_timeline_rocksdb
24/01/05 02:44:10 INFO collection.RocksDBDAO: Prefix Search for (query=) on hudi_pending_compaction__user_hive_warehouse_stock_ticks_mor. Total Time Taken (msec)=0. Serialization Time taken(micro)=0, num entries=0
24/01/05 02:44:10 INFO view.RemoteHoodieTableFileSystemView: Sending request : (http://adhoc-2:42425/v1/hoodie/view/logcompactions/pending/?basepath=%2Fuser%2Fhive%2Fwarehouse%2Fstock_ticks_mor&lastinstantts=20240105024402665&timelinehash=a2d25c4c0dfac66e565adad6b2d12f51946731409571f20219d2d4ee6dc1a8e0)
24/01/05 02:44:10 INFO collection.RocksDBDAO: Prefix Search for (query=) on hudi_pending_log_compaction__user_hive_warehouse_stock_ticks_mor. Total Time Taken (msec)=0. Serialization Time taken(micro)=0, num entries=0
24/01/05 02:44:10 INFO clean.CleanPlanner: No earliest commit to retain. No need to scan partitions !!
24/01/05 02:44:10 INFO clean.CleanPlanActionExecutor: Nothing to clean here. It is already clean
24/01/05 02:44:10 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105024402665__deltacommit__COMPLETED__20240105024410413]}
24/01/05 02:44:10 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:44:10 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:44:10 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:44:10 INFO table.HoodieTableMetaClient: Loading Active commit timeline for /user/hive/warehouse/stock_ticks_mor
24/01/05 02:44:10 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105024402665__deltacommit__COMPLETED__20240105024410413]}
24/01/05 02:44:10 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:44:10 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:44:10 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:44:10 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:44:10 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:44:10 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:44:10 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105024402665__deltacommit__COMPLETED__20240105024410330]}
24/01/05 02:44:10 INFO view.AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
24/01/05 02:44:10 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:44:10 INFO view.FileSystemViewManager: Creating View Manager with storage type :REMOTE_FIRST
24/01/05 02:44:10 INFO view.FileSystemViewManager: Creating remote first table view
24/01/05 02:44:10 INFO client.BaseHoodieWriteClient: Start to archive synchronously.
24/01/05 02:44:10 INFO transaction.TransactionManager: Transaction starting for Optional.empty with latest completed transaction instant Optional.empty
24/01/05 02:44:10 INFO lock.LockManager: LockProvider org.apache.hudi.client.transaction.lock.InProcessLockProvider
24/01/05 02:44:10 INFO lock.InProcessLockProvider: Base Path /user/hive/warehouse/stock_ticks_mor, Lock Instance java.util.concurrent.locks.ReentrantReadWriteLock@7254838[Write locks = 0, Read locks = 0], Thread main, In-process lock state ACQUIRING
24/01/05 02:44:10 INFO lock.InProcessLockProvider: Base Path /user/hive/warehouse/stock_ticks_mor, Lock Instance java.util.concurrent.locks.ReentrantReadWriteLock@7254838[Write locks = 1, Read locks = 0], Thread main, In-process lock state ACQUIRED
24/01/05 02:44:10 INFO transaction.TransactionManager: Transaction started for Optional.empty with latest completed transaction instant Optional.empty
24/01/05 02:44:10 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105024402665__deltacommit__COMPLETED__20240105024410413]}
24/01/05 02:44:10 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:44:10 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties
24/01/05 02:44:10 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_mor
24/01/05 02:44:10 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:44:10 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata/.hoodie/hoodie.properties
24/01/05 02:44:10 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from /user/hive/warehouse/stock_ticks_mor/.hoodie/metadata
24/01/05 02:44:10 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105024402665__deltacommit__COMPLETED__20240105024410330]}
24/01/05 02:44:10 INFO view.AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
24/01/05 02:44:10 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:44:10 INFO client.HoodieTimelineArchiver: Not archiving as there is no compaction yet on the metadata table
24/01/05 02:44:10 INFO client.HoodieTimelineArchiver: No Instants to archive
24/01/05 02:44:10 INFO transaction.TransactionManager: Transaction ending with transaction owner Optional.empty
24/01/05 02:44:10 INFO lock.InProcessLockProvider: Base Path /user/hive/warehouse/stock_ticks_mor, Lock Instance java.util.concurrent.locks.ReentrantReadWriteLock@7254838[Write locks = 1, Read locks = 0], Thread main, In-process lock state RELEASING
24/01/05 02:44:10 INFO lock.InProcessLockProvider: Base Path /user/hive/warehouse/stock_ticks_mor, Lock Instance java.util.concurrent.locks.ReentrantReadWriteLock@7254838[Write locks = 0, Read locks = 0], Thread main, In-process lock state RELEASED
24/01/05 02:44:10 INFO lock.InProcessLockProvider: Base Path /user/hive/warehouse/stock_ticks_mor, Lock Instance java.util.concurrent.locks.ReentrantReadWriteLock@7254838[Write locks = 0, Read locks = 0], Thread main, In-process lock state ALREADY_RELEASED
24/01/05 02:44:10 INFO lock.LockManager: Released connection created for acquiring lock
24/01/05 02:44:10 INFO transaction.TransactionManager: Transaction ended with transaction owner Optional.empty
24/01/05 02:44:10 INFO view.FileSystemViewManager: Creating remote view for basePath /user/hive/warehouse/stock_ticks_mor. Server=adhoc-2:42425, Timeout=300
24/01/05 02:44:10 INFO view.FileSystemViewManager: Creating InMemory based view for basePath /user/hive/warehouse/stock_ticks_mor
24/01/05 02:44:10 INFO view.AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
24/01/05 02:44:10 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:44:10 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105024402665__deltacommit__COMPLETED__20240105024410413]}
24/01/05 02:44:10 INFO view.RemoteHoodieTableFileSystemView: Sending request : (http://adhoc-2:42425/v1/hoodie/view/refresh/?basepath=%2Fuser%2Fhive%2Fwarehouse%2Fstock_ticks_mor&lastinstantts=20240105024402665&timelinehash=a2d25c4c0dfac66e565adad6b2d12f51946731409571f20219d2d4ee6dc1a8e0)
24/01/05 02:44:10 INFO view.RocksDbBasedFileSystemView: Closing Rocksdb !!
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : [/db_impl/db_impl.cc:478] Shutdown: canceling all background work
24/01/05 02:44:10 INFO collection.RocksDBDAO: From Rocks DB : [/db_impl/db_impl.cc:677] Shutdown complete
24/01/05 02:44:10 INFO view.RocksDbBasedFileSystemView: Closed Rocksdb !!
24/01/05 02:44:10 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105024402665__deltacommit__COMPLETED__20240105024410413]}
24/01/05 02:44:10 INFO view.AbstractTableFileSystemView: Took 1 ms to read  0 instants, 0 replaced file groups
24/01/05 02:44:10 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:44:10 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105024402665__deltacommit__COMPLETED__20240105024410413]}
24/01/05 02:44:10 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20240105024402665__deltacommit__COMPLETED__20240105024410330]}
24/01/05 02:44:10 INFO view.AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
24/01/05 02:44:10 INFO util.ClusteringUtils: Found 0 files in pending clustering operations
24/01/05 02:44:10 INFO streamer.StreamSync: Commit 20240105024402665 successful!
24/01/05 02:44:10 INFO transaction.TransactionManager: Transaction manager closed
24/01/05 02:44:10 INFO transaction.TransactionManager: Transaction manager closed
24/01/05 02:44:10 INFO streamer.StreamSync: Shutting down embedded timeline server
24/01/05 02:44:10 INFO embedded.EmbeddedTimelineService: Closing Timeline server
24/01/05 02:44:10 INFO service.TimelineService: Closing Timeline Service
24/01/05 02:44:10 INFO javalin.Javalin: Stopping Javalin ...
24/01/05 02:44:10 INFO javalin.Javalin: Javalin has stopped
24/01/05 02:44:10 INFO service.TimelineService: Closed Timeline Service
24/01/05 02:44:10 INFO embedded.EmbeddedTimelineService: Closed Timeline server
24/01/05 02:44:10 INFO ingestion.HoodieIngestionService: Ingestion service (run-once mode) has been shut down.
24/01/05 02:44:10 INFO server.AbstractConnector: Stopped Spark@61019f59{HTTP/1.1,[http/1.1]}{0.0.0.0:8090}
24/01/05 02:44:10 INFO ui.SparkUI: Stopped Spark web UI at http://adhoc-2:8090
24/01/05 02:44:10 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
24/01/05 02:44:10 INFO memory.MemoryStore: MemoryStore cleared
24/01/05 02:44:10 INFO storage.BlockManager: BlockManager stopped
24/01/05 02:44:10 INFO storage.BlockManagerMaster: BlockManagerMaster stopped
24/01/05 02:44:10 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
24/01/05 02:44:10 INFO spark.SparkContext: Successfully stopped SparkContext
24/01/05 02:44:10 INFO util.ShutdownHookManager: Shutdown hook called
24/01/05 02:44:10 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-0499c23b-2793-43c2-89ab-9ec8ad2d7d05
24/01/05 02:44:10 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-39f76c60-4b63-41da-8806-94c7b040401e
+ docker exec -i adhoc-1 hadoop --config /etc/hadoop fs -ls -R '/user/hive/warehouse/stock_ticks*/20*'
drwxr-xr-x   - root supergroup          0 2024-01-05 02:43 /user/hive/warehouse/stock_ticks_mor/2018/08
drwxr-xr-x   - root supergroup          0 2024-01-05 02:44 /user/hive/warehouse/stock_ticks_mor/2018/08/31
-rw-r--r--   1 root supergroup         96 2024-01-05 02:43 /user/hive/warehouse/stock_ticks_mor/2018/08/31/.hoodie_partition_metadata
-rw-r--r--   1 root supergroup     440911 2024-01-05 02:44 /user/hive/warehouse/stock_ticks_mor/2018/08/31/6ad87b7b-db22-4d47-afc3-cb2b288006ba-0_0-14-16_20240105024402665.parquet
-rw-r--r--   1 root supergroup     440913 2024-01-05 02:43 /user/hive/warehouse/stock_ticks_mor/2018/08/31/6ad87b7b-db22-4d47-afc3-cb2b288006ba-0_0-23-24_20240105024343265.parquet
